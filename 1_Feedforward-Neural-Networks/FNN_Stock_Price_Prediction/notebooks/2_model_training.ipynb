{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm64\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.machine())  # Should output \"arm64\" for Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/n6ycqzwx13989g40ry2djdsh0000gn/T/ipykernel_68150/3149537304.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_float.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/fc/n6ycqzwx13989g40ry2djdsh0000gn/T/ipykernel_68150/3149537304.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_float.fillna(df_float.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/stocks-COMBINED-Jan2020-Dec2024.csv\")\n",
    "\n",
    "float_columns = df.select_dtypes(include=['float64']).columns\n",
    "df_float = df[float_columns]\n",
    "\n",
    "df_float.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_float.fillna(df_float.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df_float.drop(['Open', 'High', 'Low'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>EPS</th>\n",
       "      <th>Price_Return</th>\n",
       "      <th>Log_Return</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Moving_Average</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Sharpe_Ratio</th>\n",
       "      <th>Max_Drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.86</td>\n",
       "      <td>39030000.0</td>\n",
       "      <td>-0.0153</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>143.830</td>\n",
       "      <td>411.212815</td>\n",
       "      <td>0.728616</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.07</td>\n",
       "      <td>44390000.0</td>\n",
       "      <td>-0.0305</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.018745</td>\n",
       "      <td>142.680</td>\n",
       "      <td>310.200927</td>\n",
       "      <td>0.792438</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148.60</td>\n",
       "      <td>87700000.0</td>\n",
       "      <td>-0.1062</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.031443</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.019660</td>\n",
       "      <td>143.380</td>\n",
       "      <td>247.665581</td>\n",
       "      <td>1.194777</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166.25</td>\n",
       "      <td>71900000.0</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.118775</td>\n",
       "      <td>0.112235</td>\n",
       "      <td>0.032259</td>\n",
       "      <td>148.298</td>\n",
       "      <td>148.096633</td>\n",
       "      <td>2.904362</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159.92</td>\n",
       "      <td>36010000.0</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.038075</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.033763</td>\n",
       "      <td>152.140</td>\n",
       "      <td>161.710631</td>\n",
       "      <td>3.705724</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price        Vol.  Change %  EPS  Price_Return  Log_Return  Volatility  \\\n",
       "0  141.86  39030000.0   -0.0153  0.7      0.008173    0.008140    0.020197   \n",
       "1  144.07  44390000.0   -0.0305  0.7      0.015579    0.015459    0.018745   \n",
       "2  148.60  87700000.0   -0.1062  0.7      0.031443    0.030959    0.019660   \n",
       "3  166.25  71900000.0    0.0396  0.7      0.118775    0.112235    0.032259   \n",
       "4  159.92  36010000.0    0.0236  0.7     -0.038075   -0.038819    0.033763   \n",
       "\n",
       "   Moving_Average         RSI      MACD  Sharpe_Ratio  Max_Drawdown  \n",
       "0         143.830  411.212815  0.728616     -0.010539     -0.000235  \n",
       "1         142.680  310.200927  0.792438     -0.010539     -0.000235  \n",
       "2         143.380  247.665581  1.194777     -0.010539     -0.000235  \n",
       "3         148.298  148.096633  2.904362     -0.010539     -0.000235  \n",
       "4         152.140  161.710631  3.705724     -0.010539     -0.000235  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_float.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_float.drop(['Price'], axis=1).values\n",
    "y  = df_float['Price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scalar = StandardScaler()\n",
    "\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feedforward Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 100 epochs\n",
      "Epoch 1/100\n",
      "122/122 [==============================] - 0s 910us/step - loss: 67034.9531 - mae: 224.8083 - val_loss: 57440.6641 - val_mae: 205.0255\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 0s 526us/step - loss: 34805.9609 - mae: 155.7523 - val_loss: 11329.2686 - val_mae: 89.7435\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 0s 529us/step - loss: 5576.4028 - mae: 58.2746 - val_loss: 2637.3989 - val_mae: 38.2042\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 0s 498us/step - loss: 3164.4666 - mae: 42.4284 - val_loss: 1740.2886 - val_mae: 31.1334\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 2457.4758 - mae: 37.8924 - val_loss: 1344.7085 - val_mae: 27.4117\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 0s 518us/step - loss: 2143.5872 - mae: 35.9211 - val_loss: 1138.7217 - val_mae: 25.4599\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 0s 507us/step - loss: 2019.7775 - mae: 34.7563 - val_loss: 994.8120 - val_mae: 23.8769\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 0s 498us/step - loss: 1895.8503 - mae: 33.6472 - val_loss: 914.2111 - val_mae: 23.0791\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 0s 511us/step - loss: 1749.8638 - mae: 32.6931 - val_loss: 796.0679 - val_mae: 21.5631\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 0s 513us/step - loss: 1603.7225 - mae: 31.2253 - val_loss: 716.0568 - val_mae: 20.4444\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 0s 515us/step - loss: 1558.6001 - mae: 30.8501 - val_loss: 667.2615 - val_mae: 19.7061\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 0s 516us/step - loss: 1443.3236 - mae: 29.6115 - val_loss: 578.1704 - val_mae: 18.3858\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 0s 513us/step - loss: 1309.1805 - mae: 28.2838 - val_loss: 533.5009 - val_mae: 17.6807\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 0s 506us/step - loss: 1311.1558 - mae: 28.1501 - val_loss: 485.8537 - val_mae: 16.7767\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 0s 511us/step - loss: 1161.2378 - mae: 26.7020 - val_loss: 437.2233 - val_mae: 15.8559\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 0s 497us/step - loss: 1140.6917 - mae: 26.2372 - val_loss: 391.9475 - val_mae: 15.0178\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 1052.0496 - mae: 25.2559 - val_loss: 350.8218 - val_mae: 14.3013\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 0s 494us/step - loss: 1001.8751 - mae: 24.8339 - val_loss: 317.9559 - val_mae: 13.5831\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 0s 501us/step - loss: 998.4241 - mae: 24.2709 - val_loss: 276.4756 - val_mae: 12.5534\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 0s 491us/step - loss: 934.7665 - mae: 23.5637 - val_loss: 247.9922 - val_mae: 11.8404\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 0s 504us/step - loss: 876.6525 - mae: 22.9501 - val_loss: 216.7598 - val_mae: 11.0827\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 0s 497us/step - loss: 868.4283 - mae: 22.3362 - val_loss: 202.4399 - val_mae: 10.9058\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 822.5679 - mae: 21.9553 - val_loss: 173.9619 - val_mae: 9.9723\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 0s 507us/step - loss: 788.2449 - mae: 21.5065 - val_loss: 192.9682 - val_mae: 10.7690\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 0s 500us/step - loss: 764.7181 - mae: 21.2645 - val_loss: 148.6767 - val_mae: 9.2827\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 725.6549 - mae: 20.5612 - val_loss: 138.1342 - val_mae: 9.0400\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 0s 512us/step - loss: 711.8411 - mae: 20.2366 - val_loss: 118.7438 - val_mae: 8.2948\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 0s 507us/step - loss: 766.4826 - mae: 20.8600 - val_loss: 105.8821 - val_mae: 7.7702\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 0s 512us/step - loss: 717.8033 - mae: 20.2220 - val_loss: 103.7010 - val_mae: 7.8562\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 0s 510us/step - loss: 661.7491 - mae: 19.5162 - val_loss: 83.0128 - val_mae: 6.7822\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 680.8198 - mae: 19.3680 - val_loss: 86.6532 - val_mae: 7.2486\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 610.2200 - mae: 18.4841 - val_loss: 80.6900 - val_mae: 7.0615\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 0s 486us/step - loss: 607.7568 - mae: 18.3896 - val_loss: 68.4496 - val_mae: 6.2311\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 0s 495us/step - loss: 619.9639 - mae: 18.6427 - val_loss: 86.9525 - val_mae: 7.4084\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 597.0104 - mae: 18.0381 - val_loss: 60.0201 - val_mae: 5.7901\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 584.9890 - mae: 17.6834 - val_loss: 71.5373 - val_mae: 6.8465\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 0s 499us/step - loss: 598.3578 - mae: 18.1500 - val_loss: 60.1984 - val_mae: 6.1961\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 564.3124 - mae: 17.5266 - val_loss: 57.2494 - val_mae: 6.0588\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 594.7839 - mae: 17.8963 - val_loss: 47.6884 - val_mae: 5.5103\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 0s 506us/step - loss: 563.8455 - mae: 17.3510 - val_loss: 51.0296 - val_mae: 5.6962\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 0s 497us/step - loss: 550.3708 - mae: 16.8930 - val_loss: 61.6085 - val_mae: 6.2808\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 0s 498us/step - loss: 550.1181 - mae: 16.9030 - val_loss: 39.6115 - val_mae: 4.8646\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 0s 495us/step - loss: 547.7929 - mae: 16.8181 - val_loss: 41.1107 - val_mae: 4.8034\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 0s 523us/step - loss: 533.1953 - mae: 16.5864 - val_loss: 33.7440 - val_mae: 4.5502\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 0s 511us/step - loss: 523.1506 - mae: 16.4079 - val_loss: 58.1365 - val_mae: 6.1095\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 530.7389 - mae: 16.6112 - val_loss: 46.5882 - val_mae: 5.4998\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 518.1741 - mae: 16.5439 - val_loss: 33.3768 - val_mae: 4.5252\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 0s 507us/step - loss: 517.6869 - mae: 16.1104 - val_loss: 28.0408 - val_mae: 4.0403\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 0s 484us/step - loss: 493.8682 - mae: 15.9382 - val_loss: 49.4586 - val_mae: 5.6334\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 516.6816 - mae: 16.2469 - val_loss: 37.1240 - val_mae: 4.8559\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 0s 508us/step - loss: 527.4351 - mae: 16.4005 - val_loss: 40.3372 - val_mae: 5.0797\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 0s 501us/step - loss: 527.6326 - mae: 16.4180 - val_loss: 34.8636 - val_mae: 4.6860\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 0s 501us/step - loss: 505.5697 - mae: 16.0836 - val_loss: 35.9088 - val_mae: 4.7611\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 0s 494us/step - loss: 511.3046 - mae: 15.9876 - val_loss: 27.1058 - val_mae: 3.9325\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 0s 498us/step - loss: 491.7320 - mae: 15.8405 - val_loss: 36.0723 - val_mae: 4.7979\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 0s 499us/step - loss: 478.6842 - mae: 15.5336 - val_loss: 32.0092 - val_mae: 4.3575\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 470.7565 - mae: 15.4883 - val_loss: 35.4297 - val_mae: 4.7815\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 0s 495us/step - loss: 500.1436 - mae: 15.7179 - val_loss: 39.9313 - val_mae: 5.0747\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 514.0522 - mae: 15.8143 - val_loss: 29.1596 - val_mae: 4.2238\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 0s 512us/step - loss: 455.3214 - mae: 15.1528 - val_loss: 27.5856 - val_mae: 4.1048\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 0s 511us/step - loss: 473.7106 - mae: 15.3862 - val_loss: 31.9159 - val_mae: 4.3949\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 0s 503us/step - loss: 500.9399 - mae: 15.6488 - val_loss: 33.9926 - val_mae: 4.5730\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 0s 507us/step - loss: 467.9090 - mae: 15.3762 - val_loss: 34.5572 - val_mae: 4.5729\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 488.6772 - mae: 15.5365 - val_loss: 34.4832 - val_mae: 4.5689\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 0s 508us/step - loss: 446.9595 - mae: 14.9866 - val_loss: 25.4982 - val_mae: 3.8237\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 0s 556us/step - loss: 468.9917 - mae: 15.2467 - val_loss: 55.6389 - val_mae: 5.7535\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 0s 510us/step - loss: 460.3720 - mae: 15.1306 - val_loss: 36.4011 - val_mae: 4.8635\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 0s 504us/step - loss: 454.4287 - mae: 14.9242 - val_loss: 38.6657 - val_mae: 4.9763\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 0s 509us/step - loss: 458.6146 - mae: 14.8808 - val_loss: 27.2038 - val_mae: 3.9580\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 0s 515us/step - loss: 443.1601 - mae: 14.8571 - val_loss: 38.8505 - val_mae: 4.7418\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 0s 492us/step - loss: 468.3610 - mae: 15.0247 - val_loss: 32.6785 - val_mae: 4.4801\n",
      "Epoch 72/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 420.2092 - mae: 14.4701 - val_loss: 30.9340 - val_mae: 4.3542\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 0s 510us/step - loss: 470.5297 - mae: 15.0988 - val_loss: 46.5369 - val_mae: 5.6100\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 0s 499us/step - loss: 441.0511 - mae: 14.4796 - val_loss: 30.8440 - val_mae: 4.4175\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 437.6427 - mae: 14.6980 - val_loss: 31.6090 - val_mae: 4.4065\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 0s 512us/step - loss: 436.6275 - mae: 14.4325 - val_loss: 43.3248 - val_mae: 5.1352\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 0s 519us/step - loss: 447.3366 - mae: 14.6309 - val_loss: 24.1698 - val_mae: 3.7851\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 0s 503us/step - loss: 427.7404 - mae: 14.3173 - val_loss: 52.6934 - val_mae: 5.8168\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 0s 497us/step - loss: 431.3651 - mae: 14.5903 - val_loss: 29.4301 - val_mae: 4.1093\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 0s 502us/step - loss: 428.0166 - mae: 14.0868 - val_loss: 27.3883 - val_mae: 3.9546\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 0s 507us/step - loss: 440.5423 - mae: 14.3700 - val_loss: 31.5836 - val_mae: 4.4677\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 0s 499us/step - loss: 447.4980 - mae: 14.5535 - val_loss: 27.6026 - val_mae: 4.1381\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 0s 510us/step - loss: 418.7911 - mae: 14.1009 - val_loss: 48.1495 - val_mae: 5.3550\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 0s 506us/step - loss: 431.1531 - mae: 14.3726 - val_loss: 46.7082 - val_mae: 5.4015\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 0s 503us/step - loss: 439.3050 - mae: 14.3727 - val_loss: 28.5988 - val_mae: 4.0996\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 0s 497us/step - loss: 420.8621 - mae: 14.2974 - val_loss: 25.6970 - val_mae: 3.7820\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 0s 499us/step - loss: 411.6582 - mae: 14.0726 - val_loss: 35.5036 - val_mae: 4.7748\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 0s 546us/step - loss: 417.2562 - mae: 14.0466 - val_loss: 43.8309 - val_mae: 5.4546\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 0s 503us/step - loss: 406.1552 - mae: 13.7801 - val_loss: 28.5989 - val_mae: 4.1934\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 0s 496us/step - loss: 403.4717 - mae: 13.8917 - val_loss: 46.0074 - val_mae: 5.7588\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 0s 499us/step - loss: 397.5417 - mae: 13.8167 - val_loss: 35.9588 - val_mae: 4.7770\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 0s 515us/step - loss: 402.4545 - mae: 13.7635 - val_loss: 27.1100 - val_mae: 3.9489\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 0s 510us/step - loss: 388.4118 - mae: 13.5896 - val_loss: 36.1841 - val_mae: 4.7410\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 0s 503us/step - loss: 401.2626 - mae: 13.7394 - val_loss: 29.3313 - val_mae: 4.1616\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 409.5440 - mae: 13.7859 - val_loss: 47.0729 - val_mae: 5.4274\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 0s 493us/step - loss: 430.4049 - mae: 14.1102 - val_loss: 59.1312 - val_mae: 5.9203\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 0s 506us/step - loss: 415.4341 - mae: 13.7362 - val_loss: 32.2150 - val_mae: 4.4988\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 0s 504us/step - loss: 386.6184 - mae: 13.3767 - val_loss: 66.6707 - val_mae: 6.3286\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 0s 505us/step - loss: 400.0445 - mae: 13.6008 - val_loss: 31.8828 - val_mae: 4.4538\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 0s 492us/step - loss: 400.6197 - mae: 13.6891 - val_loss: 37.6361 - val_mae: 4.8427\n",
      "31/31 [==============================] - 0s 278us/step\n",
      "Epochs: 100 | MAE: 4.842736864451874\n",
      "Training model with 150 epochs\n",
      "Epoch 1/150\n",
      "122/122 [==============================] - 0s 810us/step - loss: 66807.8203 - mae: 224.3298 - val_loss: 56228.2266 - val_mae: 203.3658\n",
      "Epoch 2/150\n",
      "122/122 [==============================] - 0s 521us/step - loss: 31814.1348 - mae: 148.4789 - val_loss: 8739.4648 - val_mae: 78.7357\n",
      "Epoch 3/150\n",
      "122/122 [==============================] - 0s 514us/step - loss: 5195.8916 - mae: 55.9558 - val_loss: 2737.5574 - val_mae: 39.5170\n",
      "Epoch 4/150\n",
      "122/122 [==============================] - 0s 517us/step - loss: 3159.9744 - mae: 42.9264 - val_loss: 1849.0347 - val_mae: 32.5418\n",
      "Epoch 5/150\n",
      "122/122 [==============================] - 0s 894us/step - loss: 2562.9297 - mae: 38.2588 - val_loss: 1366.7981 - val_mae: 27.7035\n",
      "Epoch 6/150\n",
      "122/122 [==============================] - 0s 508us/step - loss: 2159.5115 - mae: 35.4762 - val_loss: 1163.5009 - val_mae: 25.8231\n",
      "Epoch 7/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 2018.0060 - mae: 34.6281 - val_loss: 987.2460 - val_mae: 23.6948\n",
      "Epoch 8/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 1787.2395 - mae: 32.7025 - val_loss: 879.8342 - val_mae: 22.5607\n",
      "Epoch 9/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 1622.8275 - mae: 31.3431 - val_loss: 781.5672 - val_mae: 21.2645\n",
      "Epoch 10/150\n",
      "122/122 [==============================] - 0s 505us/step - loss: 1577.4895 - mae: 30.7850 - val_loss: 723.5555 - val_mae: 20.6043\n",
      "Epoch 11/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 1483.7870 - mae: 29.9676 - val_loss: 646.8071 - val_mae: 19.3604\n",
      "Epoch 12/150\n",
      "122/122 [==============================] - 0s 492us/step - loss: 1429.0411 - mae: 29.3051 - val_loss: 577.1952 - val_mae: 18.2602\n",
      "Epoch 13/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 1314.6110 - mae: 28.2987 - val_loss: 525.9892 - val_mae: 17.4357\n",
      "Epoch 14/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 1262.9344 - mae: 27.6844 - val_loss: 465.6277 - val_mae: 16.2243\n",
      "Epoch 15/150\n",
      "122/122 [==============================] - 0s 493us/step - loss: 1203.1992 - mae: 27.0332 - val_loss: 430.2022 - val_mae: 15.5729\n",
      "Epoch 16/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 1142.9641 - mae: 26.1457 - val_loss: 399.6350 - val_mae: 15.0771\n",
      "Epoch 17/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 1071.6494 - mae: 25.3976 - val_loss: 357.4294 - val_mae: 14.2607\n",
      "Epoch 18/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 1005.0721 - mae: 24.6634 - val_loss: 325.6531 - val_mae: 13.6484\n",
      "Epoch 19/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 977.0908 - mae: 24.3200 - val_loss: 298.3033 - val_mae: 13.0328\n",
      "Epoch 20/150\n",
      "122/122 [==============================] - 0s 529us/step - loss: 949.7264 - mae: 23.7950 - val_loss: 264.1244 - val_mae: 12.2250\n",
      "Epoch 21/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 902.0110 - mae: 23.1523 - val_loss: 218.0725 - val_mae: 11.0210\n",
      "Epoch 22/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 876.4685 - mae: 22.6635 - val_loss: 208.4147 - val_mae: 10.9062\n",
      "Epoch 23/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 821.1059 - mae: 21.8989 - val_loss: 187.4862 - val_mae: 10.2326\n",
      "Epoch 24/150\n",
      "122/122 [==============================] - 0s 494us/step - loss: 791.6304 - mae: 21.5571 - val_loss: 164.3860 - val_mae: 9.6661\n",
      "Epoch 25/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 770.3412 - mae: 21.3258 - val_loss: 161.2450 - val_mae: 9.7250\n",
      "Epoch 26/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 716.8387 - mae: 20.4648 - val_loss: 127.2440 - val_mae: 8.4875\n",
      "Epoch 27/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 721.7661 - mae: 20.3394 - val_loss: 126.8240 - val_mae: 8.5665\n",
      "Epoch 28/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 666.6938 - mae: 19.6010 - val_loss: 104.4671 - val_mae: 7.6903\n",
      "Epoch 29/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 683.6862 - mae: 19.6264 - val_loss: 100.2943 - val_mae: 7.5104\n",
      "Epoch 30/150\n",
      "122/122 [==============================] - 0s 495us/step - loss: 663.0895 - mae: 19.3664 - val_loss: 88.0608 - val_mae: 7.0369\n",
      "Epoch 31/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 620.2567 - mae: 18.6689 - val_loss: 99.8592 - val_mae: 7.9041\n",
      "Epoch 32/150\n",
      "122/122 [==============================] - 0s 512us/step - loss: 600.0850 - mae: 18.3684 - val_loss: 67.8725 - val_mae: 6.2072\n",
      "Epoch 33/150\n",
      "122/122 [==============================] - 0s 510us/step - loss: 613.9616 - mae: 18.0858 - val_loss: 87.3344 - val_mae: 7.3676\n",
      "Epoch 34/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 565.7186 - mae: 17.5812 - val_loss: 66.5755 - val_mae: 6.2960\n",
      "Epoch 35/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 567.0505 - mae: 17.4865 - val_loss: 61.7431 - val_mae: 6.0346\n",
      "Epoch 36/150\n",
      "122/122 [==============================] - 0s 508us/step - loss: 566.3655 - mae: 17.3349 - val_loss: 54.1626 - val_mae: 5.6810\n",
      "Epoch 37/150\n",
      "122/122 [==============================] - 0s 508us/step - loss: 568.0010 - mae: 17.6296 - val_loss: 52.0206 - val_mae: 5.3872\n",
      "Epoch 38/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 582.3897 - mae: 17.4199 - val_loss: 51.4096 - val_mae: 5.4922\n",
      "Epoch 39/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 545.9147 - mae: 17.0403 - val_loss: 46.7986 - val_mae: 5.2703\n",
      "Epoch 40/150\n",
      "122/122 [==============================] - 0s 490us/step - loss: 529.1323 - mae: 16.6679 - val_loss: 60.9573 - val_mae: 6.1312\n",
      "Epoch 41/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 535.9848 - mae: 16.6574 - val_loss: 37.6748 - val_mae: 4.5972\n",
      "Epoch 42/150\n",
      "122/122 [==============================] - 0s 534us/step - loss: 535.7509 - mae: 16.7630 - val_loss: 34.2104 - val_mae: 4.5231\n",
      "Epoch 43/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 510.1323 - mae: 16.3054 - val_loss: 63.7737 - val_mae: 6.2458\n",
      "Epoch 44/150\n",
      "122/122 [==============================] - 0s 535us/step - loss: 550.7109 - mae: 16.7590 - val_loss: 36.7787 - val_mae: 4.8224\n",
      "Epoch 45/150\n",
      "122/122 [==============================] - 0s 505us/step - loss: 528.7333 - mae: 16.3339 - val_loss: 43.9881 - val_mae: 5.0962\n",
      "Epoch 46/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 504.5300 - mae: 15.9653 - val_loss: 40.1189 - val_mae: 4.8598\n",
      "Epoch 47/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 537.0248 - mae: 16.5886 - val_loss: 41.2513 - val_mae: 5.1241\n",
      "Epoch 48/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 520.4156 - mae: 16.3335 - val_loss: 36.0211 - val_mae: 4.7797\n",
      "Epoch 49/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 481.8070 - mae: 15.7881 - val_loss: 32.7839 - val_mae: 4.2895\n",
      "Epoch 50/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 489.3138 - mae: 15.7168 - val_loss: 31.2027 - val_mae: 4.1753\n",
      "Epoch 51/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 492.0475 - mae: 15.9298 - val_loss: 53.9164 - val_mae: 5.7243\n",
      "Epoch 52/150\n",
      "122/122 [==============================] - 0s 515us/step - loss: 508.2246 - mae: 16.0826 - val_loss: 33.3012 - val_mae: 4.5013\n",
      "Epoch 53/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 495.9662 - mae: 15.8079 - val_loss: 35.4228 - val_mae: 4.6462\n",
      "Epoch 54/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 481.2838 - mae: 15.5470 - val_loss: 27.5572 - val_mae: 4.0550\n",
      "Epoch 55/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 480.6429 - mae: 15.5592 - val_loss: 32.8036 - val_mae: 4.4617\n",
      "Epoch 56/150\n",
      "122/122 [==============================] - 0s 489us/step - loss: 493.5062 - mae: 15.6103 - val_loss: 28.3802 - val_mae: 4.0548\n",
      "Epoch 57/150\n",
      "122/122 [==============================] - 0s 496us/step - loss: 482.1956 - mae: 15.4434 - val_loss: 42.4875 - val_mae: 5.1672\n",
      "Epoch 58/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 475.8123 - mae: 15.3166 - val_loss: 37.9772 - val_mae: 4.7504\n",
      "Epoch 59/150\n",
      "122/122 [==============================] - 0s 491us/step - loss: 475.0938 - mae: 15.4337 - val_loss: 30.9603 - val_mae: 4.3636\n",
      "Epoch 60/150\n",
      "122/122 [==============================] - 0s 490us/step - loss: 474.6288 - mae: 15.3079 - val_loss: 35.1173 - val_mae: 4.6478\n",
      "Epoch 61/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 477.0324 - mae: 15.4250 - val_loss: 30.7044 - val_mae: 4.2040\n",
      "Epoch 62/150\n",
      "122/122 [==============================] - 0s 492us/step - loss: 474.7284 - mae: 15.0026 - val_loss: 38.7247 - val_mae: 5.0218\n",
      "Epoch 63/150\n",
      "122/122 [==============================] - 0s 494us/step - loss: 458.2228 - mae: 14.9186 - val_loss: 29.4456 - val_mae: 4.1717\n",
      "Epoch 64/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 465.9075 - mae: 15.1147 - val_loss: 29.2540 - val_mae: 4.1122\n",
      "Epoch 65/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 494.3436 - mae: 15.3749 - val_loss: 30.8497 - val_mae: 4.3030\n",
      "Epoch 66/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 482.9572 - mae: 15.3445 - val_loss: 35.3676 - val_mae: 4.7709\n",
      "Epoch 67/150\n",
      "122/122 [==============================] - 0s 490us/step - loss: 471.5009 - mae: 15.3007 - val_loss: 42.4736 - val_mae: 5.1740\n",
      "Epoch 68/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 439.6329 - mae: 14.5828 - val_loss: 42.0420 - val_mae: 5.0845\n",
      "Epoch 69/150\n",
      "122/122 [==============================] - 0s 532us/step - loss: 464.6942 - mae: 15.0725 - val_loss: 28.4949 - val_mae: 4.0596\n",
      "Epoch 70/150\n",
      "122/122 [==============================] - 0s 510us/step - loss: 465.5813 - mae: 15.1755 - val_loss: 30.0538 - val_mae: 4.3269\n",
      "Epoch 71/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 448.1097 - mae: 14.6435 - val_loss: 29.6247 - val_mae: 4.1357\n",
      "Epoch 72/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 435.9566 - mae: 14.5717 - val_loss: 29.3496 - val_mae: 4.2730\n",
      "Epoch 73/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 451.8158 - mae: 14.7020 - val_loss: 28.9755 - val_mae: 4.1415\n",
      "Epoch 74/150\n",
      "122/122 [==============================] - 0s 508us/step - loss: 429.2783 - mae: 14.4443 - val_loss: 28.8261 - val_mae: 4.1934\n",
      "Epoch 75/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 437.5321 - mae: 14.4580 - val_loss: 32.8628 - val_mae: 4.3192\n",
      "Epoch 76/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 429.1010 - mae: 14.3350 - val_loss: 31.2376 - val_mae: 4.2632\n",
      "Epoch 77/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 401.8987 - mae: 13.8706 - val_loss: 29.6958 - val_mae: 4.2764\n",
      "Epoch 78/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 442.6636 - mae: 14.3962 - val_loss: 34.2947 - val_mae: 4.6525\n",
      "Epoch 79/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 403.3864 - mae: 14.0120 - val_loss: 24.4911 - val_mae: 3.7231\n",
      "Epoch 80/150\n",
      "122/122 [==============================] - 0s 509us/step - loss: 433.0516 - mae: 14.3468 - val_loss: 41.7706 - val_mae: 5.0763\n",
      "Epoch 81/150\n",
      "122/122 [==============================] - 0s 505us/step - loss: 405.7502 - mae: 13.9714 - val_loss: 61.1414 - val_mae: 6.0682\n",
      "Epoch 82/150\n",
      "122/122 [==============================] - 0s 496us/step - loss: 432.6215 - mae: 14.3013 - val_loss: 48.1016 - val_mae: 5.5687\n",
      "Epoch 83/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 417.7175 - mae: 14.2118 - val_loss: 41.6586 - val_mae: 5.1152\n",
      "Epoch 84/150\n",
      "122/122 [==============================] - 0s 509us/step - loss: 399.8963 - mae: 13.9462 - val_loss: 26.2208 - val_mae: 3.8899\n",
      "Epoch 85/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 433.5103 - mae: 14.2207 - val_loss: 27.5643 - val_mae: 3.9802\n",
      "Epoch 86/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 402.7818 - mae: 14.0462 - val_loss: 33.8448 - val_mae: 4.4889\n",
      "Epoch 87/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 420.2796 - mae: 14.1432 - val_loss: 28.5812 - val_mae: 4.1354\n",
      "Epoch 88/150\n",
      "122/122 [==============================] - 0s 495us/step - loss: 394.9461 - mae: 13.8286 - val_loss: 30.6018 - val_mae: 4.2074\n",
      "Epoch 89/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 412.9627 - mae: 13.8578 - val_loss: 45.2194 - val_mae: 5.2807\n",
      "Epoch 90/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 379.1619 - mae: 13.4073 - val_loss: 32.6056 - val_mae: 4.6162\n",
      "Epoch 91/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 390.6569 - mae: 13.7256 - val_loss: 40.2736 - val_mae: 5.2081\n",
      "Epoch 92/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 407.1631 - mae: 13.5465 - val_loss: 29.4777 - val_mae: 4.2511\n",
      "Epoch 93/150\n",
      "122/122 [==============================] - 0s 491us/step - loss: 399.0721 - mae: 13.7553 - val_loss: 31.6712 - val_mae: 4.3598\n",
      "Epoch 94/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 402.5540 - mae: 13.7214 - val_loss: 23.3813 - val_mae: 3.5581\n",
      "Epoch 95/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 378.3908 - mae: 13.3397 - val_loss: 32.9572 - val_mae: 4.5748\n",
      "Epoch 96/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 376.6521 - mae: 13.3834 - val_loss: 33.1997 - val_mae: 4.3935\n",
      "Epoch 97/150\n",
      "122/122 [==============================] - 0s 496us/step - loss: 410.1741 - mae: 13.7238 - val_loss: 32.6974 - val_mae: 4.5790\n",
      "Epoch 98/150\n",
      "122/122 [==============================] - 0s 518us/step - loss: 368.8610 - mae: 13.2044 - val_loss: 38.1109 - val_mae: 5.0162\n",
      "Epoch 99/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 405.0028 - mae: 13.7015 - val_loss: 34.7970 - val_mae: 4.6903\n",
      "Epoch 100/150\n",
      "122/122 [==============================] - 0s 512us/step - loss: 385.3393 - mae: 13.4039 - val_loss: 45.1486 - val_mae: 5.2562\n",
      "Epoch 101/150\n",
      "122/122 [==============================] - 0s 510us/step - loss: 358.5172 - mae: 12.9097 - val_loss: 34.7890 - val_mae: 4.6939\n",
      "Epoch 102/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 405.9169 - mae: 13.6541 - val_loss: 26.9243 - val_mae: 4.0104\n",
      "Epoch 103/150\n",
      "122/122 [==============================] - 0s 496us/step - loss: 378.2023 - mae: 13.2198 - val_loss: 29.6861 - val_mae: 4.3084\n",
      "Epoch 104/150\n",
      "122/122 [==============================] - 0s 552us/step - loss: 386.6306 - mae: 13.4114 - val_loss: 30.1682 - val_mae: 4.1624\n",
      "Epoch 105/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 373.7522 - mae: 13.3051 - val_loss: 30.5456 - val_mae: 4.2484\n",
      "Epoch 106/150\n",
      "122/122 [==============================] - 0s 532us/step - loss: 382.4404 - mae: 13.1897 - val_loss: 28.6628 - val_mae: 4.2527\n",
      "Epoch 107/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 371.6411 - mae: 13.0757 - val_loss: 52.7876 - val_mae: 5.5893\n",
      "Epoch 108/150\n",
      "122/122 [==============================] - 0s 494us/step - loss: 392.2553 - mae: 13.2542 - val_loss: 33.6085 - val_mae: 4.5691\n",
      "Epoch 109/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 379.6729 - mae: 13.1310 - val_loss: 35.2889 - val_mae: 4.6566\n",
      "Epoch 110/150\n",
      "122/122 [==============================] - 0s 506us/step - loss: 373.5363 - mae: 12.9708 - val_loss: 36.1778 - val_mae: 4.7145\n",
      "Epoch 111/150\n",
      "122/122 [==============================] - 0s 495us/step - loss: 366.3501 - mae: 13.0857 - val_loss: 30.2970 - val_mae: 4.1390\n",
      "Epoch 112/150\n",
      "122/122 [==============================] - 0s 492us/step - loss: 374.2154 - mae: 13.0667 - val_loss: 28.9699 - val_mae: 4.1665\n",
      "Epoch 113/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 367.9403 - mae: 12.9304 - val_loss: 38.6177 - val_mae: 5.0384\n",
      "Epoch 114/150\n",
      "122/122 [==============================] - 0s 486us/step - loss: 351.0515 - mae: 12.6953 - val_loss: 28.1484 - val_mae: 4.1009\n",
      "Epoch 115/150\n",
      "122/122 [==============================] - 0s 497us/step - loss: 366.6355 - mae: 12.7696 - val_loss: 28.1139 - val_mae: 4.0357\n",
      "Epoch 116/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 356.3128 - mae: 12.7435 - val_loss: 46.7099 - val_mae: 5.5197\n",
      "Epoch 117/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 347.1175 - mae: 12.6594 - val_loss: 33.6736 - val_mae: 4.6044\n",
      "Epoch 118/150\n",
      "122/122 [==============================] - 0s 505us/step - loss: 359.9861 - mae: 12.8085 - val_loss: 36.0808 - val_mae: 4.6438\n",
      "Epoch 119/150\n",
      "122/122 [==============================] - 0s 506us/step - loss: 371.4702 - mae: 12.8279 - val_loss: 28.0021 - val_mae: 4.0041\n",
      "Epoch 120/150\n",
      "122/122 [==============================] - 0s 492us/step - loss: 333.1161 - mae: 12.3529 - val_loss: 37.8288 - val_mae: 4.8443\n",
      "Epoch 121/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 338.1663 - mae: 12.3976 - val_loss: 37.6464 - val_mae: 4.8159\n",
      "Epoch 122/150\n",
      "122/122 [==============================] - 0s 494us/step - loss: 355.2342 - mae: 12.6539 - val_loss: 36.8732 - val_mae: 4.7087\n",
      "Epoch 123/150\n",
      "122/122 [==============================] - 0s 493us/step - loss: 330.0369 - mae: 12.2061 - val_loss: 28.0360 - val_mae: 4.0479\n",
      "Epoch 124/150\n",
      "122/122 [==============================] - 0s 484us/step - loss: 339.4375 - mae: 12.3516 - val_loss: 33.0819 - val_mae: 4.4437\n",
      "Epoch 125/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 367.4734 - mae: 12.6684 - val_loss: 32.6651 - val_mae: 4.5443\n",
      "Epoch 126/150\n",
      "122/122 [==============================] - 0s 500us/step - loss: 329.9122 - mae: 12.1455 - val_loss: 26.5357 - val_mae: 3.8762\n",
      "Epoch 127/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 316.3191 - mae: 12.0644 - val_loss: 31.4167 - val_mae: 4.4368\n",
      "Epoch 128/150\n",
      "122/122 [==============================] - 0s 520us/step - loss: 316.5960 - mae: 11.8990 - val_loss: 32.5257 - val_mae: 4.4493\n",
      "Epoch 129/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 310.8109 - mae: 11.8242 - val_loss: 32.9416 - val_mae: 4.5427\n",
      "Epoch 130/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 331.9934 - mae: 12.2976 - val_loss: 73.0232 - val_mae: 6.8493\n",
      "Epoch 131/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 327.4672 - mae: 12.1756 - val_loss: 35.8493 - val_mae: 4.8773\n",
      "Epoch 132/150\n",
      "122/122 [==============================] - 0s 498us/step - loss: 321.1678 - mae: 12.0541 - val_loss: 29.2070 - val_mae: 4.1701\n",
      "Epoch 133/150\n",
      "122/122 [==============================] - 0s 509us/step - loss: 333.4289 - mae: 12.1907 - val_loss: 68.1435 - val_mae: 6.4183\n",
      "Epoch 134/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 328.8519 - mae: 12.0974 - val_loss: 36.3805 - val_mae: 4.6999\n",
      "Epoch 135/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 336.1902 - mae: 12.1179 - val_loss: 37.9600 - val_mae: 4.9618\n",
      "Epoch 136/150\n",
      "122/122 [==============================] - 0s 510us/step - loss: 303.8104 - mae: 11.8366 - val_loss: 36.4022 - val_mae: 4.5763\n",
      "Epoch 137/150\n",
      "122/122 [==============================] - 0s 499us/step - loss: 303.0322 - mae: 11.7495 - val_loss: 40.6966 - val_mae: 5.0982\n",
      "Epoch 138/150\n",
      "122/122 [==============================] - 0s 489us/step - loss: 301.1786 - mae: 11.5951 - val_loss: 32.1329 - val_mae: 4.3102\n",
      "Epoch 139/150\n",
      "122/122 [==============================] - 0s 503us/step - loss: 291.7473 - mae: 11.4178 - val_loss: 31.9177 - val_mae: 4.3708\n",
      "Epoch 140/150\n",
      "122/122 [==============================] - 0s 491us/step - loss: 295.2681 - mae: 11.5877 - val_loss: 34.0628 - val_mae: 4.6273\n",
      "Epoch 141/150\n",
      "122/122 [==============================] - 0s 502us/step - loss: 295.8771 - mae: 11.5561 - val_loss: 33.4328 - val_mae: 4.5637\n",
      "Epoch 142/150\n",
      "122/122 [==============================] - 0s 494us/step - loss: 267.5582 - mae: 11.1276 - val_loss: 34.6355 - val_mae: 4.6056\n",
      "Epoch 143/150\n",
      "122/122 [==============================] - 0s 493us/step - loss: 289.2098 - mae: 11.3597 - val_loss: 37.9526 - val_mae: 4.9433\n",
      "Epoch 144/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 300.3335 - mae: 11.6025 - val_loss: 32.8305 - val_mae: 4.2636\n",
      "Epoch 145/150\n",
      "122/122 [==============================] - 0s 507us/step - loss: 299.5145 - mae: 11.5788 - val_loss: 34.9081 - val_mae: 4.7048\n",
      "Epoch 146/150\n",
      "122/122 [==============================] - 0s 501us/step - loss: 310.9810 - mae: 11.5896 - val_loss: 74.8935 - val_mae: 6.7687\n",
      "Epoch 147/150\n",
      "122/122 [==============================] - 0s 494us/step - loss: 306.4879 - mae: 11.6724 - val_loss: 29.5186 - val_mae: 4.1842\n",
      "Epoch 148/150\n",
      "122/122 [==============================] - 0s 504us/step - loss: 283.8987 - mae: 11.3017 - val_loss: 35.1078 - val_mae: 4.6212\n",
      "Epoch 149/150\n",
      "122/122 [==============================] - 0s 515us/step - loss: 295.6682 - mae: 11.5547 - val_loss: 31.9646 - val_mae: 4.3436\n",
      "Epoch 150/150\n",
      "122/122 [==============================] - 0s 534us/step - loss: 283.1152 - mae: 11.3078 - val_loss: 37.9291 - val_mae: 4.7874\n",
      "31/31 [==============================] - 0s 235us/step\n",
      "Epochs: 150 | MAE: 4.78739801917443\n",
      "Training model with 200 epochs\n",
      "Epoch 1/200\n",
      "122/122 [==============================] - 0s 787us/step - loss: 66464.8203 - mae: 223.1131 - val_loss: 54589.0469 - val_mae: 198.8524\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 0s 513us/step - loss: 28516.1133 - mae: 138.2626 - val_loss: 6530.6421 - val_mae: 66.7662\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 4718.3955 - mae: 52.0104 - val_loss: 2496.9897 - val_mae: 36.6940\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 0s 493us/step - loss: 3058.6584 - mae: 40.7052 - val_loss: 1726.3057 - val_mae: 30.5338\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 0s 490us/step - loss: 2592.9956 - mae: 37.5583 - val_loss: 1336.9899 - val_mae: 27.1698\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 2096.5911 - mae: 34.7591 - val_loss: 1138.5425 - val_mae: 25.1372\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 2044.3662 - mae: 33.9094 - val_loss: 979.2628 - val_mae: 23.3030\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 1921.6949 - mae: 32.8936 - val_loss: 852.9721 - val_mae: 21.8963\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 1680.1278 - mae: 31.4991 - val_loss: 760.0505 - val_mae: 20.6948\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 0s 495us/step - loss: 1523.1863 - mae: 29.8774 - val_loss: 670.8222 - val_mae: 19.4607\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 1431.3408 - mae: 29.0982 - val_loss: 615.0782 - val_mae: 18.7098\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 0s 505us/step - loss: 1329.2131 - mae: 28.3708 - val_loss: 569.7907 - val_mae: 18.0403\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 1229.7653 - mae: 27.0125 - val_loss: 502.0410 - val_mae: 16.9232\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 1215.5348 - mae: 27.0588 - val_loss: 442.5192 - val_mae: 15.9299\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 1140.5334 - mae: 25.9541 - val_loss: 385.3447 - val_mae: 14.9644\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 0s 507us/step - loss: 1008.6935 - mae: 24.5733 - val_loss: 353.2068 - val_mae: 14.2637\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 966.5431 - mae: 24.2258 - val_loss: 308.5951 - val_mae: 13.4100\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 0s 507us/step - loss: 894.0854 - mae: 23.1160 - val_loss: 278.4922 - val_mae: 12.6659\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 889.6217 - mae: 22.9778 - val_loss: 240.3134 - val_mae: 11.7810\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 0s 505us/step - loss: 865.8062 - mae: 22.7232 - val_loss: 215.3454 - val_mae: 11.0871\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 835.8467 - mae: 21.9113 - val_loss: 191.5924 - val_mae: 10.5237\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 0s 497us/step - loss: 798.8712 - mae: 21.5067 - val_loss: 166.6818 - val_mae: 9.8644\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 0s 509us/step - loss: 789.0045 - mae: 21.3447 - val_loss: 157.2907 - val_mae: 9.5610\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 776.5702 - mae: 21.1803 - val_loss: 137.2265 - val_mae: 9.0669\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 0s 500us/step - loss: 759.2787 - mae: 20.7081 - val_loss: 122.9004 - val_mae: 8.5232\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 0s 495us/step - loss: 704.8371 - mae: 20.1393 - val_loss: 105.7179 - val_mae: 7.8847\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - 0s 513us/step - loss: 710.1919 - mae: 19.9515 - val_loss: 104.1892 - val_mae: 7.8283\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 0s 520us/step - loss: 667.0635 - mae: 19.5951 - val_loss: 94.6752 - val_mae: 7.4880\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 0s 513us/step - loss: 667.7098 - mae: 19.2550 - val_loss: 80.4665 - val_mae: 6.9198\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 659.5099 - mae: 19.0705 - val_loss: 73.9452 - val_mae: 6.6042\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 0s 505us/step - loss: 670.5266 - mae: 19.2411 - val_loss: 78.5115 - val_mae: 6.8734\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 0s 516us/step - loss: 653.6345 - mae: 18.8638 - val_loss: 76.4222 - val_mae: 6.7735\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 0s 507us/step - loss: 624.9835 - mae: 18.5038 - val_loss: 69.1248 - val_mae: 6.3729\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 609.1430 - mae: 18.3677 - val_loss: 79.8494 - val_mae: 7.1577\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 600.9690 - mae: 18.2212 - val_loss: 58.3857 - val_mae: 5.8829\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 621.1229 - mae: 18.2442 - val_loss: 58.7708 - val_mae: 5.6570\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 0s 548us/step - loss: 570.9551 - mae: 17.5168 - val_loss: 51.4426 - val_mae: 5.3695\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 600.4280 - mae: 17.8896 - val_loss: 57.0895 - val_mae: 5.8398\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 557.3199 - mae: 17.4267 - val_loss: 50.0239 - val_mae: 5.4870\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 0s 523us/step - loss: 574.0988 - mae: 17.3171 - val_loss: 44.5569 - val_mae: 5.1244\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 0s 509us/step - loss: 559.7278 - mae: 17.0778 - val_loss: 38.5292 - val_mae: 4.7289\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 522.3195 - mae: 16.7935 - val_loss: 44.3604 - val_mae: 5.2015\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 0s 509us/step - loss: 558.8082 - mae: 16.8967 - val_loss: 45.4402 - val_mae: 5.2487\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 552.9055 - mae: 16.7190 - val_loss: 42.5278 - val_mae: 4.9024\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 0s 504us/step - loss: 544.1082 - mae: 16.7406 - val_loss: 47.6305 - val_mae: 5.3876\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 524.2304 - mae: 16.5448 - val_loss: 49.7178 - val_mae: 5.5669\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 562.0757 - mae: 16.6492 - val_loss: 46.6257 - val_mae: 5.4167\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 0s 509us/step - loss: 510.7633 - mae: 16.0395 - val_loss: 56.6335 - val_mae: 6.1317\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 537.1925 - mae: 16.6969 - val_loss: 43.8818 - val_mae: 5.0977\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 0s 500us/step - loss: 512.7083 - mae: 16.0337 - val_loss: 36.7828 - val_mae: 4.6690\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 505.6823 - mae: 15.9777 - val_loss: 36.5421 - val_mae: 4.4375\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 0s 487us/step - loss: 489.6514 - mae: 15.6725 - val_loss: 33.0262 - val_mae: 4.4610\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 488.0009 - mae: 15.5775 - val_loss: 34.0710 - val_mae: 4.2803\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 0s 506us/step - loss: 518.0434 - mae: 16.1244 - val_loss: 42.2774 - val_mae: 5.2322\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 0s 507us/step - loss: 514.7352 - mae: 15.9221 - val_loss: 34.0663 - val_mae: 4.4555\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 477.0017 - mae: 15.3860 - val_loss: 32.0667 - val_mae: 4.3485\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 0s 511us/step - loss: 499.6074 - mae: 15.6435 - val_loss: 40.3848 - val_mae: 5.1956\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 0s 487us/step - loss: 473.5541 - mae: 15.3527 - val_loss: 24.5406 - val_mae: 3.8015\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 444.6490 - mae: 15.0852 - val_loss: 25.8428 - val_mae: 3.9363\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 0s 511us/step - loss: 474.1462 - mae: 15.3185 - val_loss: 33.6445 - val_mae: 4.5278\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 0s 505us/step - loss: 469.8532 - mae: 15.2030 - val_loss: 34.5630 - val_mae: 4.6796\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 479.6665 - mae: 15.3037 - val_loss: 31.8917 - val_mae: 4.4860\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 472.8342 - mae: 15.2137 - val_loss: 74.1846 - val_mae: 6.5161\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 0s 505us/step - loss: 481.7419 - mae: 15.2560 - val_loss: 52.5109 - val_mae: 5.7042\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 477.8282 - mae: 15.1705 - val_loss: 40.1207 - val_mae: 5.2653\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 0s 506us/step - loss: 439.3396 - mae: 14.8976 - val_loss: 57.5046 - val_mae: 5.9867\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 0s 497us/step - loss: 458.0992 - mae: 15.0634 - val_loss: 39.3772 - val_mae: 5.1189\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 0s 513us/step - loss: 446.7705 - mae: 14.8025 - val_loss: 40.8005 - val_mae: 5.0712\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 0s 500us/step - loss: 482.8826 - mae: 15.2410 - val_loss: 43.2347 - val_mae: 5.4566\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 461.9904 - mae: 14.9617 - val_loss: 45.4944 - val_mae: 5.1551\n",
      "Epoch 71/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 442.5071 - mae: 14.7914 - val_loss: 35.3399 - val_mae: 4.8935\n",
      "Epoch 72/200\n",
      "122/122 [==============================] - 0s 497us/step - loss: 469.8208 - mae: 14.8515 - val_loss: 30.2262 - val_mae: 4.2307\n",
      "Epoch 73/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 444.1918 - mae: 14.7319 - val_loss: 29.2124 - val_mae: 4.2516\n",
      "Epoch 74/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 426.9919 - mae: 14.2646 - val_loss: 26.1795 - val_mae: 3.8324\n",
      "Epoch 75/200\n",
      "122/122 [==============================] - 0s 550us/step - loss: 430.2746 - mae: 14.1544 - val_loss: 38.4924 - val_mae: 4.8439\n",
      "Epoch 76/200\n",
      "122/122 [==============================] - 0s 516us/step - loss: 432.5521 - mae: 14.3735 - val_loss: 32.4289 - val_mae: 4.4916\n",
      "Epoch 77/200\n",
      "122/122 [==============================] - 0s 544us/step - loss: 401.5425 - mae: 14.0682 - val_loss: 32.5212 - val_mae: 4.3696\n",
      "Epoch 78/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 433.3668 - mae: 14.2295 - val_loss: 27.2010 - val_mae: 4.0604\n",
      "Epoch 79/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 397.2242 - mae: 13.8211 - val_loss: 35.5986 - val_mae: 4.7687\n",
      "Epoch 80/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 407.3423 - mae: 13.7695 - val_loss: 31.5685 - val_mae: 4.5130\n",
      "Epoch 81/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 400.7321 - mae: 13.7904 - val_loss: 44.0525 - val_mae: 5.2874\n",
      "Epoch 82/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 412.3694 - mae: 14.0682 - val_loss: 34.5334 - val_mae: 4.4020\n",
      "Epoch 83/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 410.7046 - mae: 14.0922 - val_loss: 63.5255 - val_mae: 6.0662\n",
      "Epoch 84/200\n",
      "122/122 [==============================] - 0s 499us/step - loss: 417.8590 - mae: 14.0072 - val_loss: 55.1759 - val_mae: 5.8826\n",
      "Epoch 85/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 398.7513 - mae: 13.8109 - val_loss: 44.6705 - val_mae: 5.3768\n",
      "Epoch 86/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 402.9093 - mae: 13.6397 - val_loss: 36.3539 - val_mae: 4.7817\n",
      "Epoch 87/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 410.0790 - mae: 13.7838 - val_loss: 67.4452 - val_mae: 6.2889\n",
      "Epoch 88/200\n",
      "122/122 [==============================] - 0s 499us/step - loss: 418.0370 - mae: 13.8695 - val_loss: 30.2822 - val_mae: 4.4989\n",
      "Epoch 89/200\n",
      "122/122 [==============================] - 0s 497us/step - loss: 388.2290 - mae: 13.6416 - val_loss: 29.6651 - val_mae: 4.0937\n",
      "Epoch 90/200\n",
      "122/122 [==============================] - 0s 505us/step - loss: 375.4924 - mae: 13.3719 - val_loss: 26.0068 - val_mae: 3.9049\n",
      "Epoch 91/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 412.0804 - mae: 13.7373 - val_loss: 43.1708 - val_mae: 5.4134\n",
      "Epoch 92/200\n",
      "122/122 [==============================] - 0s 516us/step - loss: 406.7322 - mae: 13.7841 - val_loss: 32.5331 - val_mae: 4.6427\n",
      "Epoch 93/200\n",
      "122/122 [==============================] - 0s 511us/step - loss: 373.4955 - mae: 13.1527 - val_loss: 28.9230 - val_mae: 4.2861\n",
      "Epoch 94/200\n",
      "122/122 [==============================] - 0s 519us/step - loss: 402.5607 - mae: 13.6760 - val_loss: 35.4568 - val_mae: 4.8737\n",
      "Epoch 95/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 367.7340 - mae: 13.2544 - val_loss: 45.0706 - val_mae: 5.3962\n",
      "Epoch 96/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 372.2659 - mae: 13.1225 - val_loss: 26.7511 - val_mae: 4.0319\n",
      "Epoch 97/200\n",
      "122/122 [==============================] - 0s 507us/step - loss: 373.7662 - mae: 13.1198 - val_loss: 31.4748 - val_mae: 4.4073\n",
      "Epoch 98/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 377.2870 - mae: 13.1783 - val_loss: 37.0510 - val_mae: 4.8902\n",
      "Epoch 99/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 370.1288 - mae: 13.1298 - val_loss: 47.1945 - val_mae: 5.6274\n",
      "Epoch 100/200\n",
      "122/122 [==============================] - 0s 491us/step - loss: 377.0014 - mae: 13.2352 - val_loss: 25.9942 - val_mae: 3.9361\n",
      "Epoch 101/200\n",
      "122/122 [==============================] - 0s 504us/step - loss: 358.0731 - mae: 12.8883 - val_loss: 33.7257 - val_mae: 4.7692\n",
      "Epoch 102/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 384.2717 - mae: 13.2963 - val_loss: 32.6719 - val_mae: 4.6009\n",
      "Epoch 103/200\n",
      "122/122 [==============================] - 0s 491us/step - loss: 355.4986 - mae: 12.7250 - val_loss: 31.8859 - val_mae: 4.5071\n",
      "Epoch 104/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 349.5720 - mae: 12.7624 - val_loss: 24.0748 - val_mae: 3.7346\n",
      "Epoch 105/200\n",
      "122/122 [==============================] - 0s 504us/step - loss: 369.4366 - mae: 13.0483 - val_loss: 43.9455 - val_mae: 5.2389\n",
      "Epoch 106/200\n",
      "122/122 [==============================] - 0s 506us/step - loss: 370.4931 - mae: 13.0680 - val_loss: 36.2614 - val_mae: 4.7376\n",
      "Epoch 107/200\n",
      "122/122 [==============================] - 0s 489us/step - loss: 359.4634 - mae: 12.9289 - val_loss: 33.9599 - val_mae: 4.7472\n",
      "Epoch 108/200\n",
      "122/122 [==============================] - 0s 518us/step - loss: 364.8948 - mae: 12.8321 - val_loss: 39.9501 - val_mae: 5.0957\n",
      "Epoch 109/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 355.1340 - mae: 12.7169 - val_loss: 29.1556 - val_mae: 4.1405\n",
      "Epoch 110/200\n",
      "122/122 [==============================] - 0s 575us/step - loss: 362.5513 - mae: 12.8555 - val_loss: 31.4694 - val_mae: 4.4208\n",
      "Epoch 111/200\n",
      "122/122 [==============================] - 0s 511us/step - loss: 357.8632 - mae: 12.7001 - val_loss: 30.5813 - val_mae: 4.3811\n",
      "Epoch 112/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 355.2647 - mae: 12.6932 - val_loss: 67.3492 - val_mae: 6.3261\n",
      "Epoch 113/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 342.1585 - mae: 12.5562 - val_loss: 29.5531 - val_mae: 4.1493\n",
      "Epoch 114/200\n",
      "122/122 [==============================] - 0s 491us/step - loss: 351.5857 - mae: 12.5934 - val_loss: 26.4849 - val_mae: 3.9268\n",
      "Epoch 115/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 355.4460 - mae: 12.6136 - val_loss: 28.9879 - val_mae: 4.2526\n",
      "Epoch 116/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 339.2184 - mae: 12.3428 - val_loss: 31.0372 - val_mae: 4.3707\n",
      "Epoch 117/200\n",
      "122/122 [==============================] - 0s 495us/step - loss: 338.1406 - mae: 12.4306 - val_loss: 34.4590 - val_mae: 4.6284\n",
      "Epoch 118/200\n",
      "122/122 [==============================] - 0s 497us/step - loss: 343.5494 - mae: 12.4905 - val_loss: 34.5859 - val_mae: 4.7802\n",
      "Epoch 119/200\n",
      "122/122 [==============================] - 0s 506us/step - loss: 333.7377 - mae: 12.3075 - val_loss: 30.8009 - val_mae: 4.2810\n",
      "Epoch 120/200\n",
      "122/122 [==============================] - 0s 504us/step - loss: 355.8947 - mae: 12.4721 - val_loss: 30.8897 - val_mae: 4.3526\n",
      "Epoch 121/200\n",
      "122/122 [==============================] - 0s 486us/step - loss: 337.7345 - mae: 12.3294 - val_loss: 35.7857 - val_mae: 4.7546\n",
      "Epoch 122/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 341.8473 - mae: 12.1987 - val_loss: 34.3506 - val_mae: 4.5831\n",
      "Epoch 123/200\n",
      "122/122 [==============================] - 0s 514us/step - loss: 331.2452 - mae: 12.2143 - val_loss: 34.1809 - val_mae: 4.5938\n",
      "Epoch 124/200\n",
      "122/122 [==============================] - 0s 518us/step - loss: 332.3699 - mae: 12.2112 - val_loss: 33.9738 - val_mae: 4.6299\n",
      "Epoch 125/200\n",
      "122/122 [==============================] - 0s 511us/step - loss: 342.3287 - mae: 12.4528 - val_loss: 32.5874 - val_mae: 4.4662\n",
      "Epoch 126/200\n",
      "122/122 [==============================] - 0s 492us/step - loss: 320.1256 - mae: 12.1367 - val_loss: 52.6117 - val_mae: 5.6620\n",
      "Epoch 127/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 324.7901 - mae: 12.0389 - val_loss: 29.1261 - val_mae: 4.1360\n",
      "Epoch 128/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 307.2529 - mae: 11.7332 - val_loss: 30.6607 - val_mae: 4.3103\n",
      "Epoch 129/200\n",
      "122/122 [==============================] - 0s 488us/step - loss: 325.9015 - mae: 12.0497 - val_loss: 38.2977 - val_mae: 5.0877\n",
      "Epoch 130/200\n",
      "122/122 [==============================] - 0s 499us/step - loss: 308.6028 - mae: 11.8444 - val_loss: 29.0556 - val_mae: 4.1889\n",
      "Epoch 131/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 322.9866 - mae: 11.9612 - val_loss: 70.1892 - val_mae: 6.5412\n",
      "Epoch 132/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 335.4848 - mae: 12.1063 - val_loss: 40.6696 - val_mae: 5.1557\n",
      "Epoch 133/200\n",
      "122/122 [==============================] - 0s 499us/step - loss: 297.3055 - mae: 11.5772 - val_loss: 32.0710 - val_mae: 4.5157\n",
      "Epoch 134/200\n",
      "122/122 [==============================] - 0s 497us/step - loss: 301.0723 - mae: 11.7252 - val_loss: 31.5637 - val_mae: 4.3949\n",
      "Epoch 135/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 307.8614 - mae: 11.8503 - val_loss: 54.2193 - val_mae: 5.8527\n",
      "Epoch 136/200\n",
      "122/122 [==============================] - 0s 512us/step - loss: 301.0463 - mae: 11.5977 - val_loss: 39.8387 - val_mae: 5.2044\n",
      "Epoch 137/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 318.8151 - mae: 11.8543 - val_loss: 40.8795 - val_mae: 5.0748\n",
      "Epoch 138/200\n",
      "122/122 [==============================] - 0s 523us/step - loss: 302.3279 - mae: 11.6446 - val_loss: 32.0805 - val_mae: 4.4667\n",
      "Epoch 139/200\n",
      "122/122 [==============================] - 0s 569us/step - loss: 285.5238 - mae: 11.5061 - val_loss: 27.5519 - val_mae: 4.0444\n",
      "Epoch 140/200\n",
      "122/122 [==============================] - 0s 571us/step - loss: 303.2956 - mae: 11.6548 - val_loss: 31.7434 - val_mae: 4.3821\n",
      "Epoch 141/200\n",
      "122/122 [==============================] - 0s 552us/step - loss: 292.5600 - mae: 11.3468 - val_loss: 42.2183 - val_mae: 5.1346\n",
      "Epoch 142/200\n",
      "122/122 [==============================] - 0s 580us/step - loss: 279.6679 - mae: 11.2620 - val_loss: 34.6559 - val_mae: 4.7181\n",
      "Epoch 143/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 304.1531 - mae: 11.5930 - val_loss: 45.0486 - val_mae: 5.3280\n",
      "Epoch 144/200\n",
      "122/122 [==============================] - 0s 550us/step - loss: 287.8677 - mae: 11.2382 - val_loss: 35.5118 - val_mae: 4.7251\n",
      "Epoch 145/200\n",
      "122/122 [==============================] - 0s 573us/step - loss: 304.3032 - mae: 11.4656 - val_loss: 30.4249 - val_mae: 4.2940\n",
      "Epoch 146/200\n",
      "122/122 [==============================] - 0s 577us/step - loss: 265.2760 - mae: 10.9539 - val_loss: 29.6699 - val_mae: 4.2562\n",
      "Epoch 147/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 285.0815 - mae: 11.1502 - val_loss: 40.6343 - val_mae: 5.1451\n",
      "Epoch 148/200\n",
      "122/122 [==============================] - 0s 529us/step - loss: 289.9964 - mae: 11.4452 - val_loss: 30.8760 - val_mae: 4.3512\n",
      "Epoch 149/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 279.0441 - mae: 11.1696 - val_loss: 27.3026 - val_mae: 4.0486\n",
      "Epoch 150/200\n",
      "122/122 [==============================] - 0s 523us/step - loss: 287.2122 - mae: 11.2069 - val_loss: 31.0782 - val_mae: 4.3852\n",
      "Epoch 151/200\n",
      "122/122 [==============================] - 0s 515us/step - loss: 295.8049 - mae: 11.3898 - val_loss: 39.5048 - val_mae: 5.0646\n",
      "Epoch 152/200\n",
      "122/122 [==============================] - 0s 552us/step - loss: 271.9645 - mae: 11.0833 - val_loss: 40.2706 - val_mae: 5.0976\n",
      "Epoch 153/200\n",
      "122/122 [==============================] - 0s 555us/step - loss: 278.4837 - mae: 11.2568 - val_loss: 36.3093 - val_mae: 4.7152\n",
      "Epoch 154/200\n",
      "122/122 [==============================] - 0s 551us/step - loss: 276.9437 - mae: 10.8915 - val_loss: 31.9864 - val_mae: 4.4240\n",
      "Epoch 155/200\n",
      "122/122 [==============================] - 0s 545us/step - loss: 275.8391 - mae: 11.0682 - val_loss: 36.2899 - val_mae: 4.7947\n",
      "Epoch 156/200\n",
      "122/122 [==============================] - 0s 519us/step - loss: 282.6327 - mae: 11.1457 - val_loss: 36.9941 - val_mae: 4.7223\n",
      "Epoch 157/200\n",
      "122/122 [==============================] - 0s 503us/step - loss: 251.5513 - mae: 10.5603 - val_loss: 27.4915 - val_mae: 4.0909\n",
      "Epoch 158/200\n",
      "122/122 [==============================] - 0s 547us/step - loss: 276.5065 - mae: 10.9380 - val_loss: 56.0520 - val_mae: 5.9281\n",
      "Epoch 159/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 254.2746 - mae: 10.5282 - val_loss: 36.7711 - val_mae: 4.6666\n",
      "Epoch 160/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 274.6193 - mae: 10.6806 - val_loss: 28.2554 - val_mae: 4.1464\n",
      "Epoch 161/200\n",
      "122/122 [==============================] - 0s 509us/step - loss: 256.3854 - mae: 10.6153 - val_loss: 32.5272 - val_mae: 4.4800\n",
      "Epoch 162/200\n",
      "122/122 [==============================] - 0s 494us/step - loss: 252.4259 - mae: 10.4368 - val_loss: 35.6142 - val_mae: 4.7280\n",
      "Epoch 163/200\n",
      "122/122 [==============================] - 0s 495us/step - loss: 269.1027 - mae: 10.8630 - val_loss: 38.4392 - val_mae: 5.0580\n",
      "Epoch 164/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 243.3181 - mae: 10.2608 - val_loss: 34.9304 - val_mae: 4.7532\n",
      "Epoch 165/200\n",
      "122/122 [==============================] - 0s 506us/step - loss: 256.5851 - mae: 10.5781 - val_loss: 31.2133 - val_mae: 4.2906\n",
      "Epoch 166/200\n",
      "122/122 [==============================] - 0s 498us/step - loss: 259.0323 - mae: 10.5584 - val_loss: 29.2558 - val_mae: 4.2144\n",
      "Epoch 167/200\n",
      "122/122 [==============================] - 0s 545us/step - loss: 249.0829 - mae: 10.3301 - val_loss: 29.4805 - val_mae: 4.2698\n",
      "Epoch 168/200\n",
      "122/122 [==============================] - 0s 518us/step - loss: 253.8053 - mae: 10.4423 - val_loss: 31.9002 - val_mae: 4.4515\n",
      "Epoch 169/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 235.9712 - mae: 10.2453 - val_loss: 39.1399 - val_mae: 5.0336\n",
      "Epoch 170/200\n",
      "122/122 [==============================] - 0s 512us/step - loss: 251.8762 - mae: 10.3818 - val_loss: 32.3571 - val_mae: 4.5150\n",
      "Epoch 171/200\n",
      "122/122 [==============================] - 0s 506us/step - loss: 241.8927 - mae: 10.3145 - val_loss: 84.0014 - val_mae: 6.8734\n",
      "Epoch 172/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 248.5329 - mae: 10.4942 - val_loss: 37.9069 - val_mae: 4.7906\n",
      "Epoch 173/200\n",
      "122/122 [==============================] - 0s 496us/step - loss: 237.0273 - mae: 10.1723 - val_loss: 47.8577 - val_mae: 5.3601\n",
      "Epoch 174/200\n",
      "122/122 [==============================] - 0s 501us/step - loss: 237.0038 - mae: 10.1469 - val_loss: 36.0553 - val_mae: 4.7559\n",
      "Epoch 175/200\n",
      "122/122 [==============================] - 0s 499us/step - loss: 238.4397 - mae: 10.2212 - val_loss: 34.6462 - val_mae: 4.6759\n",
      "Epoch 176/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 234.3556 - mae: 10.0237 - val_loss: 31.9811 - val_mae: 4.4220\n",
      "Epoch 177/200\n",
      "122/122 [==============================] - 0s 523us/step - loss: 229.6738 - mae: 10.0401 - val_loss: 35.2075 - val_mae: 4.6511\n",
      "Epoch 178/200\n",
      "122/122 [==============================] - 0s 510us/step - loss: 230.2501 - mae: 9.8853 - val_loss: 51.1107 - val_mae: 5.5565\n",
      "Epoch 179/200\n",
      "122/122 [==============================] - 0s 508us/step - loss: 224.3748 - mae: 9.8635 - val_loss: 55.8960 - val_mae: 5.7423\n",
      "Epoch 180/200\n",
      "122/122 [==============================] - 0s 518us/step - loss: 218.8285 - mae: 9.8003 - val_loss: 26.6552 - val_mae: 3.9361\n",
      "Epoch 181/200\n",
      "122/122 [==============================] - 0s 525us/step - loss: 238.1814 - mae: 9.9184 - val_loss: 40.2163 - val_mae: 5.2301\n",
      "Epoch 182/200\n",
      "122/122 [==============================] - 0s 523us/step - loss: 233.6694 - mae: 9.8204 - val_loss: 30.5545 - val_mae: 4.3564\n",
      "Epoch 183/200\n",
      "122/122 [==============================] - 0s 527us/step - loss: 229.8695 - mae: 9.8888 - val_loss: 31.7876 - val_mae: 4.4615\n",
      "Epoch 184/200\n",
      "122/122 [==============================] - 0s 556us/step - loss: 226.1539 - mae: 9.9318 - val_loss: 30.5556 - val_mae: 4.3028\n",
      "Epoch 185/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 221.3269 - mae: 9.8050 - val_loss: 29.1117 - val_mae: 4.2140\n",
      "Epoch 186/200\n",
      "122/122 [==============================] - 0s 512us/step - loss: 212.1257 - mae: 9.4460 - val_loss: 32.6062 - val_mae: 4.5801\n",
      "Epoch 187/200\n",
      "122/122 [==============================] - 0s 515us/step - loss: 232.3192 - mae: 9.8932 - val_loss: 28.5591 - val_mae: 4.2189\n",
      "Epoch 188/200\n",
      "122/122 [==============================] - 0s 502us/step - loss: 229.7731 - mae: 9.9135 - val_loss: 47.7328 - val_mae: 5.4194\n",
      "Epoch 189/200\n",
      "122/122 [==============================] - 0s 492us/step - loss: 222.0981 - mae: 9.7516 - val_loss: 27.9434 - val_mae: 4.1099\n",
      "Epoch 190/200\n",
      "122/122 [==============================] - 0s 433us/step - loss: 228.4240 - mae: 9.8492 - val_loss: 31.5963 - val_mae: 4.3487\n",
      "Epoch 191/200\n",
      "122/122 [==============================] - 0s 436us/step - loss: 207.8072 - mae: 9.5030 - val_loss: 31.1422 - val_mae: 4.3692\n",
      "Epoch 192/200\n",
      "122/122 [==============================] - 0s 434us/step - loss: 213.2982 - mae: 9.6638 - val_loss: 28.7819 - val_mae: 4.1575\n",
      "Epoch 193/200\n",
      "122/122 [==============================] - 0s 437us/step - loss: 215.0700 - mae: 9.4027 - val_loss: 47.9926 - val_mae: 5.4008\n",
      "Epoch 194/200\n",
      "122/122 [==============================] - 0s 428us/step - loss: 219.0037 - mae: 9.6809 - val_loss: 29.7251 - val_mae: 4.1348\n",
      "Epoch 195/200\n",
      "122/122 [==============================] - 0s 437us/step - loss: 196.9460 - mae: 9.3525 - val_loss: 33.6861 - val_mae: 4.6062\n",
      "Epoch 196/200\n",
      "122/122 [==============================] - 0s 435us/step - loss: 217.7743 - mae: 9.4100 - val_loss: 30.8965 - val_mae: 4.4195\n",
      "Epoch 197/200\n",
      "122/122 [==============================] - 0s 431us/step - loss: 204.5231 - mae: 9.3907 - val_loss: 34.8070 - val_mae: 4.7394\n",
      "Epoch 198/200\n",
      "122/122 [==============================] - 0s 435us/step - loss: 201.5293 - mae: 9.2802 - val_loss: 56.8782 - val_mae: 5.9155\n",
      "Epoch 199/200\n",
      "122/122 [==============================] - 0s 429us/step - loss: 201.2683 - mae: 9.2775 - val_loss: 44.5698 - val_mae: 5.2096\n",
      "Epoch 200/200\n",
      "122/122 [==============================] - 0s 429us/step - loss: 204.5639 - mae: 9.4369 - val_loss: 33.2474 - val_mae: 4.5765\n",
      "31/31 [==============================] - 0s 232us/step\n",
      "Epochs: 200 | MAE: 4.576545517202523\n",
      "Training model with 250 epochs\n",
      "Epoch 1/250\n",
      "122/122 [==============================] - 0s 806us/step - loss: 67501.2344 - mae: 225.5160 - val_loss: 58036.4219 - val_mae: 206.3300\n",
      "Epoch 2/250\n",
      "122/122 [==============================] - 0s 515us/step - loss: 34104.5664 - mae: 154.3896 - val_loss: 10047.5078 - val_mae: 84.9849\n",
      "Epoch 3/250\n",
      "122/122 [==============================] - 0s 476us/step - loss: 5446.1880 - mae: 57.1415 - val_loss: 2655.8882 - val_mae: 37.5519\n",
      "Epoch 4/250\n",
      "122/122 [==============================] - 0s 431us/step - loss: 3149.1746 - mae: 42.0003 - val_loss: 1691.7969 - val_mae: 30.2643\n",
      "Epoch 5/250\n",
      "122/122 [==============================] - 0s 436us/step - loss: 2517.2444 - mae: 38.2790 - val_loss: 1301.6284 - val_mae: 26.7950\n",
      "Epoch 6/250\n",
      "122/122 [==============================] - 0s 510us/step - loss: 2159.4570 - mae: 35.6886 - val_loss: 1102.0028 - val_mae: 24.7038\n",
      "Epoch 7/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 1924.0546 - mae: 33.8692 - val_loss: 967.8819 - val_mae: 23.3865\n",
      "Epoch 8/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 1782.2959 - mae: 32.6882 - val_loss: 856.9641 - val_mae: 22.1082\n",
      "Epoch 9/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 1660.4373 - mae: 31.6472 - val_loss: 784.2831 - val_mae: 21.3118\n",
      "Epoch 10/250\n",
      "122/122 [==============================] - 0s 510us/step - loss: 1572.3970 - mae: 30.6680 - val_loss: 709.2540 - val_mae: 20.2538\n",
      "Epoch 11/250\n",
      "122/122 [==============================] - 0s 432us/step - loss: 1507.9453 - mae: 30.3137 - val_loss: 654.2513 - val_mae: 19.5495\n",
      "Epoch 12/250\n",
      "122/122 [==============================] - 0s 437us/step - loss: 1488.4230 - mae: 30.1415 - val_loss: 595.7842 - val_mae: 18.5677\n",
      "Epoch 13/250\n",
      "122/122 [==============================] - 0s 433us/step - loss: 1339.3965 - mae: 28.6375 - val_loss: 544.4392 - val_mae: 17.7172\n",
      "Epoch 14/250\n",
      "122/122 [==============================] - 0s 434us/step - loss: 1260.5720 - mae: 27.7580 - val_loss: 519.5599 - val_mae: 17.4075\n",
      "Epoch 15/250\n",
      "122/122 [==============================] - 0s 426us/step - loss: 1215.8002 - mae: 27.0578 - val_loss: 465.3187 - val_mae: 16.4098\n",
      "Epoch 16/250\n",
      "122/122 [==============================] - 0s 437us/step - loss: 1135.1022 - mae: 26.3822 - val_loss: 420.7400 - val_mae: 15.6751\n",
      "Epoch 17/250\n",
      "122/122 [==============================] - 0s 447us/step - loss: 1141.7516 - mae: 26.2059 - val_loss: 391.9464 - val_mae: 15.0702\n",
      "Epoch 18/250\n",
      "122/122 [==============================] - 0s 445us/step - loss: 1071.5745 - mae: 25.3936 - val_loss: 347.0872 - val_mae: 14.2240\n",
      "Epoch 19/250\n",
      "122/122 [==============================] - 0s 439us/step - loss: 1022.2695 - mae: 24.7410 - val_loss: 294.6599 - val_mae: 13.0100\n",
      "Epoch 20/250\n",
      "122/122 [==============================] - 0s 433us/step - loss: 1001.5024 - mae: 24.7615 - val_loss: 275.9811 - val_mae: 12.6630\n",
      "Epoch 21/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 963.4692 - mae: 24.1532 - val_loss: 303.9718 - val_mae: 13.8325\n",
      "Epoch 22/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 943.3781 - mae: 23.8101 - val_loss: 211.2356 - val_mae: 11.0471\n",
      "Epoch 23/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 897.3334 - mae: 22.8914 - val_loss: 215.1830 - val_mae: 11.5133\n",
      "Epoch 24/250\n",
      "122/122 [==============================] - 0s 495us/step - loss: 831.4370 - mae: 22.0079 - val_loss: 161.8291 - val_mae: 9.6520\n",
      "Epoch 25/250\n",
      "122/122 [==============================] - 0s 489us/step - loss: 840.9193 - mae: 22.2642 - val_loss: 161.7638 - val_mae: 9.7923\n",
      "Epoch 26/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 802.7166 - mae: 21.6648 - val_loss: 142.8563 - val_mae: 9.1935\n",
      "Epoch 27/250\n",
      "122/122 [==============================] - 0s 490us/step - loss: 753.1678 - mae: 20.8431 - val_loss: 121.7461 - val_mae: 8.6028\n",
      "Epoch 28/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 725.1696 - mae: 20.3964 - val_loss: 162.3375 - val_mae: 10.4448\n",
      "Epoch 29/250\n",
      "122/122 [==============================] - 0s 493us/step - loss: 700.7184 - mae: 20.0617 - val_loss: 96.6757 - val_mae: 7.5661\n",
      "Epoch 30/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 667.9286 - mae: 19.5629 - val_loss: 83.3148 - val_mae: 6.9531\n",
      "Epoch 31/250\n",
      "122/122 [==============================] - 0s 574us/step - loss: 656.3755 - mae: 19.2461 - val_loss: 104.5679 - val_mae: 8.2068\n",
      "Epoch 32/250\n",
      "122/122 [==============================] - 0s 505us/step - loss: 691.9482 - mae: 19.3744 - val_loss: 68.4173 - val_mae: 6.3198\n",
      "Epoch 33/250\n",
      "122/122 [==============================] - 0s 491us/step - loss: 647.9833 - mae: 18.7712 - val_loss: 80.0859 - val_mae: 7.0703\n",
      "Epoch 34/250\n",
      "122/122 [==============================] - 0s 514us/step - loss: 585.4665 - mae: 18.0407 - val_loss: 89.9618 - val_mae: 7.7004\n",
      "Epoch 35/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 602.2324 - mae: 18.1168 - val_loss: 66.0902 - val_mae: 6.3748\n",
      "Epoch 36/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 567.4962 - mae: 17.5370 - val_loss: 54.8603 - val_mae: 5.7306\n",
      "Epoch 37/250\n",
      "122/122 [==============================] - 0s 487us/step - loss: 608.9955 - mae: 17.8509 - val_loss: 57.1067 - val_mae: 5.7917\n",
      "Epoch 38/250\n",
      "122/122 [==============================] - 0s 487us/step - loss: 573.6232 - mae: 17.5557 - val_loss: 59.1077 - val_mae: 6.0111\n",
      "Epoch 39/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 588.6016 - mae: 17.8382 - val_loss: 62.1383 - val_mae: 6.3864\n",
      "Epoch 40/250\n",
      "122/122 [==============================] - 0s 494us/step - loss: 578.8315 - mae: 17.4305 - val_loss: 61.6149 - val_mae: 5.8431\n",
      "Epoch 41/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 541.9836 - mae: 16.8802 - val_loss: 40.8002 - val_mae: 4.8923\n",
      "Epoch 42/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 555.0131 - mae: 16.8964 - val_loss: 44.6502 - val_mae: 5.1533\n",
      "Epoch 43/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 525.9058 - mae: 16.6911 - val_loss: 67.4834 - val_mae: 6.4587\n",
      "Epoch 44/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 548.4461 - mae: 16.7595 - val_loss: 37.4946 - val_mae: 4.6191\n",
      "Epoch 45/250\n",
      "122/122 [==============================] - 0s 488us/step - loss: 501.9565 - mae: 16.2534 - val_loss: 48.6908 - val_mae: 5.3336\n",
      "Epoch 46/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 504.1721 - mae: 16.1942 - val_loss: 33.5638 - val_mae: 4.3039\n",
      "Epoch 47/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 532.9126 - mae: 16.2753 - val_loss: 41.1491 - val_mae: 5.0477\n",
      "Epoch 48/250\n",
      "122/122 [==============================] - 0s 487us/step - loss: 488.0555 - mae: 15.7421 - val_loss: 71.6110 - val_mae: 6.5843\n",
      "Epoch 49/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 541.4697 - mae: 16.5345 - val_loss: 45.6624 - val_mae: 5.3799\n",
      "Epoch 50/250\n",
      "122/122 [==============================] - 0s 510us/step - loss: 540.3803 - mae: 16.4052 - val_loss: 32.6041 - val_mae: 4.4110\n",
      "Epoch 51/250\n",
      "122/122 [==============================] - 0s 508us/step - loss: 511.1130 - mae: 15.8470 - val_loss: 43.4038 - val_mae: 5.2005\n",
      "Epoch 52/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 505.7574 - mae: 16.0123 - val_loss: 57.5597 - val_mae: 5.9669\n",
      "Epoch 53/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 494.6428 - mae: 15.7801 - val_loss: 31.0852 - val_mae: 4.1708\n",
      "Epoch 54/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 477.0130 - mae: 15.6908 - val_loss: 42.8483 - val_mae: 5.2635\n",
      "Epoch 55/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 486.0828 - mae: 15.5899 - val_loss: 29.5720 - val_mae: 4.2640\n",
      "Epoch 56/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 493.6402 - mae: 15.6811 - val_loss: 39.0944 - val_mae: 5.1968\n",
      "Epoch 57/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 494.8875 - mae: 15.5917 - val_loss: 36.2270 - val_mae: 4.8800\n",
      "Epoch 58/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 472.0635 - mae: 15.4638 - val_loss: 39.9124 - val_mae: 4.8425\n",
      "Epoch 59/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 462.6618 - mae: 15.1214 - val_loss: 33.1060 - val_mae: 4.5769\n",
      "Epoch 60/250\n",
      "122/122 [==============================] - 0s 518us/step - loss: 471.0123 - mae: 15.4602 - val_loss: 40.0389 - val_mae: 5.1100\n",
      "Epoch 61/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 476.6057 - mae: 15.3992 - val_loss: 56.4466 - val_mae: 5.8497\n",
      "Epoch 62/250\n",
      "122/122 [==============================] - 0s 519us/step - loss: 497.3232 - mae: 15.5767 - val_loss: 40.7280 - val_mae: 5.2421\n",
      "Epoch 63/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 463.7979 - mae: 15.1816 - val_loss: 35.3400 - val_mae: 4.7776\n",
      "Epoch 64/250\n",
      "122/122 [==============================] - 0s 566us/step - loss: 443.7117 - mae: 14.8604 - val_loss: 37.4492 - val_mae: 4.7919\n",
      "Epoch 65/250\n",
      "122/122 [==============================] - 0s 585us/step - loss: 480.5473 - mae: 15.1394 - val_loss: 30.5197 - val_mae: 4.2903\n",
      "Epoch 66/250\n",
      "122/122 [==============================] - 0s 596us/step - loss: 441.2297 - mae: 14.7514 - val_loss: 42.8390 - val_mae: 5.2571\n",
      "Epoch 67/250\n",
      "122/122 [==============================] - 0s 587us/step - loss: 451.8115 - mae: 14.9467 - val_loss: 40.3224 - val_mae: 4.9566\n",
      "Epoch 68/250\n",
      "122/122 [==============================] - 0s 569us/step - loss: 416.5837 - mae: 14.1374 - val_loss: 44.0521 - val_mae: 5.1158\n",
      "Epoch 69/250\n",
      "122/122 [==============================] - 0s 572us/step - loss: 459.8174 - mae: 14.8348 - val_loss: 39.3591 - val_mae: 5.1067\n",
      "Epoch 70/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 452.0855 - mae: 14.6178 - val_loss: 48.7567 - val_mae: 5.4251\n",
      "Epoch 71/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 422.4018 - mae: 14.2978 - val_loss: 32.0231 - val_mae: 4.2614\n",
      "Epoch 72/250\n",
      "122/122 [==============================] - 0s 506us/step - loss: 442.5036 - mae: 14.5755 - val_loss: 55.2247 - val_mae: 5.8028\n",
      "Epoch 73/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 412.0299 - mae: 14.2116 - val_loss: 32.9448 - val_mae: 4.5358\n",
      "Epoch 74/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 424.8560 - mae: 14.3707 - val_loss: 25.4512 - val_mae: 3.9208\n",
      "Epoch 75/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 417.0751 - mae: 14.3084 - val_loss: 34.1764 - val_mae: 4.6140\n",
      "Epoch 76/250\n",
      "122/122 [==============================] - 0s 430us/step - loss: 388.6837 - mae: 13.7946 - val_loss: 46.9773 - val_mae: 5.6424\n",
      "Epoch 77/250\n",
      "122/122 [==============================] - 0s 428us/step - loss: 431.1854 - mae: 14.2668 - val_loss: 83.5782 - val_mae: 6.9779\n",
      "Epoch 78/250\n",
      "122/122 [==============================] - 0s 430us/step - loss: 380.5085 - mae: 13.5634 - val_loss: 47.1929 - val_mae: 5.2868\n",
      "Epoch 79/250\n",
      "122/122 [==============================] - 0s 438us/step - loss: 423.1652 - mae: 14.1727 - val_loss: 84.1172 - val_mae: 7.0093\n",
      "Epoch 80/250\n",
      "122/122 [==============================] - 0s 432us/step - loss: 406.4138 - mae: 14.0315 - val_loss: 51.1217 - val_mae: 5.6767\n",
      "Epoch 81/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 414.0319 - mae: 14.0630 - val_loss: 49.6165 - val_mae: 5.5636\n",
      "Epoch 82/250\n",
      "122/122 [==============================] - 0s 516us/step - loss: 407.2861 - mae: 14.0096 - val_loss: 38.1022 - val_mae: 4.8644\n",
      "Epoch 83/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 413.2654 - mae: 13.9964 - val_loss: 40.3689 - val_mae: 5.1432\n",
      "Epoch 84/250\n",
      "122/122 [==============================] - 0s 508us/step - loss: 411.2645 - mae: 13.9400 - val_loss: 33.3258 - val_mae: 4.3344\n",
      "Epoch 85/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 393.3645 - mae: 13.6841 - val_loss: 43.5313 - val_mae: 5.1807\n",
      "Epoch 86/250\n",
      "122/122 [==============================] - 0s 485us/step - loss: 391.8167 - mae: 13.6362 - val_loss: 37.6870 - val_mae: 4.8638\n",
      "Epoch 87/250\n",
      "122/122 [==============================] - 0s 562us/step - loss: 431.4809 - mae: 14.1167 - val_loss: 43.7235 - val_mae: 5.5432\n",
      "Epoch 88/250\n",
      "122/122 [==============================] - 0s 522us/step - loss: 399.0151 - mae: 13.6468 - val_loss: 45.2023 - val_mae: 5.1469\n",
      "Epoch 89/250\n",
      "122/122 [==============================] - 0s 563us/step - loss: 404.3424 - mae: 13.6762 - val_loss: 32.3360 - val_mae: 4.4599\n",
      "Epoch 90/250\n",
      "122/122 [==============================] - 0s 562us/step - loss: 394.8808 - mae: 13.4659 - val_loss: 25.7334 - val_mae: 3.8498\n",
      "Epoch 91/250\n",
      "122/122 [==============================] - 0s 505us/step - loss: 393.5541 - mae: 13.3966 - val_loss: 51.5911 - val_mae: 5.5297\n",
      "Epoch 92/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 404.5331 - mae: 13.6223 - val_loss: 53.3025 - val_mae: 5.7989\n",
      "Epoch 93/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 379.5671 - mae: 13.3886 - val_loss: 39.0254 - val_mae: 5.0644\n",
      "Epoch 94/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 372.0458 - mae: 13.2718 - val_loss: 32.2628 - val_mae: 4.4447\n",
      "Epoch 95/250\n",
      "122/122 [==============================] - 0s 488us/step - loss: 392.7967 - mae: 13.3916 - val_loss: 33.8415 - val_mae: 4.5871\n",
      "Epoch 96/250\n",
      "122/122 [==============================] - 0s 435us/step - loss: 347.4471 - mae: 12.6680 - val_loss: 39.5360 - val_mae: 5.1918\n",
      "Epoch 97/250\n",
      "122/122 [==============================] - 0s 426us/step - loss: 365.1935 - mae: 12.8729 - val_loss: 55.5811 - val_mae: 5.7946\n",
      "Epoch 98/250\n",
      "122/122 [==============================] - 0s 442us/step - loss: 361.0501 - mae: 12.9933 - val_loss: 38.4885 - val_mae: 5.1529\n",
      "Epoch 99/250\n",
      "122/122 [==============================] - 0s 442us/step - loss: 353.5649 - mae: 12.9090 - val_loss: 53.2089 - val_mae: 5.7599\n",
      "Epoch 100/250\n",
      "122/122 [==============================] - 0s 432us/step - loss: 390.2197 - mae: 13.4241 - val_loss: 44.9011 - val_mae: 5.3737\n",
      "Epoch 101/250\n",
      "122/122 [==============================] - 0s 431us/step - loss: 356.6468 - mae: 12.8741 - val_loss: 29.2750 - val_mae: 4.1845\n",
      "Epoch 102/250\n",
      "122/122 [==============================] - 0s 433us/step - loss: 370.5529 - mae: 12.9618 - val_loss: 31.1155 - val_mae: 4.2550\n",
      "Epoch 103/250\n",
      "122/122 [==============================] - 0s 426us/step - loss: 355.3092 - mae: 12.7835 - val_loss: 51.6338 - val_mae: 5.5378\n",
      "Epoch 104/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 384.9397 - mae: 13.1548 - val_loss: 46.8378 - val_mae: 5.4661\n",
      "Epoch 105/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 350.2275 - mae: 12.6362 - val_loss: 31.4544 - val_mae: 4.5229\n",
      "Epoch 106/250\n",
      "122/122 [==============================] - 0s 493us/step - loss: 365.0095 - mae: 12.9866 - val_loss: 43.6734 - val_mae: 5.2916\n",
      "Epoch 107/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 363.6313 - mae: 12.8006 - val_loss: 47.4900 - val_mae: 5.5268\n",
      "Epoch 108/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 345.9395 - mae: 12.5692 - val_loss: 40.9406 - val_mae: 5.0204\n",
      "Epoch 109/250\n",
      "122/122 [==============================] - 0s 508us/step - loss: 354.8073 - mae: 12.7327 - val_loss: 40.2864 - val_mae: 4.9719\n",
      "Epoch 110/250\n",
      "122/122 [==============================] - 0s 521us/step - loss: 336.0547 - mae: 12.4398 - val_loss: 59.7599 - val_mae: 5.8808\n",
      "Epoch 111/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 343.4297 - mae: 12.4576 - val_loss: 71.3552 - val_mae: 6.5231\n",
      "Epoch 112/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 328.7094 - mae: 12.2825 - val_loss: 55.2477 - val_mae: 5.8636\n",
      "Epoch 113/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 334.8831 - mae: 12.3946 - val_loss: 53.1566 - val_mae: 5.5889\n",
      "Epoch 114/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 360.0477 - mae: 12.7410 - val_loss: 26.0866 - val_mae: 3.9169\n",
      "Epoch 115/250\n",
      "122/122 [==============================] - 0s 516us/step - loss: 350.1198 - mae: 12.5801 - val_loss: 43.0494 - val_mae: 5.2618\n",
      "Epoch 116/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 331.1464 - mae: 12.2704 - val_loss: 45.4513 - val_mae: 5.3665\n",
      "Epoch 117/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 319.6779 - mae: 12.1767 - val_loss: 45.3769 - val_mae: 5.3043\n",
      "Epoch 118/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 334.8210 - mae: 12.3976 - val_loss: 34.2855 - val_mae: 4.6502\n",
      "Epoch 119/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 325.3005 - mae: 12.3448 - val_loss: 43.0414 - val_mae: 5.1952\n",
      "Epoch 120/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 330.8152 - mae: 12.1198 - val_loss: 69.6189 - val_mae: 6.4098\n",
      "Epoch 121/250\n",
      "122/122 [==============================] - 0s 489us/step - loss: 298.8705 - mae: 11.8270 - val_loss: 44.0051 - val_mae: 5.0817\n",
      "Epoch 122/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 292.5572 - mae: 11.6067 - val_loss: 43.6280 - val_mae: 5.2840\n",
      "Epoch 123/250\n",
      "122/122 [==============================] - 0s 483us/step - loss: 306.1985 - mae: 11.8278 - val_loss: 50.3936 - val_mae: 5.6010\n",
      "Epoch 124/250\n",
      "122/122 [==============================] - 0s 495us/step - loss: 299.1153 - mae: 11.5460 - val_loss: 93.8507 - val_mae: 7.3218\n",
      "Epoch 125/250\n",
      "122/122 [==============================] - 0s 487us/step - loss: 306.4626 - mae: 11.7194 - val_loss: 55.1954 - val_mae: 5.8671\n",
      "Epoch 126/250\n",
      "122/122 [==============================] - 0s 510us/step - loss: 311.8507 - mae: 11.7871 - val_loss: 66.9061 - val_mae: 6.2178\n",
      "Epoch 127/250\n",
      "122/122 [==============================] - 0s 494us/step - loss: 308.2110 - mae: 11.8090 - val_loss: 49.2501 - val_mae: 5.7094\n",
      "Epoch 128/250\n",
      "122/122 [==============================] - 0s 550us/step - loss: 301.0196 - mae: 11.8417 - val_loss: 34.6704 - val_mae: 4.7088\n",
      "Epoch 129/250\n",
      "122/122 [==============================] - 0s 517us/step - loss: 297.3857 - mae: 11.6015 - val_loss: 42.2040 - val_mae: 5.2234\n",
      "Epoch 130/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 289.6298 - mae: 11.4262 - val_loss: 48.9272 - val_mae: 5.6280\n",
      "Epoch 131/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 290.8354 - mae: 11.5565 - val_loss: 56.1854 - val_mae: 5.8898\n",
      "Epoch 132/250\n",
      "122/122 [==============================] - 0s 508us/step - loss: 283.2644 - mae: 11.4312 - val_loss: 62.6352 - val_mae: 6.1853\n",
      "Epoch 133/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 285.0495 - mae: 11.3102 - val_loss: 73.6803 - val_mae: 6.5429\n",
      "Epoch 134/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 288.9125 - mae: 11.3262 - val_loss: 67.5286 - val_mae: 6.3500\n",
      "Epoch 135/250\n",
      "122/122 [==============================] - 0s 494us/step - loss: 317.7901 - mae: 11.6894 - val_loss: 38.4988 - val_mae: 5.0756\n",
      "Epoch 136/250\n",
      "122/122 [==============================] - 0s 507us/step - loss: 273.5061 - mae: 11.1009 - val_loss: 115.1165 - val_mae: 7.7227\n",
      "Epoch 137/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 279.2459 - mae: 11.1311 - val_loss: 90.9061 - val_mae: 6.9145\n",
      "Epoch 138/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 285.2143 - mae: 11.4651 - val_loss: 93.8696 - val_mae: 7.1291\n",
      "Epoch 139/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 259.2659 - mae: 10.9266 - val_loss: 99.9841 - val_mae: 7.2535\n",
      "Epoch 140/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 271.8312 - mae: 11.0465 - val_loss: 102.7299 - val_mae: 7.8615\n",
      "Epoch 141/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 272.1660 - mae: 11.0474 - val_loss: 114.7909 - val_mae: 7.6991\n",
      "Epoch 142/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 261.4384 - mae: 10.9422 - val_loss: 81.7424 - val_mae: 6.9647\n",
      "Epoch 143/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 242.1052 - mae: 10.5203 - val_loss: 85.6063 - val_mae: 7.1681\n",
      "Epoch 144/250\n",
      "122/122 [==============================] - 0s 495us/step - loss: 242.7878 - mae: 10.5135 - val_loss: 85.8788 - val_mae: 7.3050\n",
      "Epoch 145/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 242.3055 - mae: 10.5646 - val_loss: 95.9791 - val_mae: 7.5536\n",
      "Epoch 146/250\n",
      "122/122 [==============================] - 0s 507us/step - loss: 243.9374 - mae: 10.5656 - val_loss: 85.4458 - val_mae: 6.9689\n",
      "Epoch 147/250\n",
      "122/122 [==============================] - 0s 519us/step - loss: 237.8163 - mae: 10.5235 - val_loss: 53.0170 - val_mae: 5.7906\n",
      "Epoch 148/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 241.1032 - mae: 10.4508 - val_loss: 98.2502 - val_mae: 7.6244\n",
      "Epoch 149/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 238.3975 - mae: 10.4274 - val_loss: 102.1288 - val_mae: 7.7986\n",
      "Epoch 150/250\n",
      "122/122 [==============================] - 0s 508us/step - loss: 230.2022 - mae: 10.3291 - val_loss: 116.7712 - val_mae: 7.9523\n",
      "Epoch 151/250\n",
      "122/122 [==============================] - 0s 571us/step - loss: 226.9382 - mae: 10.1670 - val_loss: 67.1278 - val_mae: 6.5754\n",
      "Epoch 152/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 237.2602 - mae: 10.3943 - val_loss: 145.0052 - val_mae: 8.8404\n",
      "Epoch 153/250\n",
      "122/122 [==============================] - 0s 506us/step - loss: 218.6808 - mae: 10.0133 - val_loss: 97.6451 - val_mae: 7.3760\n",
      "Epoch 154/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 211.9786 - mae: 9.9569 - val_loss: 80.9297 - val_mae: 6.7778\n",
      "Epoch 155/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 221.4222 - mae: 9.9679 - val_loss: 180.9205 - val_mae: 9.3906\n",
      "Epoch 156/250\n",
      "122/122 [==============================] - 0s 507us/step - loss: 209.3232 - mae: 9.7926 - val_loss: 122.1903 - val_mae: 8.0358\n",
      "Epoch 157/250\n",
      "122/122 [==============================] - 0s 493us/step - loss: 210.9041 - mae: 9.8049 - val_loss: 150.3566 - val_mae: 8.6115\n",
      "Epoch 158/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 209.3997 - mae: 9.8241 - val_loss: 134.5788 - val_mae: 8.1564\n",
      "Epoch 159/250\n",
      "122/122 [==============================] - 0s 500us/step - loss: 208.4987 - mae: 9.7676 - val_loss: 204.7366 - val_mae: 9.8928\n",
      "Epoch 160/250\n",
      "122/122 [==============================] - 0s 513us/step - loss: 197.6373 - mae: 9.7109 - val_loss: 174.3748 - val_mae: 9.4911\n",
      "Epoch 161/250\n",
      "122/122 [==============================] - 0s 515us/step - loss: 212.7722 - mae: 9.8457 - val_loss: 131.8658 - val_mae: 8.4094\n",
      "Epoch 162/250\n",
      "122/122 [==============================] - 0s 515us/step - loss: 213.0414 - mae: 9.8529 - val_loss: 212.0079 - val_mae: 10.0485\n",
      "Epoch 163/250\n",
      "122/122 [==============================] - 0s 526us/step - loss: 210.0605 - mae: 9.7405 - val_loss: 84.9286 - val_mae: 7.3293\n",
      "Epoch 164/250\n",
      "122/122 [==============================] - 0s 512us/step - loss: 192.5035 - mae: 9.4941 - val_loss: 205.4681 - val_mae: 9.8920\n",
      "Epoch 165/250\n",
      "122/122 [==============================] - 0s 514us/step - loss: 201.5705 - mae: 9.3515 - val_loss: 112.6757 - val_mae: 7.8110\n",
      "Epoch 166/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 197.2896 - mae: 9.4608 - val_loss: 160.8550 - val_mae: 9.0450\n",
      "Epoch 167/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 197.3000 - mae: 9.4373 - val_loss: 88.1543 - val_mae: 7.0804\n",
      "Epoch 168/250\n",
      "122/122 [==============================] - 0s 512us/step - loss: 169.7990 - mae: 9.0256 - val_loss: 200.3756 - val_mae: 9.7270\n",
      "Epoch 169/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 182.3689 - mae: 9.1399 - val_loss: 122.4044 - val_mae: 7.9933\n",
      "Epoch 170/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 193.7716 - mae: 9.3290 - val_loss: 111.7374 - val_mae: 8.3801\n",
      "Epoch 171/250\n",
      "122/122 [==============================] - 0s 507us/step - loss: 181.8475 - mae: 9.1554 - val_loss: 117.8114 - val_mae: 8.1398\n",
      "Epoch 172/250\n",
      "122/122 [==============================] - 0s 506us/step - loss: 176.7997 - mae: 9.0233 - val_loss: 136.9991 - val_mae: 8.5526\n",
      "Epoch 173/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 163.4120 - mae: 8.7531 - val_loss: 176.8475 - val_mae: 9.5474\n",
      "Epoch 174/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 178.2371 - mae: 8.9720 - val_loss: 283.7753 - val_mae: 11.2436\n",
      "Epoch 175/250\n",
      "122/122 [==============================] - 0s 516us/step - loss: 160.7810 - mae: 8.7864 - val_loss: 166.3982 - val_mae: 8.9953\n",
      "Epoch 176/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 161.5436 - mae: 8.5787 - val_loss: 148.5524 - val_mae: 9.2081\n",
      "Epoch 177/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 177.9973 - mae: 8.8926 - val_loss: 196.0881 - val_mae: 9.8650\n",
      "Epoch 178/250\n",
      "122/122 [==============================] - 0s 511us/step - loss: 161.9283 - mae: 8.6079 - val_loss: 200.4614 - val_mae: 10.1807\n",
      "Epoch 179/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 151.0768 - mae: 8.5148 - val_loss: 167.5062 - val_mae: 9.3192\n",
      "Epoch 180/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 160.5074 - mae: 8.4533 - val_loss: 216.6371 - val_mae: 10.6433\n",
      "Epoch 181/250\n",
      "122/122 [==============================] - 0s 506us/step - loss: 158.7481 - mae: 8.4838 - val_loss: 164.3306 - val_mae: 9.4087\n",
      "Epoch 182/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 157.2105 - mae: 8.4664 - val_loss: 194.4096 - val_mae: 9.8605\n",
      "Epoch 183/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 159.1657 - mae: 8.5263 - val_loss: 203.9338 - val_mae: 10.1891\n",
      "Epoch 184/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 167.1675 - mae: 8.8154 - val_loss: 216.0824 - val_mae: 10.3274\n",
      "Epoch 185/250\n",
      "122/122 [==============================] - 0s 506us/step - loss: 153.7873 - mae: 8.4866 - val_loss: 198.6671 - val_mae: 10.0079\n",
      "Epoch 186/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 149.3544 - mae: 8.3433 - val_loss: 217.0312 - val_mae: 10.0944\n",
      "Epoch 187/250\n",
      "122/122 [==============================] - 0s 522us/step - loss: 146.5890 - mae: 8.2428 - val_loss: 231.7727 - val_mae: 10.8955\n",
      "Epoch 188/250\n",
      "122/122 [==============================] - 0s 509us/step - loss: 143.0298 - mae: 8.0020 - val_loss: 291.2902 - val_mae: 11.6258\n",
      "Epoch 189/250\n",
      "122/122 [==============================] - 0s 503us/step - loss: 146.6136 - mae: 8.2281 - val_loss: 306.1567 - val_mae: 12.3934\n",
      "Epoch 190/250\n",
      "122/122 [==============================] - 0s 495us/step - loss: 147.9790 - mae: 8.0499 - val_loss: 191.0341 - val_mae: 9.8875\n",
      "Epoch 191/250\n",
      "122/122 [==============================] - 0s 496us/step - loss: 138.3248 - mae: 7.9526 - val_loss: 180.4684 - val_mae: 9.7195\n",
      "Epoch 192/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 135.3696 - mae: 7.8836 - val_loss: 113.4397 - val_mae: 8.1823\n",
      "Epoch 193/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 143.3084 - mae: 7.9831 - val_loss: 260.5791 - val_mae: 11.2391\n",
      "Epoch 194/250\n",
      "122/122 [==============================] - 0s 501us/step - loss: 136.3961 - mae: 7.9414 - val_loss: 140.1037 - val_mae: 8.5626\n",
      "Epoch 195/250\n",
      "122/122 [==============================] - 0s 567us/step - loss: 139.6845 - mae: 7.9375 - val_loss: 130.1709 - val_mae: 8.4613\n",
      "Epoch 196/250\n",
      "122/122 [==============================] - 0s 517us/step - loss: 134.1904 - mae: 7.8857 - val_loss: 200.2172 - val_mae: 10.3457\n",
      "Epoch 197/250\n",
      "122/122 [==============================] - 0s 499us/step - loss: 137.7836 - mae: 7.9609 - val_loss: 309.1519 - val_mae: 12.0270\n",
      "Epoch 198/250\n",
      "122/122 [==============================] - 0s 498us/step - loss: 144.4571 - mae: 7.9654 - val_loss: 229.3995 - val_mae: 10.7101\n",
      "Epoch 199/250\n",
      "122/122 [==============================] - 0s 497us/step - loss: 131.0170 - mae: 7.6704 - val_loss: 283.0351 - val_mae: 11.8957\n",
      "Epoch 200/250\n",
      "122/122 [==============================] - 0s 505us/step - loss: 126.5228 - mae: 7.6061 - val_loss: 243.6634 - val_mae: 11.0225\n",
      "Epoch 201/250\n",
      "122/122 [==============================] - 0s 518us/step - loss: 130.1328 - mae: 7.7122 - val_loss: 167.1524 - val_mae: 9.7763\n",
      "Epoch 202/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 130.8996 - mae: 7.7204 - val_loss: 236.7748 - val_mae: 10.8948\n",
      "Epoch 203/250\n",
      "122/122 [==============================] - 0s 526us/step - loss: 140.7890 - mae: 7.7818 - val_loss: 202.4985 - val_mae: 10.3104\n",
      "Epoch 204/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 125.9841 - mae: 7.5265 - val_loss: 250.0725 - val_mae: 11.4079\n",
      "Epoch 205/250\n",
      "122/122 [==============================] - 0s 511us/step - loss: 114.3527 - mae: 7.2976 - val_loss: 277.9010 - val_mae: 11.8558\n",
      "Epoch 206/250\n",
      "122/122 [==============================] - 0s 507us/step - loss: 113.9027 - mae: 7.1311 - val_loss: 351.0600 - val_mae: 13.0913\n",
      "Epoch 207/250\n",
      "122/122 [==============================] - 0s 511us/step - loss: 127.3273 - mae: 7.3458 - val_loss: 243.3316 - val_mae: 11.4339\n",
      "Epoch 208/250\n",
      "122/122 [==============================] - 0s 519us/step - loss: 127.2182 - mae: 7.5318 - val_loss: 301.7108 - val_mae: 12.0806\n",
      "Epoch 209/250\n",
      "122/122 [==============================] - 0s 513us/step - loss: 111.5903 - mae: 7.1040 - val_loss: 243.8241 - val_mae: 11.3576\n",
      "Epoch 210/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 121.9204 - mae: 7.2930 - val_loss: 285.7876 - val_mae: 12.0100\n",
      "Epoch 211/250\n",
      "122/122 [==============================] - 0s 563us/step - loss: 113.8913 - mae: 7.2059 - val_loss: 229.5056 - val_mae: 10.6100\n",
      "Epoch 212/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 116.7032 - mae: 7.2242 - val_loss: 200.8729 - val_mae: 10.5627\n",
      "Epoch 213/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 116.5139 - mae: 7.2391 - val_loss: 259.2106 - val_mae: 11.5294\n",
      "Epoch 214/250\n",
      "122/122 [==============================] - 0s 602us/step - loss: 114.4197 - mae: 7.2133 - val_loss: 308.6689 - val_mae: 12.1917\n",
      "Epoch 215/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 103.5859 - mae: 6.9704 - val_loss: 278.5597 - val_mae: 11.6077\n",
      "Epoch 216/250\n",
      "122/122 [==============================] - 0s 564us/step - loss: 110.4473 - mae: 6.9321 - val_loss: 260.0151 - val_mae: 11.4383\n",
      "Epoch 217/250\n",
      "122/122 [==============================] - 0s 527us/step - loss: 104.1982 - mae: 7.0248 - val_loss: 270.2910 - val_mae: 11.6829\n",
      "Epoch 218/250\n",
      "122/122 [==============================] - 0s 519us/step - loss: 114.4065 - mae: 6.9780 - val_loss: 245.3114 - val_mae: 11.3348\n",
      "Epoch 219/250\n",
      "122/122 [==============================] - 0s 502us/step - loss: 113.6750 - mae: 7.1689 - val_loss: 327.8613 - val_mae: 12.5647\n",
      "Epoch 220/250\n",
      "122/122 [==============================] - 0s 512us/step - loss: 98.1026 - mae: 6.7571 - val_loss: 323.8640 - val_mae: 12.4360\n",
      "Epoch 221/250\n",
      "122/122 [==============================] - 0s 510us/step - loss: 99.0518 - mae: 6.7730 - val_loss: 265.1944 - val_mae: 11.4842\n",
      "Epoch 222/250\n",
      "122/122 [==============================] - 0s 515us/step - loss: 105.5354 - mae: 6.8749 - val_loss: 441.8845 - val_mae: 14.5042\n",
      "Epoch 223/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 97.8540 - mae: 6.7256 - val_loss: 329.4856 - val_mae: 12.7035\n",
      "Epoch 224/250\n",
      "122/122 [==============================] - 0s 521us/step - loss: 102.8997 - mae: 6.8115 - val_loss: 360.3217 - val_mae: 13.3428\n",
      "Epoch 225/250\n",
      "122/122 [==============================] - 0s 536us/step - loss: 102.9639 - mae: 6.8127 - val_loss: 428.5069 - val_mae: 14.1948\n",
      "Epoch 226/250\n",
      "122/122 [==============================] - 0s 519us/step - loss: 103.0086 - mae: 6.7614 - val_loss: 403.1563 - val_mae: 13.5226\n",
      "Epoch 227/250\n",
      "122/122 [==============================] - 0s 515us/step - loss: 100.3235 - mae: 6.7564 - val_loss: 275.8238 - val_mae: 11.9733\n",
      "Epoch 228/250\n",
      "122/122 [==============================] - 0s 511us/step - loss: 100.4044 - mae: 6.7080 - val_loss: 273.1080 - val_mae: 11.7086\n",
      "Epoch 229/250\n",
      "122/122 [==============================] - 0s 513us/step - loss: 98.4464 - mae: 6.6449 - val_loss: 314.3123 - val_mae: 12.0730\n",
      "Epoch 230/250\n",
      "122/122 [==============================] - 0s 504us/step - loss: 101.6572 - mae: 6.7012 - val_loss: 289.8845 - val_mae: 12.0809\n",
      "Epoch 231/250\n",
      "122/122 [==============================] - 0s 528us/step - loss: 95.0293 - mae: 6.5086 - val_loss: 310.4560 - val_mae: 12.1922\n",
      "Epoch 232/250\n",
      "122/122 [==============================] - 0s 551us/step - loss: 91.0073 - mae: 6.4013 - val_loss: 333.5506 - val_mae: 12.4916\n",
      "Epoch 233/250\n",
      "122/122 [==============================] - 0s 643us/step - loss: 90.4477 - mae: 6.4781 - val_loss: 281.5936 - val_mae: 11.7445\n",
      "Epoch 234/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 91.7547 - mae: 6.4601 - val_loss: 329.0258 - val_mae: 12.5814\n",
      "Epoch 235/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 100.8160 - mae: 6.5676 - val_loss: 323.4533 - val_mae: 12.4990\n",
      "Epoch 236/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 90.9478 - mae: 6.4280 - val_loss: 349.9918 - val_mae: 12.7720\n",
      "Epoch 237/250\n",
      "122/122 [==============================] - 0s 527us/step - loss: 95.4616 - mae: 6.4934 - val_loss: 332.1648 - val_mae: 12.1943\n",
      "Epoch 238/250\n",
      "122/122 [==============================] - 0s 524us/step - loss: 96.0097 - mae: 6.5193 - val_loss: 343.2499 - val_mae: 12.7501\n",
      "Epoch 239/250\n",
      "122/122 [==============================] - 0s 510us/step - loss: 85.5383 - mae: 6.1717 - val_loss: 293.1988 - val_mae: 11.9950\n",
      "Epoch 240/250\n",
      "122/122 [==============================] - 0s 527us/step - loss: 89.9756 - mae: 6.3846 - val_loss: 320.6656 - val_mae: 12.4611\n",
      "Epoch 241/250\n",
      "122/122 [==============================] - 0s 524us/step - loss: 100.4399 - mae: 6.5014 - val_loss: 392.6313 - val_mae: 13.5255\n",
      "Epoch 242/250\n",
      "122/122 [==============================] - 0s 513us/step - loss: 90.6540 - mae: 6.3795 - val_loss: 482.0410 - val_mae: 14.4835\n",
      "Epoch 243/250\n",
      "122/122 [==============================] - 0s 536us/step - loss: 90.9987 - mae: 6.5223 - val_loss: 343.8664 - val_mae: 12.3901\n",
      "Epoch 244/250\n",
      "122/122 [==============================] - 0s 520us/step - loss: 91.1667 - mae: 6.3040 - val_loss: 249.6292 - val_mae: 10.7684\n",
      "Epoch 245/250\n",
      "122/122 [==============================] - 0s 508us/step - loss: 83.8302 - mae: 6.1993 - val_loss: 268.9763 - val_mae: 11.4654\n",
      "Epoch 246/250\n",
      "122/122 [==============================] - 0s 516us/step - loss: 88.6710 - mae: 6.2897 - val_loss: 357.6472 - val_mae: 13.3268\n",
      "Epoch 247/250\n",
      "122/122 [==============================] - 0s 518us/step - loss: 92.8245 - mae: 6.3807 - val_loss: 307.5286 - val_mae: 12.3283\n",
      "Epoch 248/250\n",
      "122/122 [==============================] - 0s 514us/step - loss: 91.5353 - mae: 6.3138 - val_loss: 359.6507 - val_mae: 12.8332\n",
      "Epoch 249/250\n",
      "122/122 [==============================] - 0s 505us/step - loss: 97.8035 - mae: 6.3521 - val_loss: 332.6721 - val_mae: 12.3768\n",
      "Epoch 250/250\n",
      "122/122 [==============================] - 0s 556us/step - loss: 90.1176 - mae: 6.3192 - val_loss: 376.5760 - val_mae: 13.1071\n",
      "31/31 [==============================] - 0s 243us/step\n",
      "Epochs: 250 | MAE: 13.10711109806941\n",
      "Training model with 300 epochs\n",
      "Epoch 1/300\n",
      "122/122 [==============================] - 0s 831us/step - loss: 66698.2578 - mae: 223.7924 - val_loss: 55731.9570 - val_mae: 201.5838\n",
      "Epoch 2/300\n",
      "122/122 [==============================] - 0s 600us/step - loss: 31869.3320 - mae: 147.9503 - val_loss: 9018.1484 - val_mae: 79.7316\n",
      "Epoch 3/300\n",
      "122/122 [==============================] - 0s 528us/step - loss: 5247.8955 - mae: 55.9506 - val_loss: 2727.4355 - val_mae: 38.4531\n",
      "Epoch 4/300\n",
      "122/122 [==============================] - 0s 510us/step - loss: 3024.6182 - mae: 41.8993 - val_loss: 1723.7595 - val_mae: 30.3900\n",
      "Epoch 5/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 2496.5254 - mae: 37.8635 - val_loss: 1322.6315 - val_mae: 27.0561\n",
      "Epoch 6/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 2136.7961 - mae: 35.6334 - val_loss: 1109.3411 - val_mae: 24.8863\n",
      "Epoch 7/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 1964.0757 - mae: 34.0342 - val_loss: 964.9655 - val_mae: 23.4029\n",
      "Epoch 8/300\n",
      "122/122 [==============================] - 0s 516us/step - loss: 1753.8672 - mae: 32.6159 - val_loss: 862.1584 - val_mae: 22.1485\n",
      "Epoch 9/300\n",
      "122/122 [==============================] - 0s 529us/step - loss: 1663.0505 - mae: 31.7656 - val_loss: 783.8610 - val_mae: 21.2534\n",
      "Epoch 10/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 1588.7472 - mae: 31.2662 - val_loss: 728.1964 - val_mae: 20.5042\n",
      "Epoch 11/300\n",
      "122/122 [==============================] - 0s 632us/step - loss: 1531.2574 - mae: 30.5721 - val_loss: 661.7161 - val_mae: 19.6046\n",
      "Epoch 12/300\n",
      "122/122 [==============================] - 0s 525us/step - loss: 1389.6855 - mae: 29.2478 - val_loss: 611.8430 - val_mae: 18.8092\n",
      "Epoch 13/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 1334.2223 - mae: 28.4924 - val_loss: 545.1179 - val_mae: 17.7755\n",
      "Epoch 14/300\n",
      "122/122 [==============================] - 0s 529us/step - loss: 1230.2012 - mae: 27.7352 - val_loss: 498.0292 - val_mae: 17.0171\n",
      "Epoch 15/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 1217.1941 - mae: 27.0141 - val_loss: 451.4092 - val_mae: 16.1641\n",
      "Epoch 16/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 1107.8352 - mae: 25.8378 - val_loss: 414.5397 - val_mae: 15.4229\n",
      "Epoch 17/300\n",
      "122/122 [==============================] - 0s 523us/step - loss: 1086.0994 - mae: 25.6988 - val_loss: 378.5946 - val_mae: 14.8477\n",
      "Epoch 18/300\n",
      "122/122 [==============================] - 0s 598us/step - loss: 969.2677 - mae: 24.4894 - val_loss: 343.3152 - val_mae: 14.1704\n",
      "Epoch 19/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 980.8018 - mae: 24.2391 - val_loss: 313.7295 - val_mae: 13.4935\n",
      "Epoch 20/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 897.6404 - mae: 23.3836 - val_loss: 293.0055 - val_mae: 13.1303\n",
      "Epoch 21/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 895.5106 - mae: 23.2266 - val_loss: 250.5962 - val_mae: 12.0657\n",
      "Epoch 22/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 878.9715 - mae: 23.0071 - val_loss: 226.5192 - val_mae: 11.5698\n",
      "Epoch 23/300\n",
      "122/122 [==============================] - 0s 536us/step - loss: 833.1756 - mae: 22.2969 - val_loss: 204.4365 - val_mae: 10.9483\n",
      "Epoch 24/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 802.8570 - mae: 21.7970 - val_loss: 171.1770 - val_mae: 9.8138\n",
      "Epoch 25/300\n",
      "122/122 [==============================] - 0s 519us/step - loss: 761.7380 - mae: 21.2327 - val_loss: 153.2626 - val_mae: 9.3223\n",
      "Epoch 26/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 747.6813 - mae: 20.6532 - val_loss: 135.3398 - val_mae: 8.7911\n",
      "Epoch 27/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 745.0457 - mae: 20.7025 - val_loss: 140.0155 - val_mae: 9.2425\n",
      "Epoch 28/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 704.4467 - mae: 20.3136 - val_loss: 118.0247 - val_mae: 8.2235\n",
      "Epoch 29/300\n",
      "122/122 [==============================] - 0s 499us/step - loss: 641.9083 - mae: 19.1142 - val_loss: 110.7186 - val_mae: 8.0734\n",
      "Epoch 30/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 648.2963 - mae: 19.0742 - val_loss: 88.8248 - val_mae: 7.1572\n",
      "Epoch 31/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 627.5502 - mae: 18.9327 - val_loss: 98.3152 - val_mae: 7.7720\n",
      "Epoch 32/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 647.4520 - mae: 18.9632 - val_loss: 77.1679 - val_mae: 6.7828\n",
      "Epoch 33/300\n",
      "122/122 [==============================] - 0s 613us/step - loss: 623.9868 - mae: 18.4871 - val_loss: 113.1200 - val_mae: 8.4886\n",
      "Epoch 34/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 604.1787 - mae: 18.1834 - val_loss: 107.1637 - val_mae: 8.2843\n",
      "Epoch 35/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 641.4828 - mae: 18.5307 - val_loss: 57.1250 - val_mae: 5.7415\n",
      "Epoch 36/300\n",
      "122/122 [==============================] - 0s 527us/step - loss: 580.1048 - mae: 17.5357 - val_loss: 63.9089 - val_mae: 6.2420\n",
      "Epoch 37/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 570.5767 - mae: 17.2925 - val_loss: 47.6365 - val_mae: 5.2438\n",
      "Epoch 38/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 542.7274 - mae: 17.0544 - val_loss: 60.6160 - val_mae: 6.0976\n",
      "Epoch 39/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 584.0339 - mae: 17.5202 - val_loss: 54.0790 - val_mae: 5.6691\n",
      "Epoch 40/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 569.5650 - mae: 17.2892 - val_loss: 43.8605 - val_mae: 5.1405\n",
      "Epoch 41/300\n",
      "122/122 [==============================] - 0s 524us/step - loss: 528.0198 - mae: 16.5536 - val_loss: 33.4707 - val_mae: 4.4379\n",
      "Epoch 42/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 557.9637 - mae: 17.1981 - val_loss: 35.8612 - val_mae: 4.6701\n",
      "Epoch 43/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 532.0917 - mae: 16.6261 - val_loss: 57.1565 - val_mae: 6.0993\n",
      "Epoch 44/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 545.0548 - mae: 16.5561 - val_loss: 33.5242 - val_mae: 4.5257\n",
      "Epoch 45/300\n",
      "122/122 [==============================] - 0s 536us/step - loss: 529.7769 - mae: 16.5017 - val_loss: 39.4598 - val_mae: 4.9091\n",
      "Epoch 46/300\n",
      "122/122 [==============================] - 0s 536us/step - loss: 510.3981 - mae: 16.0646 - val_loss: 64.5706 - val_mae: 6.3786\n",
      "Epoch 47/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 520.4757 - mae: 16.1300 - val_loss: 29.6371 - val_mae: 4.2857\n",
      "Epoch 48/300\n",
      "122/122 [==============================] - 0s 581us/step - loss: 517.0160 - mae: 16.0912 - val_loss: 49.0194 - val_mae: 5.5051\n",
      "Epoch 49/300\n",
      "122/122 [==============================] - 0s 556us/step - loss: 475.2651 - mae: 15.5693 - val_loss: 44.9367 - val_mae: 5.5151\n",
      "Epoch 50/300\n",
      "122/122 [==============================] - 0s 531us/step - loss: 491.4155 - mae: 15.7762 - val_loss: 32.7198 - val_mae: 4.3460\n",
      "Epoch 51/300\n",
      "122/122 [==============================] - 0s 524us/step - loss: 505.1759 - mae: 15.6849 - val_loss: 62.1385 - val_mae: 6.1574\n",
      "Epoch 52/300\n",
      "122/122 [==============================] - 0s 539us/step - loss: 477.5817 - mae: 15.5479 - val_loss: 29.8476 - val_mae: 4.1230\n",
      "Epoch 53/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 478.2489 - mae: 15.5332 - val_loss: 42.1861 - val_mae: 5.0757\n",
      "Epoch 54/300\n",
      "122/122 [==============================] - 0s 516us/step - loss: 467.6408 - mae: 15.4261 - val_loss: 41.6439 - val_mae: 5.1451\n",
      "Epoch 55/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 450.4388 - mae: 15.1785 - val_loss: 54.8198 - val_mae: 6.0719\n",
      "Epoch 56/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 475.1054 - mae: 15.5277 - val_loss: 29.4539 - val_mae: 4.1037\n",
      "Epoch 57/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 488.1339 - mae: 15.4219 - val_loss: 31.1946 - val_mae: 4.3009\n",
      "Epoch 58/300\n",
      "122/122 [==============================] - 0s 575us/step - loss: 460.7291 - mae: 15.0092 - val_loss: 40.1650 - val_mae: 5.0552\n",
      "Epoch 59/300\n",
      "122/122 [==============================] - 0s 525us/step - loss: 450.9146 - mae: 14.7731 - val_loss: 29.6051 - val_mae: 4.2937\n",
      "Epoch 60/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 455.5903 - mae: 15.0134 - val_loss: 26.7322 - val_mae: 4.0033\n",
      "Epoch 61/300\n",
      "122/122 [==============================] - 0s 524us/step - loss: 456.8333 - mae: 14.8594 - val_loss: 33.9519 - val_mae: 4.7181\n",
      "Epoch 62/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 456.3868 - mae: 14.9870 - val_loss: 26.1599 - val_mae: 3.9546\n",
      "Epoch 63/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 429.3259 - mae: 14.7148 - val_loss: 35.8825 - val_mae: 4.8181\n",
      "Epoch 64/300\n",
      "122/122 [==============================] - 0s 581us/step - loss: 433.6062 - mae: 14.5997 - val_loss: 46.0243 - val_mae: 5.4765\n",
      "Epoch 65/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 463.5480 - mae: 14.8432 - val_loss: 33.3532 - val_mae: 4.5980\n",
      "Epoch 66/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 435.8702 - mae: 14.4267 - val_loss: 34.6442 - val_mae: 4.8227\n",
      "Epoch 67/300\n",
      "122/122 [==============================] - 0s 521us/step - loss: 437.5017 - mae: 14.6190 - val_loss: 49.2611 - val_mae: 5.4493\n",
      "Epoch 68/300\n",
      "122/122 [==============================] - 0s 531us/step - loss: 423.4008 - mae: 14.2661 - val_loss: 35.0919 - val_mae: 4.6322\n",
      "Epoch 69/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 428.2809 - mae: 14.4534 - val_loss: 44.2304 - val_mae: 5.1496\n",
      "Epoch 70/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 440.5654 - mae: 14.5077 - val_loss: 48.8659 - val_mae: 5.5255\n",
      "Epoch 71/300\n",
      "122/122 [==============================] - 0s 510us/step - loss: 437.0208 - mae: 14.6311 - val_loss: 39.1987 - val_mae: 4.9160\n",
      "Epoch 72/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 434.3372 - mae: 14.4956 - val_loss: 25.0850 - val_mae: 3.7640\n",
      "Epoch 73/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 419.4208 - mae: 14.1056 - val_loss: 42.4760 - val_mae: 5.1952\n",
      "Epoch 74/300\n",
      "122/122 [==============================] - 0s 494us/step - loss: 427.0474 - mae: 14.3098 - val_loss: 35.5923 - val_mae: 4.7640\n",
      "Epoch 75/300\n",
      "122/122 [==============================] - 0s 498us/step - loss: 403.1173 - mae: 13.9708 - val_loss: 36.4064 - val_mae: 4.8378\n",
      "Epoch 76/300\n",
      "122/122 [==============================] - 0s 494us/step - loss: 387.3063 - mae: 13.5561 - val_loss: 46.1033 - val_mae: 5.3430\n",
      "Epoch 77/300\n",
      "122/122 [==============================] - 0s 500us/step - loss: 391.9969 - mae: 13.5302 - val_loss: 38.4442 - val_mae: 4.8954\n",
      "Epoch 78/300\n",
      "122/122 [==============================] - 0s 558us/step - loss: 410.8551 - mae: 13.9126 - val_loss: 31.2887 - val_mae: 4.3995\n",
      "Epoch 79/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 381.5730 - mae: 13.3998 - val_loss: 103.6441 - val_mae: 7.5688\n",
      "Epoch 80/300\n",
      "122/122 [==============================] - 0s 517us/step - loss: 409.7266 - mae: 13.8808 - val_loss: 41.5788 - val_mae: 5.1000\n",
      "Epoch 81/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 392.3365 - mae: 13.4239 - val_loss: 32.4143 - val_mae: 4.5061\n",
      "Epoch 82/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 399.2378 - mae: 13.7025 - val_loss: 44.5829 - val_mae: 5.4104\n",
      "Epoch 83/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 396.7497 - mae: 13.6334 - val_loss: 28.6577 - val_mae: 4.0416\n",
      "Epoch 84/300\n",
      "122/122 [==============================] - 0s 523us/step - loss: 398.6895 - mae: 13.6230 - val_loss: 28.9606 - val_mae: 4.2582\n",
      "Epoch 85/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 383.6260 - mae: 13.6350 - val_loss: 55.0271 - val_mae: 5.8766\n",
      "Epoch 86/300\n",
      "122/122 [==============================] - 0s 516us/step - loss: 376.2889 - mae: 13.3534 - val_loss: 51.4309 - val_mae: 5.8905\n",
      "Epoch 87/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 369.4221 - mae: 13.1277 - val_loss: 38.6625 - val_mae: 4.8975\n",
      "Epoch 88/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 355.9050 - mae: 12.9813 - val_loss: 68.0852 - val_mae: 6.4791\n",
      "Epoch 89/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 403.4539 - mae: 13.6913 - val_loss: 39.4283 - val_mae: 4.8818\n",
      "Epoch 90/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 367.7643 - mae: 13.2580 - val_loss: 27.4203 - val_mae: 4.0350\n",
      "Epoch 91/300\n",
      "122/122 [==============================] - 0s 524us/step - loss: 373.3803 - mae: 13.1058 - val_loss: 28.1481 - val_mae: 4.1562\n",
      "Epoch 92/300\n",
      "122/122 [==============================] - 0s 497us/step - loss: 399.3641 - mae: 13.5654 - val_loss: 31.2038 - val_mae: 4.4117\n",
      "Epoch 93/300\n",
      "122/122 [==============================] - 0s 563us/step - loss: 371.8288 - mae: 13.1059 - val_loss: 37.8585 - val_mae: 4.9971\n",
      "Epoch 94/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 364.7442 - mae: 12.9304 - val_loss: 54.3450 - val_mae: 5.7751\n",
      "Epoch 95/300\n",
      "122/122 [==============================] - 0s 531us/step - loss: 358.9444 - mae: 12.8595 - val_loss: 39.9551 - val_mae: 5.1178\n",
      "Epoch 96/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 360.6758 - mae: 12.7671 - val_loss: 36.1887 - val_mae: 4.7938\n",
      "Epoch 97/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 332.1845 - mae: 12.3906 - val_loss: 28.8455 - val_mae: 4.1889\n",
      "Epoch 98/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 358.1563 - mae: 12.7589 - val_loss: 36.0560 - val_mae: 4.8323\n",
      "Epoch 99/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 346.3358 - mae: 12.8750 - val_loss: 26.5240 - val_mae: 3.9288\n",
      "Epoch 100/300\n",
      "122/122 [==============================] - 0s 531us/step - loss: 335.9115 - mae: 12.4309 - val_loss: 38.2639 - val_mae: 4.9672\n",
      "Epoch 101/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 367.2169 - mae: 12.7964 - val_loss: 42.8045 - val_mae: 5.3654\n",
      "Epoch 102/300\n",
      "122/122 [==============================] - 0s 526us/step - loss: 339.2793 - mae: 12.4787 - val_loss: 44.3191 - val_mae: 5.1459\n",
      "Epoch 103/300\n",
      "122/122 [==============================] - 0s 523us/step - loss: 349.7450 - mae: 12.5878 - val_loss: 57.2861 - val_mae: 5.9430\n",
      "Epoch 104/300\n",
      "122/122 [==============================] - 0s 500us/step - loss: 350.4495 - mae: 12.6729 - val_loss: 30.3861 - val_mae: 4.3110\n",
      "Epoch 105/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 337.8838 - mae: 12.4978 - val_loss: 45.0249 - val_mae: 5.4733\n",
      "Epoch 106/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 335.9523 - mae: 12.5127 - val_loss: 34.9781 - val_mae: 4.6858\n",
      "Epoch 107/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 343.3664 - mae: 12.5015 - val_loss: 43.5626 - val_mae: 5.4876\n",
      "Epoch 108/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 339.6390 - mae: 12.2617 - val_loss: 45.4389 - val_mae: 5.4341\n",
      "Epoch 109/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 318.8470 - mae: 12.0366 - val_loss: 33.4506 - val_mae: 4.6069\n",
      "Epoch 110/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 328.2366 - mae: 12.1928 - val_loss: 36.6079 - val_mae: 4.8274\n",
      "Epoch 111/300\n",
      "122/122 [==============================] - 0s 521us/step - loss: 332.2040 - mae: 12.1461 - val_loss: 44.4888 - val_mae: 5.2171\n",
      "Epoch 112/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 317.9562 - mae: 12.1729 - val_loss: 32.3026 - val_mae: 4.4569\n",
      "Epoch 113/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 325.3964 - mae: 12.0883 - val_loss: 57.7752 - val_mae: 5.9431\n",
      "Epoch 114/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 328.1578 - mae: 12.3415 - val_loss: 31.8282 - val_mae: 4.4003\n",
      "Epoch 115/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 327.9295 - mae: 12.1767 - val_loss: 33.5071 - val_mae: 4.5650\n",
      "Epoch 116/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 317.3974 - mae: 12.1138 - val_loss: 33.4213 - val_mae: 4.5413\n",
      "Epoch 117/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 314.7102 - mae: 11.9965 - val_loss: 34.9551 - val_mae: 4.7428\n",
      "Epoch 118/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 317.6224 - mae: 12.0124 - val_loss: 34.4163 - val_mae: 4.7187\n",
      "Epoch 119/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 314.6433 - mae: 12.0188 - val_loss: 25.1458 - val_mae: 3.8020\n",
      "Epoch 120/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 300.4008 - mae: 11.7349 - val_loss: 46.6640 - val_mae: 5.5572\n",
      "Epoch 121/300\n",
      "122/122 [==============================] - 0s 517us/step - loss: 323.9432 - mae: 11.9652 - val_loss: 48.7936 - val_mae: 5.6194\n",
      "Epoch 122/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 326.6947 - mae: 11.9937 - val_loss: 58.0825 - val_mae: 5.9476\n",
      "Epoch 123/300\n",
      "122/122 [==============================] - 0s 537us/step - loss: 299.8701 - mae: 11.6098 - val_loss: 32.1843 - val_mae: 4.4191\n",
      "Epoch 124/300\n",
      "122/122 [==============================] - 0s 540us/step - loss: 307.9716 - mae: 11.7829 - val_loss: 43.6719 - val_mae: 5.3023\n",
      "Epoch 125/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 274.4855 - mae: 11.2905 - val_loss: 36.3740 - val_mae: 4.7743\n",
      "Epoch 126/300\n",
      "122/122 [==============================] - 0s 517us/step - loss: 262.9109 - mae: 11.1921 - val_loss: 34.2638 - val_mae: 4.6379\n",
      "Epoch 127/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 290.5395 - mae: 11.4575 - val_loss: 51.8083 - val_mae: 5.6489\n",
      "Epoch 128/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 305.3557 - mae: 11.6462 - val_loss: 45.7297 - val_mae: 5.3591\n",
      "Epoch 129/300\n",
      "122/122 [==============================] - 0s 509us/step - loss: 292.9910 - mae: 11.5460 - val_loss: 44.5759 - val_mae: 5.2719\n",
      "Epoch 130/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 314.5293 - mae: 11.6785 - val_loss: 42.1017 - val_mae: 5.2743\n",
      "Epoch 131/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 299.9558 - mae: 11.5993 - val_loss: 53.7580 - val_mae: 5.9061\n",
      "Epoch 132/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 287.6135 - mae: 11.2568 - val_loss: 41.3941 - val_mae: 5.0344\n",
      "Epoch 133/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 282.1257 - mae: 11.2860 - val_loss: 34.1025 - val_mae: 4.6024\n",
      "Epoch 134/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 281.6979 - mae: 11.2252 - val_loss: 45.4472 - val_mae: 5.2935\n",
      "Epoch 135/300\n",
      "122/122 [==============================] - 0s 499us/step - loss: 292.9501 - mae: 11.2260 - val_loss: 43.4039 - val_mae: 5.3463\n",
      "Epoch 136/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 275.4408 - mae: 10.9814 - val_loss: 36.3019 - val_mae: 4.8721\n",
      "Epoch 137/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 275.2773 - mae: 11.0370 - val_loss: 55.8251 - val_mae: 5.9968\n",
      "Epoch 138/300\n",
      "122/122 [==============================] - 0s 567us/step - loss: 287.9245 - mae: 11.0740 - val_loss: 35.9425 - val_mae: 4.7746\n",
      "Epoch 139/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 276.4141 - mae: 11.1176 - val_loss: 48.6465 - val_mae: 5.7640\n",
      "Epoch 140/300\n",
      "122/122 [==============================] - 0s 492us/step - loss: 256.4230 - mae: 10.8271 - val_loss: 42.7847 - val_mae: 5.2581\n",
      "Epoch 141/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 260.6049 - mae: 10.7935 - val_loss: 39.8879 - val_mae: 5.0935\n",
      "Epoch 142/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 265.3831 - mae: 10.9575 - val_loss: 41.3353 - val_mae: 5.1754\n",
      "Epoch 143/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 255.8483 - mae: 10.7250 - val_loss: 52.4149 - val_mae: 5.8752\n",
      "Epoch 144/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 269.5115 - mae: 10.9143 - val_loss: 47.2689 - val_mae: 5.4521\n",
      "Epoch 145/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 255.8302 - mae: 10.7409 - val_loss: 63.7112 - val_mae: 6.1342\n",
      "Epoch 146/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 274.3732 - mae: 11.0611 - val_loss: 39.0871 - val_mae: 4.9763\n",
      "Epoch 147/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 260.1129 - mae: 10.7367 - val_loss: 46.4662 - val_mae: 5.5326\n",
      "Epoch 148/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 257.5257 - mae: 10.6243 - val_loss: 30.3383 - val_mae: 4.3061\n",
      "Epoch 149/300\n",
      "122/122 [==============================] - 0s 497us/step - loss: 271.4016 - mae: 10.9443 - val_loss: 36.4279 - val_mae: 4.7984\n",
      "Epoch 150/300\n",
      "122/122 [==============================] - 0s 501us/step - loss: 255.3171 - mae: 10.6454 - val_loss: 58.8292 - val_mae: 5.9279\n",
      "Epoch 151/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 263.6949 - mae: 10.7301 - val_loss: 49.1657 - val_mae: 5.4513\n",
      "Epoch 152/300\n",
      "122/122 [==============================] - 0s 561us/step - loss: 265.8068 - mae: 10.8064 - val_loss: 51.1585 - val_mae: 5.6510\n",
      "Epoch 153/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 265.3082 - mae: 10.6817 - val_loss: 52.3214 - val_mae: 5.7779\n",
      "Epoch 154/300\n",
      "122/122 [==============================] - 0s 500us/step - loss: 241.8979 - mae: 10.3947 - val_loss: 56.8471 - val_mae: 5.8816\n",
      "Epoch 155/300\n",
      "122/122 [==============================] - 0s 499us/step - loss: 256.8380 - mae: 10.4749 - val_loss: 46.1084 - val_mae: 5.4249\n",
      "Epoch 156/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 228.8845 - mae: 10.1001 - val_loss: 40.0305 - val_mae: 5.0903\n",
      "Epoch 157/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 238.2947 - mae: 10.2361 - val_loss: 44.5247 - val_mae: 5.3242\n",
      "Epoch 158/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 238.8175 - mae: 10.2095 - val_loss: 30.3988 - val_mae: 4.2852\n",
      "Epoch 159/300\n",
      "122/122 [==============================] - 0s 567us/step - loss: 234.6097 - mae: 10.1504 - val_loss: 48.5592 - val_mae: 5.7597\n",
      "Epoch 160/300\n",
      "122/122 [==============================] - 0s 524us/step - loss: 239.5700 - mae: 10.1757 - val_loss: 48.3437 - val_mae: 5.5683\n",
      "Epoch 161/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 204.1835 - mae: 9.7103 - val_loss: 43.8674 - val_mae: 5.3584\n",
      "Epoch 162/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 250.5479 - mae: 10.4154 - val_loss: 34.6362 - val_mae: 4.6385\n",
      "Epoch 163/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 231.6129 - mae: 9.9974 - val_loss: 45.3663 - val_mae: 5.5120\n",
      "Epoch 164/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 222.7399 - mae: 9.8615 - val_loss: 33.7857 - val_mae: 4.5367\n",
      "Epoch 165/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 225.4651 - mae: 9.9156 - val_loss: 54.6834 - val_mae: 5.7317\n",
      "Epoch 166/300\n",
      "122/122 [==============================] - 0s 572us/step - loss: 239.5790 - mae: 10.1293 - val_loss: 31.9831 - val_mae: 4.3790\n",
      "Epoch 167/300\n",
      "122/122 [==============================] - 0s 517us/step - loss: 215.6455 - mae: 9.7431 - val_loss: 28.5511 - val_mae: 4.0979\n",
      "Epoch 168/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 227.3724 - mae: 9.7775 - val_loss: 46.4599 - val_mae: 5.3699\n",
      "Epoch 169/300\n",
      "122/122 [==============================] - 0s 500us/step - loss: 227.0112 - mae: 9.9820 - val_loss: 43.9738 - val_mae: 5.1080\n",
      "Epoch 170/300\n",
      "122/122 [==============================] - 0s 510us/step - loss: 223.4162 - mae: 9.9047 - val_loss: 49.3461 - val_mae: 5.4234\n",
      "Epoch 171/300\n",
      "122/122 [==============================] - 0s 533us/step - loss: 225.6181 - mae: 9.8945 - val_loss: 35.9399 - val_mae: 4.7561\n",
      "Epoch 172/300\n",
      "122/122 [==============================] - 0s 513us/step - loss: 213.7633 - mae: 9.5464 - val_loss: 54.5064 - val_mae: 5.7836\n",
      "Epoch 173/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 228.2437 - mae: 9.7373 - val_loss: 43.6456 - val_mae: 5.3134\n",
      "Epoch 174/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 228.3772 - mae: 9.8859 - val_loss: 35.5536 - val_mae: 4.6217\n",
      "Epoch 175/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 227.1567 - mae: 9.6930 - val_loss: 39.5087 - val_mae: 5.0425\n",
      "Epoch 176/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 213.6842 - mae: 9.5636 - val_loss: 35.3637 - val_mae: 4.7714\n",
      "Epoch 177/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 225.4956 - mae: 9.6739 - val_loss: 31.0284 - val_mae: 4.2540\n",
      "Epoch 178/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 219.6994 - mae: 9.6885 - val_loss: 38.5929 - val_mae: 4.9437\n",
      "Epoch 179/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 218.5880 - mae: 9.6646 - val_loss: 40.8093 - val_mae: 5.0660\n",
      "Epoch 180/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 210.5772 - mae: 9.5528 - val_loss: 36.8199 - val_mae: 4.7848\n",
      "Epoch 181/300\n",
      "122/122 [==============================] - 0s 626us/step - loss: 198.6536 - mae: 9.2351 - val_loss: 39.1134 - val_mae: 4.9689\n",
      "Epoch 182/300\n",
      "122/122 [==============================] - 0s 565us/step - loss: 208.1806 - mae: 9.6171 - val_loss: 41.9995 - val_mae: 5.1284\n",
      "Epoch 183/300\n",
      "122/122 [==============================] - 0s 619us/step - loss: 195.3289 - mae: 9.3533 - val_loss: 33.2338 - val_mae: 4.4636\n",
      "Epoch 184/300\n",
      "122/122 [==============================] - 0s 616us/step - loss: 209.3976 - mae: 9.4090 - val_loss: 34.9320 - val_mae: 4.6111\n",
      "Epoch 185/300\n",
      "122/122 [==============================] - 0s 594us/step - loss: 205.4482 - mae: 9.3492 - val_loss: 39.6815 - val_mae: 5.0004\n",
      "Epoch 186/300\n",
      "122/122 [==============================] - 0s 613us/step - loss: 195.0249 - mae: 9.2088 - val_loss: 47.2292 - val_mae: 5.5294\n",
      "Epoch 187/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 196.0311 - mae: 9.1172 - val_loss: 43.9875 - val_mae: 5.2619\n",
      "Epoch 188/300\n",
      "122/122 [==============================] - 0s 598us/step - loss: 193.7892 - mae: 9.2493 - val_loss: 56.5384 - val_mae: 5.7741\n",
      "Epoch 189/300\n",
      "122/122 [==============================] - 0s 578us/step - loss: 198.0785 - mae: 9.1859 - val_loss: 53.5793 - val_mae: 5.6680\n",
      "Epoch 190/300\n",
      "122/122 [==============================] - 0s 594us/step - loss: 200.4545 - mae: 9.1966 - val_loss: 35.3456 - val_mae: 4.5934\n",
      "Epoch 191/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 186.5463 - mae: 9.0015 - val_loss: 42.7536 - val_mae: 5.2314\n",
      "Epoch 192/300\n",
      "122/122 [==============================] - 0s 510us/step - loss: 202.1874 - mae: 9.1735 - val_loss: 41.6904 - val_mae: 5.0993\n",
      "Epoch 193/300\n",
      "122/122 [==============================] - 0s 498us/step - loss: 192.4609 - mae: 9.0634 - val_loss: 27.2237 - val_mae: 3.9236\n",
      "Epoch 194/300\n",
      "122/122 [==============================] - 0s 499us/step - loss: 194.0827 - mae: 8.9983 - val_loss: 46.4828 - val_mae: 5.3660\n",
      "Epoch 195/300\n",
      "122/122 [==============================] - 0s 492us/step - loss: 193.9024 - mae: 9.0195 - val_loss: 60.5594 - val_mae: 6.0228\n",
      "Epoch 196/300\n",
      "122/122 [==============================] - 0s 525us/step - loss: 191.2195 - mae: 9.0781 - val_loss: 42.8740 - val_mae: 4.9653\n",
      "Epoch 197/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 199.7991 - mae: 9.1053 - val_loss: 45.6428 - val_mae: 5.3638\n",
      "Epoch 198/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 179.5446 - mae: 8.7824 - val_loss: 54.4401 - val_mae: 5.6800\n",
      "Epoch 199/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 189.7468 - mae: 9.0408 - val_loss: 36.9839 - val_mae: 4.8506\n",
      "Epoch 200/300\n",
      "122/122 [==============================] - 0s 497us/step - loss: 190.4709 - mae: 9.0549 - val_loss: 31.1624 - val_mae: 4.4465\n",
      "Epoch 201/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 192.4143 - mae: 9.1053 - val_loss: 43.4662 - val_mae: 5.1696\n",
      "Epoch 202/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 174.9181 - mae: 8.6526 - val_loss: 34.8938 - val_mae: 4.6115\n",
      "Epoch 203/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 180.2139 - mae: 8.8049 - val_loss: 33.5063 - val_mae: 4.4817\n",
      "Epoch 204/300\n",
      "122/122 [==============================] - 0s 569us/step - loss: 173.4034 - mae: 8.7516 - val_loss: 34.0026 - val_mae: 4.5398\n",
      "Epoch 205/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 177.6891 - mae: 8.8622 - val_loss: 35.4505 - val_mae: 4.7137\n",
      "Epoch 206/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 193.5034 - mae: 8.9256 - val_loss: 42.8432 - val_mae: 5.2952\n",
      "Epoch 207/300\n",
      "122/122 [==============================] - 0s 586us/step - loss: 196.7097 - mae: 8.9855 - val_loss: 49.8423 - val_mae: 5.3776\n",
      "Epoch 208/300\n",
      "122/122 [==============================] - 0s 493us/step - loss: 190.7399 - mae: 8.9543 - val_loss: 41.6641 - val_mae: 5.1977\n",
      "Epoch 209/300\n",
      "122/122 [==============================] - 0s 499us/step - loss: 176.8969 - mae: 8.8154 - val_loss: 38.7898 - val_mae: 4.9353\n",
      "Epoch 210/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 181.8553 - mae: 8.7121 - val_loss: 38.0088 - val_mae: 4.8029\n",
      "Epoch 211/300\n",
      "122/122 [==============================] - 0s 499us/step - loss: 168.9062 - mae: 8.4861 - val_loss: 50.9192 - val_mae: 5.8063\n",
      "Epoch 212/300\n",
      "122/122 [==============================] - 0s 540us/step - loss: 178.6039 - mae: 8.8032 - val_loss: 55.5289 - val_mae: 5.8173\n",
      "Epoch 213/300\n",
      "122/122 [==============================] - 0s 510us/step - loss: 179.3524 - mae: 8.7142 - val_loss: 32.5738 - val_mae: 4.4176\n",
      "Epoch 214/300\n",
      "122/122 [==============================] - 0s 501us/step - loss: 184.4313 - mae: 8.7832 - val_loss: 61.4877 - val_mae: 6.0925\n",
      "Epoch 215/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 176.9405 - mae: 8.7065 - val_loss: 33.5889 - val_mae: 4.4297\n",
      "Epoch 216/300\n",
      "122/122 [==============================] - 0s 539us/step - loss: 164.1330 - mae: 8.4839 - val_loss: 36.6243 - val_mae: 4.8122\n",
      "Epoch 217/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 163.1135 - mae: 8.5058 - val_loss: 39.4979 - val_mae: 4.9433\n",
      "Epoch 218/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 172.0712 - mae: 8.5697 - val_loss: 44.2033 - val_mae: 5.3588\n",
      "Epoch 219/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 178.1968 - mae: 8.7383 - val_loss: 35.9173 - val_mae: 4.7270\n",
      "Epoch 220/300\n",
      "122/122 [==============================] - 0s 504us/step - loss: 182.3243 - mae: 8.7914 - val_loss: 41.0228 - val_mae: 5.1457\n",
      "Epoch 221/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 156.7326 - mae: 8.3291 - val_loss: 32.0365 - val_mae: 4.3576\n",
      "Epoch 222/300\n",
      "122/122 [==============================] - 0s 494us/step - loss: 180.1705 - mae: 8.7431 - val_loss: 34.3591 - val_mae: 4.5274\n",
      "Epoch 223/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 171.9207 - mae: 8.6453 - val_loss: 37.3648 - val_mae: 4.8343\n",
      "Epoch 224/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 170.5069 - mae: 8.5466 - val_loss: 38.9960 - val_mae: 4.9942\n",
      "Epoch 225/300\n",
      "122/122 [==============================] - 0s 509us/step - loss: 159.4205 - mae: 8.2687 - val_loss: 48.8921 - val_mae: 5.4871\n",
      "Epoch 226/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 174.9902 - mae: 8.5247 - val_loss: 42.5703 - val_mae: 5.2219\n",
      "Epoch 227/300\n",
      "122/122 [==============================] - 0s 509us/step - loss: 153.8914 - mae: 8.2175 - val_loss: 29.0056 - val_mae: 4.1735\n",
      "Epoch 228/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 156.7583 - mae: 8.2425 - val_loss: 30.0034 - val_mae: 4.1749\n",
      "Epoch 229/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 164.3075 - mae: 8.3369 - val_loss: 36.9542 - val_mae: 4.8011\n",
      "Epoch 230/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 154.2764 - mae: 8.2559 - val_loss: 42.0317 - val_mae: 5.1682\n",
      "Epoch 231/300\n",
      "122/122 [==============================] - 0s 509us/step - loss: 150.7718 - mae: 8.1668 - val_loss: 39.3251 - val_mae: 4.9577\n",
      "Epoch 232/300\n",
      "122/122 [==============================] - 0s 502us/step - loss: 160.3621 - mae: 8.3214 - val_loss: 36.5859 - val_mae: 4.7527\n",
      "Epoch 233/300\n",
      "122/122 [==============================] - 0s 501us/step - loss: 157.9026 - mae: 8.2432 - val_loss: 48.8055 - val_mae: 5.4608\n",
      "Epoch 234/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 152.8869 - mae: 8.1869 - val_loss: 39.7778 - val_mae: 5.0772\n",
      "Epoch 235/300\n",
      "122/122 [==============================] - 0s 558us/step - loss: 160.3794 - mae: 8.3377 - val_loss: 41.7068 - val_mae: 5.0421\n",
      "Epoch 236/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 155.4077 - mae: 8.3157 - val_loss: 32.5849 - val_mae: 4.3440\n",
      "Epoch 237/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 157.0517 - mae: 8.3745 - val_loss: 34.6980 - val_mae: 4.6082\n",
      "Epoch 238/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 159.8637 - mae: 8.3038 - val_loss: 37.7999 - val_mae: 4.8322\n",
      "Epoch 239/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 156.0214 - mae: 8.0851 - val_loss: 32.1203 - val_mae: 4.3786\n",
      "Epoch 240/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 158.1141 - mae: 8.3524 - val_loss: 36.3607 - val_mae: 4.7808\n",
      "Epoch 241/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 159.2641 - mae: 8.2011 - val_loss: 60.8281 - val_mae: 6.0134\n",
      "Epoch 242/300\n",
      "122/122 [==============================] - 0s 505us/step - loss: 163.6352 - mae: 8.2976 - val_loss: 43.2086 - val_mae: 5.2104\n",
      "Epoch 243/300\n",
      "122/122 [==============================] - 0s 500us/step - loss: 148.8998 - mae: 8.1239 - val_loss: 40.7815 - val_mae: 5.0396\n",
      "Epoch 244/300\n",
      "122/122 [==============================] - 0s 501us/step - loss: 152.6962 - mae: 8.0890 - val_loss: 38.6953 - val_mae: 4.9903\n",
      "Epoch 245/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 160.2273 - mae: 8.2196 - val_loss: 52.0423 - val_mae: 5.4437\n",
      "Epoch 246/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 141.1622 - mae: 8.0186 - val_loss: 34.7073 - val_mae: 4.6375\n",
      "Epoch 247/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 148.2343 - mae: 8.0773 - val_loss: 35.8315 - val_mae: 4.8201\n",
      "Epoch 248/300\n",
      "122/122 [==============================] - 0s 535us/step - loss: 144.3377 - mae: 8.0090 - val_loss: 62.6326 - val_mae: 6.1124\n",
      "Epoch 249/300\n",
      "122/122 [==============================] - 0s 598us/step - loss: 146.2064 - mae: 8.0548 - val_loss: 32.9597 - val_mae: 4.3697\n",
      "Epoch 250/300\n",
      "122/122 [==============================] - 0s 578us/step - loss: 146.5931 - mae: 8.0156 - val_loss: 36.9126 - val_mae: 4.8259\n",
      "Epoch 251/300\n",
      "122/122 [==============================] - 0s 525us/step - loss: 158.4118 - mae: 8.2760 - val_loss: 42.1739 - val_mae: 5.2520\n",
      "Epoch 252/300\n",
      "122/122 [==============================] - 0s 517us/step - loss: 145.7113 - mae: 7.9940 - val_loss: 37.5101 - val_mae: 4.7652\n",
      "Epoch 253/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 135.2635 - mae: 7.8241 - val_loss: 39.7880 - val_mae: 4.8748\n",
      "Epoch 254/300\n",
      "122/122 [==============================] - 0s 521us/step - loss: 139.0569 - mae: 7.8661 - val_loss: 30.9201 - val_mae: 4.1359\n",
      "Epoch 255/300\n",
      "122/122 [==============================] - 0s 509us/step - loss: 148.0701 - mae: 7.9588 - val_loss: 37.2825 - val_mae: 4.7042\n",
      "Epoch 256/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 133.1992 - mae: 7.7300 - val_loss: 40.2853 - val_mae: 4.9081\n",
      "Epoch 257/300\n",
      "122/122 [==============================] - 0s 671us/step - loss: 153.4158 - mae: 8.1435 - val_loss: 38.1018 - val_mae: 4.9230\n",
      "Epoch 258/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 140.0713 - mae: 7.8082 - val_loss: 32.9799 - val_mae: 4.4262\n",
      "Epoch 259/300\n",
      "122/122 [==============================] - 0s 526us/step - loss: 148.0445 - mae: 8.0060 - val_loss: 36.6077 - val_mae: 4.7047\n",
      "Epoch 260/300\n",
      "122/122 [==============================] - 0s 519us/step - loss: 146.0384 - mae: 8.0252 - val_loss: 29.1990 - val_mae: 4.0995\n",
      "Epoch 261/300\n",
      "122/122 [==============================] - 0s 509us/step - loss: 145.3463 - mae: 7.8687 - val_loss: 44.4619 - val_mae: 5.3616\n",
      "Epoch 262/300\n",
      "122/122 [==============================] - 0s 516us/step - loss: 144.1208 - mae: 7.8976 - val_loss: 36.8247 - val_mae: 4.7437\n",
      "Epoch 263/300\n",
      "122/122 [==============================] - 0s 568us/step - loss: 140.8470 - mae: 7.8059 - val_loss: 37.1440 - val_mae: 4.8612\n",
      "Epoch 264/300\n",
      "122/122 [==============================] - 0s 559us/step - loss: 134.5537 - mae: 7.6761 - val_loss: 37.1230 - val_mae: 4.6841\n",
      "Epoch 265/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 136.7742 - mae: 7.6954 - val_loss: 45.5712 - val_mae: 5.3851\n",
      "Epoch 266/300\n",
      "122/122 [==============================] - 0s 514us/step - loss: 131.1815 - mae: 7.6697 - val_loss: 38.4939 - val_mae: 4.9923\n",
      "Epoch 267/300\n",
      "122/122 [==============================] - 0s 517us/step - loss: 147.3573 - mae: 7.9153 - val_loss: 36.6084 - val_mae: 4.8611\n",
      "Epoch 268/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 134.5687 - mae: 7.7358 - val_loss: 36.3487 - val_mae: 4.6798\n",
      "Epoch 269/300\n",
      "122/122 [==============================] - 0s 507us/step - loss: 131.3129 - mae: 7.6967 - val_loss: 30.5094 - val_mae: 4.3006\n",
      "Epoch 270/300\n",
      "122/122 [==============================] - 0s 515us/step - loss: 135.7149 - mae: 7.6063 - val_loss: 44.8907 - val_mae: 5.4744\n",
      "Epoch 271/300\n",
      "122/122 [==============================] - 0s 512us/step - loss: 141.5243 - mae: 7.9702 - val_loss: 37.9193 - val_mae: 4.9092\n",
      "Epoch 272/300\n",
      "122/122 [==============================] - 0s 537us/step - loss: 131.9236 - mae: 7.6613 - val_loss: 33.6878 - val_mae: 4.5119\n",
      "Epoch 273/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 128.8019 - mae: 7.6363 - val_loss: 31.5833 - val_mae: 4.2501\n",
      "Epoch 274/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 132.3780 - mae: 7.6953 - val_loss: 36.5072 - val_mae: 4.6107\n",
      "Epoch 275/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 131.9862 - mae: 7.7265 - val_loss: 55.5683 - val_mae: 5.9812\n",
      "Epoch 276/300\n",
      "122/122 [==============================] - 0s 530us/step - loss: 135.1965 - mae: 7.7675 - val_loss: 37.7670 - val_mae: 4.8432\n",
      "Epoch 277/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 133.0860 - mae: 7.6962 - val_loss: 47.1916 - val_mae: 5.4423\n",
      "Epoch 278/300\n",
      "122/122 [==============================] - 0s 519us/step - loss: 128.7011 - mae: 7.5180 - val_loss: 34.8503 - val_mae: 4.5787\n",
      "Epoch 279/300\n",
      "122/122 [==============================] - 0s 521us/step - loss: 136.4314 - mae: 7.6599 - val_loss: 31.4527 - val_mae: 4.3712\n",
      "Epoch 280/300\n",
      "122/122 [==============================] - 0s 525us/step - loss: 136.9146 - mae: 7.7554 - val_loss: 33.9602 - val_mae: 4.5861\n",
      "Epoch 281/300\n",
      "122/122 [==============================] - 0s 516us/step - loss: 126.2699 - mae: 7.6069 - val_loss: 41.5061 - val_mae: 5.1946\n",
      "Epoch 282/300\n",
      "122/122 [==============================] - 0s 510us/step - loss: 133.4292 - mae: 7.7296 - val_loss: 38.7766 - val_mae: 4.9652\n",
      "Epoch 283/300\n",
      "122/122 [==============================] - 0s 520us/step - loss: 126.0095 - mae: 7.4551 - val_loss: 37.7984 - val_mae: 4.8288\n",
      "Epoch 284/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 115.9890 - mae: 7.3664 - val_loss: 30.3395 - val_mae: 4.2509\n",
      "Epoch 285/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 131.2287 - mae: 7.4981 - val_loss: 38.9256 - val_mae: 4.9435\n",
      "Epoch 286/300\n",
      "122/122 [==============================] - 0s 610us/step - loss: 123.6426 - mae: 7.4291 - val_loss: 31.7806 - val_mae: 4.3697\n",
      "Epoch 287/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 117.5994 - mae: 7.3376 - val_loss: 50.0764 - val_mae: 5.6743\n",
      "Epoch 288/300\n",
      "122/122 [==============================] - 0s 518us/step - loss: 131.0987 - mae: 7.6056 - val_loss: 42.4737 - val_mae: 5.1374\n",
      "Epoch 289/300\n",
      "122/122 [==============================] - 0s 508us/step - loss: 127.4271 - mae: 7.6815 - val_loss: 30.7289 - val_mae: 4.3438\n",
      "Epoch 290/300\n",
      "122/122 [==============================] - 0s 506us/step - loss: 129.4459 - mae: 7.6548 - val_loss: 29.4049 - val_mae: 4.2698\n",
      "Epoch 291/300\n",
      "122/122 [==============================] - 0s 522us/step - loss: 126.7190 - mae: 7.5649 - val_loss: 48.0781 - val_mae: 5.7599\n",
      "Epoch 292/300\n",
      "122/122 [==============================] - 0s 528us/step - loss: 125.6851 - mae: 7.5516 - val_loss: 37.9142 - val_mae: 4.7651\n",
      "Epoch 293/300\n",
      "122/122 [==============================] - 0s 513us/step - loss: 113.3530 - mae: 7.2605 - val_loss: 43.0480 - val_mae: 5.2358\n",
      "Epoch 294/300\n",
      "122/122 [==============================] - 0s 513us/step - loss: 113.6853 - mae: 7.3166 - val_loss: 32.4902 - val_mae: 4.4598\n",
      "Epoch 295/300\n",
      "122/122 [==============================] - 0s 578us/step - loss: 115.2392 - mae: 7.2951 - val_loss: 33.9925 - val_mae: 4.5483\n",
      "Epoch 296/300\n",
      "122/122 [==============================] - 0s 513us/step - loss: 122.5919 - mae: 7.5399 - val_loss: 45.2215 - val_mae: 5.4720\n",
      "Epoch 297/300\n",
      "122/122 [==============================] - 0s 511us/step - loss: 118.1271 - mae: 7.2491 - val_loss: 36.1520 - val_mae: 4.6704\n",
      "Epoch 298/300\n",
      "122/122 [==============================] - 0s 503us/step - loss: 120.4508 - mae: 7.4589 - val_loss: 37.8443 - val_mae: 4.9379\n",
      "Epoch 299/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 117.5455 - mae: 7.3150 - val_loss: 33.2540 - val_mae: 4.5110\n",
      "Epoch 300/300\n",
      "122/122 [==============================] - 0s 525us/step - loss: 124.3877 - mae: 7.3993 - val_loss: 46.4485 - val_mae: 5.2388\n",
      "31/31 [==============================] - 0s 249us/step\n",
      "Epochs: 300 | MAE: 5.238770446464343\n",
      "Training model with 350 epochs\n",
      "Epoch 1/350\n",
      "122/122 [==============================] - 0s 827us/step - loss: 68376.8359 - mae: 227.2388 - val_loss: 61207.7188 - val_mae: 212.4549\n",
      "Epoch 2/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 41755.7188 - mae: 173.3714 - val_loss: 18442.1562 - val_mae: 115.9458\n",
      "Epoch 3/350\n",
      "122/122 [==============================] - 0s 526us/step - loss: 8162.7754 - mae: 71.9436 - val_loss: 3300.2119 - val_mae: 43.6616\n",
      "Epoch 4/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 3478.6008 - mae: 44.9436 - val_loss: 1948.3728 - val_mae: 33.1197\n",
      "Epoch 5/350\n",
      "122/122 [==============================] - 0s 519us/step - loss: 2671.4011 - mae: 39.0887 - val_loss: 1480.4829 - val_mae: 29.2693\n",
      "Epoch 6/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 2221.7913 - mae: 36.1123 - val_loss: 1194.1998 - val_mae: 26.0863\n",
      "Epoch 7/350\n",
      "122/122 [==============================] - 0s 545us/step - loss: 2011.5380 - mae: 34.3247 - val_loss: 1041.6488 - val_mae: 24.5396\n",
      "Epoch 8/350\n",
      "122/122 [==============================] - 0s 528us/step - loss: 1845.5999 - mae: 33.4546 - val_loss: 945.9227 - val_mae: 23.4788\n",
      "Epoch 9/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 1743.9192 - mae: 32.3574 - val_loss: 849.6737 - val_mae: 22.1956\n",
      "Epoch 10/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 1583.3413 - mae: 31.2319 - val_loss: 764.1849 - val_mae: 21.0654\n",
      "Epoch 11/350\n",
      "122/122 [==============================] - 0s 531us/step - loss: 1469.2238 - mae: 29.9112 - val_loss: 687.2229 - val_mae: 19.8940\n",
      "Epoch 12/350\n",
      "122/122 [==============================] - 0s 671us/step - loss: 1402.1044 - mae: 29.2816 - val_loss: 630.7831 - val_mae: 19.1452\n",
      "Epoch 13/350\n",
      "122/122 [==============================] - 0s 534us/step - loss: 1336.3777 - mae: 28.3705 - val_loss: 561.3379 - val_mae: 18.0185\n",
      "Epoch 14/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 1282.1873 - mae: 27.6265 - val_loss: 511.8809 - val_mae: 17.2233\n",
      "Epoch 15/350\n",
      "122/122 [==============================] - 0s 535us/step - loss: 1156.9489 - mae: 26.3908 - val_loss: 457.1762 - val_mae: 16.2428\n",
      "Epoch 16/350\n",
      "122/122 [==============================] - 0s 522us/step - loss: 1130.6825 - mae: 26.1315 - val_loss: 392.0199 - val_mae: 14.8973\n",
      "Epoch 17/350\n",
      "122/122 [==============================] - 0s 521us/step - loss: 1061.6594 - mae: 25.4949 - val_loss: 360.4886 - val_mae: 14.3756\n",
      "Epoch 18/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 1025.4025 - mae: 24.7994 - val_loss: 338.5871 - val_mae: 13.9895\n",
      "Epoch 19/350\n",
      "122/122 [==============================] - 0s 529us/step - loss: 956.8801 - mae: 23.9092 - val_loss: 297.5995 - val_mae: 13.0444\n",
      "Epoch 20/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 942.3365 - mae: 23.5101 - val_loss: 250.2451 - val_mae: 11.8809\n",
      "Epoch 21/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 872.5129 - mae: 22.8177 - val_loss: 237.1744 - val_mae: 11.6589\n",
      "Epoch 22/350\n",
      "122/122 [==============================] - 0s 574us/step - loss: 865.4320 - mae: 22.4334 - val_loss: 230.0653 - val_mae: 11.6902\n",
      "Epoch 23/350\n",
      "122/122 [==============================] - 0s 538us/step - loss: 777.6018 - mae: 21.4071 - val_loss: 178.2944 - val_mae: 10.0766\n",
      "Epoch 24/350\n",
      "122/122 [==============================] - 0s 527us/step - loss: 812.3159 - mae: 21.6048 - val_loss: 158.1763 - val_mae: 9.4018\n",
      "Epoch 25/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 776.5286 - mae: 21.0897 - val_loss: 146.2952 - val_mae: 9.0173\n",
      "Epoch 26/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 720.6334 - mae: 20.2597 - val_loss: 124.2512 - val_mae: 8.2782\n",
      "Epoch 27/350\n",
      "122/122 [==============================] - 0s 541us/step - loss: 705.6478 - mae: 20.2472 - val_loss: 126.4849 - val_mae: 8.6447\n",
      "Epoch 28/350\n",
      "122/122 [==============================] - 0s 535us/step - loss: 686.4408 - mae: 19.7594 - val_loss: 98.8284 - val_mae: 7.4669\n",
      "Epoch 29/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 658.0479 - mae: 19.1995 - val_loss: 90.3065 - val_mae: 7.0834\n",
      "Epoch 30/350\n",
      "122/122 [==============================] - 0s 533us/step - loss: 671.4056 - mae: 19.2548 - val_loss: 76.6352 - val_mae: 6.4396\n",
      "Epoch 31/350\n",
      "122/122 [==============================] - 0s 526us/step - loss: 637.7009 - mae: 18.7493 - val_loss: 73.5286 - val_mae: 6.2923\n",
      "Epoch 32/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 619.6387 - mae: 18.5271 - val_loss: 94.9961 - val_mae: 7.7839\n",
      "Epoch 33/350\n",
      "122/122 [==============================] - 0s 543us/step - loss: 590.9011 - mae: 18.0382 - val_loss: 64.4427 - val_mae: 5.9949\n",
      "Epoch 34/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 602.2087 - mae: 17.9159 - val_loss: 74.2882 - val_mae: 6.6452\n",
      "Epoch 35/350\n",
      "122/122 [==============================] - 0s 535us/step - loss: 586.8802 - mae: 17.9045 - val_loss: 66.0863 - val_mae: 6.3285\n",
      "Epoch 36/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 581.3905 - mae: 17.6186 - val_loss: 56.7439 - val_mae: 5.6946\n",
      "Epoch 37/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 575.4363 - mae: 17.6367 - val_loss: 57.0085 - val_mae: 5.9559\n",
      "Epoch 38/350\n",
      "122/122 [==============================] - 0s 543us/step - loss: 563.8383 - mae: 17.1453 - val_loss: 59.2906 - val_mae: 6.1417\n",
      "Epoch 39/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 578.1468 - mae: 17.4077 - val_loss: 60.6313 - val_mae: 6.2836\n",
      "Epoch 40/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 562.7927 - mae: 16.9734 - val_loss: 45.2214 - val_mae: 5.3420\n",
      "Epoch 41/350\n",
      "122/122 [==============================] - 0s 534us/step - loss: 565.5867 - mae: 17.1416 - val_loss: 69.8989 - val_mae: 6.7420\n",
      "Epoch 42/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 543.2924 - mae: 16.8824 - val_loss: 46.4560 - val_mae: 5.5376\n",
      "Epoch 43/350\n",
      "122/122 [==============================] - 0s 527us/step - loss: 534.0151 - mae: 16.7782 - val_loss: 48.2931 - val_mae: 5.4432\n",
      "Epoch 44/350\n",
      "122/122 [==============================] - 0s 524us/step - loss: 540.0972 - mae: 16.5920 - val_loss: 38.1029 - val_mae: 4.7120\n",
      "Epoch 45/350\n",
      "122/122 [==============================] - 0s 521us/step - loss: 525.6116 - mae: 16.4252 - val_loss: 41.4262 - val_mae: 5.1062\n",
      "Epoch 46/350\n",
      "122/122 [==============================] - 0s 587us/step - loss: 537.2520 - mae: 16.5113 - val_loss: 42.2004 - val_mae: 5.0302\n",
      "Epoch 47/350\n",
      "122/122 [==============================] - 0s 570us/step - loss: 520.1599 - mae: 16.5150 - val_loss: 37.0435 - val_mae: 4.7129\n",
      "Epoch 48/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 511.4918 - mae: 16.2727 - val_loss: 44.3002 - val_mae: 5.4131\n",
      "Epoch 49/350\n",
      "122/122 [==============================] - 0s 571us/step - loss: 522.2526 - mae: 16.2765 - val_loss: 39.5942 - val_mae: 4.8379\n",
      "Epoch 50/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 513.6672 - mae: 16.1617 - val_loss: 50.8589 - val_mae: 5.8075\n",
      "Epoch 51/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 494.3268 - mae: 15.9325 - val_loss: 42.5729 - val_mae: 5.3632\n",
      "Epoch 52/350\n",
      "122/122 [==============================] - 0s 543us/step - loss: 504.2665 - mae: 16.0177 - val_loss: 55.3719 - val_mae: 5.9870\n",
      "Epoch 53/350\n",
      "122/122 [==============================] - 0s 543us/step - loss: 474.9234 - mae: 15.6706 - val_loss: 28.1842 - val_mae: 4.1259\n",
      "Epoch 54/350\n",
      "122/122 [==============================] - 0s 521us/step - loss: 478.6254 - mae: 15.6524 - val_loss: 36.0814 - val_mae: 4.8068\n",
      "Epoch 55/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 474.3027 - mae: 15.4842 - val_loss: 48.5766 - val_mae: 5.6546\n",
      "Epoch 56/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 515.3648 - mae: 16.0117 - val_loss: 46.4306 - val_mae: 5.6839\n",
      "Epoch 57/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 495.4383 - mae: 15.7795 - val_loss: 36.7032 - val_mae: 4.9023\n",
      "Epoch 58/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 492.5628 - mae: 15.6305 - val_loss: 58.0796 - val_mae: 6.1695\n",
      "Epoch 59/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 487.6018 - mae: 15.5528 - val_loss: 36.4794 - val_mae: 4.8570\n",
      "Epoch 60/350\n",
      "122/122 [==============================] - 0s 529us/step - loss: 505.7708 - mae: 15.8106 - val_loss: 31.9349 - val_mae: 4.3529\n",
      "Epoch 61/350\n",
      "122/122 [==============================] - 0s 531us/step - loss: 480.1751 - mae: 15.2786 - val_loss: 26.8527 - val_mae: 3.9980\n",
      "Epoch 62/350\n",
      "122/122 [==============================] - 0s 528us/step - loss: 521.7780 - mae: 15.8829 - val_loss: 47.3057 - val_mae: 5.6604\n",
      "Epoch 63/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 467.6182 - mae: 15.1687 - val_loss: 33.2313 - val_mae: 4.5835\n",
      "Epoch 64/350\n",
      "122/122 [==============================] - 0s 530us/step - loss: 438.3002 - mae: 14.8079 - val_loss: 31.3481 - val_mae: 4.3640\n",
      "Epoch 65/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 463.1441 - mae: 15.1098 - val_loss: 60.6691 - val_mae: 6.1461\n",
      "Epoch 66/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 463.8382 - mae: 15.1600 - val_loss: 55.4262 - val_mae: 5.8629\n",
      "Epoch 67/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 475.8899 - mae: 15.2473 - val_loss: 42.4662 - val_mae: 5.3644\n",
      "Epoch 68/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 472.6203 - mae: 15.3117 - val_loss: 29.7158 - val_mae: 4.1932\n",
      "Epoch 69/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 456.2418 - mae: 15.0509 - val_loss: 47.2877 - val_mae: 5.5723\n",
      "Epoch 70/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 469.2733 - mae: 14.8830 - val_loss: 85.5016 - val_mae: 7.0913\n",
      "Epoch 71/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 468.7660 - mae: 15.0737 - val_loss: 35.3403 - val_mae: 4.7939\n",
      "Epoch 72/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 455.8413 - mae: 14.8512 - val_loss: 32.1599 - val_mae: 4.4709\n",
      "Epoch 73/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 460.7000 - mae: 15.0156 - val_loss: 35.1143 - val_mae: 4.6184\n",
      "Epoch 74/350\n",
      "122/122 [==============================] - 0s 540us/step - loss: 442.9690 - mae: 14.7196 - val_loss: 31.3126 - val_mae: 4.2014\n",
      "Epoch 75/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 443.5622 - mae: 14.7178 - val_loss: 34.4573 - val_mae: 4.5333\n",
      "Epoch 76/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 455.6967 - mae: 14.9036 - val_loss: 28.5373 - val_mae: 4.0896\n",
      "Epoch 77/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 456.0089 - mae: 14.7814 - val_loss: 50.4041 - val_mae: 5.4407\n",
      "Epoch 78/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 433.5725 - mae: 14.5225 - val_loss: 39.4014 - val_mae: 4.8841\n",
      "Epoch 79/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 428.7603 - mae: 14.4260 - val_loss: 28.9344 - val_mae: 4.1109\n",
      "Epoch 80/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 428.9582 - mae: 14.5194 - val_loss: 51.1780 - val_mae: 5.6707\n",
      "Epoch 81/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 427.2259 - mae: 14.5371 - val_loss: 30.7811 - val_mae: 4.3547\n",
      "Epoch 82/350\n",
      "122/122 [==============================] - 0s 520us/step - loss: 426.0706 - mae: 14.2764 - val_loss: 28.6702 - val_mae: 4.2610\n",
      "Epoch 83/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 447.3542 - mae: 14.6793 - val_loss: 29.3555 - val_mae: 4.2807\n",
      "Epoch 84/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 406.2573 - mae: 13.9057 - val_loss: 31.3537 - val_mae: 4.4530\n",
      "Epoch 85/350\n",
      "122/122 [==============================] - 0s 519us/step - loss: 429.1983 - mae: 14.3878 - val_loss: 30.4889 - val_mae: 4.2935\n",
      "Epoch 86/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 429.9494 - mae: 14.2970 - val_loss: 29.4982 - val_mae: 4.1733\n",
      "Epoch 87/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 425.0454 - mae: 14.4256 - val_loss: 45.7864 - val_mae: 5.4078\n",
      "Epoch 88/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 413.5468 - mae: 13.8910 - val_loss: 26.6741 - val_mae: 4.0026\n",
      "Epoch 89/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 431.7590 - mae: 14.1333 - val_loss: 24.8498 - val_mae: 3.8121\n",
      "Epoch 90/350\n",
      "122/122 [==============================] - 0s 521us/step - loss: 402.2617 - mae: 13.9029 - val_loss: 46.6866 - val_mae: 5.4104\n",
      "Epoch 91/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 427.6331 - mae: 14.2918 - val_loss: 45.9377 - val_mae: 5.4051\n",
      "Epoch 92/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 391.5206 - mae: 13.6906 - val_loss: 57.2072 - val_mae: 5.8945\n",
      "Epoch 93/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 413.4574 - mae: 13.9391 - val_loss: 32.6212 - val_mae: 4.5134\n",
      "Epoch 94/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 404.9277 - mae: 13.7133 - val_loss: 27.6190 - val_mae: 4.0249\n",
      "Epoch 95/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 412.8987 - mae: 14.0346 - val_loss: 33.2739 - val_mae: 4.3031\n",
      "Epoch 96/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 417.9015 - mae: 13.9959 - val_loss: 38.4610 - val_mae: 4.7647\n",
      "Epoch 97/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 386.2968 - mae: 13.5672 - val_loss: 41.4650 - val_mae: 5.2847\n",
      "Epoch 98/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 412.7562 - mae: 13.7903 - val_loss: 32.7622 - val_mae: 4.5753\n",
      "Epoch 99/350\n",
      "122/122 [==============================] - 0s 526us/step - loss: 381.7496 - mae: 13.4756 - val_loss: 42.0136 - val_mae: 5.1839\n",
      "Epoch 100/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 418.6544 - mae: 13.7462 - val_loss: 36.6525 - val_mae: 4.9355\n",
      "Epoch 101/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 401.0276 - mae: 13.7680 - val_loss: 30.6066 - val_mae: 4.2980\n",
      "Epoch 102/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 400.9078 - mae: 13.7853 - val_loss: 31.1872 - val_mae: 4.1905\n",
      "Epoch 103/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 392.2154 - mae: 13.7193 - val_loss: 32.0538 - val_mae: 4.4819\n",
      "Epoch 104/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 394.9910 - mae: 13.5872 - val_loss: 44.6302 - val_mae: 5.3438\n",
      "Epoch 105/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 381.3679 - mae: 13.4970 - val_loss: 43.9213 - val_mae: 5.3654\n",
      "Epoch 106/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 389.7754 - mae: 13.4257 - val_loss: 29.5910 - val_mae: 4.2968\n",
      "Epoch 107/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 386.8881 - mae: 13.4698 - val_loss: 27.3242 - val_mae: 3.9753\n",
      "Epoch 108/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 361.3328 - mae: 13.0498 - val_loss: 32.2589 - val_mae: 4.4675\n",
      "Epoch 109/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 364.4866 - mae: 13.1138 - val_loss: 40.5710 - val_mae: 5.0111\n",
      "Epoch 110/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 378.2795 - mae: 13.2954 - val_loss: 49.6788 - val_mae: 5.6246\n",
      "Epoch 111/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 375.9051 - mae: 13.1285 - val_loss: 30.8155 - val_mae: 4.1803\n",
      "Epoch 112/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 361.3922 - mae: 12.9221 - val_loss: 48.9626 - val_mae: 5.6140\n",
      "Epoch 113/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 373.6401 - mae: 13.1569 - val_loss: 39.3329 - val_mae: 5.0170\n",
      "Epoch 114/350\n",
      "122/122 [==============================] - 0s 519us/step - loss: 371.0821 - mae: 13.0713 - val_loss: 35.2799 - val_mae: 4.6824\n",
      "Epoch 115/350\n",
      "122/122 [==============================] - 0s 520us/step - loss: 373.6706 - mae: 13.0985 - val_loss: 33.8417 - val_mae: 4.5926\n",
      "Epoch 116/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 350.7180 - mae: 12.7527 - val_loss: 53.9282 - val_mae: 5.8658\n",
      "Epoch 117/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 364.4605 - mae: 12.8727 - val_loss: 39.0668 - val_mae: 5.1258\n",
      "Epoch 118/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 348.7237 - mae: 12.7011 - val_loss: 38.1063 - val_mae: 4.9371\n",
      "Epoch 119/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 368.7244 - mae: 12.9115 - val_loss: 36.5029 - val_mae: 4.8254\n",
      "Epoch 120/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 363.2520 - mae: 12.7033 - val_loss: 36.8608 - val_mae: 4.8594\n",
      "Epoch 121/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 371.7014 - mae: 13.0459 - val_loss: 35.4254 - val_mae: 4.7755\n",
      "Epoch 122/350\n",
      "122/122 [==============================] - 0s 538us/step - loss: 370.0265 - mae: 12.6948 - val_loss: 29.1163 - val_mae: 4.2326\n",
      "Epoch 123/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 377.1587 - mae: 13.0388 - val_loss: 34.3096 - val_mae: 4.6626\n",
      "Epoch 124/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 342.7524 - mae: 12.5426 - val_loss: 49.9613 - val_mae: 5.5836\n",
      "Epoch 125/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 376.1761 - mae: 12.9903 - val_loss: 35.0090 - val_mae: 4.6803\n",
      "Epoch 126/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 350.1943 - mae: 12.4606 - val_loss: 44.7416 - val_mae: 5.4829\n",
      "Epoch 127/350\n",
      "122/122 [==============================] - 0s 519us/step - loss: 358.1383 - mae: 12.5164 - val_loss: 34.2881 - val_mae: 4.5562\n",
      "Epoch 128/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 349.8762 - mae: 12.6095 - val_loss: 39.7570 - val_mae: 5.0733\n",
      "Epoch 129/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 346.7717 - mae: 12.4456 - val_loss: 32.7765 - val_mae: 4.3763\n",
      "Epoch 130/350\n",
      "122/122 [==============================] - 0s 527us/step - loss: 342.6483 - mae: 12.3496 - val_loss: 28.9864 - val_mae: 4.1694\n",
      "Epoch 131/350\n",
      "122/122 [==============================] - 0s 542us/step - loss: 348.7545 - mae: 12.5032 - val_loss: 43.7528 - val_mae: 5.3540\n",
      "Epoch 132/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 347.0935 - mae: 12.4139 - val_loss: 38.8854 - val_mae: 5.1233\n",
      "Epoch 133/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 327.8341 - mae: 12.1981 - val_loss: 29.6338 - val_mae: 4.2050\n",
      "Epoch 134/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 310.1034 - mae: 11.8675 - val_loss: 35.3717 - val_mae: 4.7482\n",
      "Epoch 135/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 332.3344 - mae: 12.2516 - val_loss: 31.3345 - val_mae: 4.3274\n",
      "Epoch 136/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 336.6650 - mae: 12.2686 - val_loss: 51.1336 - val_mae: 6.0828\n",
      "Epoch 137/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 325.3985 - mae: 12.1082 - val_loss: 41.8625 - val_mae: 5.1888\n",
      "Epoch 138/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 345.0111 - mae: 12.2907 - val_loss: 37.1281 - val_mae: 4.8267\n",
      "Epoch 139/350\n",
      "122/122 [==============================] - 0s 500us/step - loss: 318.1645 - mae: 12.0366 - val_loss: 32.2000 - val_mae: 4.4412\n",
      "Epoch 140/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 306.6457 - mae: 11.6562 - val_loss: 30.7084 - val_mae: 4.2100\n",
      "Epoch 141/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 360.6785 - mae: 12.4522 - val_loss: 30.2938 - val_mae: 4.2116\n",
      "Epoch 142/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 334.1566 - mae: 12.2246 - val_loss: 36.6253 - val_mae: 4.7006\n",
      "Epoch 143/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 313.3068 - mae: 11.6954 - val_loss: 40.1236 - val_mae: 5.1515\n",
      "Epoch 144/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 309.0140 - mae: 11.6593 - val_loss: 32.8509 - val_mae: 4.4842\n",
      "Epoch 145/350\n",
      "122/122 [==============================] - 0s 521us/step - loss: 325.1833 - mae: 11.8820 - val_loss: 49.4895 - val_mae: 5.6084\n",
      "Epoch 146/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 299.2587 - mae: 11.5625 - val_loss: 47.7925 - val_mae: 5.5211\n",
      "Epoch 147/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 297.8032 - mae: 11.4736 - val_loss: 49.0257 - val_mae: 5.4653\n",
      "Epoch 148/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 305.8041 - mae: 11.7202 - val_loss: 35.3464 - val_mae: 4.5928\n",
      "Epoch 149/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 328.9786 - mae: 12.0365 - val_loss: 35.5458 - val_mae: 4.5972\n",
      "Epoch 150/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 298.2278 - mae: 11.5602 - val_loss: 32.0831 - val_mae: 4.4688\n",
      "Epoch 151/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 289.5727 - mae: 11.3053 - val_loss: 39.9109 - val_mae: 5.0578\n",
      "Epoch 152/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 325.6873 - mae: 11.8011 - val_loss: 47.5865 - val_mae: 5.6995\n",
      "Epoch 153/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 324.6299 - mae: 11.9175 - val_loss: 48.8365 - val_mae: 5.5518\n",
      "Epoch 154/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 304.1751 - mae: 11.4571 - val_loss: 43.2176 - val_mae: 5.2728\n",
      "Epoch 155/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 279.5698 - mae: 11.1289 - val_loss: 37.7702 - val_mae: 4.7385\n",
      "Epoch 156/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 292.3955 - mae: 11.2324 - val_loss: 63.4343 - val_mae: 6.3087\n",
      "Epoch 157/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 305.9406 - mae: 11.3817 - val_loss: 35.1394 - val_mae: 4.7591\n",
      "Epoch 158/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 280.1394 - mae: 11.1181 - val_loss: 34.7848 - val_mae: 4.5910\n",
      "Epoch 159/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 291.7883 - mae: 11.3158 - val_loss: 34.4969 - val_mae: 4.7495\n",
      "Epoch 160/350\n",
      "122/122 [==============================] - 0s 536us/step - loss: 290.7656 - mae: 11.1387 - val_loss: 35.0813 - val_mae: 4.6530\n",
      "Epoch 161/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 270.6586 - mae: 10.8934 - val_loss: 46.9849 - val_mae: 5.7278\n",
      "Epoch 162/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 287.3372 - mae: 11.0924 - val_loss: 31.7809 - val_mae: 4.3077\n",
      "Epoch 163/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 281.4194 - mae: 10.9929 - val_loss: 36.1411 - val_mae: 4.7995\n",
      "Epoch 164/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 279.2071 - mae: 10.9947 - val_loss: 35.9639 - val_mae: 4.5859\n",
      "Epoch 165/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 273.0685 - mae: 10.8651 - val_loss: 41.7360 - val_mae: 5.2602\n",
      "Epoch 166/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 288.5174 - mae: 11.0192 - val_loss: 43.1387 - val_mae: 5.2018\n",
      "Epoch 167/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 269.9251 - mae: 10.7921 - val_loss: 38.1751 - val_mae: 4.9326\n",
      "Epoch 168/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 270.9690 - mae: 10.8909 - val_loss: 48.4549 - val_mae: 5.7893\n",
      "Epoch 169/350\n",
      "122/122 [==============================] - 0s 539us/step - loss: 262.7761 - mae: 10.7551 - val_loss: 34.7841 - val_mae: 4.7375\n",
      "Epoch 170/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 283.5939 - mae: 11.0063 - val_loss: 39.8533 - val_mae: 5.1132\n",
      "Epoch 171/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 282.9569 - mae: 10.9096 - val_loss: 41.2415 - val_mae: 5.2050\n",
      "Epoch 172/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 265.4437 - mae: 10.7132 - val_loss: 48.8128 - val_mae: 5.5515\n",
      "Epoch 173/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 266.6658 - mae: 10.6481 - val_loss: 30.8488 - val_mae: 4.3716\n",
      "Epoch 174/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 280.1852 - mae: 10.7518 - val_loss: 36.3861 - val_mae: 4.8104\n",
      "Epoch 175/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 270.4906 - mae: 10.7039 - val_loss: 40.1875 - val_mae: 5.0854\n",
      "Epoch 176/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 288.9171 - mae: 10.8692 - val_loss: 34.9217 - val_mae: 4.5963\n",
      "Epoch 177/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 252.3343 - mae: 10.4618 - val_loss: 30.9858 - val_mae: 4.2725\n",
      "Epoch 178/350\n",
      "122/122 [==============================] - 0s 570us/step - loss: 252.3706 - mae: 10.4374 - val_loss: 35.4112 - val_mae: 4.5279\n",
      "Epoch 179/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 256.0303 - mae: 10.4629 - val_loss: 46.7810 - val_mae: 5.4857\n",
      "Epoch 180/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 239.8931 - mae: 10.2502 - val_loss: 35.9309 - val_mae: 4.7864\n",
      "Epoch 181/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 256.6016 - mae: 10.4593 - val_loss: 48.6453 - val_mae: 5.7215\n",
      "Epoch 182/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 246.7217 - mae: 10.2804 - val_loss: 33.9484 - val_mae: 4.4216\n",
      "Epoch 183/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 244.2770 - mae: 10.2530 - val_loss: 51.8912 - val_mae: 5.8531\n",
      "Epoch 184/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 269.9648 - mae: 10.5825 - val_loss: 49.1233 - val_mae: 5.8775\n",
      "Epoch 185/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 244.1516 - mae: 10.0732 - val_loss: 38.2751 - val_mae: 4.8160\n",
      "Epoch 186/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 236.9103 - mae: 10.0520 - val_loss: 31.0865 - val_mae: 4.1351\n",
      "Epoch 187/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 252.0421 - mae: 10.4475 - val_loss: 43.9420 - val_mae: 5.3405\n",
      "Epoch 188/350\n",
      "122/122 [==============================] - 0s 500us/step - loss: 242.8231 - mae: 10.0794 - val_loss: 37.4772 - val_mae: 4.7674\n",
      "Epoch 189/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 242.4873 - mae: 10.0540 - val_loss: 37.2118 - val_mae: 4.8974\n",
      "Epoch 190/350\n",
      "122/122 [==============================] - 0s 539us/step - loss: 253.3152 - mae: 10.2389 - val_loss: 50.9164 - val_mae: 5.8393\n",
      "Epoch 191/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 246.8250 - mae: 10.1316 - val_loss: 39.7658 - val_mae: 4.9798\n",
      "Epoch 192/350\n",
      "122/122 [==============================] - 0s 519us/step - loss: 259.6478 - mae: 10.3243 - val_loss: 41.9235 - val_mae: 5.0679\n",
      "Epoch 193/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 225.0777 - mae: 9.7209 - val_loss: 35.7354 - val_mae: 4.5803\n",
      "Epoch 194/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 237.7537 - mae: 9.9654 - val_loss: 39.7013 - val_mae: 5.0203\n",
      "Epoch 195/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 225.9308 - mae: 9.7522 - val_loss: 34.4188 - val_mae: 4.5952\n",
      "Epoch 196/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 236.8281 - mae: 9.9991 - val_loss: 31.6178 - val_mae: 4.3645\n",
      "Epoch 197/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 228.3157 - mae: 9.7167 - val_loss: 39.1027 - val_mae: 5.0208\n",
      "Epoch 198/350\n",
      "122/122 [==============================] - 0s 543us/step - loss: 234.6720 - mae: 9.7420 - val_loss: 45.1374 - val_mae: 5.4681\n",
      "Epoch 199/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 227.8241 - mae: 9.8256 - val_loss: 47.0038 - val_mae: 5.5838\n",
      "Epoch 200/350\n",
      "122/122 [==============================] - 0s 498us/step - loss: 239.2359 - mae: 9.9269 - val_loss: 45.6968 - val_mae: 5.4404\n",
      "Epoch 201/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 223.9752 - mae: 9.7594 - val_loss: 47.8862 - val_mae: 5.5314\n",
      "Epoch 202/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 228.2994 - mae: 9.6451 - val_loss: 35.5776 - val_mae: 4.6191\n",
      "Epoch 203/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 203.4532 - mae: 9.3224 - val_loss: 36.7837 - val_mae: 4.6826\n",
      "Epoch 204/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 217.4274 - mae: 9.4254 - val_loss: 44.4764 - val_mae: 5.3523\n",
      "Epoch 205/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 220.4095 - mae: 9.4008 - val_loss: 47.1869 - val_mae: 5.5721\n",
      "Epoch 206/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 224.2175 - mae: 9.5476 - val_loss: 37.9198 - val_mae: 4.8504\n",
      "Epoch 207/350\n",
      "122/122 [==============================] - 0s 542us/step - loss: 220.4454 - mae: 9.6307 - val_loss: 34.5912 - val_mae: 4.6041\n",
      "Epoch 208/350\n",
      "122/122 [==============================] - 0s 527us/step - loss: 203.4653 - mae: 9.2275 - val_loss: 43.1007 - val_mae: 5.1608\n",
      "Epoch 209/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 219.5650 - mae: 9.5641 - val_loss: 47.2393 - val_mae: 5.5130\n",
      "Epoch 210/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 207.6573 - mae: 9.2692 - val_loss: 44.8414 - val_mae: 5.5031\n",
      "Epoch 211/350\n",
      "122/122 [==============================] - 0s 524us/step - loss: 212.1647 - mae: 9.4025 - val_loss: 54.1202 - val_mae: 5.9993\n",
      "Epoch 212/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 224.8414 - mae: 9.5503 - val_loss: 54.3394 - val_mae: 6.0714\n",
      "Epoch 213/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 202.3817 - mae: 9.1691 - val_loss: 45.7247 - val_mae: 5.4290\n",
      "Epoch 214/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 200.5345 - mae: 9.1328 - val_loss: 46.3455 - val_mae: 5.3719\n",
      "Epoch 215/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 215.0856 - mae: 9.4014 - val_loss: 52.0249 - val_mae: 5.8184\n",
      "Epoch 216/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 194.1622 - mae: 8.9972 - val_loss: 44.1733 - val_mae: 5.4720\n",
      "Epoch 217/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 201.1120 - mae: 9.2816 - val_loss: 64.6305 - val_mae: 6.3534\n",
      "Epoch 218/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 217.3494 - mae: 9.3927 - val_loss: 42.0978 - val_mae: 5.1437\n",
      "Epoch 219/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 205.6885 - mae: 9.0647 - val_loss: 60.8062 - val_mae: 6.3902\n",
      "Epoch 220/350\n",
      "122/122 [==============================] - 0s 506us/step - loss: 206.1217 - mae: 9.3630 - val_loss: 58.3443 - val_mae: 6.1975\n",
      "Epoch 221/350\n",
      "122/122 [==============================] - 0s 545us/step - loss: 208.9941 - mae: 9.2168 - val_loss: 50.9661 - val_mae: 5.6971\n",
      "Epoch 222/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 196.9274 - mae: 9.0026 - val_loss: 55.1288 - val_mae: 6.2468\n",
      "Epoch 223/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 208.7810 - mae: 9.4069 - val_loss: 45.1016 - val_mae: 5.3424\n",
      "Epoch 224/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 207.2500 - mae: 9.2071 - val_loss: 40.8885 - val_mae: 5.1271\n",
      "Epoch 225/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 207.7853 - mae: 9.1425 - val_loss: 40.6325 - val_mae: 5.1736\n",
      "Epoch 226/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 201.3308 - mae: 8.9730 - val_loss: 33.4834 - val_mae: 4.4828\n",
      "Epoch 227/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 208.3344 - mae: 9.2782 - val_loss: 47.0102 - val_mae: 5.5451\n",
      "Epoch 228/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 189.0366 - mae: 8.9783 - val_loss: 41.4706 - val_mae: 5.1516\n",
      "Epoch 229/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 193.4848 - mae: 8.8575 - val_loss: 52.6074 - val_mae: 5.8942\n",
      "Epoch 230/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 188.5019 - mae: 8.8764 - val_loss: 40.3264 - val_mae: 5.0884\n",
      "Epoch 231/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 197.3833 - mae: 8.9274 - val_loss: 39.3473 - val_mae: 5.0470\n",
      "Epoch 232/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 184.1175 - mae: 8.6781 - val_loss: 55.9697 - val_mae: 6.2275\n",
      "Epoch 233/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 178.4261 - mae: 8.6884 - val_loss: 40.7776 - val_mae: 5.0001\n",
      "Epoch 234/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 188.9979 - mae: 8.9126 - val_loss: 50.5440 - val_mae: 5.7149\n",
      "Epoch 235/350\n",
      "122/122 [==============================] - 0s 538us/step - loss: 177.1284 - mae: 8.6050 - val_loss: 36.3752 - val_mae: 4.7626\n",
      "Epoch 236/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 190.2642 - mae: 8.7766 - val_loss: 47.5718 - val_mae: 5.4760\n",
      "Epoch 237/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 183.6307 - mae: 8.6963 - val_loss: 38.6996 - val_mae: 4.9441\n",
      "Epoch 238/350\n",
      "122/122 [==============================] - 0s 501us/step - loss: 181.6356 - mae: 8.7695 - val_loss: 38.9835 - val_mae: 4.8198\n",
      "Epoch 239/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 193.2475 - mae: 8.9554 - val_loss: 66.0902 - val_mae: 6.6148\n",
      "Epoch 240/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 180.2098 - mae: 8.6402 - val_loss: 45.0153 - val_mae: 5.2095\n",
      "Epoch 241/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 172.3702 - mae: 8.5395 - val_loss: 42.9198 - val_mae: 5.0177\n",
      "Epoch 242/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 179.2145 - mae: 8.7363 - val_loss: 39.8149 - val_mae: 5.0309\n",
      "Epoch 243/350\n",
      "122/122 [==============================] - 0s 567us/step - loss: 181.8011 - mae: 8.6678 - val_loss: 54.8243 - val_mae: 5.9493\n",
      "Epoch 244/350\n",
      "122/122 [==============================] - 0s 539us/step - loss: 173.3347 - mae: 8.5220 - val_loss: 41.9883 - val_mae: 5.2969\n",
      "Epoch 245/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 168.6362 - mae: 8.4001 - val_loss: 43.5913 - val_mae: 5.2365\n",
      "Epoch 246/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 169.2392 - mae: 8.5931 - val_loss: 67.5436 - val_mae: 6.7072\n",
      "Epoch 247/350\n",
      "122/122 [==============================] - 0s 519us/step - loss: 174.3522 - mae: 8.5852 - val_loss: 56.7471 - val_mae: 6.1355\n",
      "Epoch 248/350\n",
      "122/122 [==============================] - 0s 502us/step - loss: 175.2148 - mae: 8.5714 - val_loss: 37.9521 - val_mae: 4.9099\n",
      "Epoch 249/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 182.1251 - mae: 8.6478 - val_loss: 38.5929 - val_mae: 4.9165\n",
      "Epoch 250/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 174.9060 - mae: 8.5708 - val_loss: 57.1905 - val_mae: 5.9475\n",
      "Epoch 251/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 179.8874 - mae: 8.5892 - val_loss: 46.8800 - val_mae: 5.6827\n",
      "Epoch 252/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 175.2997 - mae: 8.5723 - val_loss: 64.8637 - val_mae: 6.4343\n",
      "Epoch 253/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 183.3769 - mae: 8.6729 - val_loss: 44.0087 - val_mae: 5.3558\n",
      "Epoch 254/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 161.4728 - mae: 8.3209 - val_loss: 39.3159 - val_mae: 4.8888\n",
      "Epoch 255/350\n",
      "122/122 [==============================] - 0s 527us/step - loss: 170.2983 - mae: 8.4375 - val_loss: 33.1570 - val_mae: 4.4587\n",
      "Epoch 256/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 160.6853 - mae: 8.3729 - val_loss: 40.2279 - val_mae: 5.0764\n",
      "Epoch 257/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 183.7572 - mae: 8.6330 - val_loss: 43.2234 - val_mae: 5.2011\n",
      "Epoch 258/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 175.4622 - mae: 8.4975 - val_loss: 52.3310 - val_mae: 5.9564\n",
      "Epoch 259/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 168.8153 - mae: 8.2831 - val_loss: 54.2536 - val_mae: 6.0448\n",
      "Epoch 260/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 170.2981 - mae: 8.5164 - val_loss: 43.9910 - val_mae: 5.3433\n",
      "Epoch 261/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 166.3869 - mae: 8.2969 - val_loss: 46.5504 - val_mae: 5.6007\n",
      "Epoch 262/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 169.3460 - mae: 8.4898 - val_loss: 37.6690 - val_mae: 4.8820\n",
      "Epoch 263/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 164.2086 - mae: 8.3635 - val_loss: 52.2376 - val_mae: 5.8617\n",
      "Epoch 264/350\n",
      "122/122 [==============================] - 0s 517us/step - loss: 154.8482 - mae: 8.1863 - val_loss: 42.7392 - val_mae: 5.2508\n",
      "Epoch 265/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 159.0890 - mae: 8.2547 - val_loss: 38.7973 - val_mae: 4.9731\n",
      "Epoch 266/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 163.4763 - mae: 8.3381 - val_loss: 52.6941 - val_mae: 5.8719\n",
      "Epoch 267/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 169.0857 - mae: 8.3262 - val_loss: 50.5928 - val_mae: 5.7729\n",
      "Epoch 268/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 151.1583 - mae: 8.0840 - val_loss: 37.5440 - val_mae: 4.6680\n",
      "Epoch 269/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 149.5861 - mae: 8.1814 - val_loss: 42.8574 - val_mae: 5.2059\n",
      "Epoch 270/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 158.7784 - mae: 8.1760 - val_loss: 43.2616 - val_mae: 5.1532\n",
      "Epoch 271/350\n",
      "122/122 [==============================] - 0s 539us/step - loss: 154.2848 - mae: 8.1666 - val_loss: 40.0443 - val_mae: 5.0784\n",
      "Epoch 272/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 160.7223 - mae: 8.1612 - val_loss: 41.3271 - val_mae: 5.0733\n",
      "Epoch 273/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 154.2342 - mae: 8.0806 - val_loss: 54.5627 - val_mae: 5.9874\n",
      "Epoch 274/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 163.3219 - mae: 8.2654 - val_loss: 44.6005 - val_mae: 5.4442\n",
      "Epoch 275/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 147.1266 - mae: 7.9216 - val_loss: 35.5001 - val_mae: 4.6010\n",
      "Epoch 276/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 148.3072 - mae: 8.0431 - val_loss: 46.3912 - val_mae: 5.4527\n",
      "Epoch 277/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 151.8981 - mae: 8.0247 - val_loss: 53.0927 - val_mae: 5.8749\n",
      "Epoch 278/350\n",
      "122/122 [==============================] - 0s 541us/step - loss: 148.3794 - mae: 8.0965 - val_loss: 55.9747 - val_mae: 6.1139\n",
      "Epoch 279/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 155.9072 - mae: 8.1117 - val_loss: 39.9143 - val_mae: 4.8318\n",
      "Epoch 280/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 144.3187 - mae: 8.0462 - val_loss: 42.1964 - val_mae: 5.0258\n",
      "Epoch 281/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 155.7643 - mae: 8.1177 - val_loss: 60.8079 - val_mae: 6.3272\n",
      "Epoch 282/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 139.8901 - mae: 7.8363 - val_loss: 43.5841 - val_mae: 5.2381\n",
      "Epoch 283/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 155.5767 - mae: 8.0653 - val_loss: 61.1271 - val_mae: 6.3417\n",
      "Epoch 284/350\n",
      "122/122 [==============================] - 0s 528us/step - loss: 145.2968 - mae: 7.9957 - val_loss: 44.9460 - val_mae: 5.4520\n",
      "Epoch 285/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 152.0760 - mae: 8.0257 - val_loss: 49.0677 - val_mae: 5.6036\n",
      "Epoch 286/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 138.6688 - mae: 7.9032 - val_loss: 46.4362 - val_mae: 5.5116\n",
      "Epoch 287/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 158.9261 - mae: 8.2758 - val_loss: 68.4526 - val_mae: 6.5462\n",
      "Epoch 288/350\n",
      "122/122 [==============================] - 0s 522us/step - loss: 141.1973 - mae: 7.8464 - val_loss: 54.2655 - val_mae: 5.8056\n",
      "Epoch 289/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 150.1392 - mae: 8.0653 - val_loss: 58.0902 - val_mae: 6.2817\n",
      "Epoch 290/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 139.5889 - mae: 7.9356 - val_loss: 49.2222 - val_mae: 5.6570\n",
      "Epoch 291/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 138.3966 - mae: 7.8100 - val_loss: 50.3990 - val_mae: 5.6029\n",
      "Epoch 292/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 138.3601 - mae: 7.8844 - val_loss: 41.1202 - val_mae: 5.1754\n",
      "Epoch 293/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 142.4276 - mae: 7.8787 - val_loss: 36.4636 - val_mae: 4.7304\n",
      "Epoch 294/350\n",
      "122/122 [==============================] - 0s 567us/step - loss: 139.0022 - mae: 7.7830 - val_loss: 39.6815 - val_mae: 4.9775\n",
      "Epoch 295/350\n",
      "122/122 [==============================] - 0s 542us/step - loss: 150.0262 - mae: 8.0310 - val_loss: 48.8107 - val_mae: 5.6141\n",
      "Epoch 296/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 142.7228 - mae: 7.8627 - val_loss: 42.0326 - val_mae: 5.0733\n",
      "Epoch 297/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 147.9174 - mae: 8.0538 - val_loss: 46.5182 - val_mae: 5.3074\n",
      "Epoch 298/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 146.6794 - mae: 7.9409 - val_loss: 44.7596 - val_mae: 5.3228\n",
      "Epoch 299/350\n",
      "122/122 [==============================] - 0s 501us/step - loss: 138.7523 - mae: 7.7876 - val_loss: 46.3332 - val_mae: 5.5020\n",
      "Epoch 300/350\n",
      "122/122 [==============================] - 0s 514us/step - loss: 143.1946 - mae: 8.0166 - val_loss: 43.8314 - val_mae: 5.1856\n",
      "Epoch 301/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 148.1284 - mae: 8.0038 - val_loss: 37.7107 - val_mae: 4.7025\n",
      "Epoch 302/350\n",
      "122/122 [==============================] - 0s 522us/step - loss: 140.5388 - mae: 7.7820 - val_loss: 51.7176 - val_mae: 5.7407\n",
      "Epoch 303/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 136.0630 - mae: 7.7857 - val_loss: 47.3027 - val_mae: 5.5059\n",
      "Epoch 304/350\n",
      "122/122 [==============================] - 0s 584us/step - loss: 138.8538 - mae: 7.8357 - val_loss: 43.5740 - val_mae: 5.3328\n",
      "Epoch 305/350\n",
      "122/122 [==============================] - 0s 524us/step - loss: 130.7155 - mae: 7.6380 - val_loss: 49.6151 - val_mae: 5.7016\n",
      "Epoch 306/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 135.2868 - mae: 7.7480 - val_loss: 35.3484 - val_mae: 4.5009\n",
      "Epoch 307/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 136.0583 - mae: 7.7018 - val_loss: 43.4579 - val_mae: 5.2485\n",
      "Epoch 308/350\n",
      "122/122 [==============================] - 0s 602us/step - loss: 132.4178 - mae: 7.6280 - val_loss: 52.6013 - val_mae: 5.9459\n",
      "Epoch 309/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 137.1784 - mae: 7.8621 - val_loss: 64.3031 - val_mae: 6.4486\n",
      "Epoch 310/350\n",
      "122/122 [==============================] - 0s 530us/step - loss: 138.0598 - mae: 7.7480 - val_loss: 47.0638 - val_mae: 5.4894\n",
      "Epoch 311/350\n",
      "122/122 [==============================] - 0s 504us/step - loss: 140.8093 - mae: 7.7993 - val_loss: 45.6034 - val_mae: 5.4899\n",
      "Epoch 312/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 137.8903 - mae: 7.7722 - val_loss: 55.9238 - val_mae: 6.2128\n",
      "Epoch 313/350\n",
      "122/122 [==============================] - 0s 499us/step - loss: 129.5258 - mae: 7.5940 - val_loss: 41.2924 - val_mae: 5.1418\n",
      "Epoch 314/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 126.1123 - mae: 7.5911 - val_loss: 46.4857 - val_mae: 5.4352\n",
      "Epoch 315/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 128.8740 - mae: 7.6448 - val_loss: 44.1837 - val_mae: 5.1732\n",
      "Epoch 316/350\n",
      "122/122 [==============================] - 0s 525us/step - loss: 124.8163 - mae: 7.4324 - val_loss: 57.3667 - val_mae: 6.1228\n",
      "Epoch 317/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 127.7663 - mae: 7.5432 - val_loss: 51.6426 - val_mae: 5.6933\n",
      "Epoch 318/350\n",
      "122/122 [==============================] - 0s 508us/step - loss: 127.5012 - mae: 7.4326 - val_loss: 49.8399 - val_mae: 5.7025\n",
      "Epoch 319/350\n",
      "122/122 [==============================] - 0s 539us/step - loss: 127.5200 - mae: 7.5394 - val_loss: 73.7828 - val_mae: 7.2336\n",
      "Epoch 320/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 124.3439 - mae: 7.5346 - val_loss: 48.8878 - val_mae: 5.6693\n",
      "Epoch 321/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 122.4481 - mae: 7.5303 - val_loss: 54.0861 - val_mae: 5.7596\n",
      "Epoch 322/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 120.7545 - mae: 7.5129 - val_loss: 45.7307 - val_mae: 5.3999\n",
      "Epoch 323/350\n",
      "122/122 [==============================] - 0s 542us/step - loss: 119.7582 - mae: 7.3695 - val_loss: 42.8109 - val_mae: 5.2043\n",
      "Epoch 324/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 129.2589 - mae: 7.5087 - val_loss: 45.7949 - val_mae: 5.5205\n",
      "Epoch 325/350\n",
      "122/122 [==============================] - 0s 503us/step - loss: 120.9191 - mae: 7.5034 - val_loss: 55.5544 - val_mae: 6.0932\n",
      "Epoch 326/350\n",
      "122/122 [==============================] - 0s 543us/step - loss: 121.0998 - mae: 7.3692 - val_loss: 48.4922 - val_mae: 5.7732\n",
      "Epoch 327/350\n",
      "122/122 [==============================] - 0s 513us/step - loss: 120.8554 - mae: 7.4518 - val_loss: 46.6085 - val_mae: 5.4660\n",
      "Epoch 328/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 131.9658 - mae: 7.6082 - val_loss: 52.1389 - val_mae: 5.7426\n",
      "Epoch 329/350\n",
      "122/122 [==============================] - 0s 505us/step - loss: 122.0674 - mae: 7.3644 - val_loss: 37.7443 - val_mae: 4.8582\n",
      "Epoch 330/350\n",
      "122/122 [==============================] - 0s 523us/step - loss: 127.7869 - mae: 7.6844 - val_loss: 45.9199 - val_mae: 5.4487\n",
      "Epoch 331/350\n",
      "122/122 [==============================] - 0s 540us/step - loss: 113.7507 - mae: 7.3149 - val_loss: 41.8559 - val_mae: 5.0705\n",
      "Epoch 332/350\n",
      "122/122 [==============================] - 0s 528us/step - loss: 122.1736 - mae: 7.4753 - val_loss: 41.9501 - val_mae: 5.1298\n",
      "Epoch 333/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 112.7212 - mae: 7.2695 - val_loss: 44.3533 - val_mae: 5.1235\n",
      "Epoch 334/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 125.9089 - mae: 7.5291 - val_loss: 40.1876 - val_mae: 5.0480\n",
      "Epoch 335/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 123.6930 - mae: 7.4953 - val_loss: 56.5547 - val_mae: 6.0460\n",
      "Epoch 336/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 116.6243 - mae: 7.3557 - val_loss: 40.9393 - val_mae: 4.9510\n",
      "Epoch 337/350\n",
      "122/122 [==============================] - 0s 509us/step - loss: 110.7051 - mae: 7.2574 - val_loss: 37.2701 - val_mae: 4.8542\n",
      "Epoch 338/350\n",
      "122/122 [==============================] - 0s 511us/step - loss: 122.7361 - mae: 7.5033 - val_loss: 58.1433 - val_mae: 6.2373\n",
      "Epoch 339/350\n",
      "122/122 [==============================] - 0s 510us/step - loss: 118.2478 - mae: 7.4553 - val_loss: 42.8665 - val_mae: 5.1545\n",
      "Epoch 340/350\n",
      "122/122 [==============================] - 0s 518us/step - loss: 122.2342 - mae: 7.3297 - val_loss: 43.8452 - val_mae: 5.4754\n",
      "Epoch 341/350\n",
      "122/122 [==============================] - 0s 660us/step - loss: 107.2785 - mae: 7.1405 - val_loss: 51.6405 - val_mae: 5.7955\n",
      "Epoch 342/350\n",
      "122/122 [==============================] - 0s 521us/step - loss: 123.1886 - mae: 7.4749 - val_loss: 36.9631 - val_mae: 4.7062\n",
      "Epoch 343/350\n",
      "122/122 [==============================] - 0s 516us/step - loss: 121.1324 - mae: 7.3269 - val_loss: 42.6340 - val_mae: 5.0169\n",
      "Epoch 344/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 121.9205 - mae: 7.4905 - val_loss: 37.8338 - val_mae: 4.7284\n",
      "Epoch 345/350\n",
      "122/122 [==============================] - 0s 515us/step - loss: 113.2974 - mae: 7.3135 - val_loss: 43.2183 - val_mae: 5.2382\n",
      "Epoch 346/350\n",
      "122/122 [==============================] - 0s 507us/step - loss: 113.8725 - mae: 7.2606 - val_loss: 46.5968 - val_mae: 5.4086\n",
      "Epoch 347/350\n",
      "122/122 [==============================] - 0s 520us/step - loss: 122.2941 - mae: 7.4516 - val_loss: 45.0807 - val_mae: 5.2941\n",
      "Epoch 348/350\n",
      "122/122 [==============================] - 0s 524us/step - loss: 114.7642 - mae: 7.2804 - val_loss: 56.2952 - val_mae: 6.1123\n",
      "Epoch 349/350\n",
      "122/122 [==============================] - 0s 520us/step - loss: 113.2141 - mae: 7.2907 - val_loss: 46.9344 - val_mae: 5.4307\n",
      "Epoch 350/350\n",
      "122/122 [==============================] - 0s 512us/step - loss: 114.6275 - mae: 7.3072 - val_loss: 48.7607 - val_mae: 5.4784\n",
      "31/31 [==============================] - 0s 239us/step\n",
      "Epochs: 350 | MAE: 5.478413896140074\n",
      "Training model with 400 epochs\n",
      "Epoch 1/400\n",
      "122/122 [==============================] - 0s 817us/step - loss: 67055.8203 - mae: 224.4613 - val_loss: 56836.4141 - val_mae: 203.7169\n",
      "Epoch 2/400\n",
      "122/122 [==============================] - 0s 529us/step - loss: 33123.6914 - mae: 150.8911 - val_loss: 9676.7676 - val_mae: 81.9493\n",
      "Epoch 3/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 5444.6929 - mae: 57.4190 - val_loss: 2942.7397 - val_mae: 40.2117\n",
      "Epoch 4/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 3292.0920 - mae: 43.2471 - val_loss: 1804.8085 - val_mae: 31.4580\n",
      "Epoch 5/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 2361.1228 - mae: 37.0715 - val_loss: 1348.1045 - val_mae: 27.0124\n",
      "Epoch 6/400\n",
      "122/122 [==============================] - 0s 506us/step - loss: 2051.2917 - mae: 35.1096 - val_loss: 1124.2107 - val_mae: 25.0714\n",
      "Epoch 7/400\n",
      "122/122 [==============================] - 0s 542us/step - loss: 1871.0830 - mae: 33.7032 - val_loss: 958.8254 - val_mae: 23.4059\n",
      "Epoch 8/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 1714.8729 - mae: 32.0943 - val_loss: 881.1635 - val_mae: 22.6002\n",
      "Epoch 9/400\n",
      "122/122 [==============================] - 0s 529us/step - loss: 1702.5400 - mae: 31.9572 - val_loss: 779.9787 - val_mae: 21.2272\n",
      "Epoch 10/400\n",
      "122/122 [==============================] - 0s 518us/step - loss: 1568.9486 - mae: 30.8544 - val_loss: 680.7833 - val_mae: 19.8668\n",
      "Epoch 11/400\n",
      "122/122 [==============================] - 0s 507us/step - loss: 1460.1338 - mae: 29.8378 - val_loss: 626.2957 - val_mae: 19.0580\n",
      "Epoch 12/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 1401.0587 - mae: 28.8008 - val_loss: 575.2272 - val_mae: 18.2735\n",
      "Epoch 13/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 1314.0963 - mae: 28.1057 - val_loss: 497.1882 - val_mae: 16.9503\n",
      "Epoch 14/400\n",
      "122/122 [==============================] - 0s 511us/step - loss: 1179.6656 - mae: 26.8082 - val_loss: 449.9846 - val_mae: 16.1276\n",
      "Epoch 15/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 1095.3654 - mae: 25.8536 - val_loss: 422.3149 - val_mae: 15.6702\n",
      "Epoch 16/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 1117.7329 - mae: 25.9238 - val_loss: 365.5605 - val_mae: 14.6652\n",
      "Epoch 17/400\n",
      "122/122 [==============================] - 0s 512us/step - loss: 1001.1465 - mae: 24.7161 - val_loss: 319.6544 - val_mae: 13.6323\n",
      "Epoch 18/400\n",
      "122/122 [==============================] - 0s 544us/step - loss: 969.3190 - mae: 23.9962 - val_loss: 306.6101 - val_mae: 13.4971\n",
      "Epoch 19/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 896.5529 - mae: 23.1109 - val_loss: 252.8030 - val_mae: 12.0937\n",
      "Epoch 20/400\n",
      "122/122 [==============================] - 0s 508us/step - loss: 898.8328 - mae: 22.8607 - val_loss: 224.8488 - val_mae: 11.3623\n",
      "Epoch 21/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 818.9519 - mae: 22.0299 - val_loss: 210.3177 - val_mae: 11.0710\n",
      "Epoch 22/400\n",
      "122/122 [==============================] - 0s 542us/step - loss: 844.3527 - mae: 21.8995 - val_loss: 192.8735 - val_mae: 10.6123\n",
      "Epoch 23/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 779.6812 - mae: 21.4098 - val_loss: 151.0834 - val_mae: 9.2681\n",
      "Epoch 24/400\n",
      "122/122 [==============================] - 0s 507us/step - loss: 733.2685 - mae: 20.6736 - val_loss: 140.3276 - val_mae: 8.9886\n",
      "Epoch 25/400\n",
      "122/122 [==============================] - 0s 526us/step - loss: 720.3934 - mae: 20.3903 - val_loss: 115.1042 - val_mae: 8.0641\n",
      "Epoch 26/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 725.5366 - mae: 20.2699 - val_loss: 111.0799 - val_mae: 8.0313\n",
      "Epoch 27/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 701.7153 - mae: 19.8094 - val_loss: 102.1671 - val_mae: 7.8403\n",
      "Epoch 28/400\n",
      "122/122 [==============================] - 0s 512us/step - loss: 666.8802 - mae: 19.4285 - val_loss: 109.3825 - val_mae: 8.3689\n",
      "Epoch 29/400\n",
      "122/122 [==============================] - 0s 538us/step - loss: 643.2563 - mae: 19.0198 - val_loss: 84.7330 - val_mae: 7.0535\n",
      "Epoch 30/400\n",
      "122/122 [==============================] - 0s 511us/step - loss: 625.8553 - mae: 18.6893 - val_loss: 65.0577 - val_mae: 6.1055\n",
      "Epoch 31/400\n",
      "122/122 [==============================] - 0s 505us/step - loss: 652.0009 - mae: 18.9500 - val_loss: 65.5592 - val_mae: 6.0515\n",
      "Epoch 32/400\n",
      "122/122 [==============================] - 0s 533us/step - loss: 602.6035 - mae: 18.3026 - val_loss: 61.2755 - val_mae: 6.0305\n",
      "Epoch 33/400\n",
      "122/122 [==============================] - 0s 524us/step - loss: 620.5422 - mae: 18.3080 - val_loss: 48.3326 - val_mae: 5.2560\n",
      "Epoch 34/400\n",
      "122/122 [==============================] - 0s 530us/step - loss: 595.7210 - mae: 17.8660 - val_loss: 51.6022 - val_mae: 5.6599\n",
      "Epoch 35/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 592.2353 - mae: 17.5378 - val_loss: 49.5252 - val_mae: 5.4510\n",
      "Epoch 36/400\n",
      "122/122 [==============================] - 0s 667us/step - loss: 588.8107 - mae: 17.6087 - val_loss: 40.4313 - val_mae: 4.9074\n",
      "Epoch 37/400\n",
      "122/122 [==============================] - 0s 592us/step - loss: 563.7796 - mae: 17.3998 - val_loss: 60.6067 - val_mae: 6.3121\n",
      "Epoch 38/400\n",
      "122/122 [==============================] - 0s 600us/step - loss: 585.8353 - mae: 17.4644 - val_loss: 44.3772 - val_mae: 5.2841\n",
      "Epoch 39/400\n",
      "122/122 [==============================] - 0s 591us/step - loss: 551.2332 - mae: 16.9264 - val_loss: 35.1208 - val_mae: 4.4885\n",
      "Epoch 40/400\n",
      "122/122 [==============================] - 0s 591us/step - loss: 534.5881 - mae: 16.6961 - val_loss: 33.5657 - val_mae: 4.4428\n",
      "Epoch 41/400\n",
      "122/122 [==============================] - 0s 621us/step - loss: 542.8954 - mae: 16.8262 - val_loss: 39.0177 - val_mae: 4.8934\n",
      "Epoch 42/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 519.2163 - mae: 16.4455 - val_loss: 44.2488 - val_mae: 5.3291\n",
      "Epoch 43/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 512.0626 - mae: 16.3780 - val_loss: 41.4447 - val_mae: 5.1207\n",
      "Epoch 44/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 503.4538 - mae: 16.3545 - val_loss: 39.5215 - val_mae: 5.0556\n",
      "Epoch 45/400\n",
      "122/122 [==============================] - 0s 523us/step - loss: 533.6915 - mae: 16.6266 - val_loss: 41.6829 - val_mae: 5.1456\n",
      "Epoch 46/400\n",
      "122/122 [==============================] - 0s 516us/step - loss: 568.1064 - mae: 16.7171 - val_loss: 48.8605 - val_mae: 5.7817\n",
      "Epoch 47/400\n",
      "122/122 [==============================] - 0s 543us/step - loss: 521.7463 - mae: 16.2476 - val_loss: 40.4836 - val_mae: 5.0463\n",
      "Epoch 48/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 539.1902 - mae: 16.3140 - val_loss: 33.6642 - val_mae: 4.4549\n",
      "Epoch 49/400\n",
      "122/122 [==============================] - 0s 512us/step - loss: 511.6187 - mae: 16.1079 - val_loss: 28.3378 - val_mae: 4.0864\n",
      "Epoch 50/400\n",
      "122/122 [==============================] - 0s 510us/step - loss: 522.1436 - mae: 16.1460 - val_loss: 41.0649 - val_mae: 5.1670\n",
      "Epoch 51/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 500.2895 - mae: 15.9008 - val_loss: 34.0900 - val_mae: 4.3933\n",
      "Epoch 52/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 468.1590 - mae: 15.3392 - val_loss: 25.6299 - val_mae: 3.7984\n",
      "Epoch 53/400\n",
      "122/122 [==============================] - 0s 513us/step - loss: 469.6841 - mae: 15.4128 - val_loss: 32.0784 - val_mae: 4.3297\n",
      "Epoch 54/400\n",
      "122/122 [==============================] - 0s 543us/step - loss: 489.3415 - mae: 15.7022 - val_loss: 44.9741 - val_mae: 5.3243\n",
      "Epoch 55/400\n",
      "122/122 [==============================] - 0s 523us/step - loss: 496.2791 - mae: 15.6581 - val_loss: 33.9657 - val_mae: 4.5476\n",
      "Epoch 56/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 475.2133 - mae: 15.3438 - val_loss: 39.5458 - val_mae: 4.4827\n",
      "Epoch 57/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 467.3707 - mae: 15.3799 - val_loss: 29.7316 - val_mae: 4.2590\n",
      "Epoch 58/400\n",
      "122/122 [==============================] - 0s 606us/step - loss: 460.9965 - mae: 15.1716 - val_loss: 52.0973 - val_mae: 5.7303\n",
      "Epoch 59/400\n",
      "122/122 [==============================] - 0s 532us/step - loss: 474.2451 - mae: 15.1245 - val_loss: 29.9800 - val_mae: 4.2121\n",
      "Epoch 60/400\n",
      "122/122 [==============================] - 0s 594us/step - loss: 483.5708 - mae: 15.1401 - val_loss: 39.8671 - val_mae: 5.0217\n",
      "Epoch 61/400\n",
      "122/122 [==============================] - 0s 593us/step - loss: 460.6301 - mae: 14.9338 - val_loss: 32.0761 - val_mae: 4.3953\n",
      "Epoch 62/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 463.0986 - mae: 15.0773 - val_loss: 29.7077 - val_mae: 4.2269\n",
      "Epoch 63/400\n",
      "122/122 [==============================] - 0s 508us/step - loss: 460.2988 - mae: 14.8469 - val_loss: 28.0657 - val_mae: 4.0979\n",
      "Epoch 64/400\n",
      "122/122 [==============================] - 0s 510us/step - loss: 486.4915 - mae: 15.4748 - val_loss: 35.9999 - val_mae: 4.7781\n",
      "Epoch 65/400\n",
      "122/122 [==============================] - 0s 512us/step - loss: 462.2240 - mae: 15.0717 - val_loss: 37.5826 - val_mae: 4.9062\n",
      "Epoch 66/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 439.9197 - mae: 14.6381 - val_loss: 68.7070 - val_mae: 6.3031\n",
      "Epoch 67/400\n",
      "122/122 [==============================] - 0s 545us/step - loss: 466.7614 - mae: 14.9929 - val_loss: 45.4668 - val_mae: 5.5991\n",
      "Epoch 68/400\n",
      "122/122 [==============================] - 0s 521us/step - loss: 454.5621 - mae: 14.6036 - val_loss: 46.1392 - val_mae: 5.6273\n",
      "Epoch 69/400\n",
      "122/122 [==============================] - 0s 505us/step - loss: 469.1551 - mae: 15.0249 - val_loss: 33.2555 - val_mae: 4.5047\n",
      "Epoch 70/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 463.1002 - mae: 14.8438 - val_loss: 32.6867 - val_mae: 4.6316\n",
      "Epoch 71/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 440.9698 - mae: 14.6473 - val_loss: 41.4917 - val_mae: 5.3235\n",
      "Epoch 72/400\n",
      "122/122 [==============================] - 0s 516us/step - loss: 411.5911 - mae: 14.0601 - val_loss: 37.4021 - val_mae: 4.9392\n",
      "Epoch 73/400\n",
      "122/122 [==============================] - 0s 506us/step - loss: 431.5418 - mae: 14.4648 - val_loss: 33.6200 - val_mae: 4.4854\n",
      "Epoch 74/400\n",
      "122/122 [==============================] - 0s 542us/step - loss: 428.8090 - mae: 14.3405 - val_loss: 31.1643 - val_mae: 4.4224\n",
      "Epoch 75/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 418.8915 - mae: 14.3260 - val_loss: 29.4865 - val_mae: 4.1711\n",
      "Epoch 76/400\n",
      "122/122 [==============================] - 0s 508us/step - loss: 427.4844 - mae: 14.3816 - val_loss: 31.5601 - val_mae: 4.2838\n",
      "Epoch 77/400\n",
      "122/122 [==============================] - 0s 503us/step - loss: 426.3560 - mae: 14.1885 - val_loss: 34.5871 - val_mae: 4.6331\n",
      "Epoch 78/400\n",
      "122/122 [==============================] - 0s 504us/step - loss: 444.7709 - mae: 14.4223 - val_loss: 34.2945 - val_mae: 4.6242\n",
      "Epoch 79/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 425.9233 - mae: 14.0707 - val_loss: 36.6416 - val_mae: 4.8486\n",
      "Epoch 80/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 426.5721 - mae: 14.4763 - val_loss: 39.5999 - val_mae: 5.0977\n",
      "Epoch 81/400\n",
      "122/122 [==============================] - 0s 538us/step - loss: 438.9471 - mae: 14.3586 - val_loss: 39.5107 - val_mae: 5.0642\n",
      "Epoch 82/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 430.7460 - mae: 14.3172 - val_loss: 25.2178 - val_mae: 3.8056\n",
      "Epoch 83/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 419.6729 - mae: 14.1880 - val_loss: 29.0817 - val_mae: 4.1689\n",
      "Epoch 84/400\n",
      "122/122 [==============================] - 0s 512us/step - loss: 433.2152 - mae: 14.2646 - val_loss: 52.1202 - val_mae: 5.6600\n",
      "Epoch 85/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 411.1230 - mae: 13.9892 - val_loss: 29.2361 - val_mae: 4.3058\n",
      "Epoch 86/400\n",
      "122/122 [==============================] - 0s 522us/step - loss: 426.1765 - mae: 14.0788 - val_loss: 30.0592 - val_mae: 4.0809\n",
      "Epoch 87/400\n",
      "122/122 [==============================] - 0s 498us/step - loss: 400.2370 - mae: 13.6238 - val_loss: 60.1410 - val_mae: 5.9949\n",
      "Epoch 88/400\n",
      "122/122 [==============================] - 0s 510us/step - loss: 398.6041 - mae: 13.6554 - val_loss: 34.5462 - val_mae: 4.5599\n",
      "Epoch 89/400\n",
      "122/122 [==============================] - 0s 612us/step - loss: 401.3648 - mae: 13.7189 - val_loss: 24.3746 - val_mae: 3.7105\n",
      "Epoch 90/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 408.4809 - mae: 13.8336 - val_loss: 30.8358 - val_mae: 4.3817\n",
      "Epoch 91/400\n",
      "122/122 [==============================] - 0s 500us/step - loss: 414.2186 - mae: 13.7903 - val_loss: 46.3455 - val_mae: 5.6853\n",
      "Epoch 92/400\n",
      "122/122 [==============================] - 0s 510us/step - loss: 402.9263 - mae: 13.5645 - val_loss: 39.6376 - val_mae: 5.0688\n",
      "Epoch 93/400\n",
      "122/122 [==============================] - 0s 512us/step - loss: 412.2524 - mae: 13.6070 - val_loss: 37.3377 - val_mae: 4.9315\n",
      "Epoch 94/400\n",
      "122/122 [==============================] - 0s 500us/step - loss: 418.6245 - mae: 13.9542 - val_loss: 48.7846 - val_mae: 5.6801\n",
      "Epoch 95/400\n",
      "122/122 [==============================] - 0s 502us/step - loss: 400.1996 - mae: 13.6267 - val_loss: 32.1148 - val_mae: 4.4773\n",
      "Epoch 96/400\n",
      "122/122 [==============================] - 0s 508us/step - loss: 385.8728 - mae: 13.4074 - val_loss: 43.9034 - val_mae: 5.4207\n",
      "Epoch 97/400\n",
      "122/122 [==============================] - 0s 508us/step - loss: 379.4293 - mae: 13.2579 - val_loss: 32.4847 - val_mae: 4.5640\n",
      "Epoch 98/400\n",
      "122/122 [==============================] - 0s 631us/step - loss: 382.7667 - mae: 13.2660 - val_loss: 30.3030 - val_mae: 4.3486\n",
      "Epoch 99/400\n",
      "122/122 [==============================] - 0s 518us/step - loss: 394.4113 - mae: 13.4152 - val_loss: 26.5109 - val_mae: 3.9419\n",
      "Epoch 100/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 388.3726 - mae: 13.2115 - val_loss: 27.6926 - val_mae: 4.1094\n",
      "Epoch 101/400\n",
      "122/122 [==============================] - 0s 524us/step - loss: 402.2227 - mae: 13.5744 - val_loss: 57.6466 - val_mae: 5.9400\n",
      "Epoch 102/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 364.9004 - mae: 13.1418 - val_loss: 30.4910 - val_mae: 4.2368\n",
      "Epoch 103/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 366.8072 - mae: 13.1942 - val_loss: 42.1933 - val_mae: 5.3016\n",
      "Epoch 104/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 393.2973 - mae: 13.4725 - val_loss: 52.5125 - val_mae: 5.6763\n",
      "Epoch 105/400\n",
      "122/122 [==============================] - 0s 523us/step - loss: 384.3518 - mae: 13.2581 - val_loss: 37.9463 - val_mae: 4.8802\n",
      "Epoch 106/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 383.8233 - mae: 13.2115 - val_loss: 51.4894 - val_mae: 6.0859\n",
      "Epoch 107/400\n",
      "122/122 [==============================] - 0s 592us/step - loss: 368.3398 - mae: 12.9541 - val_loss: 43.2993 - val_mae: 5.4140\n",
      "Epoch 108/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 384.0988 - mae: 13.2382 - val_loss: 36.1911 - val_mae: 4.7975\n",
      "Epoch 109/400\n",
      "122/122 [==============================] - 0s 598us/step - loss: 353.7567 - mae: 12.7412 - val_loss: 34.3124 - val_mae: 4.6569\n",
      "Epoch 110/400\n",
      "122/122 [==============================] - 0s 602us/step - loss: 387.9644 - mae: 13.1243 - val_loss: 46.1893 - val_mae: 5.4094\n",
      "Epoch 111/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 367.3604 - mae: 12.9449 - val_loss: 30.3504 - val_mae: 4.2830\n",
      "Epoch 112/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 360.6350 - mae: 12.7326 - val_loss: 36.2845 - val_mae: 4.8707\n",
      "Epoch 113/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 368.2366 - mae: 12.8427 - val_loss: 33.5201 - val_mae: 4.4538\n",
      "Epoch 114/400\n",
      "122/122 [==============================] - 0s 592us/step - loss: 361.8950 - mae: 12.8397 - val_loss: 36.9698 - val_mae: 4.9099\n",
      "Epoch 115/400\n",
      "122/122 [==============================] - 0s 600us/step - loss: 374.3457 - mae: 13.0578 - val_loss: 40.8307 - val_mae: 5.0981\n",
      "Epoch 116/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 347.3017 - mae: 12.6778 - val_loss: 27.1581 - val_mae: 3.9564\n",
      "Epoch 117/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 348.9101 - mae: 12.6295 - val_loss: 38.8990 - val_mae: 5.1414\n",
      "Epoch 118/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 351.7982 - mae: 12.5345 - val_loss: 33.1753 - val_mae: 4.5890\n",
      "Epoch 119/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 355.3903 - mae: 12.7386 - val_loss: 36.9851 - val_mae: 4.8932\n",
      "Epoch 120/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 338.2205 - mae: 12.4857 - val_loss: 32.4506 - val_mae: 4.4652\n",
      "Epoch 121/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 322.9831 - mae: 12.0927 - val_loss: 31.3849 - val_mae: 4.3668\n",
      "Epoch 122/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 346.4479 - mae: 12.5372 - val_loss: 26.3074 - val_mae: 3.9160\n",
      "Epoch 123/400\n",
      "122/122 [==============================] - 0s 549us/step - loss: 340.8284 - mae: 12.4541 - val_loss: 34.4366 - val_mae: 4.7122\n",
      "Epoch 124/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 336.5463 - mae: 12.4124 - val_loss: 31.0789 - val_mae: 4.3716\n",
      "Epoch 125/400\n",
      "122/122 [==============================] - 0s 530us/step - loss: 346.6400 - mae: 12.5949 - val_loss: 28.5976 - val_mae: 4.0936\n",
      "Epoch 126/400\n",
      "122/122 [==============================] - 0s 516us/step - loss: 337.4633 - mae: 12.2413 - val_loss: 43.3584 - val_mae: 5.2653\n",
      "Epoch 127/400\n",
      "122/122 [==============================] - 0s 506us/step - loss: 342.4660 - mae: 12.2860 - val_loss: 27.7000 - val_mae: 4.1010\n",
      "Epoch 128/400\n",
      "122/122 [==============================] - 0s 522us/step - loss: 341.5243 - mae: 12.3928 - val_loss: 36.8321 - val_mae: 4.9179\n",
      "Epoch 129/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 334.5561 - mae: 12.3327 - val_loss: 31.6819 - val_mae: 4.3907\n",
      "Epoch 130/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 316.3719 - mae: 11.9930 - val_loss: 30.4970 - val_mae: 4.2944\n",
      "Epoch 131/400\n",
      "122/122 [==============================] - 0s 553us/step - loss: 344.6770 - mae: 12.1993 - val_loss: 40.5170 - val_mae: 4.9793\n",
      "Epoch 132/400\n",
      "122/122 [==============================] - 0s 528us/step - loss: 328.6467 - mae: 12.2719 - val_loss: 34.8175 - val_mae: 4.5769\n",
      "Epoch 133/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 333.6148 - mae: 12.1400 - val_loss: 33.8910 - val_mae: 4.4479\n",
      "Epoch 134/400\n",
      "122/122 [==============================] - 0s 505us/step - loss: 332.0260 - mae: 12.1913 - val_loss: 34.1931 - val_mae: 4.6653\n",
      "Epoch 135/400\n",
      "122/122 [==============================] - 0s 518us/step - loss: 328.8060 - mae: 12.0064 - val_loss: 41.0891 - val_mae: 5.1826\n",
      "Epoch 136/400\n",
      "122/122 [==============================] - 0s 522us/step - loss: 322.8628 - mae: 12.0918 - val_loss: 26.6368 - val_mae: 3.8839\n",
      "Epoch 137/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 325.8783 - mae: 12.0935 - val_loss: 38.3620 - val_mae: 4.9159\n",
      "Epoch 138/400\n",
      "122/122 [==============================] - 0s 610us/step - loss: 318.3236 - mae: 11.8664 - val_loss: 49.0457 - val_mae: 5.5792\n",
      "Epoch 139/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 306.1071 - mae: 11.8523 - val_loss: 40.7774 - val_mae: 5.1604\n",
      "Epoch 140/400\n",
      "122/122 [==============================] - 0s 528us/step - loss: 287.8531 - mae: 11.4043 - val_loss: 32.8758 - val_mae: 4.4490\n",
      "Epoch 141/400\n",
      "122/122 [==============================] - 0s 522us/step - loss: 300.9201 - mae: 11.5863 - val_loss: 33.4382 - val_mae: 4.6398\n",
      "Epoch 142/400\n",
      "122/122 [==============================] - 0s 551us/step - loss: 317.2243 - mae: 11.8424 - val_loss: 37.0604 - val_mae: 4.9121\n",
      "Epoch 143/400\n",
      "122/122 [==============================] - 0s 520us/step - loss: 315.1755 - mae: 11.7830 - val_loss: 37.0359 - val_mae: 4.9217\n",
      "Epoch 144/400\n",
      "122/122 [==============================] - 0s 516us/step - loss: 312.7130 - mae: 11.7685 - val_loss: 35.8479 - val_mae: 4.7599\n",
      "Epoch 145/400\n",
      "122/122 [==============================] - 0s 531us/step - loss: 324.4373 - mae: 11.9354 - val_loss: 31.7599 - val_mae: 4.4194\n",
      "Epoch 146/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 305.0486 - mae: 11.6626 - val_loss: 31.0421 - val_mae: 4.2414\n",
      "Epoch 147/400\n",
      "122/122 [==============================] - 0s 528us/step - loss: 308.0798 - mae: 11.6480 - val_loss: 34.7823 - val_mae: 4.5907\n",
      "Epoch 148/400\n",
      "122/122 [==============================] - 0s 518us/step - loss: 285.1469 - mae: 11.3460 - val_loss: 36.3544 - val_mae: 4.7969\n",
      "Epoch 149/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 295.7082 - mae: 11.3751 - val_loss: 66.2859 - val_mae: 6.9282\n",
      "Epoch 150/400\n",
      "122/122 [==============================] - 0s 521us/step - loss: 305.9924 - mae: 11.6344 - val_loss: 34.9769 - val_mae: 4.5669\n",
      "Epoch 151/400\n",
      "122/122 [==============================] - 0s 509us/step - loss: 295.9814 - mae: 11.5962 - val_loss: 33.8872 - val_mae: 4.6020\n",
      "Epoch 152/400\n",
      "122/122 [==============================] - 0s 511us/step - loss: 292.3689 - mae: 11.3325 - val_loss: 31.7804 - val_mae: 4.4168\n",
      "Epoch 153/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 284.6158 - mae: 11.2697 - val_loss: 38.9815 - val_mae: 4.9333\n",
      "Epoch 154/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 284.8570 - mae: 11.3102 - val_loss: 36.7402 - val_mae: 4.8607\n",
      "Epoch 155/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 285.8239 - mae: 11.2806 - val_loss: 41.9103 - val_mae: 5.2816\n",
      "Epoch 156/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 294.5433 - mae: 11.4053 - val_loss: 33.6964 - val_mae: 4.6249\n",
      "Epoch 157/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 288.4311 - mae: 11.2217 - val_loss: 43.1417 - val_mae: 5.2996\n",
      "Epoch 158/400\n",
      "122/122 [==============================] - 0s 519us/step - loss: 288.0640 - mae: 11.3411 - val_loss: 37.2691 - val_mae: 4.7627\n",
      "Epoch 159/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 275.6853 - mae: 11.0425 - val_loss: 31.1338 - val_mae: 4.3090\n",
      "Epoch 160/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 282.1863 - mae: 11.1780 - val_loss: 34.7989 - val_mae: 4.7245\n",
      "Epoch 161/400\n",
      "122/122 [==============================] - 0s 579us/step - loss: 287.8653 - mae: 11.0442 - val_loss: 38.7228 - val_mae: 4.9700\n",
      "Epoch 162/400\n",
      "122/122 [==============================] - 0s 528us/step - loss: 281.0155 - mae: 11.1768 - val_loss: 37.5108 - val_mae: 4.7571\n",
      "Epoch 163/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 286.1475 - mae: 11.1257 - val_loss: 26.3260 - val_mae: 3.9106\n",
      "Epoch 164/400\n",
      "122/122 [==============================] - 0s 525us/step - loss: 278.5296 - mae: 11.1644 - val_loss: 30.3474 - val_mae: 4.2002\n",
      "Epoch 165/400\n",
      "122/122 [==============================] - 0s 524us/step - loss: 283.8769 - mae: 11.0277 - val_loss: 29.0434 - val_mae: 4.1796\n",
      "Epoch 166/400\n",
      "122/122 [==============================] - 0s 541us/step - loss: 275.3507 - mae: 10.8529 - val_loss: 32.2817 - val_mae: 4.4369\n",
      "Epoch 167/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 268.6124 - mae: 10.8675 - val_loss: 44.8235 - val_mae: 5.5029\n",
      "Epoch 168/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 278.6327 - mae: 10.9161 - val_loss: 36.6496 - val_mae: 4.7248\n",
      "Epoch 169/400\n",
      "122/122 [==============================] - 0s 621us/step - loss: 275.3306 - mae: 10.8788 - val_loss: 41.1529 - val_mae: 5.2422\n",
      "Epoch 170/400\n",
      "122/122 [==============================] - 0s 530us/step - loss: 248.6020 - mae: 10.4732 - val_loss: 47.6855 - val_mae: 5.7677\n",
      "Epoch 171/400\n",
      "122/122 [==============================] - 0s 529us/step - loss: 275.6914 - mae: 10.8294 - val_loss: 40.4708 - val_mae: 5.1782\n",
      "Epoch 172/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 288.1932 - mae: 10.9838 - val_loss: 32.3338 - val_mae: 4.5200\n",
      "Epoch 173/400\n",
      "122/122 [==============================] - 0s 521us/step - loss: 266.3414 - mae: 10.6940 - val_loss: 34.2828 - val_mae: 4.7076\n",
      "Epoch 174/400\n",
      "122/122 [==============================] - 0s 525us/step - loss: 253.7945 - mae: 10.5450 - val_loss: 62.0600 - val_mae: 6.1750\n",
      "Epoch 175/400\n",
      "122/122 [==============================] - 0s 548us/step - loss: 258.0662 - mae: 10.5732 - val_loss: 30.2800 - val_mae: 4.2948\n",
      "Epoch 176/400\n",
      "122/122 [==============================] - 0s 537us/step - loss: 276.1654 - mae: 10.6144 - val_loss: 33.8164 - val_mae: 4.5783\n",
      "Epoch 177/400\n",
      "122/122 [==============================] - 0s 574us/step - loss: 265.6229 - mae: 10.6492 - val_loss: 33.8620 - val_mae: 4.5658\n",
      "Epoch 178/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 264.6899 - mae: 10.6685 - val_loss: 45.4308 - val_mae: 5.5532\n",
      "Epoch 179/400\n",
      "122/122 [==============================] - 0s 530us/step - loss: 259.4255 - mae: 10.5564 - val_loss: 37.7247 - val_mae: 4.9785\n",
      "Epoch 180/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 259.3610 - mae: 10.5252 - val_loss: 36.6075 - val_mae: 4.8208\n",
      "Epoch 181/400\n",
      "122/122 [==============================] - 0s 514us/step - loss: 245.8072 - mae: 10.4245 - val_loss: 31.0254 - val_mae: 4.3804\n",
      "Epoch 182/400\n",
      "122/122 [==============================] - 0s 533us/step - loss: 264.2181 - mae: 10.5651 - val_loss: 37.8921 - val_mae: 5.1159\n",
      "Epoch 183/400\n",
      "122/122 [==============================] - 0s 528us/step - loss: 220.5690 - mae: 9.9002 - val_loss: 46.2538 - val_mae: 5.5499\n",
      "Epoch 184/400\n",
      "122/122 [==============================] - 0s 544us/step - loss: 236.3555 - mae: 10.2077 - val_loss: 60.7181 - val_mae: 6.3802\n",
      "Epoch 185/400\n",
      "122/122 [==============================] - 0s 636us/step - loss: 254.6663 - mae: 10.3679 - val_loss: 40.3242 - val_mae: 5.1721\n",
      "Epoch 186/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 240.6324 - mae: 10.1055 - val_loss: 37.5881 - val_mae: 4.8742\n",
      "Epoch 187/400\n",
      "122/122 [==============================] - 0s 585us/step - loss: 243.7024 - mae: 10.2586 - val_loss: 30.4730 - val_mae: 4.2405\n",
      "Epoch 188/400\n",
      "122/122 [==============================] - 0s 602us/step - loss: 245.2117 - mae: 10.2958 - val_loss: 34.1322 - val_mae: 4.6286\n",
      "Epoch 189/400\n",
      "122/122 [==============================] - 0s 595us/step - loss: 254.3860 - mae: 10.1902 - val_loss: 35.0229 - val_mae: 4.6334\n",
      "Epoch 190/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 233.7066 - mae: 9.9509 - val_loss: 33.7799 - val_mae: 4.5316\n",
      "Epoch 191/400\n",
      "122/122 [==============================] - 0s 607us/step - loss: 236.7396 - mae: 10.0835 - val_loss: 43.9811 - val_mae: 5.3087\n",
      "Epoch 192/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 238.9220 - mae: 9.9314 - val_loss: 49.8306 - val_mae: 5.5766\n",
      "Epoch 193/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 226.0020 - mae: 9.9323 - val_loss: 40.8823 - val_mae: 5.1847\n",
      "Epoch 194/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 243.1560 - mae: 10.1398 - val_loss: 35.5688 - val_mae: 4.8089\n",
      "Epoch 195/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 236.3108 - mae: 9.9531 - val_loss: 37.8238 - val_mae: 4.9248\n",
      "Epoch 196/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 238.5167 - mae: 10.0105 - val_loss: 32.2126 - val_mae: 4.3954\n",
      "Epoch 197/400\n",
      "122/122 [==============================] - 0s 619us/step - loss: 241.7915 - mae: 10.2025 - val_loss: 30.8093 - val_mae: 4.3537\n",
      "Epoch 198/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 246.6207 - mae: 10.1155 - val_loss: 34.1000 - val_mae: 4.6909\n",
      "Epoch 199/400\n",
      "122/122 [==============================] - 0s 544us/step - loss: 235.9358 - mae: 9.8787 - val_loss: 40.5837 - val_mae: 5.1784\n",
      "Epoch 200/400\n",
      "122/122 [==============================] - 0s 526us/step - loss: 236.7496 - mae: 9.9159 - val_loss: 43.5338 - val_mae: 5.1895\n",
      "Epoch 201/400\n",
      "122/122 [==============================] - 0s 527us/step - loss: 231.4293 - mae: 9.8705 - val_loss: 44.4379 - val_mae: 5.4469\n",
      "Epoch 202/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 213.6730 - mae: 9.5171 - val_loss: 25.5584 - val_mae: 3.7783\n",
      "Epoch 203/400\n",
      "122/122 [==============================] - 0s 530us/step - loss: 212.5598 - mae: 9.5205 - val_loss: 39.1614 - val_mae: 5.0955\n",
      "Epoch 204/400\n",
      "122/122 [==============================] - 0s 547us/step - loss: 217.5552 - mae: 9.6951 - val_loss: 47.4376 - val_mae: 5.4727\n",
      "Epoch 205/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 213.4763 - mae: 9.4695 - val_loss: 36.4838 - val_mae: 4.8975\n",
      "Epoch 206/400\n",
      "122/122 [==============================] - 0s 534us/step - loss: 224.1259 - mae: 9.6948 - val_loss: 40.9263 - val_mae: 5.1384\n",
      "Epoch 207/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 220.5750 - mae: 9.5578 - val_loss: 37.2939 - val_mae: 4.9276\n",
      "Epoch 208/400\n",
      "122/122 [==============================] - 0s 531us/step - loss: 212.9061 - mae: 9.4739 - val_loss: 36.8291 - val_mae: 4.8049\n",
      "Epoch 209/400\n",
      "122/122 [==============================] - 0s 528us/step - loss: 222.2110 - mae: 9.6566 - val_loss: 40.8345 - val_mae: 5.1839\n",
      "Epoch 210/400\n",
      "122/122 [==============================] - 0s 523us/step - loss: 220.4519 - mae: 9.4862 - val_loss: 41.2127 - val_mae: 5.0959\n",
      "Epoch 211/400\n",
      "122/122 [==============================] - 0s 527us/step - loss: 221.6110 - mae: 9.6843 - val_loss: 39.9150 - val_mae: 5.1123\n",
      "Epoch 212/400\n",
      "122/122 [==============================] - 0s 515us/step - loss: 217.2215 - mae: 9.4899 - val_loss: 31.8116 - val_mae: 4.4177\n",
      "Epoch 213/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 201.0137 - mae: 9.3642 - val_loss: 30.0491 - val_mae: 4.2125\n",
      "Epoch 214/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 211.9716 - mae: 9.5872 - val_loss: 34.9377 - val_mae: 4.6842\n",
      "Epoch 215/400\n",
      "122/122 [==============================] - 0s 545us/step - loss: 205.7879 - mae: 9.2524 - val_loss: 31.4082 - val_mae: 4.3689\n",
      "Epoch 216/400\n",
      "122/122 [==============================] - 0s 541us/step - loss: 215.9973 - mae: 9.4122 - val_loss: 41.4682 - val_mae: 5.1389\n",
      "Epoch 217/400\n",
      "122/122 [==============================] - 0s 542us/step - loss: 199.3663 - mae: 9.1249 - val_loss: 53.5361 - val_mae: 5.8667\n",
      "Epoch 218/400\n",
      "122/122 [==============================] - 0s 532us/step - loss: 201.8718 - mae: 9.2978 - val_loss: 37.0604 - val_mae: 4.9216\n",
      "Epoch 219/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 200.8841 - mae: 9.2623 - val_loss: 47.3450 - val_mae: 5.3758\n",
      "Epoch 220/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 203.5373 - mae: 9.2015 - val_loss: 34.5201 - val_mae: 4.5790\n",
      "Epoch 221/400\n",
      "122/122 [==============================] - 0s 535us/step - loss: 212.7621 - mae: 9.4689 - val_loss: 31.3792 - val_mae: 4.3623\n",
      "Epoch 222/400\n",
      "122/122 [==============================] - 0s 525us/step - loss: 185.6359 - mae: 8.9682 - val_loss: 29.3997 - val_mae: 4.1448\n",
      "Epoch 223/400\n",
      "122/122 [==============================] - 0s 510us/step - loss: 188.1288 - mae: 8.9668 - val_loss: 38.5970 - val_mae: 4.9483\n",
      "Epoch 224/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 191.5371 - mae: 8.9161 - val_loss: 37.3115 - val_mae: 4.9845\n",
      "Epoch 225/400\n",
      "122/122 [==============================] - 0s 549us/step - loss: 200.7818 - mae: 9.1267 - val_loss: 47.1208 - val_mae: 5.5355\n",
      "Epoch 226/400\n",
      "122/122 [==============================] - 0s 535us/step - loss: 198.4408 - mae: 9.2357 - val_loss: 37.7145 - val_mae: 4.9205\n",
      "Epoch 227/400\n",
      "122/122 [==============================] - 0s 546us/step - loss: 195.7126 - mae: 9.1369 - val_loss: 38.2180 - val_mae: 4.9644\n",
      "Epoch 228/400\n",
      "122/122 [==============================] - 0s 525us/step - loss: 196.3500 - mae: 9.0737 - val_loss: 34.5814 - val_mae: 4.6508\n",
      "Epoch 229/400\n",
      "122/122 [==============================] - 0s 517us/step - loss: 179.7943 - mae: 8.8223 - val_loss: 31.7182 - val_mae: 4.4022\n",
      "Epoch 230/400\n",
      "122/122 [==============================] - 0s 529us/step - loss: 203.9582 - mae: 9.1680 - val_loss: 34.5299 - val_mae: 4.7659\n",
      "Epoch 231/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 196.5095 - mae: 9.1250 - val_loss: 37.6001 - val_mae: 4.8983\n",
      "Epoch 232/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 186.0338 - mae: 8.8475 - val_loss: 41.4255 - val_mae: 5.1962\n",
      "Epoch 233/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 184.5258 - mae: 8.9000 - val_loss: 31.4329 - val_mae: 4.2945\n",
      "Epoch 234/400\n",
      "122/122 [==============================] - 0s 590us/step - loss: 169.5331 - mae: 8.6391 - val_loss: 33.5595 - val_mae: 4.4988\n",
      "Epoch 235/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 190.2805 - mae: 8.8206 - val_loss: 46.6543 - val_mae: 5.5097\n",
      "Epoch 236/400\n",
      "122/122 [==============================] - 0s 547us/step - loss: 178.5532 - mae: 8.7698 - val_loss: 33.9532 - val_mae: 4.5565\n",
      "Epoch 237/400\n",
      "122/122 [==============================] - 0s 527us/step - loss: 188.4719 - mae: 8.7361 - val_loss: 34.7907 - val_mae: 4.6233\n",
      "Epoch 238/400\n",
      "122/122 [==============================] - 0s 522us/step - loss: 189.1642 - mae: 8.8386 - val_loss: 47.5406 - val_mae: 5.3101\n",
      "Epoch 239/400\n",
      "122/122 [==============================] - 0s 531us/step - loss: 180.7521 - mae: 8.8463 - val_loss: 32.5525 - val_mae: 4.4034\n",
      "Epoch 240/400\n",
      "122/122 [==============================] - 0s 531us/step - loss: 175.2312 - mae: 8.6231 - val_loss: 37.2947 - val_mae: 4.8888\n",
      "Epoch 241/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 182.2835 - mae: 8.6570 - val_loss: 29.6312 - val_mae: 4.2342\n",
      "Epoch 242/400\n",
      "  1/122 [..............................] - ETA: 0s - loss: 286.8730 - mae: 9.7334"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# adam -> adaptive gradient based optimizer (type of gradient descent): AdaGrad & RMSProp       \u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     24\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, predictions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/FNN_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mae_results = {}\n",
    "for epoch in range(100, 1001, 50):\n",
    "    print(f\"Training model with {epoch} epochs\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=[X_train.shape[1],]),\n",
    "        Dropout(0.3), # randomly set 30% of neurons to 0 during training to prevent overfitting\n",
    "        Dense(32, activation='relu'), # 2nd layer with only 32 neurons to reduce model complexity\n",
    "        Dense(1, activation='linear') # single out representing the Price\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics='mae')\n",
    "    # adam -> adaptive gradient based optimizer (type of gradient descent): AdaGrad & RMSProp       \n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=epoch,\n",
    "                    batch_size=32\n",
    "                    )\n",
    "    \n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mae_results[epoch] = mae\n",
    "    print(f\"Epochs: {epoch} | MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 3.7399122877854563,\n",
       " 150: 3.673137145996094,\n",
       " 200: 3.5558574425330525,\n",
       " 250: 4.3641277916541465,\n",
       " 300: 4.341572904146635,\n",
       " 350: 3.674060824506711,\n",
       " 400: 9.223212161646135,\n",
       " 450: 11.699360898844402,\n",
       " 500: 4.194033297181741,\n",
       " 550: 4.009001143391926,\n",
       " 600: 3.8716499240973046,\n",
       " 650: 4.038987292010968,\n",
       " 700: 8.05191028129382,\n",
       " 750: 4.2953251377203525,\n",
       " 800: 4.485844051889273,\n",
       " 850: 4.4830111594175674,\n",
       " 900: 4.284747272354518,\n",
       " 950: 4.666406692270132,\n",
       " 1000: 17.59313472290039}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15568f610>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = list(mae_results.keys())\n",
    "mae_values = list(mae_results.values())\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae_values, marker='o', linestyle='-', color='blue', label='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 454us/step - loss: 24.4421 - mae: 3.3766\n",
      "Test Loss: 24.442119598388672, Test MAE: 3.376631021499634\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 391us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 53.16598 ],\n",
       "       [165.90982 ],\n",
       "       [234.89175 ],\n",
       "       [186.91599 ],\n",
       "       [200.98889 ],\n",
       "       [327.57306 ],\n",
       "       [124.425934],\n",
       "       [310.511   ],\n",
       "       [232.30785 ],\n",
       "       [311.1548  ],\n",
       "       [219.70325 ],\n",
       "       [326.35556 ],\n",
       "       [360.28595 ],\n",
       "       [454.15796 ],\n",
       "       [149.31961 ],\n",
       "       [394.00458 ],\n",
       "       [232.8606  ],\n",
       "       [246.82654 ],\n",
       "       [468.94507 ],\n",
       "       [443.3084  ],\n",
       "       [222.89096 ],\n",
       "       [504.56244 ],\n",
       "       [469.98267 ],\n",
       "       [248.65472 ],\n",
       "       [ 76.61895 ],\n",
       "       [214.89973 ],\n",
       "       [176.9992  ],\n",
       "       [ 88.414215],\n",
       "       [236.2771  ],\n",
       "       [ 96.0325  ],\n",
       "       [149.27368 ],\n",
       "       [444.11676 ],\n",
       "       [204.35237 ],\n",
       "       [120.032265],\n",
       "       [383.1347  ],\n",
       "       [205.66064 ],\n",
       "       [228.03241 ],\n",
       "       [265.5633  ],\n",
       "       [202.01688 ],\n",
       "       [ 54.16947 ],\n",
       "       [207.01474 ],\n",
       "       [ 93.06757 ],\n",
       "       [555.56354 ],\n",
       "       [ 39.60001 ],\n",
       "       [213.3809  ],\n",
       "       [ 89.845184],\n",
       "       [149.4267  ],\n",
       "       [472.58984 ],\n",
       "       [419.63425 ],\n",
       "       [151.9839  ],\n",
       "       [382.3241  ],\n",
       "       [226.34207 ],\n",
       "       [371.7729  ],\n",
       "       [182.34692 ],\n",
       "       [ 96.41562 ],\n",
       "       [ 46.648884],\n",
       "       [ 80.93788 ],\n",
       "       [561.7192  ],\n",
       "       [ 75.56683 ],\n",
       "       [122.4536  ],\n",
       "       [454.9203  ],\n",
       "       [101.728584],\n",
       "       [184.86731 ],\n",
       "       [406.36    ],\n",
       "       [ 79.96712 ],\n",
       "       [282.35663 ],\n",
       "       [427.32263 ],\n",
       "       [ 94.46002 ],\n",
       "       [ 90.75644 ],\n",
       "       [220.3081  ],\n",
       "       [477.6519  ],\n",
       "       [211.80685 ],\n",
       "       [272.48505 ],\n",
       "       [ 78.698364],\n",
       "       [ 88.37825 ],\n",
       "       [138.8206  ],\n",
       "       [226.95444 ],\n",
       "       [513.02405 ],\n",
       "       [ 54.83838 ],\n",
       "       [214.22258 ],\n",
       "       [251.94989 ],\n",
       "       [387.23584 ],\n",
       "       [ 54.600945],\n",
       "       [219.03885 ],\n",
       "       [200.99411 ],\n",
       "       [174.07301 ],\n",
       "       [391.02826 ],\n",
       "       [ 53.995255],\n",
       "       [176.93834 ],\n",
       "       [535.9425  ],\n",
       "       [219.57921 ],\n",
       "       [234.43549 ],\n",
       "       [215.03159 ],\n",
       "       [ 98.681435],\n",
       "       [104.391174],\n",
       "       [285.17786 ],\n",
       "       [175.92557 ],\n",
       "       [107.903656],\n",
       "       [416.1335  ],\n",
       "       [273.4048  ],\n",
       "       [118.22502 ],\n",
       "       [477.03107 ],\n",
       "       [204.52295 ],\n",
       "       [104.959915],\n",
       "       [188.03827 ],\n",
       "       [ 45.265324],\n",
       "       [232.80934 ],\n",
       "       [393.94232 ],\n",
       "       [269.2106  ],\n",
       "       [199.61118 ],\n",
       "       [208.98996 ],\n",
       "       [217.98486 ],\n",
       "       [201.21475 ],\n",
       "       [198.88916 ],\n",
       "       [107.30747 ],\n",
       "       [163.3222  ],\n",
       "       [265.74097 ],\n",
       "       [227.51413 ],\n",
       "       [210.77588 ],\n",
       "       [ 83.90452 ],\n",
       "       [189.29633 ],\n",
       "       [228.5412  ],\n",
       "       [180.31012 ],\n",
       "       [188.65736 ],\n",
       "       [191.00316 ],\n",
       "       [ 81.000916],\n",
       "       [147.85803 ],\n",
       "       [ 54.409893],\n",
       "       [149.0635  ],\n",
       "       [208.84367 ],\n",
       "       [ 72.06065 ],\n",
       "       [223.51146 ],\n",
       "       [403.60828 ],\n",
       "       [178.42693 ],\n",
       "       [233.79237 ],\n",
       "       [222.50882 ],\n",
       "       [452.0839  ],\n",
       "       [252.27759 ],\n",
       "       [ 52.23481 ],\n",
       "       [393.40735 ],\n",
       "       [346.00555 ],\n",
       "       [181.91243 ],\n",
       "       [149.13687 ],\n",
       "       [168.8343  ],\n",
       "       [ 45.511887],\n",
       "       [230.87233 ],\n",
       "       [442.2616  ],\n",
       "       [442.30994 ],\n",
       "       [246.47705 ],\n",
       "       [533.92523 ],\n",
       "       [ 54.789463],\n",
       "       [147.68878 ],\n",
       "       [190.67065 ],\n",
       "       [151.59225 ],\n",
       "       [330.59247 ],\n",
       "       [209.48962 ],\n",
       "       [431.146   ],\n",
       "       [195.10797 ],\n",
       "       [441.47415 ],\n",
       "       [424.9444  ],\n",
       "       [156.34378 ],\n",
       "       [242.49826 ],\n",
       "       [200.25461 ],\n",
       "       [248.17378 ],\n",
       "       [162.2724  ],\n",
       "       [205.82204 ],\n",
       "       [518.3587  ],\n",
       "       [196.25258 ],\n",
       "       [276.5123  ],\n",
       "       [446.96353 ],\n",
       "       [333.4757  ],\n",
       "       [ 89.25505 ],\n",
       "       [177.24908 ],\n",
       "       [297.851   ],\n",
       "       [ 84.07837 ],\n",
       "       [141.6719  ],\n",
       "       [153.1887  ],\n",
       "       [111.70059 ],\n",
       "       [273.7993  ],\n",
       "       [440.32202 ],\n",
       "       [107.69114 ],\n",
       "       [459.14532 ],\n",
       "       [191.29807 ],\n",
       "       [129.83351 ],\n",
       "       [202.74619 ],\n",
       "       [196.51474 ],\n",
       "       [ 61.655827],\n",
       "       [114.57672 ],\n",
       "       [137.10574 ],\n",
       "       [431.52808 ],\n",
       "       [546.90027 ],\n",
       "       [385.2504  ],\n",
       "       [297.9452  ],\n",
       "       [189.80783 ],\n",
       "       [121.42656 ],\n",
       "       [212.22108 ],\n",
       "       [274.70795 ],\n",
       "       [414.6675  ],\n",
       "       [244.54767 ],\n",
       "       [192.40887 ],\n",
       "       [446.28687 ],\n",
       "       [193.22043 ],\n",
       "       [104.55856 ],\n",
       "       [157.38477 ],\n",
       "       [324.8484  ],\n",
       "       [ 89.96269 ],\n",
       "       [ 54.50828 ],\n",
       "       [176.59464 ],\n",
       "       [ 99.41199 ],\n",
       "       [ 95.070404],\n",
       "       [196.23746 ],\n",
       "       [ 94.1871  ],\n",
       "       [112.14079 ],\n",
       "       [199.68616 ],\n",
       "       [499.68103 ],\n",
       "       [416.93835 ],\n",
       "       [ 74.31957 ],\n",
       "       [327.43146 ],\n",
       "       [ 82.331276],\n",
       "       [420.06866 ],\n",
       "       [191.26335 ],\n",
       "       [428.83005 ],\n",
       "       [205.31973 ],\n",
       "       [296.79462 ],\n",
       "       [ 92.826385],\n",
       "       [110.92507 ],\n",
       "       [137.93015 ],\n",
       "       [393.95422 ],\n",
       "       [133.41345 ],\n",
       "       [171.49176 ],\n",
       "       [292.9835  ],\n",
       "       [171.56781 ],\n",
       "       [255.5546  ],\n",
       "       [ 75.32592 ],\n",
       "       [281.64877 ],\n",
       "       [135.17159 ],\n",
       "       [195.94499 ],\n",
       "       [394.43118 ],\n",
       "       [550.7935  ],\n",
       "       [219.02655 ],\n",
       "       [458.0878  ],\n",
       "       [328.7787  ],\n",
       "       [251.95862 ],\n",
       "       [217.98297 ],\n",
       "       [286.05994 ],\n",
       "       [142.6987  ],\n",
       "       [451.1034  ],\n",
       "       [296.36676 ],\n",
       "       [105.31892 ],\n",
       "       [158.4385  ],\n",
       "       [280.4724  ],\n",
       "       [183.56621 ],\n",
       "       [260.88165 ],\n",
       "       [123.03997 ],\n",
       "       [228.90454 ],\n",
       "       [215.16246 ],\n",
       "       [211.0863  ],\n",
       "       [ 92.3774  ],\n",
       "       [155.42868 ],\n",
       "       [301.37042 ],\n",
       "       [165.77951 ],\n",
       "       [ 85.70642 ],\n",
       "       [143.16034 ],\n",
       "       [187.1231  ],\n",
       "       [437.06805 ],\n",
       "       [344.53748 ],\n",
       "       [ 54.609974],\n",
       "       [398.0123  ],\n",
       "       [177.83228 ],\n",
       "       [446.1078  ],\n",
       "       [100.85843 ],\n",
       "       [153.97348 ],\n",
       "       [361.5453  ],\n",
       "       [234.65054 ],\n",
       "       [273.45795 ],\n",
       "       [149.93211 ],\n",
       "       [151.69868 ],\n",
       "       [276.7955  ],\n",
       "       [396.98682 ],\n",
       "       [132.24925 ],\n",
       "       [247.90033 ],\n",
       "       [253.599   ],\n",
       "       [354.44293 ],\n",
       "       [ 65.7958  ],\n",
       "       [437.82678 ],\n",
       "       [220.56685 ],\n",
       "       [288.75555 ],\n",
       "       [467.5988  ],\n",
       "       [275.12686 ],\n",
       "       [237.18661 ],\n",
       "       [ 54.168472],\n",
       "       [329.22046 ],\n",
       "       [196.90332 ],\n",
       "       [ 78.06577 ],\n",
       "       [405.64752 ],\n",
       "       [465.42834 ],\n",
       "       [ 55.67928 ],\n",
       "       [ 65.50137 ],\n",
       "       [241.19766 ],\n",
       "       [ 90.06091 ],\n",
       "       [498.40753 ],\n",
       "       [168.02005 ],\n",
       "       [256.85754 ],\n",
       "       [403.70068 ],\n",
       "       [ 54.486523],\n",
       "       [203.96545 ],\n",
       "       [211.38362 ],\n",
       "       [448.10596 ],\n",
       "       [ 93.6434  ],\n",
       "       [ 70.73516 ],\n",
       "       [160.6665  ],\n",
       "       [ 49.15272 ],\n",
       "       [440.15668 ],\n",
       "       [391.87527 ],\n",
       "       [198.6276  ],\n",
       "       [ 83.005844],\n",
       "       [295.52087 ],\n",
       "       [ 53.286724],\n",
       "       [ 96.910286],\n",
       "       [431.48068 ],\n",
       "       [360.65967 ],\n",
       "       [165.35611 ],\n",
       "       [219.93079 ],\n",
       "       [151.90182 ],\n",
       "       [165.73386 ],\n",
       "       [ 76.96058 ],\n",
       "       [249.1739  ],\n",
       "       [245.58298 ],\n",
       "       [131.09714 ],\n",
       "       [172.33582 ],\n",
       "       [184.64813 ],\n",
       "       [ 78.488434],\n",
       "       [141.15198 ],\n",
       "       [390.27856 ],\n",
       "       [451.19678 ],\n",
       "       [385.27524 ],\n",
       "       [448.08228 ],\n",
       "       [ 80.97507 ],\n",
       "       [202.2386  ],\n",
       "       [335.2176  ],\n",
       "       [137.49332 ],\n",
       "       [441.73914 ],\n",
       "       [ 55.711407],\n",
       "       [134.27982 ],\n",
       "       [219.04816 ],\n",
       "       [214.83041 ],\n",
       "       [175.59811 ],\n",
       "       [ 76.94468 ],\n",
       "       [104.340744],\n",
       "       [222.81744 ],\n",
       "       [210.81113 ],\n",
       "       [333.0959  ],\n",
       "       [ 79.888336],\n",
       "       [ 75.09836 ],\n",
       "       [258.65332 ],\n",
       "       [104.76988 ],\n",
       "       [336.16553 ],\n",
       "       [365.18994 ],\n",
       "       [169.50462 ],\n",
       "       [436.5202  ],\n",
       "       [174.66422 ],\n",
       "       [155.84686 ],\n",
       "       [108.9616  ],\n",
       "       [ 74.472885],\n",
       "       [470.19858 ],\n",
       "       [154.95    ],\n",
       "       [253.04947 ],\n",
       "       [467.99634 ],\n",
       "       [327.99817 ],\n",
       "       [ 53.51035 ],\n",
       "       [271.296   ],\n",
       "       [211.03632 ],\n",
       "       [439.22882 ],\n",
       "       [141.51883 ],\n",
       "       [205.41019 ],\n",
       "       [122.867905],\n",
       "       [150.1407  ],\n",
       "       [186.1463  ],\n",
       "       [557.7056  ],\n",
       "       [187.60406 ],\n",
       "       [105.73828 ],\n",
       "       [231.41838 ],\n",
       "       [178.29948 ],\n",
       "       [395.14508 ],\n",
       "       [161.92497 ],\n",
       "       [228.25719 ],\n",
       "       [546.7597  ],\n",
       "       [229.42003 ],\n",
       "       [194.84311 ],\n",
       "       [444.62216 ],\n",
       "       [ 87.31375 ],\n",
       "       [ 71.52969 ],\n",
       "       [213.37105 ],\n",
       "       [ 70.55266 ],\n",
       "       [415.67987 ],\n",
       "       [ 73.10517 ],\n",
       "       [191.43083 ],\n",
       "       [242.69897 ],\n",
       "       [ 84.40695 ],\n",
       "       [171.58055 ],\n",
       "       [212.66257 ],\n",
       "       [167.16977 ],\n",
       "       [140.63593 ],\n",
       "       [177.33542 ],\n",
       "       [198.51526 ],\n",
       "       [ 72.57573 ],\n",
       "       [265.8629  ],\n",
       "       [ 76.09474 ],\n",
       "       [340.6093  ],\n",
       "       [382.4914  ],\n",
       "       [355.65027 ],\n",
       "       [247.87772 ],\n",
       "       [146.17323 ],\n",
       "       [217.29448 ],\n",
       "       [262.92355 ],\n",
       "       [157.81616 ],\n",
       "       [167.69623 ],\n",
       "       [532.73505 ],\n",
       "       [ 55.48021 ],\n",
       "       [ 96.21471 ],\n",
       "       [ 94.9068  ],\n",
       "       [226.78708 ],\n",
       "       [255.29524 ],\n",
       "       [188.61719 ],\n",
       "       [190.31104 ],\n",
       "       [432.2074  ],\n",
       "       [136.8785  ],\n",
       "       [ 92.0446  ],\n",
       "       [269.24597 ],\n",
       "       [374.48547 ],\n",
       "       [142.81409 ],\n",
       "       [ 92.521286],\n",
       "       [174.3484  ],\n",
       "       [257.10852 ],\n",
       "       [173.03624 ],\n",
       "       [138.88469 ],\n",
       "       [253.87233 ],\n",
       "       [285.45267 ],\n",
       "       [233.33362 ],\n",
       "       [433.95972 ],\n",
       "       [ 77.683525],\n",
       "       [263.12384 ],\n",
       "       [339.18842 ],\n",
       "       [ 55.402706],\n",
       "       [208.90051 ],\n",
       "       [139.4941  ],\n",
       "       [365.4381  ],\n",
       "       [205.65948 ],\n",
       "       [ 64.78601 ],\n",
       "       [220.44449 ],\n",
       "       [ 83.767746],\n",
       "       [265.82977 ],\n",
       "       [176.13663 ],\n",
       "       [248.09155 ],\n",
       "       [ 78.52801 ],\n",
       "       [ 40.755825],\n",
       "       [120.52573 ],\n",
       "       [262.47495 ],\n",
       "       [176.05405 ],\n",
       "       [ 73.85079 ],\n",
       "       [215.48456 ],\n",
       "       [267.27493 ],\n",
       "       [532.73737 ],\n",
       "       [331.98718 ],\n",
       "       [180.29239 ],\n",
       "       [207.63039 ],\n",
       "       [415.23877 ],\n",
       "       [170.3675  ],\n",
       "       [306.74255 ],\n",
       "       [251.54083 ],\n",
       "       [215.08463 ],\n",
       "       [210.51706 ],\n",
       "       [ 79.0093  ],\n",
       "       [152.49469 ],\n",
       "       [203.08345 ],\n",
       "       [137.9231  ],\n",
       "       [ 61.54017 ],\n",
       "       [180.89702 ],\n",
       "       [187.67218 ],\n",
       "       [ 65.513054],\n",
       "       [144.4398  ],\n",
       "       [ 93.15943 ],\n",
       "       [ 53.625973],\n",
       "       [213.38124 ],\n",
       "       [160.13531 ],\n",
       "       [210.18027 ],\n",
       "       [ 98.415306],\n",
       "       [188.54623 ],\n",
       "       [210.51321 ],\n",
       "       [465.96606 ],\n",
       "       [100.05829 ],\n",
       "       [157.01761 ],\n",
       "       [414.37372 ],\n",
       "       [335.94012 ],\n",
       "       [ 50.114254],\n",
       "       [547.4964  ],\n",
       "       [530.3478  ],\n",
       "       [209.32053 ],\n",
       "       [204.43158 ],\n",
       "       [ 93.64869 ],\n",
       "       [153.00778 ],\n",
       "       [ 82.6789  ],\n",
       "       [ 82.162704],\n",
       "       [509.26672 ],\n",
       "       [ 54.28235 ],\n",
       "       [ 60.034542],\n",
       "       [112.94473 ],\n",
       "       [ 83.96289 ],\n",
       "       [ 42.4265  ],\n",
       "       [356.63058 ],\n",
       "       [318.85098 ],\n",
       "       [139.69496 ],\n",
       "       [201.08945 ],\n",
       "       [437.22388 ],\n",
       "       [379.47235 ],\n",
       "       [444.5249  ],\n",
       "       [217.40865 ],\n",
       "       [234.1676  ],\n",
       "       [198.66672 ],\n",
       "       [222.93738 ],\n",
       "       [ 63.583755],\n",
       "       [108.51978 ],\n",
       "       [351.4131  ],\n",
       "       [440.32684 ],\n",
       "       [159.31622 ],\n",
       "       [103.319626],\n",
       "       [211.63351 ],\n",
       "       [151.81995 ],\n",
       "       [220.65703 ],\n",
       "       [115.16348 ],\n",
       "       [153.1676  ],\n",
       "       [200.74503 ],\n",
       "       [220.22356 ],\n",
       "       [202.15765 ],\n",
       "       [ 75.726906],\n",
       "       [ 74.11703 ],\n",
       "       [433.5261  ],\n",
       "       [271.74023 ],\n",
       "       [427.3383  ],\n",
       "       [290.31952 ],\n",
       "       [331.3852  ],\n",
       "       [368.5559  ],\n",
       "       [142.01854 ],\n",
       "       [340.28833 ],\n",
       "       [194.46753 ],\n",
       "       [341.98315 ],\n",
       "       [415.43552 ],\n",
       "       [369.67474 ],\n",
       "       [ 60.820225],\n",
       "       [413.18448 ],\n",
       "       [387.74216 ],\n",
       "       [155.28857 ],\n",
       "       [195.82382 ],\n",
       "       [204.66086 ],\n",
       "       [177.7648  ],\n",
       "       [240.91536 ],\n",
       "       [ 97.81653 ],\n",
       "       [184.96747 ],\n",
       "       [456.02606 ],\n",
       "       [383.79523 ],\n",
       "       [215.88522 ],\n",
       "       [561.778   ],\n",
       "       [207.52069 ],\n",
       "       [229.99698 ],\n",
       "       [548.0411  ],\n",
       "       [447.79727 ],\n",
       "       [424.4806  ],\n",
       "       [524.3222  ],\n",
       "       [434.47812 ],\n",
       "       [179.68323 ],\n",
       "       [182.4552  ],\n",
       "       [181.6506  ],\n",
       "       [155.91122 ],\n",
       "       [140.74292 ],\n",
       "       [197.05408 ],\n",
       "       [578.86273 ],\n",
       "       [161.52365 ],\n",
       "       [169.79987 ],\n",
       "       [113.30812 ],\n",
       "       [338.0014  ],\n",
       "       [ 90.35028 ],\n",
       "       [433.1021  ],\n",
       "       [330.72726 ],\n",
       "       [102.50719 ],\n",
       "       [194.06361 ],\n",
       "       [201.2749  ],\n",
       "       [418.1432  ],\n",
       "       [424.98358 ],\n",
       "       [266.60806 ],\n",
       "       [335.04993 ],\n",
       "       [394.86743 ],\n",
       "       [528.6772  ],\n",
       "       [175.795   ],\n",
       "       [413.41998 ],\n",
       "       [256.79523 ],\n",
       "       [440.2574  ],\n",
       "       [423.17838 ],\n",
       "       [249.64404 ],\n",
       "       [204.00092 ],\n",
       "       [210.33772 ],\n",
       "       [213.78377 ],\n",
       "       [450.28983 ],\n",
       "       [194.69522 ],\n",
       "       [186.31244 ],\n",
       "       [190.8125  ],\n",
       "       [173.23645 ],\n",
       "       [219.15974 ],\n",
       "       [406.70642 ],\n",
       "       [149.78236 ],\n",
       "       [331.3764  ],\n",
       "       [111.18307 ],\n",
       "       [ 55.071064],\n",
       "       [ 80.64192 ],\n",
       "       [ 83.446045],\n",
       "       [167.57599 ],\n",
       "       [229.49362 ],\n",
       "       [ 93.56363 ],\n",
       "       [365.58054 ],\n",
       "       [549.4071  ],\n",
       "       [157.49713 ],\n",
       "       [210.02704 ],\n",
       "       [456.60464 ],\n",
       "       [ 52.25643 ],\n",
       "       [194.41629 ],\n",
       "       [ 52.863117],\n",
       "       [248.83649 ],\n",
       "       [249.85236 ],\n",
       "       [401.25836 ],\n",
       "       [119.36061 ],\n",
       "       [ 70.66106 ],\n",
       "       [129.46841 ],\n",
       "       [320.6396  ],\n",
       "       [ 86.08963 ],\n",
       "       [175.07004 ],\n",
       "       [197.89336 ],\n",
       "       [257.28046 ],\n",
       "       [194.93231 ],\n",
       "       [ 92.00125 ],\n",
       "       [107.28505 ],\n",
       "       [144.43494 ],\n",
       "       [386.65717 ],\n",
       "       [433.63538 ],\n",
       "       [171.43124 ],\n",
       "       [305.31824 ],\n",
       "       [197.07451 ],\n",
       "       [213.09473 ],\n",
       "       [168.48184 ],\n",
       "       [205.94312 ],\n",
       "       [ 97.56881 ],\n",
       "       [194.25362 ],\n",
       "       [112.75345 ],\n",
       "       [230.4794  ],\n",
       "       [ 94.36884 ],\n",
       "       [174.20999 ],\n",
       "       [207.94826 ],\n",
       "       [447.07327 ],\n",
       "       [ 69.30382 ],\n",
       "       [141.35701 ],\n",
       "       [154.6493  ],\n",
       "       [182.44263 ],\n",
       "       [258.43234 ],\n",
       "       [430.59732 ],\n",
       "       [448.6768  ],\n",
       "       [139.6204  ],\n",
       "       [ 59.087276],\n",
       "       [400.03268 ],\n",
       "       [352.46313 ],\n",
       "       [159.76436 ],\n",
       "       [ 78.37677 ],\n",
       "       [520.368   ],\n",
       "       [234.69485 ],\n",
       "       [211.89282 ],\n",
       "       [239.73462 ],\n",
       "       [ 92.90773 ],\n",
       "       [122.85094 ],\n",
       "       [ 76.236145],\n",
       "       [325.93915 ],\n",
       "       [155.29973 ],\n",
       "       [107.951675],\n",
       "       [ 91.6836  ],\n",
       "       [199.8965  ],\n",
       "       [ 98.07656 ],\n",
       "       [ 96.667076],\n",
       "       [177.26721 ],\n",
       "       [104.25322 ],\n",
       "       [381.9376  ],\n",
       "       [489.8933  ],\n",
       "       [198.69714 ],\n",
       "       [ 57.068913],\n",
       "       [173.38101 ],\n",
       "       [137.07437 ],\n",
       "       [ 94.33805 ],\n",
       "       [ 77.998764],\n",
       "       [430.51425 ],\n",
       "       [246.7104  ],\n",
       "       [188.11897 ],\n",
       "       [315.2862  ],\n",
       "       [213.49066 ],\n",
       "       [396.7091  ],\n",
       "       [338.78293 ],\n",
       "       [392.53812 ],\n",
       "       [ 90.85628 ],\n",
       "       [446.9511  ],\n",
       "       [465.4096  ],\n",
       "       [524.9233  ],\n",
       "       [ 63.942146],\n",
       "       [203.45146 ],\n",
       "       [361.26035 ],\n",
       "       [173.30603 ],\n",
       "       [259.03754 ],\n",
       "       [163.94394 ],\n",
       "       [319.00708 ],\n",
       "       [460.76773 ],\n",
       "       [ 54.63291 ],\n",
       "       [460.3075  ],\n",
       "       [400.52112 ],\n",
       "       [399.46512 ],\n",
       "       [176.56848 ],\n",
       "       [359.60974 ],\n",
       "       [387.0302  ],\n",
       "       [ 68.847305],\n",
       "       [109.70752 ],\n",
       "       [443.45242 ],\n",
       "       [139.83025 ],\n",
       "       [147.76762 ],\n",
       "       [116.04649 ],\n",
       "       [ 77.45726 ],\n",
       "       [ 95.44869 ],\n",
       "       [172.25269 ],\n",
       "       [ 92.70474 ],\n",
       "       [247.04459 ],\n",
       "       [231.22723 ],\n",
       "       [236.108   ],\n",
       "       [168.28485 ],\n",
       "       [557.6163  ],\n",
       "       [324.98596 ],\n",
       "       [174.56946 ],\n",
       "       [ 72.70747 ],\n",
       "       [281.57242 ],\n",
       "       [246.37979 ],\n",
       "       [182.62198 ],\n",
       "       [206.00844 ],\n",
       "       [210.95113 ],\n",
       "       [258.93945 ],\n",
       "       [565.6817  ],\n",
       "       [219.5816  ],\n",
       "       [ 89.333954],\n",
       "       [227.45595 ],\n",
       "       [173.21884 ],\n",
       "       [122.7138  ],\n",
       "       [469.76212 ],\n",
       "       [155.43289 ],\n",
       "       [232.3573  ],\n",
       "       [170.76193 ],\n",
       "       [395.45093 ],\n",
       "       [214.11458 ],\n",
       "       [435.88104 ],\n",
       "       [164.16856 ],\n",
       "       [136.46487 ],\n",
       "       [423.42023 ],\n",
       "       [373.8127  ],\n",
       "       [128.70697 ],\n",
       "       [207.73668 ],\n",
       "       [186.73822 ],\n",
       "       [368.7639  ],\n",
       "       [417.151   ],\n",
       "       [386.17566 ],\n",
       "       [415.24835 ],\n",
       "       [434.98697 ],\n",
       "       [143.21661 ],\n",
       "       [504.63467 ],\n",
       "       [218.00601 ],\n",
       "       [252.2127  ],\n",
       "       [152.65417 ],\n",
       "       [182.03436 ],\n",
       "       [ 37.725037],\n",
       "       [199.33008 ],\n",
       "       [237.01666 ],\n",
       "       [126.87466 ],\n",
       "       [212.75946 ],\n",
       "       [ 80.05762 ],\n",
       "       [210.78854 ],\n",
       "       [247.36804 ],\n",
       "       [201.50021 ],\n",
       "       [335.88516 ],\n",
       "       [ 54.295994],\n",
       "       [213.78946 ],\n",
       "       [330.93573 ],\n",
       "       [ 64.64043 ],\n",
       "       [ 61.41445 ],\n",
       "       [312.4168  ],\n",
       "       [ 52.066654],\n",
       "       [459.51917 ],\n",
       "       [383.2283  ],\n",
       "       [500.5954  ],\n",
       "       [165.89073 ],\n",
       "       [182.89107 ],\n",
       "       [ 75.20189 ],\n",
       "       [222.99748 ],\n",
       "       [142.94958 ],\n",
       "       [191.35837 ],\n",
       "       [188.38194 ],\n",
       "       [455.56625 ],\n",
       "       [111.977264],\n",
       "       [ 81.529045],\n",
       "       [397.8982  ],\n",
       "       [338.49664 ],\n",
       "       [166.33423 ],\n",
       "       [173.1927  ],\n",
       "       [233.7894  ],\n",
       "       [ 42.790833],\n",
       "       [423.10184 ],\n",
       "       [222.30719 ],\n",
       "       [514.2171  ],\n",
       "       [ 51.999203],\n",
       "       [437.11365 ],\n",
       "       [239.26752 ],\n",
       "       [113.417435],\n",
       "       [351.51526 ],\n",
       "       [186.9158  ],\n",
       "       [466.36945 ],\n",
       "       [290.05103 ],\n",
       "       [132.28442 ],\n",
       "       [396.1631  ],\n",
       "       [ 81.88468 ],\n",
       "       [204.45796 ],\n",
       "       [210.96127 ],\n",
       "       [ 73.722275],\n",
       "       [167.47137 ],\n",
       "       [419.54266 ],\n",
       "       [208.85277 ],\n",
       "       [174.27228 ],\n",
       "       [107.7222  ],\n",
       "       [156.03029 ],\n",
       "       [206.1576  ],\n",
       "       [188.51599 ],\n",
       "       [135.16158 ],\n",
       "       [468.65787 ],\n",
       "       [254.33273 ],\n",
       "       [ 80.250275],\n",
       "       [150.66104 ],\n",
       "       [179.4578  ],\n",
       "       [128.72041 ],\n",
       "       [ 77.084015],\n",
       "       [128.46251 ],\n",
       "       [204.03394 ],\n",
       "       [253.22108 ],\n",
       "       [408.04102 ],\n",
       "       [218.88422 ],\n",
       "       [425.22824 ],\n",
       "       [361.4726  ],\n",
       "       [222.01678 ],\n",
       "       [162.33163 ],\n",
       "       [159.28668 ],\n",
       "       [ 82.980865],\n",
       "       [ 85.07941 ],\n",
       "       [158.04736 ],\n",
       "       [234.1456  ],\n",
       "       [266.7356  ],\n",
       "       [ 52.869423],\n",
       "       [158.97137 ],\n",
       "       [ 84.46469 ],\n",
       "       [202.06633 ],\n",
       "       [157.55112 ],\n",
       "       [211.19342 ],\n",
       "       [257.69266 ],\n",
       "       [ 74.17545 ],\n",
       "       [171.14557 ],\n",
       "       [175.76228 ],\n",
       "       [370.94766 ],\n",
       "       [201.72644 ],\n",
       "       [ 39.745796],\n",
       "       [ 58.733128],\n",
       "       [366.63843 ],\n",
       "       [447.4835  ],\n",
       "       [200.48544 ],\n",
       "       [344.4508  ],\n",
       "       [213.23465 ],\n",
       "       [182.84662 ],\n",
       "       [214.55478 ],\n",
       "       [386.6138  ],\n",
       "       [134.80547 ],\n",
       "       [195.90947 ],\n",
       "       [148.54205 ],\n",
       "       [422.84546 ],\n",
       "       [ 51.277935],\n",
       "       [422.20142 ],\n",
       "       [ 94.373985],\n",
       "       [411.15955 ],\n",
       "       [244.22684 ],\n",
       "       [257.86197 ],\n",
       "       [361.373   ],\n",
       "       [231.4841  ],\n",
       "       [219.71704 ],\n",
       "       [452.3695  ],\n",
       "       [203.7473  ],\n",
       "       [ 67.54126 ],\n",
       "       [188.63373 ],\n",
       "       [347.45892 ],\n",
       "       [ 86.51808 ],\n",
       "       [227.06197 ],\n",
       "       [414.12692 ],\n",
       "       [335.14542 ],\n",
       "       [171.01611 ],\n",
       "       [ 66.87251 ],\n",
       "       [239.94351 ],\n",
       "       [ 53.938953],\n",
       "       [239.60417 ],\n",
       "       [ 53.17544 ],\n",
       "       [189.0611  ],\n",
       "       [107.01341 ],\n",
       "       [400.48013 ],\n",
       "       [250.5404  ],\n",
       "       [179.733   ],\n",
       "       [155.71161 ],\n",
       "       [144.07945 ],\n",
       "       [156.54642 ],\n",
       "       [133.92505 ],\n",
       "       [279.6244  ],\n",
       "       [386.06583 ],\n",
       "       [239.28671 ],\n",
       "       [171.52441 ],\n",
       "       [137.00322 ],\n",
       "       [455.68933 ],\n",
       "       [147.45761 ],\n",
       "       [510.77902 ],\n",
       "       [251.42514 ],\n",
       "       [159.94531 ],\n",
       "       [217.80057 ],\n",
       "       [ 96.021866],\n",
       "       [139.35846 ],\n",
       "       [211.59799 ],\n",
       "       [367.729   ],\n",
       "       [429.2738  ],\n",
       "       [108.969406],\n",
       "       [114.5213  ],\n",
       "       [183.95615 ],\n",
       "       [ 87.87697 ],\n",
       "       [516.9567  ],\n",
       "       [186.11194 ],\n",
       "       [107.06177 ],\n",
       "       [358.52008 ],\n",
       "       [244.74207 ],\n",
       "       [452.328   ],\n",
       "       [121.18526 ],\n",
       "       [138.49329 ],\n",
       "       [104.01097 ],\n",
       "       [257.58783 ],\n",
       "       [213.46794 ],\n",
       "       [561.32465 ],\n",
       "       [572.27    ],\n",
       "       [106.55514 ],\n",
       "       [ 85.506996],\n",
       "       [202.83275 ],\n",
       "       [202.97195 ],\n",
       "       [112.822464],\n",
       "       [444.5357  ],\n",
       "       [222.9412  ],\n",
       "       [ 54.08428 ],\n",
       "       [ 38.050163],\n",
       "       [166.62296 ],\n",
       "       [ 86.277306],\n",
       "       [333.37244 ],\n",
       "       [337.0853  ],\n",
       "       [254.39015 ],\n",
       "       [203.57954 ],\n",
       "       [174.38876 ],\n",
       "       [ 73.264244],\n",
       "       [215.71866 ],\n",
       "       [329.30035 ],\n",
       "       [499.04507 ],\n",
       "       [ 81.65787 ],\n",
       "       [164.18326 ],\n",
       "       [212.26752 ],\n",
       "       [436.74124 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.78</td>\n",
       "      <td>53.165981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.74</td>\n",
       "      <td>165.909821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246.79</td>\n",
       "      <td>234.891754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187.82</td>\n",
       "      <td>186.915985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201.62</td>\n",
       "      <td>200.988892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>496.39</td>\n",
       "      <td>499.045074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>85.95</td>\n",
       "      <td>81.657867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>166.01</td>\n",
       "      <td>164.183258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>207.90</td>\n",
       "      <td>212.267517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>434.45</td>\n",
       "      <td>436.741241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual   Predicted\n",
       "0     48.78   53.165981\n",
       "1    168.74  165.909821\n",
       "2    246.79  234.891754\n",
       "3    187.82  186.915985\n",
       "4    201.62  200.988892\n",
       "..      ...         ...\n",
       "970  496.39  499.045074\n",
       "971   85.95   81.657867\n",
       "972  166.01  164.183258\n",
       "973  207.90  212.267517\n",
       "974  434.45  436.741241\n",
       "\n",
       "[975 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions.flatten()\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.376630204107822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate the residuals (errors)\n",
    "comparison_df['Error'] = abs(comparison_df['Actual'] - comparison_df['Predicted'])\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(comparison_df['Actual'], comparison_df['Predicted'])\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot individual absolute errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(comparison_df)), comparison_df['Error'])\n",
    "plt.title('Absolute Errors for Predictions')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
