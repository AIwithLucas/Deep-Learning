{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/n6ycqzwx13989g40ry2djdsh0000gn/T/ipykernel_44739/3149537304.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_float.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/fc/n6ycqzwx13989g40ry2djdsh0000gn/T/ipykernel_44739/3149537304.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_float.fillna(df_float.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/stocks-COMBINED-Jan2020-Dec2024.csv\")\n",
    "\n",
    "float_columns = df.select_dtypes(include=['float64']).columns\n",
    "df_float = df[float_columns]\n",
    "\n",
    "df_float.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_float.fillna(df_float.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df_float.drop(['Open', 'High', 'Low'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Change %</th>\n",
       "      <th>EPS</th>\n",
       "      <th>Price_Return</th>\n",
       "      <th>Log_Return</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Moving_Average</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>Sharpe_Ratio</th>\n",
       "      <th>Max_Drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.86</td>\n",
       "      <td>39030000.0</td>\n",
       "      <td>-0.0153</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>143.830</td>\n",
       "      <td>411.212815</td>\n",
       "      <td>0.728616</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.07</td>\n",
       "      <td>44390000.0</td>\n",
       "      <td>-0.0305</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.018745</td>\n",
       "      <td>142.680</td>\n",
       "      <td>310.200927</td>\n",
       "      <td>0.792438</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148.60</td>\n",
       "      <td>87700000.0</td>\n",
       "      <td>-0.1062</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.031443</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.019660</td>\n",
       "      <td>143.380</td>\n",
       "      <td>247.665581</td>\n",
       "      <td>1.194777</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166.25</td>\n",
       "      <td>71900000.0</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.118775</td>\n",
       "      <td>0.112235</td>\n",
       "      <td>0.032259</td>\n",
       "      <td>148.298</td>\n",
       "      <td>148.096633</td>\n",
       "      <td>2.904362</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159.92</td>\n",
       "      <td>36010000.0</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.038075</td>\n",
       "      <td>-0.038819</td>\n",
       "      <td>0.033763</td>\n",
       "      <td>152.140</td>\n",
       "      <td>161.710631</td>\n",
       "      <td>3.705724</td>\n",
       "      <td>-0.010539</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price        Vol.  Change %  EPS  Price_Return  Log_Return  Volatility  \\\n",
       "0  141.86  39030000.0   -0.0153  0.7      0.008173    0.008140    0.020197   \n",
       "1  144.07  44390000.0   -0.0305  0.7      0.015579    0.015459    0.018745   \n",
       "2  148.60  87700000.0   -0.1062  0.7      0.031443    0.030959    0.019660   \n",
       "3  166.25  71900000.0    0.0396  0.7      0.118775    0.112235    0.032259   \n",
       "4  159.92  36010000.0    0.0236  0.7     -0.038075   -0.038819    0.033763   \n",
       "\n",
       "   Moving_Average         RSI      MACD  Sharpe_Ratio  Max_Drawdown  \n",
       "0         143.830  411.212815  0.728616     -0.010539     -0.000235  \n",
       "1         142.680  310.200927  0.792438     -0.010539     -0.000235  \n",
       "2         143.380  247.665581  1.194777     -0.010539     -0.000235  \n",
       "3         148.298  148.096633  2.904362     -0.010539     -0.000235  \n",
       "4         152.140  161.710631  3.705724     -0.010539     -0.000235  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_float.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_float.drop(['Price'], axis=1).values\n",
    "y  = df_float['Price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scalar = StandardScaler()\n",
    "\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feedforward Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 100 epochs\n",
      "Epoch 1/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 67328.5547 - mae: 224.9558 - val_loss: 56539.7891 - val_mae: 205.9457\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 0s 606us/step - loss: 32324.3379 - mae: 150.6947 - val_loss: 8974.3887 - val_mae: 79.8327\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 0s 715us/step - loss: 5556.7529 - mae: 57.8313 - val_loss: 3126.1799 - val_mae: 42.3481\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 0s 622us/step - loss: 3168.7871 - mae: 42.5325 - val_loss: 2055.1318 - val_mae: 33.7816\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 0s 603us/step - loss: 2448.5405 - mae: 37.5585 - val_loss: 1601.3794 - val_mae: 29.8976\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 0s 557us/step - loss: 2102.8127 - mae: 34.6340 - val_loss: 1387.3743 - val_mae: 27.8549\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 0s 554us/step - loss: 1969.9646 - mae: 33.6601 - val_loss: 1206.4191 - val_mae: 25.9367\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 0s 554us/step - loss: 1724.8667 - mae: 32.4537 - val_loss: 1108.4801 - val_mae: 24.7898\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 0s 549us/step - loss: 1736.0316 - mae: 32.3319 - val_loss: 997.4844 - val_mae: 23.4676\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 0s 582us/step - loss: 1599.5231 - mae: 30.9179 - val_loss: 937.9937 - val_mae: 23.1422\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 0s 554us/step - loss: 1482.7395 - mae: 30.0542 - val_loss: 811.6826 - val_mae: 21.1769\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 0s 565us/step - loss: 1354.1515 - mae: 28.8495 - val_loss: 768.6937 - val_mae: 20.5623\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 0s 557us/step - loss: 1287.2892 - mae: 27.9196 - val_loss: 677.5590 - val_mae: 19.6453\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 0s 557us/step - loss: 1257.8462 - mae: 27.8431 - val_loss: 599.8137 - val_mae: 18.3832\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 0s 552us/step - loss: 1197.2649 - mae: 26.7000 - val_loss: 542.3536 - val_mae: 17.7547\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 0s 545us/step - loss: 1121.9391 - mae: 26.0921 - val_loss: 481.0423 - val_mae: 16.3225\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 0s 542us/step - loss: 1063.3962 - mae: 25.2973 - val_loss: 430.3988 - val_mae: 15.4450\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 0s 550us/step - loss: 958.4023 - mae: 24.0205 - val_loss: 374.8300 - val_mae: 14.4345\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 0s 545us/step - loss: 938.2870 - mae: 23.7462 - val_loss: 365.0052 - val_mae: 14.1816\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 0s 541us/step - loss: 903.8026 - mae: 23.1262 - val_loss: 318.8636 - val_mae: 13.5187\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 0s 546us/step - loss: 893.5411 - mae: 23.0862 - val_loss: 290.4563 - val_mae: 13.1230\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 0s 550us/step - loss: 818.9848 - mae: 22.1303 - val_loss: 246.5138 - val_mae: 11.8826\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 0s 555us/step - loss: 804.6246 - mae: 21.8615 - val_loss: 220.6419 - val_mae: 11.0597\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 0s 554us/step - loss: 768.8530 - mae: 21.2343 - val_loss: 198.4423 - val_mae: 10.5729\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 0s 543us/step - loss: 743.3286 - mae: 20.9766 - val_loss: 182.6003 - val_mae: 9.7572\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 0s 543us/step - loss: 724.8145 - mae: 20.3889 - val_loss: 158.3281 - val_mae: 9.5443\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 0s 541us/step - loss: 681.5073 - mae: 19.8023 - val_loss: 149.6773 - val_mae: 9.1315\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 0s 542us/step - loss: 691.7515 - mae: 19.9157 - val_loss: 140.9203 - val_mae: 8.6326\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 0s 543us/step - loss: 661.5567 - mae: 19.4641 - val_loss: 122.8867 - val_mae: 8.2931\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 0s 536us/step - loss: 643.0591 - mae: 18.9655 - val_loss: 95.2135 - val_mae: 7.1846\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 619.4688 - mae: 18.6315 - val_loss: 94.4472 - val_mae: 7.1142\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 630.6804 - mae: 18.5885 - val_loss: 81.3738 - val_mae: 6.6123\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 591.4821 - mae: 18.1719 - val_loss: 83.9047 - val_mae: 6.7785\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 590.2309 - mae: 17.9055 - val_loss: 65.9131 - val_mae: 5.9760\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 611.0608 - mae: 18.2586 - val_loss: 82.0544 - val_mae: 7.3224\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 0s 552us/step - loss: 561.4846 - mae: 17.5545 - val_loss: 62.0907 - val_mae: 6.2430\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 584.8281 - mae: 17.7980 - val_loss: 48.6891 - val_mae: 5.2577\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 0s 536us/step - loss: 598.7039 - mae: 17.8204 - val_loss: 66.2196 - val_mae: 6.5179\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 567.6839 - mae: 17.2005 - val_loss: 44.9805 - val_mae: 5.2358\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 548.3062 - mae: 17.1915 - val_loss: 44.0789 - val_mae: 4.9543\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 0s 543us/step - loss: 564.2593 - mae: 17.0763 - val_loss: 46.1198 - val_mae: 5.3796\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 543.1331 - mae: 16.9778 - val_loss: 43.5103 - val_mae: 4.9950\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 0s 541us/step - loss: 545.6217 - mae: 16.8236 - val_loss: 44.4057 - val_mae: 5.1021\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 0s 546us/step - loss: 554.2662 - mae: 16.9249 - val_loss: 47.0872 - val_mae: 5.3878\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 0s 542us/step - loss: 491.7693 - mae: 15.9700 - val_loss: 61.1410 - val_mae: 6.2068\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 0s 543us/step - loss: 498.7158 - mae: 16.0992 - val_loss: 33.9358 - val_mae: 4.4586\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 0s 538us/step - loss: 524.3046 - mae: 16.3736 - val_loss: 34.0364 - val_mae: 4.4119\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 527.5879 - mae: 16.1941 - val_loss: 31.9520 - val_mae: 4.3451\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 526.1337 - mae: 16.3729 - val_loss: 36.2935 - val_mae: 4.4696\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 0s 530us/step - loss: 515.9959 - mae: 16.3417 - val_loss: 35.1194 - val_mae: 4.5583\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 0s 541us/step - loss: 484.0748 - mae: 15.7268 - val_loss: 27.5817 - val_mae: 3.9295\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 501.3389 - mae: 15.9721 - val_loss: 36.3210 - val_mae: 4.7098\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 500.8403 - mae: 15.7695 - val_loss: 42.2708 - val_mae: 4.9583\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 502.5146 - mae: 15.8974 - val_loss: 38.3515 - val_mae: 4.7879\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 0s 541us/step - loss: 494.3624 - mae: 15.7645 - val_loss: 28.1035 - val_mae: 3.9038\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 0s 562us/step - loss: 483.5348 - mae: 15.5716 - val_loss: 29.1214 - val_mae: 4.0887\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 497.6863 - mae: 15.8449 - val_loss: 40.4365 - val_mae: 4.8544\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 482.2188 - mae: 15.5203 - val_loss: 29.4256 - val_mae: 3.9691\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 482.4535 - mae: 15.4475 - val_loss: 33.3802 - val_mae: 4.5178\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 0s 537us/step - loss: 479.1014 - mae: 15.3670 - val_loss: 30.8462 - val_mae: 4.2338\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 0s 542us/step - loss: 473.1111 - mae: 15.4079 - val_loss: 24.9261 - val_mae: 3.6995\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 472.2359 - mae: 15.4297 - val_loss: 24.9550 - val_mae: 3.7971\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 0s 541us/step - loss: 507.8018 - mae: 15.6346 - val_loss: 31.5563 - val_mae: 4.3278\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 0s 542us/step - loss: 475.0479 - mae: 15.2565 - val_loss: 38.0246 - val_mae: 4.4790\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 483.4360 - mae: 15.4474 - val_loss: 26.7741 - val_mae: 3.8642\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 0s 536us/step - loss: 464.4050 - mae: 15.2222 - val_loss: 29.2850 - val_mae: 4.1067\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 0s 571us/step - loss: 473.9826 - mae: 15.1917 - val_loss: 34.2871 - val_mae: 4.5612\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 453.6302 - mae: 14.8690 - val_loss: 24.8725 - val_mae: 3.7756\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 0s 539us/step - loss: 439.0272 - mae: 14.7531 - val_loss: 27.9048 - val_mae: 3.7651\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 0s 538us/step - loss: 453.5544 - mae: 14.7550 - val_loss: 37.7638 - val_mae: 4.9818\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 0s 534us/step - loss: 447.0837 - mae: 14.7598 - val_loss: 29.0663 - val_mae: 4.1777\n",
      "Epoch 72/100\n",
      "122/122 [==============================] - 0s 560us/step - loss: 442.5034 - mae: 14.5693 - val_loss: 21.9843 - val_mae: 3.4217\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 0s 566us/step - loss: 456.1054 - mae: 14.9516 - val_loss: 28.9968 - val_mae: 3.9624\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 0s 550us/step - loss: 469.0913 - mae: 15.1091 - val_loss: 33.4062 - val_mae: 4.1176\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 0s 558us/step - loss: 446.2958 - mae: 14.6013 - val_loss: 26.6668 - val_mae: 3.8435\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 0s 553us/step - loss: 423.7797 - mae: 14.4093 - val_loss: 31.4429 - val_mae: 4.2683\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 0s 548us/step - loss: 421.1933 - mae: 14.4653 - val_loss: 30.7350 - val_mae: 4.1620\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 0s 547us/step - loss: 427.1198 - mae: 14.3317 - val_loss: 22.2224 - val_mae: 3.4016\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 0s 555us/step - loss: 426.7040 - mae: 14.3485 - val_loss: 34.5962 - val_mae: 4.4750\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 0s 550us/step - loss: 433.5744 - mae: 14.4899 - val_loss: 23.8827 - val_mae: 3.4628\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 0s 549us/step - loss: 431.7483 - mae: 14.3545 - val_loss: 23.1875 - val_mae: 3.4090\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 0s 555us/step - loss: 426.2196 - mae: 14.1821 - val_loss: 31.3271 - val_mae: 4.1108\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 0s 554us/step - loss: 447.2818 - mae: 14.5938 - val_loss: 24.8877 - val_mae: 3.7374\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 0s 543us/step - loss: 416.8185 - mae: 14.1554 - val_loss: 31.6239 - val_mae: 4.3483\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 0s 562us/step - loss: 439.5704 - mae: 14.2538 - val_loss: 29.4741 - val_mae: 3.9459\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 0s 553us/step - loss: 412.1956 - mae: 13.9456 - val_loss: 23.2671 - val_mae: 3.3613\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 0s 550us/step - loss: 427.0998 - mae: 14.3439 - val_loss: 23.2128 - val_mae: 3.3903\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 0s 548us/step - loss: 421.9722 - mae: 14.3273 - val_loss: 26.7839 - val_mae: 3.7906\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 0s 548us/step - loss: 415.6349 - mae: 14.1363 - val_loss: 26.4630 - val_mae: 3.6527\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 0s 547us/step - loss: 405.9524 - mae: 14.0536 - val_loss: 27.4517 - val_mae: 3.9637\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 0s 547us/step - loss: 398.0398 - mae: 13.7811 - val_loss: 27.4337 - val_mae: 3.8615\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 0s 540us/step - loss: 407.1465 - mae: 13.8919 - val_loss: 26.9087 - val_mae: 3.7714\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 0s 546us/step - loss: 391.2864 - mae: 13.6025 - val_loss: 28.8118 - val_mae: 3.9547\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 0s 552us/step - loss: 396.0238 - mae: 13.7154 - val_loss: 22.7427 - val_mae: 3.4525\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 0s 551us/step - loss: 416.2145 - mae: 13.9536 - val_loss: 25.7229 - val_mae: 3.5815\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 0s 545us/step - loss: 419.9895 - mae: 14.0730 - val_loss: 23.0491 - val_mae: 3.4738\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 0s 544us/step - loss: 395.9539 - mae: 13.6578 - val_loss: 29.2360 - val_mae: 4.1869\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 0s 553us/step - loss: 388.8006 - mae: 13.5382 - val_loss: 28.1224 - val_mae: 4.0996\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 0s 550us/step - loss: 387.9667 - mae: 13.4583 - val_loss: 26.6695 - val_mae: 3.5398\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 0s 549us/step - loss: 399.1122 - mae: 13.6502 - val_loss: 27.1403 - val_mae: 3.7399\n",
      "31/31 [==============================] - 0s 307us/step\n",
      "Epochs: 100 | MAE: 3.7399122877854563\n",
      "Training model with 150 epochs\n",
      "Epoch 1/150\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 66033.6797 - mae: 222.6225 - val_loss: 53834.8477 - val_mae: 201.7627\n",
      "Epoch 2/150\n",
      "122/122 [==============================] - 0s 557us/step - loss: 30668.6562 - mae: 148.4413 - val_loss: 8659.9775 - val_mae: 79.9248\n",
      "Epoch 3/150\n",
      "122/122 [==============================] - 0s 553us/step - loss: 5132.2168 - mae: 56.6097 - val_loss: 2884.8064 - val_mae: 40.5494\n",
      "Epoch 4/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 2973.9194 - mae: 40.8941 - val_loss: 1973.0793 - val_mae: 32.9162\n",
      "Epoch 5/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 2390.0574 - mae: 37.1028 - val_loss: 1554.2596 - val_mae: 29.1339\n",
      "Epoch 6/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 1985.0771 - mae: 34.2582 - val_loss: 1355.4818 - val_mae: 27.1155\n",
      "Epoch 7/150\n",
      "122/122 [==============================] - 0s 549us/step - loss: 1935.0994 - mae: 33.7810 - val_loss: 1219.7770 - val_mae: 25.9566\n",
      "Epoch 8/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 1745.9893 - mae: 32.4034 - val_loss: 1057.8330 - val_mae: 24.2471\n",
      "Epoch 9/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 1647.8989 - mae: 31.7467 - val_loss: 977.0527 - val_mae: 23.3242\n",
      "Epoch 10/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 1595.9688 - mae: 30.6878 - val_loss: 887.7448 - val_mae: 22.1753\n",
      "Epoch 11/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 1509.1251 - mae: 30.1086 - val_loss: 813.1478 - val_mae: 21.2031\n",
      "Epoch 12/150\n",
      "122/122 [==============================] - 0s 570us/step - loss: 1369.3145 - mae: 28.9923 - val_loss: 758.7890 - val_mae: 20.7410\n",
      "Epoch 13/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 1280.4622 - mae: 27.9951 - val_loss: 675.1309 - val_mae: 19.3457\n",
      "Epoch 14/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 1292.6981 - mae: 28.1683 - val_loss: 628.5216 - val_mae: 18.7734\n",
      "Epoch 15/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 1168.3972 - mae: 26.8339 - val_loss: 544.3196 - val_mae: 17.3555\n",
      "Epoch 16/150\n",
      "122/122 [==============================] - 0s 549us/step - loss: 1103.1610 - mae: 25.9236 - val_loss: 531.2534 - val_mae: 17.7937\n",
      "Epoch 17/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 1054.8252 - mae: 25.2317 - val_loss: 471.3230 - val_mae: 16.4340\n",
      "Epoch 18/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 962.8873 - mae: 24.3528 - val_loss: 411.4911 - val_mae: 15.3078\n",
      "Epoch 19/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 966.9181 - mae: 24.1491 - val_loss: 353.2333 - val_mae: 14.0196\n",
      "Epoch 20/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 905.8793 - mae: 23.5487 - val_loss: 326.5933 - val_mae: 13.5970\n",
      "Epoch 21/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 883.2703 - mae: 22.9036 - val_loss: 304.3060 - val_mae: 13.1062\n",
      "Epoch 22/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 884.6005 - mae: 23.1088 - val_loss: 269.0874 - val_mae: 12.0553\n",
      "Epoch 23/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 799.3994 - mae: 21.9356 - val_loss: 243.9178 - val_mae: 11.5378\n",
      "Epoch 24/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 785.6148 - mae: 21.4822 - val_loss: 225.6078 - val_mae: 11.7246\n",
      "Epoch 25/150\n",
      "122/122 [==============================] - 0s 556us/step - loss: 781.5016 - mae: 21.5017 - val_loss: 201.2263 - val_mae: 10.9981\n",
      "Epoch 26/150\n",
      "122/122 [==============================] - 0s 663us/step - loss: 770.3434 - mae: 21.2711 - val_loss: 166.5748 - val_mae: 9.8304\n",
      "Epoch 27/150\n",
      "122/122 [==============================] - 0s 553us/step - loss: 731.4284 - mae: 20.6444 - val_loss: 160.9503 - val_mae: 9.8647\n",
      "Epoch 28/150\n",
      "122/122 [==============================] - 0s 553us/step - loss: 715.7377 - mae: 20.1793 - val_loss: 135.2899 - val_mae: 8.8089\n",
      "Epoch 29/150\n",
      "122/122 [==============================] - 0s 555us/step - loss: 666.9281 - mae: 19.6216 - val_loss: 120.9778 - val_mae: 8.4231\n",
      "Epoch 30/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 678.7988 - mae: 19.4544 - val_loss: 114.0809 - val_mae: 8.2962\n",
      "Epoch 31/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 647.7640 - mae: 19.1717 - val_loss: 97.2343 - val_mae: 7.3586\n",
      "Epoch 32/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 636.5256 - mae: 18.5439 - val_loss: 88.4870 - val_mae: 7.1137\n",
      "Epoch 33/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 601.8379 - mae: 18.3103 - val_loss: 86.8445 - val_mae: 7.3479\n",
      "Epoch 34/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 625.7502 - mae: 18.5600 - val_loss: 68.2580 - val_mae: 6.3504\n",
      "Epoch 35/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 610.0331 - mae: 18.2378 - val_loss: 65.0441 - val_mae: 5.9606\n",
      "Epoch 36/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 597.2509 - mae: 18.0532 - val_loss: 57.0359 - val_mae: 5.8952\n",
      "Epoch 37/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 569.6852 - mae: 17.5859 - val_loss: 57.1785 - val_mae: 5.6088\n",
      "Epoch 38/150\n",
      "122/122 [==============================] - 0s 551us/step - loss: 565.9565 - mae: 17.4343 - val_loss: 52.1548 - val_mae: 5.3929\n",
      "Epoch 39/150\n",
      "122/122 [==============================] - 0s 535us/step - loss: 557.1780 - mae: 17.1081 - val_loss: 52.3721 - val_mae: 5.7167\n",
      "Epoch 40/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 540.6685 - mae: 16.8299 - val_loss: 47.8256 - val_mae: 5.0594\n",
      "Epoch 41/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 518.2774 - mae: 16.4694 - val_loss: 51.7892 - val_mae: 5.6001\n",
      "Epoch 42/150\n",
      "122/122 [==============================] - 0s 533us/step - loss: 568.7043 - mae: 17.2028 - val_loss: 53.1779 - val_mae: 5.3100\n",
      "Epoch 43/150\n",
      "122/122 [==============================] - 0s 543us/step - loss: 541.9580 - mae: 16.6683 - val_loss: 36.5969 - val_mae: 4.5505\n",
      "Epoch 44/150\n",
      "122/122 [==============================] - 0s 536us/step - loss: 526.8114 - mae: 16.5851 - val_loss: 46.7145 - val_mae: 5.1433\n",
      "Epoch 45/150\n",
      "122/122 [==============================] - 0s 533us/step - loss: 534.1849 - mae: 16.6655 - val_loss: 34.1261 - val_mae: 4.2532\n",
      "Epoch 46/150\n",
      "122/122 [==============================] - 0s 534us/step - loss: 508.3891 - mae: 16.2951 - val_loss: 34.7952 - val_mae: 4.3621\n",
      "Epoch 47/150\n",
      "122/122 [==============================] - 0s 536us/step - loss: 494.7464 - mae: 15.8995 - val_loss: 36.9246 - val_mae: 4.4065\n",
      "Epoch 48/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 504.3541 - mae: 16.0464 - val_loss: 32.3421 - val_mae: 4.3149\n",
      "Epoch 49/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 525.2816 - mae: 16.2039 - val_loss: 32.5943 - val_mae: 4.1499\n",
      "Epoch 50/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 529.3229 - mae: 16.3395 - val_loss: 28.3112 - val_mae: 3.8214\n",
      "Epoch 51/150\n",
      "122/122 [==============================] - 0s 537us/step - loss: 516.6210 - mae: 15.8991 - val_loss: 28.8111 - val_mae: 3.9558\n",
      "Epoch 52/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 513.6363 - mae: 15.9960 - val_loss: 30.3517 - val_mae: 4.0773\n",
      "Epoch 53/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 483.1902 - mae: 15.5204 - val_loss: 27.8889 - val_mae: 3.9179\n",
      "Epoch 54/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 478.2524 - mae: 15.6299 - val_loss: 25.1151 - val_mae: 3.6178\n",
      "Epoch 55/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 499.3309 - mae: 15.7696 - val_loss: 28.7866 - val_mae: 3.8890\n",
      "Epoch 56/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 474.1789 - mae: 15.3907 - val_loss: 24.5741 - val_mae: 3.5076\n",
      "Epoch 57/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 482.4309 - mae: 15.5478 - val_loss: 37.7374 - val_mae: 4.5533\n",
      "Epoch 58/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 482.3218 - mae: 15.3798 - val_loss: 27.9348 - val_mae: 3.8683\n",
      "Epoch 59/150\n",
      "122/122 [==============================] - 0s 534us/step - loss: 465.2229 - mae: 15.1599 - val_loss: 29.2016 - val_mae: 4.1241\n",
      "Epoch 60/150\n",
      "122/122 [==============================] - 0s 537us/step - loss: 471.5758 - mae: 15.1502 - val_loss: 23.2142 - val_mae: 3.3982\n",
      "Epoch 61/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 439.5170 - mae: 14.8075 - val_loss: 38.1824 - val_mae: 4.6922\n",
      "Epoch 62/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 486.2982 - mae: 15.3113 - val_loss: 27.1898 - val_mae: 3.9254\n",
      "Epoch 63/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 431.7464 - mae: 14.7857 - val_loss: 29.5648 - val_mae: 4.1900\n",
      "Epoch 64/150\n",
      "122/122 [==============================] - 0s 533us/step - loss: 464.3391 - mae: 15.1971 - val_loss: 28.5039 - val_mae: 4.0305\n",
      "Epoch 65/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 461.8825 - mae: 14.9534 - val_loss: 38.1647 - val_mae: 4.7652\n",
      "Epoch 66/150\n",
      "122/122 [==============================] - 0s 536us/step - loss: 468.0899 - mae: 15.0470 - val_loss: 25.3087 - val_mae: 3.5247\n",
      "Epoch 67/150\n",
      "122/122 [==============================] - 0s 569us/step - loss: 483.7232 - mae: 15.2569 - val_loss: 22.8915 - val_mae: 3.5386\n",
      "Epoch 68/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 438.2833 - mae: 14.5271 - val_loss: 25.2102 - val_mae: 3.5544\n",
      "Epoch 69/150\n",
      "122/122 [==============================] - 0s 534us/step - loss: 418.2415 - mae: 14.3795 - val_loss: 30.7671 - val_mae: 4.0650\n",
      "Epoch 70/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 451.6077 - mae: 14.5828 - val_loss: 22.6720 - val_mae: 3.3633\n",
      "Epoch 71/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 428.0054 - mae: 14.4878 - val_loss: 30.0753 - val_mae: 4.1068\n",
      "Epoch 72/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 424.7234 - mae: 14.5706 - val_loss: 32.0393 - val_mae: 4.3320\n",
      "Epoch 73/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 423.7584 - mae: 14.2031 - val_loss: 31.7418 - val_mae: 4.2699\n",
      "Epoch 74/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 449.7839 - mae: 14.7810 - val_loss: 26.6474 - val_mae: 3.7115\n",
      "Epoch 75/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 419.7162 - mae: 14.3031 - val_loss: 27.7989 - val_mae: 3.8461\n",
      "Epoch 76/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 449.2281 - mae: 14.6180 - val_loss: 26.7199 - val_mae: 3.7429\n",
      "Epoch 77/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 419.0533 - mae: 14.0645 - val_loss: 29.0686 - val_mae: 4.0061\n",
      "Epoch 78/150\n",
      "122/122 [==============================] - 0s 532us/step - loss: 437.2388 - mae: 14.4183 - val_loss: 35.6590 - val_mae: 4.4913\n",
      "Epoch 79/150\n",
      "122/122 [==============================] - 0s 537us/step - loss: 421.0644 - mae: 14.1962 - val_loss: 33.2020 - val_mae: 4.4412\n",
      "Epoch 80/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 412.0690 - mae: 14.0961 - val_loss: 62.7337 - val_mae: 6.4559\n",
      "Epoch 81/150\n",
      "122/122 [==============================] - 0s 535us/step - loss: 398.1311 - mae: 14.0270 - val_loss: 29.2122 - val_mae: 3.8633\n",
      "Epoch 82/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 433.0728 - mae: 14.2998 - val_loss: 28.5014 - val_mae: 3.7898\n",
      "Epoch 83/150\n",
      "122/122 [==============================] - 0s 543us/step - loss: 398.0250 - mae: 13.8511 - val_loss: 30.9939 - val_mae: 4.0683\n",
      "Epoch 84/150\n",
      "122/122 [==============================] - 0s 533us/step - loss: 406.8049 - mae: 13.8049 - val_loss: 26.2381 - val_mae: 3.6204\n",
      "Epoch 85/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 399.8437 - mae: 13.8387 - val_loss: 25.1240 - val_mae: 3.6730\n",
      "Epoch 86/150\n",
      "122/122 [==============================] - 0s 563us/step - loss: 402.4526 - mae: 13.8488 - val_loss: 29.0102 - val_mae: 3.8401\n",
      "Epoch 87/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 407.2278 - mae: 13.8133 - val_loss: 25.8913 - val_mae: 3.7633\n",
      "Epoch 88/150\n",
      "122/122 [==============================] - 0s 556us/step - loss: 409.5128 - mae: 13.7891 - val_loss: 26.3175 - val_mae: 3.7774\n",
      "Epoch 89/150\n",
      "122/122 [==============================] - 0s 561us/step - loss: 398.5407 - mae: 13.7633 - val_loss: 25.0020 - val_mae: 3.6840\n",
      "Epoch 90/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 378.6998 - mae: 13.4680 - val_loss: 27.8345 - val_mae: 3.6261\n",
      "Epoch 91/150\n",
      "122/122 [==============================] - 0s 558us/step - loss: 404.9078 - mae: 13.9305 - val_loss: 23.0030 - val_mae: 3.4774\n",
      "Epoch 92/150\n",
      "122/122 [==============================] - 0s 553us/step - loss: 405.9573 - mae: 13.7182 - val_loss: 32.3840 - val_mae: 4.3381\n",
      "Epoch 93/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 377.5714 - mae: 13.3962 - val_loss: 25.4513 - val_mae: 3.7625\n",
      "Epoch 94/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 396.8138 - mae: 13.5871 - val_loss: 32.9640 - val_mae: 4.2655\n",
      "Epoch 95/150\n",
      "122/122 [==============================] - 0s 553us/step - loss: 395.4063 - mae: 13.5782 - val_loss: 25.7193 - val_mae: 3.6947\n",
      "Epoch 96/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 397.6759 - mae: 13.6656 - val_loss: 24.7364 - val_mae: 3.5555\n",
      "Epoch 97/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 388.1260 - mae: 13.3266 - val_loss: 26.6224 - val_mae: 3.7536\n",
      "Epoch 98/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 388.1362 - mae: 13.2654 - val_loss: 32.1360 - val_mae: 4.2205\n",
      "Epoch 99/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 368.1639 - mae: 13.1739 - val_loss: 47.6020 - val_mae: 5.3679\n",
      "Epoch 100/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 388.7966 - mae: 13.2866 - val_loss: 24.2728 - val_mae: 3.5804\n",
      "Epoch 101/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 376.3840 - mae: 13.3061 - val_loss: 37.6200 - val_mae: 4.4813\n",
      "Epoch 102/150\n",
      "122/122 [==============================] - 0s 550us/step - loss: 372.1329 - mae: 13.2482 - val_loss: 24.1770 - val_mae: 3.5029\n",
      "Epoch 103/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 376.5670 - mae: 13.1109 - val_loss: 45.5138 - val_mae: 5.1314\n",
      "Epoch 104/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 377.1084 - mae: 13.0864 - val_loss: 23.5402 - val_mae: 3.4893\n",
      "Epoch 105/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 375.7838 - mae: 13.0423 - val_loss: 23.0539 - val_mae: 3.4099\n",
      "Epoch 106/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 369.4526 - mae: 13.0981 - val_loss: 22.7195 - val_mae: 3.4027\n",
      "Epoch 107/150\n",
      "122/122 [==============================] - 0s 642us/step - loss: 383.8209 - mae: 12.8999 - val_loss: 22.9678 - val_mae: 3.4442\n",
      "Epoch 108/150\n",
      "122/122 [==============================] - 0s 551us/step - loss: 360.0394 - mae: 12.7342 - val_loss: 25.3669 - val_mae: 3.5103\n",
      "Epoch 109/150\n",
      "122/122 [==============================] - 0s 540us/step - loss: 330.2144 - mae: 12.4439 - val_loss: 27.5454 - val_mae: 3.8303\n",
      "Epoch 110/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 355.1803 - mae: 12.7736 - val_loss: 26.9284 - val_mae: 3.7586\n",
      "Epoch 111/150\n",
      "122/122 [==============================] - 0s 544us/step - loss: 362.4737 - mae: 12.8098 - val_loss: 24.8280 - val_mae: 3.5870\n",
      "Epoch 112/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 335.5326 - mae: 12.2826 - val_loss: 31.0746 - val_mae: 4.1839\n",
      "Epoch 113/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 361.5606 - mae: 12.8794 - val_loss: 28.5214 - val_mae: 3.6528\n",
      "Epoch 114/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 352.9026 - mae: 12.7815 - val_loss: 28.4798 - val_mae: 3.9929\n",
      "Epoch 115/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 353.6063 - mae: 12.6187 - val_loss: 29.4104 - val_mae: 3.9804\n",
      "Epoch 116/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 350.9111 - mae: 12.5314 - val_loss: 24.3118 - val_mae: 3.4687\n",
      "Epoch 117/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 344.6711 - mae: 12.5506 - val_loss: 28.1681 - val_mae: 3.6697\n",
      "Epoch 118/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 347.4489 - mae: 12.4957 - val_loss: 22.2505 - val_mae: 3.3132\n",
      "Epoch 119/150\n",
      "122/122 [==============================] - 0s 549us/step - loss: 340.5380 - mae: 12.4405 - val_loss: 25.4011 - val_mae: 3.4825\n",
      "Epoch 120/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 337.2234 - mae: 12.1916 - val_loss: 25.6017 - val_mae: 3.7257\n",
      "Epoch 121/150\n",
      "122/122 [==============================] - 0s 531us/step - loss: 328.6699 - mae: 12.1301 - val_loss: 24.6643 - val_mae: 3.4947\n",
      "Epoch 122/150\n",
      "122/122 [==============================] - 0s 539us/step - loss: 320.7577 - mae: 12.1301 - val_loss: 25.9359 - val_mae: 3.7291\n",
      "Epoch 123/150\n",
      "122/122 [==============================] - 0s 569us/step - loss: 344.9616 - mae: 12.2790 - val_loss: 26.3376 - val_mae: 3.8308\n",
      "Epoch 124/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 338.9270 - mae: 12.2288 - val_loss: 31.2330 - val_mae: 4.2178\n",
      "Epoch 125/150\n",
      "122/122 [==============================] - 0s 543us/step - loss: 333.2391 - mae: 12.3341 - val_loss: 27.3231 - val_mae: 3.8670\n",
      "Epoch 126/150\n",
      "122/122 [==============================] - 0s 542us/step - loss: 340.8356 - mae: 12.3710 - val_loss: 24.7271 - val_mae: 3.6463\n",
      "Epoch 127/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 317.1905 - mae: 11.9186 - val_loss: 28.8367 - val_mae: 4.0270\n",
      "Epoch 128/150\n",
      "122/122 [==============================] - 0s 551us/step - loss: 330.5189 - mae: 12.0689 - val_loss: 21.6118 - val_mae: 3.2940\n",
      "Epoch 129/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 318.9398 - mae: 11.9966 - val_loss: 25.5715 - val_mae: 3.7557\n",
      "Epoch 130/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 303.9867 - mae: 11.7801 - val_loss: 27.8470 - val_mae: 3.8281\n",
      "Epoch 131/150\n",
      "122/122 [==============================] - 0s 546us/step - loss: 322.4193 - mae: 11.9564 - val_loss: 37.2296 - val_mae: 4.8649\n",
      "Epoch 132/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 321.5294 - mae: 12.1288 - val_loss: 25.1017 - val_mae: 3.5962\n",
      "Epoch 133/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 323.1364 - mae: 12.1341 - val_loss: 28.0918 - val_mae: 4.0177\n",
      "Epoch 134/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 325.8925 - mae: 11.8927 - val_loss: 22.8457 - val_mae: 3.4287\n",
      "Epoch 135/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 309.4776 - mae: 11.7461 - val_loss: 33.7390 - val_mae: 4.3670\n",
      "Epoch 136/150\n",
      "122/122 [==============================] - 0s 538us/step - loss: 304.2537 - mae: 11.6635 - val_loss: 40.5173 - val_mae: 4.7646\n",
      "Epoch 137/150\n",
      "122/122 [==============================] - 0s 550us/step - loss: 322.9855 - mae: 11.8223 - val_loss: 23.1542 - val_mae: 3.4606\n",
      "Epoch 138/150\n",
      "122/122 [==============================] - 0s 543us/step - loss: 294.0617 - mae: 11.4302 - val_loss: 37.2629 - val_mae: 4.5947\n",
      "Epoch 139/150\n",
      "122/122 [==============================] - 0s 537us/step - loss: 308.8022 - mae: 11.7539 - val_loss: 22.9118 - val_mae: 3.4228\n",
      "Epoch 140/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 313.3715 - mae: 11.7493 - val_loss: 22.9478 - val_mae: 3.4205\n",
      "Epoch 141/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 285.4719 - mae: 11.2581 - val_loss: 21.9316 - val_mae: 3.3580\n",
      "Epoch 142/150\n",
      "122/122 [==============================] - 0s 541us/step - loss: 291.0521 - mae: 11.5282 - val_loss: 28.3814 - val_mae: 3.9674\n",
      "Epoch 143/150\n",
      "122/122 [==============================] - 0s 556us/step - loss: 300.9890 - mae: 11.4940 - val_loss: 26.4086 - val_mae: 3.6947\n",
      "Epoch 144/150\n",
      "122/122 [==============================] - 0s 543us/step - loss: 290.2900 - mae: 11.3464 - val_loss: 25.2929 - val_mae: 3.6142\n",
      "Epoch 145/150\n",
      "122/122 [==============================] - 0s 560us/step - loss: 290.6945 - mae: 11.3641 - val_loss: 23.6499 - val_mae: 3.5086\n",
      "Epoch 146/150\n",
      "122/122 [==============================] - 0s 553us/step - loss: 269.1367 - mae: 10.9688 - val_loss: 27.1647 - val_mae: 3.8387\n",
      "Epoch 147/150\n",
      "122/122 [==============================] - 0s 545us/step - loss: 272.8177 - mae: 11.1816 - val_loss: 29.6948 - val_mae: 4.0583\n",
      "Epoch 148/150\n",
      "122/122 [==============================] - 0s 549us/step - loss: 281.3132 - mae: 11.1007 - val_loss: 29.5561 - val_mae: 4.0320\n",
      "Epoch 149/150\n",
      "122/122 [==============================] - 0s 547us/step - loss: 295.8890 - mae: 11.3662 - val_loss: 23.0522 - val_mae: 3.4176\n",
      "Epoch 150/150\n",
      "122/122 [==============================] - 0s 548us/step - loss: 295.4053 - mae: 11.2533 - val_loss: 24.9266 - val_mae: 3.6731\n",
      "31/31 [==============================] - 0s 295us/step\n",
      "Epochs: 150 | MAE: 3.673137145996094\n",
      "Training model with 200 epochs\n",
      "Epoch 1/200\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68092.0781 - mae: 226.2975 - val_loss: 59658.6289 - val_mae: 211.0540\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 0s 547us/step - loss: 39102.9492 - mae: 165.7045 - val_loss: 14168.7334 - val_mae: 101.4202\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 6636.3237 - mae: 64.0046 - val_loss: 3179.7839 - val_mae: 43.2921\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 3312.6060 - mae: 43.9206 - val_loss: 2056.1641 - val_mae: 34.8502\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 2593.1848 - mae: 38.4409 - val_loss: 1627.7728 - val_mae: 30.9703\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 2116.6309 - mae: 35.7996 - val_loss: 1399.3542 - val_mae: 28.6610\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 2050.0271 - mae: 34.2389 - val_loss: 1238.6982 - val_mae: 26.9162\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 0s 527us/step - loss: 1884.9187 - mae: 33.3008 - val_loss: 1104.5435 - val_mae: 25.3145\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 1659.1561 - mae: 31.4422 - val_loss: 1006.0520 - val_mae: 24.2211\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 0s 647us/step - loss: 1555.0323 - mae: 30.3955 - val_loss: 898.0362 - val_mae: 22.6717\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 0s 539us/step - loss: 1483.9576 - mae: 29.8865 - val_loss: 817.1494 - val_mae: 21.5662\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 1399.0836 - mae: 29.0449 - val_loss: 726.9482 - val_mae: 20.5351\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 1295.7594 - mae: 28.1235 - val_loss: 668.3848 - val_mae: 19.5719\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 1259.6754 - mae: 27.6113 - val_loss: 584.1418 - val_mae: 18.1523\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 0s 553us/step - loss: 1158.5895 - mae: 26.6357 - val_loss: 533.6866 - val_mae: 17.4382\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 0s 526us/step - loss: 1185.1367 - mae: 26.4404 - val_loss: 496.2819 - val_mae: 16.7156\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 1059.1826 - mae: 25.3153 - val_loss: 438.1662 - val_mae: 15.9489\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 0s 611us/step - loss: 1000.5787 - mae: 24.6723 - val_loss: 376.0653 - val_mae: 14.4076\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 0s 657us/step - loss: 964.9728 - mae: 23.8705 - val_loss: 345.2553 - val_mae: 13.9325\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 934.0887 - mae: 23.7327 - val_loss: 332.1167 - val_mae: 13.7164\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 876.1299 - mae: 23.1829 - val_loss: 302.2512 - val_mae: 13.6786\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 838.5692 - mae: 22.2892 - val_loss: 257.7337 - val_mae: 11.9178\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 0s 522us/step - loss: 799.4420 - mae: 21.8966 - val_loss: 215.7827 - val_mae: 10.9387\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 0s 527us/step - loss: 744.0154 - mae: 20.9881 - val_loss: 194.6809 - val_mae: 10.7377\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 773.5956 - mae: 21.1256 - val_loss: 183.5246 - val_mae: 10.4144\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 0s 527us/step - loss: 710.4302 - mae: 20.3825 - val_loss: 148.6928 - val_mae: 9.1847\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - 0s 525us/step - loss: 691.7303 - mae: 19.9968 - val_loss: 134.2628 - val_mae: 8.5337\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 0s 528us/step - loss: 702.6034 - mae: 19.9633 - val_loss: 126.0166 - val_mae: 8.3825\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 0s 528us/step - loss: 653.8306 - mae: 19.3135 - val_loss: 113.8970 - val_mae: 8.3280\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 0s 550us/step - loss: 642.8981 - mae: 19.1419 - val_loss: 98.7080 - val_mae: 7.4974\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 679.9074 - mae: 19.4094 - val_loss: 105.3321 - val_mae: 8.0849\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 595.1484 - mae: 18.3869 - val_loss: 102.8775 - val_mae: 7.6863\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 600.6644 - mae: 18.2330 - val_loss: 80.2888 - val_mae: 6.6383\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 0s 528us/step - loss: 581.5710 - mae: 17.7371 - val_loss: 72.5469 - val_mae: 6.4804\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 584.2283 - mae: 17.6563 - val_loss: 57.3118 - val_mae: 5.5891\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 0s 528us/step - loss: 565.5713 - mae: 17.2960 - val_loss: 58.8850 - val_mae: 6.0528\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 580.4256 - mae: 17.5464 - val_loss: 53.0382 - val_mae: 5.7165\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 561.9648 - mae: 17.3304 - val_loss: 56.4906 - val_mae: 5.6762\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 560.9913 - mae: 17.3293 - val_loss: 50.0445 - val_mae: 5.5818\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 0s 522us/step - loss: 564.1913 - mae: 17.3075 - val_loss: 41.7585 - val_mae: 4.9227\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 577.6837 - mae: 17.4773 - val_loss: 35.9117 - val_mae: 4.4515\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 0s 529us/step - loss: 539.8427 - mae: 16.8603 - val_loss: 35.4786 - val_mae: 4.4109\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 0s 525us/step - loss: 562.7229 - mae: 17.1173 - val_loss: 44.2400 - val_mae: 4.9695\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 541.4568 - mae: 16.7220 - val_loss: 38.1431 - val_mae: 4.4664\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 543.5532 - mae: 16.7195 - val_loss: 34.8309 - val_mae: 4.3256\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 0s 520us/step - loss: 512.4007 - mae: 16.0794 - val_loss: 30.1633 - val_mae: 4.1310\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 536.1136 - mae: 16.6976 - val_loss: 36.2606 - val_mae: 4.4096\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 517.0117 - mae: 16.3442 - val_loss: 39.3423 - val_mae: 4.8799\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 0s 545us/step - loss: 506.8486 - mae: 16.0422 - val_loss: 38.9266 - val_mae: 4.8053\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 0s 552us/step - loss: 522.1417 - mae: 16.3701 - val_loss: 36.0589 - val_mae: 4.3648\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 492.9825 - mae: 15.7431 - val_loss: 29.0810 - val_mae: 3.8913\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 496.0992 - mae: 15.8671 - val_loss: 30.6714 - val_mae: 4.2338\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 0s 545us/step - loss: 520.1166 - mae: 15.9739 - val_loss: 23.4909 - val_mae: 3.5448\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 524.3589 - mae: 16.0963 - val_loss: 35.4958 - val_mae: 4.3703\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 0s 566us/step - loss: 505.6406 - mae: 15.7848 - val_loss: 34.1151 - val_mae: 4.4587\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 0s 545us/step - loss: 501.9900 - mae: 15.8175 - val_loss: 41.3805 - val_mae: 4.7108\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 492.4340 - mae: 15.6615 - val_loss: 26.0455 - val_mae: 3.7042\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 492.1655 - mae: 15.5970 - val_loss: 28.7912 - val_mae: 3.9618\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 463.3974 - mae: 15.1995 - val_loss: 25.4176 - val_mae: 3.5484\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 476.3347 - mae: 15.3696 - val_loss: 28.5120 - val_mae: 3.8573\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 0s 528us/step - loss: 486.4772 - mae: 15.4245 - val_loss: 32.7039 - val_mae: 4.4258\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 486.2211 - mae: 15.4717 - val_loss: 35.9440 - val_mae: 4.4978\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 483.4106 - mae: 15.5071 - val_loss: 47.9001 - val_mae: 5.1906\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 473.2335 - mae: 15.1569 - val_loss: 29.3796 - val_mae: 4.0383\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 461.0190 - mae: 15.1551 - val_loss: 24.8205 - val_mae: 3.5514\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 475.5132 - mae: 15.2761 - val_loss: 32.7437 - val_mae: 4.4388\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 481.7494 - mae: 15.3261 - val_loss: 25.4764 - val_mae: 3.6463\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 482.4023 - mae: 15.3251 - val_loss: 33.0684 - val_mae: 4.3354\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 458.2043 - mae: 14.9478 - val_loss: 22.5830 - val_mae: 3.4141\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 444.7802 - mae: 14.8368 - val_loss: 30.3998 - val_mae: 4.2012\n",
      "Epoch 71/200\n",
      "122/122 [==============================] - 0s 627us/step - loss: 447.4610 - mae: 14.5789 - val_loss: 45.3690 - val_mae: 5.5059\n",
      "Epoch 72/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 440.6144 - mae: 14.6509 - val_loss: 34.1125 - val_mae: 4.5226\n",
      "Epoch 73/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 439.0763 - mae: 14.6822 - val_loss: 25.3592 - val_mae: 3.6615\n",
      "Epoch 74/200\n",
      "122/122 [==============================] - 0s 528us/step - loss: 441.5151 - mae: 14.4772 - val_loss: 29.8145 - val_mae: 3.9727\n",
      "Epoch 75/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 429.4951 - mae: 14.3210 - val_loss: 23.0589 - val_mae: 3.3922\n",
      "Epoch 76/200\n",
      "122/122 [==============================] - 0s 529us/step - loss: 449.6614 - mae: 14.7015 - val_loss: 31.1047 - val_mae: 4.0169\n",
      "Epoch 77/200\n",
      "122/122 [==============================] - 0s 547us/step - loss: 457.1564 - mae: 14.8195 - val_loss: 24.2923 - val_mae: 3.5824\n",
      "Epoch 78/200\n",
      "122/122 [==============================] - 0s 543us/step - loss: 422.9440 - mae: 14.2276 - val_loss: 23.1756 - val_mae: 3.4219\n",
      "Epoch 79/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 444.2684 - mae: 14.5838 - val_loss: 27.1163 - val_mae: 3.6697\n",
      "Epoch 80/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 428.3300 - mae: 14.2317 - val_loss: 25.2945 - val_mae: 3.5714\n",
      "Epoch 81/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 442.6138 - mae: 14.6329 - val_loss: 29.7400 - val_mae: 3.7376\n",
      "Epoch 82/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 451.4256 - mae: 14.5740 - val_loss: 25.1377 - val_mae: 3.5948\n",
      "Epoch 83/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 436.7966 - mae: 14.4176 - val_loss: 22.7050 - val_mae: 3.4460\n",
      "Epoch 84/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 428.6069 - mae: 14.1900 - val_loss: 25.5081 - val_mae: 3.8308\n",
      "Epoch 85/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 429.6406 - mae: 14.2728 - val_loss: 32.4690 - val_mae: 4.2984\n",
      "Epoch 86/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 418.4967 - mae: 14.2590 - val_loss: 24.9931 - val_mae: 3.5307\n",
      "Epoch 87/200\n",
      "122/122 [==============================] - 0s 526us/step - loss: 427.7878 - mae: 14.2469 - val_loss: 24.1768 - val_mae: 3.4214\n",
      "Epoch 88/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 401.9549 - mae: 13.7913 - val_loss: 27.0511 - val_mae: 3.8903\n",
      "Epoch 89/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 390.3323 - mae: 13.8001 - val_loss: 29.2925 - val_mae: 4.0052\n",
      "Epoch 90/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 413.2078 - mae: 13.9002 - val_loss: 24.7531 - val_mae: 3.6107\n",
      "Epoch 91/200\n",
      "122/122 [==============================] - 0s 563us/step - loss: 416.8418 - mae: 13.9940 - val_loss: 30.9286 - val_mae: 3.9422\n",
      "Epoch 92/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 418.7157 - mae: 14.1641 - val_loss: 28.7872 - val_mae: 3.7173\n",
      "Epoch 93/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 411.2407 - mae: 13.7224 - val_loss: 23.4779 - val_mae: 3.4448\n",
      "Epoch 94/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 391.7497 - mae: 13.5540 - val_loss: 27.4019 - val_mae: 3.8281\n",
      "Epoch 95/200\n",
      "122/122 [==============================] - 0s 539us/step - loss: 408.7030 - mae: 13.8522 - val_loss: 22.1311 - val_mae: 3.3762\n",
      "Epoch 96/200\n",
      "122/122 [==============================] - 0s 525us/step - loss: 407.4386 - mae: 13.6162 - val_loss: 26.4415 - val_mae: 3.5416\n",
      "Epoch 97/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 417.2072 - mae: 13.8699 - val_loss: 23.7430 - val_mae: 3.5475\n",
      "Epoch 98/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 415.3262 - mae: 13.8826 - val_loss: 28.1642 - val_mae: 3.7956\n",
      "Epoch 99/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 390.0765 - mae: 13.4674 - val_loss: 24.9399 - val_mae: 3.6185\n",
      "Epoch 100/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 410.2607 - mae: 13.8637 - val_loss: 24.4013 - val_mae: 3.5223\n",
      "Epoch 101/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 378.2061 - mae: 13.3605 - val_loss: 30.9438 - val_mae: 4.1160\n",
      "Epoch 102/200\n",
      "122/122 [==============================] - 0s 539us/step - loss: 393.5308 - mae: 13.3229 - val_loss: 47.2194 - val_mae: 4.9705\n",
      "Epoch 103/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 371.1268 - mae: 13.3177 - val_loss: 25.1436 - val_mae: 3.5391\n",
      "Epoch 104/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 385.3464 - mae: 13.3358 - val_loss: 28.5766 - val_mae: 3.7869\n",
      "Epoch 105/200\n",
      "122/122 [==============================] - 0s 525us/step - loss: 360.9332 - mae: 12.9810 - val_loss: 30.2908 - val_mae: 4.0813\n",
      "Epoch 106/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 389.2250 - mae: 13.3307 - val_loss: 28.3542 - val_mae: 3.8591\n",
      "Epoch 107/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 388.2873 - mae: 13.3491 - val_loss: 28.0473 - val_mae: 3.8809\n",
      "Epoch 108/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 371.6246 - mae: 13.1616 - val_loss: 26.3186 - val_mae: 3.7004\n",
      "Epoch 109/200\n",
      "122/122 [==============================] - 0s 544us/step - loss: 378.2089 - mae: 13.3228 - val_loss: 44.1580 - val_mae: 4.8533\n",
      "Epoch 110/200\n",
      "122/122 [==============================] - 0s 553us/step - loss: 373.0155 - mae: 13.0077 - val_loss: 25.6104 - val_mae: 3.6447\n",
      "Epoch 111/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 377.3824 - mae: 13.0505 - val_loss: 26.1582 - val_mae: 3.8123\n",
      "Epoch 112/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 375.2189 - mae: 12.8346 - val_loss: 22.7386 - val_mae: 3.4416\n",
      "Epoch 113/200\n",
      "122/122 [==============================] - 0s 545us/step - loss: 370.0756 - mae: 12.9342 - val_loss: 26.1901 - val_mae: 3.5491\n",
      "Epoch 114/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 371.1313 - mae: 13.0896 - val_loss: 28.8897 - val_mae: 3.9295\n",
      "Epoch 115/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 383.2104 - mae: 13.0850 - val_loss: 33.6417 - val_mae: 4.4722\n",
      "Epoch 116/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 351.4240 - mae: 12.7197 - val_loss: 26.7770 - val_mae: 3.7628\n",
      "Epoch 117/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 365.3050 - mae: 13.0142 - val_loss: 26.8343 - val_mae: 3.7918\n",
      "Epoch 118/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 359.1936 - mae: 12.8139 - val_loss: 28.0493 - val_mae: 3.8266\n",
      "Epoch 119/200\n",
      "122/122 [==============================] - 0s 616us/step - loss: 351.9655 - mae: 12.6243 - val_loss: 28.0926 - val_mae: 3.8700\n",
      "Epoch 120/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 356.1414 - mae: 12.5675 - val_loss: 23.2666 - val_mae: 3.4595\n",
      "Epoch 121/200\n",
      "122/122 [==============================] - 0s 539us/step - loss: 346.1025 - mae: 12.5570 - val_loss: 22.0434 - val_mae: 3.3823\n",
      "Epoch 122/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 382.5864 - mae: 12.8022 - val_loss: 60.7271 - val_mae: 5.6015\n",
      "Epoch 123/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 374.1363 - mae: 12.6969 - val_loss: 23.9953 - val_mae: 3.5244\n",
      "Epoch 124/200\n",
      "122/122 [==============================] - 0s 563us/step - loss: 374.3173 - mae: 12.7046 - val_loss: 28.3978 - val_mae: 3.9075\n",
      "Epoch 125/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 367.6874 - mae: 12.7351 - val_loss: 25.0971 - val_mae: 3.5531\n",
      "Epoch 126/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 346.3258 - mae: 12.4201 - val_loss: 29.3049 - val_mae: 4.0335\n",
      "Epoch 127/200\n",
      "122/122 [==============================] - 0s 557us/step - loss: 347.8513 - mae: 12.3051 - val_loss: 23.5335 - val_mae: 3.3454\n",
      "Epoch 128/200\n",
      "122/122 [==============================] - 0s 544us/step - loss: 358.4862 - mae: 12.4554 - val_loss: 26.6308 - val_mae: 3.7581\n",
      "Epoch 129/200\n",
      "122/122 [==============================] - 0s 543us/step - loss: 323.3770 - mae: 12.1325 - val_loss: 25.4402 - val_mae: 3.5182\n",
      "Epoch 130/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 341.4028 - mae: 12.2496 - val_loss: 23.7174 - val_mae: 3.5564\n",
      "Epoch 131/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 323.9134 - mae: 12.1462 - val_loss: 23.6974 - val_mae: 3.5783\n",
      "Epoch 132/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 332.0482 - mae: 12.1787 - val_loss: 24.1565 - val_mae: 3.4653\n",
      "Epoch 133/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 299.3540 - mae: 11.6623 - val_loss: 23.2068 - val_mae: 3.3075\n",
      "Epoch 134/200\n",
      "122/122 [==============================] - 0s 544us/step - loss: 329.9857 - mae: 12.1547 - val_loss: 22.1511 - val_mae: 3.3421\n",
      "Epoch 135/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 348.8911 - mae: 12.2309 - val_loss: 20.2799 - val_mae: 3.1673\n",
      "Epoch 136/200\n",
      "122/122 [==============================] - 0s 527us/step - loss: 340.9954 - mae: 12.2920 - val_loss: 26.5406 - val_mae: 3.6332\n",
      "Epoch 137/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 333.6189 - mae: 12.0631 - val_loss: 24.2191 - val_mae: 3.4207\n",
      "Epoch 138/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 313.4962 - mae: 11.6878 - val_loss: 28.9791 - val_mae: 4.0118\n",
      "Epoch 139/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 357.8382 - mae: 12.2268 - val_loss: 24.6445 - val_mae: 3.6802\n",
      "Epoch 140/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 308.7146 - mae: 11.7143 - val_loss: 28.3652 - val_mae: 3.7724\n",
      "Epoch 141/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 327.5830 - mae: 11.8265 - val_loss: 24.1031 - val_mae: 3.5785\n",
      "Epoch 142/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 310.7436 - mae: 11.6845 - val_loss: 42.7449 - val_mae: 4.8980\n",
      "Epoch 143/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 309.8259 - mae: 11.7046 - val_loss: 29.5285 - val_mae: 3.9964\n",
      "Epoch 144/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 311.1835 - mae: 11.7034 - val_loss: 28.7422 - val_mae: 3.8208\n",
      "Epoch 145/200\n",
      "122/122 [==============================] - 0s 529us/step - loss: 311.8008 - mae: 11.7528 - val_loss: 22.9592 - val_mae: 3.3782\n",
      "Epoch 146/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 337.1613 - mae: 12.0415 - val_loss: 22.0323 - val_mae: 3.3252\n",
      "Epoch 147/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 293.8033 - mae: 11.3664 - val_loss: 23.2424 - val_mae: 3.4033\n",
      "Epoch 148/200\n",
      "122/122 [==============================] - 0s 563us/step - loss: 327.4785 - mae: 11.7660 - val_loss: 23.5835 - val_mae: 3.4195\n",
      "Epoch 149/200\n",
      "122/122 [==============================] - 0s 559us/step - loss: 314.3941 - mae: 11.7312 - val_loss: 25.3401 - val_mae: 3.5154\n",
      "Epoch 150/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 312.8247 - mae: 11.5370 - val_loss: 44.9005 - val_mae: 5.0872\n",
      "Epoch 151/200\n",
      "122/122 [==============================] - 0s 549us/step - loss: 302.1766 - mae: 11.3580 - val_loss: 25.3208 - val_mae: 3.5225\n",
      "Epoch 152/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 300.5086 - mae: 11.5195 - val_loss: 37.1339 - val_mae: 4.3490\n",
      "Epoch 153/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 299.9044 - mae: 11.3039 - val_loss: 39.4992 - val_mae: 4.6945\n",
      "Epoch 154/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 298.2542 - mae: 11.3636 - val_loss: 28.3797 - val_mae: 3.9378\n",
      "Epoch 155/200\n",
      "122/122 [==============================] - 0s 566us/step - loss: 297.5008 - mae: 11.4276 - val_loss: 25.0078 - val_mae: 3.6128\n",
      "Epoch 156/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 292.7358 - mae: 11.3552 - val_loss: 30.9112 - val_mae: 4.0242\n",
      "Epoch 157/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 297.6433 - mae: 11.3432 - val_loss: 22.2504 - val_mae: 3.3820\n",
      "Epoch 158/200\n",
      "122/122 [==============================] - 0s 531us/step - loss: 295.7050 - mae: 11.1706 - val_loss: 42.5489 - val_mae: 5.0389\n",
      "Epoch 159/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 277.7107 - mae: 10.9509 - val_loss: 21.4431 - val_mae: 3.3053\n",
      "Epoch 160/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 300.3604 - mae: 11.3602 - val_loss: 24.0370 - val_mae: 3.5013\n",
      "Epoch 161/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 279.8023 - mae: 10.9114 - val_loss: 31.1349 - val_mae: 4.0745\n",
      "Epoch 162/200\n",
      "122/122 [==============================] - 0s 529us/step - loss: 278.4689 - mae: 11.0000 - val_loss: 25.2781 - val_mae: 3.6822\n",
      "Epoch 163/200\n",
      "122/122 [==============================] - 0s 546us/step - loss: 268.9446 - mae: 10.8281 - val_loss: 30.0070 - val_mae: 4.0020\n",
      "Epoch 164/200\n",
      "122/122 [==============================] - 0s 633us/step - loss: 305.8982 - mae: 11.2732 - val_loss: 25.8116 - val_mae: 3.7722\n",
      "Epoch 165/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 293.8823 - mae: 11.0246 - val_loss: 23.4621 - val_mae: 3.4739\n",
      "Epoch 166/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 257.8724 - mae: 10.5498 - val_loss: 30.2847 - val_mae: 4.1642\n",
      "Epoch 167/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 292.4700 - mae: 11.0553 - val_loss: 22.1311 - val_mae: 3.3502\n",
      "Epoch 168/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 275.1440 - mae: 10.7494 - val_loss: 24.2807 - val_mae: 3.5007\n",
      "Epoch 169/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 276.2565 - mae: 10.7682 - val_loss: 24.2321 - val_mae: 3.4600\n",
      "Epoch 170/200\n",
      "122/122 [==============================] - 0s 557us/step - loss: 265.2335 - mae: 10.6708 - val_loss: 41.5148 - val_mae: 4.8000\n",
      "Epoch 171/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 275.8376 - mae: 10.7586 - val_loss: 24.4825 - val_mae: 3.5789\n",
      "Epoch 172/200\n",
      "122/122 [==============================] - 0s 541us/step - loss: 272.9287 - mae: 10.6301 - val_loss: 26.6639 - val_mae: 3.7721\n",
      "Epoch 173/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 261.8875 - mae: 10.5546 - val_loss: 30.2885 - val_mae: 4.1782\n",
      "Epoch 174/200\n",
      "122/122 [==============================] - 0s 530us/step - loss: 285.3686 - mae: 10.9291 - val_loss: 24.7405 - val_mae: 3.6366\n",
      "Epoch 175/200\n",
      "122/122 [==============================] - 0s 543us/step - loss: 277.0300 - mae: 10.7666 - val_loss: 30.7172 - val_mae: 4.1708\n",
      "Epoch 176/200\n",
      "122/122 [==============================] - 0s 548us/step - loss: 244.9520 - mae: 10.3153 - val_loss: 24.7822 - val_mae: 3.5255\n",
      "Epoch 177/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 272.3507 - mae: 10.5493 - val_loss: 25.0119 - val_mae: 3.5944\n",
      "Epoch 178/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 253.0717 - mae: 10.4515 - val_loss: 26.1256 - val_mae: 3.7666\n",
      "Epoch 179/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 266.4516 - mae: 10.5147 - val_loss: 27.2664 - val_mae: 3.8057\n",
      "Epoch 180/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 279.6030 - mae: 10.6165 - val_loss: 25.6940 - val_mae: 3.6915\n",
      "Epoch 181/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 251.0409 - mae: 10.2762 - val_loss: 50.6547 - val_mae: 5.4026\n",
      "Epoch 182/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 273.0818 - mae: 10.5691 - val_loss: 22.9752 - val_mae: 3.4407\n",
      "Epoch 183/200\n",
      "122/122 [==============================] - 0s 563us/step - loss: 275.7499 - mae: 10.6292 - val_loss: 23.9673 - val_mae: 3.5046\n",
      "Epoch 184/200\n",
      "122/122 [==============================] - 0s 542us/step - loss: 256.7139 - mae: 10.4205 - val_loss: 24.2326 - val_mae: 3.5001\n",
      "Epoch 185/200\n",
      "122/122 [==============================] - 0s 536us/step - loss: 261.1110 - mae: 10.4413 - val_loss: 26.1820 - val_mae: 3.7842\n",
      "Epoch 186/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 251.4995 - mae: 10.2725 - val_loss: 23.8901 - val_mae: 3.5342\n",
      "Epoch 187/200\n",
      "122/122 [==============================] - 0s 538us/step - loss: 252.9010 - mae: 10.2990 - val_loss: 29.0673 - val_mae: 3.9382\n",
      "Epoch 188/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 251.8862 - mae: 10.2531 - val_loss: 25.1967 - val_mae: 3.7206\n",
      "Epoch 189/200\n",
      "122/122 [==============================] - 0s 529us/step - loss: 247.4500 - mae: 10.1512 - val_loss: 35.8526 - val_mae: 4.4489\n",
      "Epoch 190/200\n",
      "122/122 [==============================] - 0s 539us/step - loss: 243.3839 - mae: 10.0835 - val_loss: 27.1659 - val_mae: 3.7825\n",
      "Epoch 191/200\n",
      "122/122 [==============================] - 0s 537us/step - loss: 241.2982 - mae: 10.0878 - val_loss: 25.4579 - val_mae: 3.7192\n",
      "Epoch 192/200\n",
      "122/122 [==============================] - 0s 533us/step - loss: 256.7167 - mae: 10.2985 - val_loss: 26.7999 - val_mae: 3.8252\n",
      "Epoch 193/200\n",
      "122/122 [==============================] - 0s 550us/step - loss: 245.0748 - mae: 9.9886 - val_loss: 28.1673 - val_mae: 3.9814\n",
      "Epoch 194/200\n",
      "122/122 [==============================] - 0s 534us/step - loss: 233.0240 - mae: 9.8459 - val_loss: 55.7189 - val_mae: 5.8403\n",
      "Epoch 195/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 241.8154 - mae: 9.9505 - val_loss: 24.6629 - val_mae: 3.6098\n",
      "Epoch 196/200\n",
      "122/122 [==============================] - 0s 543us/step - loss: 227.1186 - mae: 9.7204 - val_loss: 28.7268 - val_mae: 3.9884\n",
      "Epoch 197/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 248.0720 - mae: 10.0814 - val_loss: 25.5864 - val_mae: 3.6445\n",
      "Epoch 198/200\n",
      "122/122 [==============================] - 0s 535us/step - loss: 235.6907 - mae: 9.8268 - val_loss: 26.0654 - val_mae: 3.6769\n",
      "Epoch 199/200\n",
      "122/122 [==============================] - 0s 540us/step - loss: 237.8069 - mae: 9.9234 - val_loss: 37.4826 - val_mae: 4.6414\n",
      "Epoch 200/200\n",
      "122/122 [==============================] - 0s 532us/step - loss: 236.6118 - mae: 9.9535 - val_loss: 24.8198 - val_mae: 3.5559\n",
      "31/31 [==============================] - 0s 296us/step\n",
      "Epochs: 200 | MAE: 3.5558574425330525\n",
      "Training model with 250 epochs\n",
      "Epoch 1/250\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 65646.8594 - mae: 221.6272 - val_loss: 52726.0117 - val_mae: 198.0651\n",
      "Epoch 2/250\n",
      "122/122 [==============================] - 0s 557us/step - loss: 29805.8555 - mae: 142.5268 - val_loss: 8041.1265 - val_mae: 74.3379\n",
      "Epoch 3/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 5020.9175 - mae: 54.1747 - val_loss: 2987.4736 - val_mae: 40.8551\n",
      "Epoch 4/250\n",
      "122/122 [==============================] - 0s 645us/step - loss: 3091.6260 - mae: 41.6100 - val_loss: 2017.3340 - val_mae: 33.4549\n",
      "Epoch 5/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 2434.2229 - mae: 37.0244 - val_loss: 1598.1304 - val_mae: 29.8337\n",
      "Epoch 6/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 2083.8821 - mae: 34.7418 - val_loss: 1378.3970 - val_mae: 27.4869\n",
      "Epoch 7/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 1830.5443 - mae: 32.8715 - val_loss: 1214.1436 - val_mae: 25.7866\n",
      "Epoch 8/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 1808.8387 - mae: 32.6082 - val_loss: 1068.9403 - val_mae: 24.1533\n",
      "Epoch 9/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 1714.5940 - mae: 31.5268 - val_loss: 952.5960 - val_mae: 22.8104\n",
      "Epoch 10/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 1530.9917 - mae: 30.1583 - val_loss: 854.9695 - val_mae: 21.6315\n",
      "Epoch 11/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 1386.0135 - mae: 28.8472 - val_loss: 760.6689 - val_mae: 20.3841\n",
      "Epoch 12/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 1292.6224 - mae: 28.1473 - val_loss: 702.6368 - val_mae: 19.5858\n",
      "Epoch 13/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 1260.4248 - mae: 27.2771 - val_loss: 629.5920 - val_mae: 18.3372\n",
      "Epoch 14/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 1202.4333 - mae: 26.9368 - val_loss: 566.8132 - val_mae: 17.3843\n",
      "Epoch 15/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 1102.5242 - mae: 25.8318 - val_loss: 504.4115 - val_mae: 16.5185\n",
      "Epoch 16/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 1083.0438 - mae: 25.7531 - val_loss: 459.6116 - val_mae: 15.7375\n",
      "Epoch 17/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 1042.4235 - mae: 24.8667 - val_loss: 402.6563 - val_mae: 14.5481\n",
      "Epoch 18/250\n",
      "122/122 [==============================] - 0s 527us/step - loss: 958.8674 - mae: 24.1281 - val_loss: 364.8687 - val_mae: 14.0434\n",
      "Epoch 19/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 962.5991 - mae: 23.8898 - val_loss: 333.1761 - val_mae: 13.3153\n",
      "Epoch 20/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 899.4114 - mae: 23.2251 - val_loss: 296.5525 - val_mae: 12.5719\n",
      "Epoch 21/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 906.8583 - mae: 23.1016 - val_loss: 263.9349 - val_mae: 11.9396\n",
      "Epoch 22/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 812.1243 - mae: 22.0127 - val_loss: 237.6305 - val_mae: 11.1643\n",
      "Epoch 23/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 787.0999 - mae: 21.7531 - val_loss: 219.7929 - val_mae: 11.0660\n",
      "Epoch 24/250\n",
      "122/122 [==============================] - 0s 558us/step - loss: 784.5384 - mae: 21.6086 - val_loss: 194.7109 - val_mae: 10.5561\n",
      "Epoch 25/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 723.7151 - mae: 20.6222 - val_loss: 180.0029 - val_mae: 9.8727\n",
      "Epoch 26/250\n",
      "122/122 [==============================] - 0s 552us/step - loss: 730.4667 - mae: 20.4975 - val_loss: 163.2422 - val_mae: 9.3709\n",
      "Epoch 27/250\n",
      "122/122 [==============================] - 0s 547us/step - loss: 677.3199 - mae: 19.9665 - val_loss: 136.7548 - val_mae: 8.5502\n",
      "Epoch 28/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 695.7077 - mae: 19.9468 - val_loss: 162.0633 - val_mae: 10.2842\n",
      "Epoch 29/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 690.5834 - mae: 19.8253 - val_loss: 112.9370 - val_mae: 7.7450\n",
      "Epoch 30/250\n",
      "122/122 [==============================] - 0s 570us/step - loss: 641.7812 - mae: 18.9984 - val_loss: 108.7161 - val_mae: 7.9594\n",
      "Epoch 31/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 645.8338 - mae: 19.2409 - val_loss: 121.3086 - val_mae: 8.8227\n",
      "Epoch 32/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 636.8239 - mae: 18.8166 - val_loss: 94.0668 - val_mae: 7.1677\n",
      "Epoch 33/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 614.9341 - mae: 18.6707 - val_loss: 85.4977 - val_mae: 6.7331\n",
      "Epoch 34/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 581.1301 - mae: 18.0388 - val_loss: 74.5250 - val_mae: 6.5608\n",
      "Epoch 35/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 599.8278 - mae: 18.0079 - val_loss: 70.2787 - val_mae: 6.2440\n",
      "Epoch 36/250\n",
      "122/122 [==============================] - 0s 536us/step - loss: 595.6597 - mae: 18.0187 - val_loss: 89.3714 - val_mae: 7.7122\n",
      "Epoch 37/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 582.3707 - mae: 17.8088 - val_loss: 59.2767 - val_mae: 5.6460\n",
      "Epoch 38/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 561.2675 - mae: 17.2424 - val_loss: 69.9810 - val_mae: 6.3611\n",
      "Epoch 39/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 582.2888 - mae: 17.5548 - val_loss: 50.4638 - val_mae: 5.4453\n",
      "Epoch 40/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 607.0340 - mae: 17.8488 - val_loss: 46.7464 - val_mae: 5.1225\n",
      "Epoch 41/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 560.4193 - mae: 17.2444 - val_loss: 47.8651 - val_mae: 5.2924\n",
      "Epoch 42/250\n",
      "122/122 [==============================] - 0s 642us/step - loss: 546.1710 - mae: 16.9573 - val_loss: 55.3865 - val_mae: 5.8645\n",
      "Epoch 43/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 542.7071 - mae: 16.8182 - val_loss: 42.9950 - val_mae: 4.9244\n",
      "Epoch 44/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 500.2081 - mae: 16.3517 - val_loss: 33.3221 - val_mae: 4.2650\n",
      "Epoch 45/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 509.1263 - mae: 16.2646 - val_loss: 48.6992 - val_mae: 5.1359\n",
      "Epoch 46/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 518.6787 - mae: 16.2173 - val_loss: 36.6203 - val_mae: 4.5768\n",
      "Epoch 47/250\n",
      "122/122 [==============================] - 0s 556us/step - loss: 505.8997 - mae: 16.0766 - val_loss: 33.8448 - val_mae: 4.2652\n",
      "Epoch 48/250\n",
      "122/122 [==============================] - 0s 650us/step - loss: 497.0514 - mae: 15.8903 - val_loss: 30.7572 - val_mae: 4.0271\n",
      "Epoch 49/250\n",
      "122/122 [==============================] - 0s 744us/step - loss: 502.8116 - mae: 16.0478 - val_loss: 39.5528 - val_mae: 4.9037\n",
      "Epoch 50/250\n",
      "122/122 [==============================] - 0s 792us/step - loss: 486.9267 - mae: 15.8668 - val_loss: 28.5554 - val_mae: 3.8541\n",
      "Epoch 51/250\n",
      "122/122 [==============================] - 0s 864us/step - loss: 499.7789 - mae: 16.0391 - val_loss: 27.6103 - val_mae: 3.8116\n",
      "Epoch 52/250\n",
      "122/122 [==============================] - 0s 753us/step - loss: 485.8979 - mae: 15.6933 - val_loss: 26.1740 - val_mae: 3.6575\n",
      "Epoch 53/250\n",
      "122/122 [==============================] - 0s 701us/step - loss: 549.8719 - mae: 16.4684 - val_loss: 25.5775 - val_mae: 3.6324\n",
      "Epoch 54/250\n",
      "122/122 [==============================] - 0s 595us/step - loss: 504.7179 - mae: 15.7425 - val_loss: 37.3709 - val_mae: 4.6175\n",
      "Epoch 55/250\n",
      "122/122 [==============================] - 0s 560us/step - loss: 527.1301 - mae: 15.9668 - val_loss: 44.4517 - val_mae: 5.1706\n",
      "Epoch 56/250\n",
      "122/122 [==============================] - 0s 549us/step - loss: 501.0000 - mae: 15.9095 - val_loss: 26.5634 - val_mae: 3.7257\n",
      "Epoch 57/250\n",
      "122/122 [==============================] - 0s 629us/step - loss: 476.3919 - mae: 15.5679 - val_loss: 28.2667 - val_mae: 3.7569\n",
      "Epoch 58/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 505.3140 - mae: 15.6879 - val_loss: 24.9214 - val_mae: 3.6878\n",
      "Epoch 59/250\n",
      "122/122 [==============================] - 0s 549us/step - loss: 448.5036 - mae: 15.0623 - val_loss: 36.9882 - val_mae: 4.6028\n",
      "Epoch 60/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 468.2480 - mae: 15.4650 - val_loss: 28.3303 - val_mae: 3.7334\n",
      "Epoch 61/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 478.8510 - mae: 15.3655 - val_loss: 32.2737 - val_mae: 4.2598\n",
      "Epoch 62/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 459.9058 - mae: 15.1414 - val_loss: 38.9027 - val_mae: 4.9422\n",
      "Epoch 63/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 455.3538 - mae: 15.1130 - val_loss: 28.7940 - val_mae: 4.0561\n",
      "Epoch 64/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 460.4882 - mae: 15.0162 - val_loss: 37.2521 - val_mae: 4.6782\n",
      "Epoch 65/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 463.0595 - mae: 15.1712 - val_loss: 38.6198 - val_mae: 4.3697\n",
      "Epoch 66/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 464.6402 - mae: 15.1473 - val_loss: 30.5836 - val_mae: 4.1447\n",
      "Epoch 67/250\n",
      "122/122 [==============================] - 0s 529us/step - loss: 454.6995 - mae: 14.9186 - val_loss: 30.0458 - val_mae: 3.8429\n",
      "Epoch 68/250\n",
      "122/122 [==============================] - 0s 558us/step - loss: 450.0170 - mae: 14.7644 - val_loss: 28.0131 - val_mae: 3.9202\n",
      "Epoch 69/250\n",
      "122/122 [==============================] - 0s 674us/step - loss: 438.5076 - mae: 14.6117 - val_loss: 25.8759 - val_mae: 3.6599\n",
      "Epoch 70/250\n",
      "122/122 [==============================] - 0s 710us/step - loss: 434.8433 - mae: 14.6847 - val_loss: 32.1244 - val_mae: 3.8919\n",
      "Epoch 71/250\n",
      "122/122 [==============================] - 0s 592us/step - loss: 444.7020 - mae: 14.6464 - val_loss: 39.9811 - val_mae: 4.7776\n",
      "Epoch 72/250\n",
      "122/122 [==============================] - 0s 554us/step - loss: 446.9068 - mae: 14.5754 - val_loss: 24.8787 - val_mae: 3.6749\n",
      "Epoch 73/250\n",
      "122/122 [==============================] - 0s 549us/step - loss: 456.2878 - mae: 14.8660 - val_loss: 26.1254 - val_mae: 3.8236\n",
      "Epoch 74/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 453.8149 - mae: 14.7893 - val_loss: 24.8041 - val_mae: 3.5541\n",
      "Epoch 75/250\n",
      "122/122 [==============================] - 0s 651us/step - loss: 453.8522 - mae: 14.7275 - val_loss: 27.7288 - val_mae: 3.8563\n",
      "Epoch 76/250\n",
      "122/122 [==============================] - 0s 536us/step - loss: 424.1080 - mae: 14.4606 - val_loss: 27.4247 - val_mae: 3.8456\n",
      "Epoch 77/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 453.9790 - mae: 14.7370 - val_loss: 27.8534 - val_mae: 3.8354\n",
      "Epoch 78/250\n",
      "122/122 [==============================] - 0s 546us/step - loss: 427.5691 - mae: 14.3350 - val_loss: 24.9507 - val_mae: 3.5218\n",
      "Epoch 79/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 435.6352 - mae: 14.4082 - val_loss: 29.0215 - val_mae: 3.9186\n",
      "Epoch 80/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 418.5066 - mae: 14.2654 - val_loss: 29.7280 - val_mae: 4.1690\n",
      "Epoch 81/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 428.4447 - mae: 14.3802 - val_loss: 33.7238 - val_mae: 4.3959\n",
      "Epoch 82/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 427.9552 - mae: 14.2051 - val_loss: 44.4020 - val_mae: 5.3872\n",
      "Epoch 83/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 442.1667 - mae: 14.5179 - val_loss: 42.2511 - val_mae: 4.9983\n",
      "Epoch 84/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 396.3915 - mae: 13.8512 - val_loss: 23.0486 - val_mae: 3.4034\n",
      "Epoch 85/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 425.2643 - mae: 14.1549 - val_loss: 26.6308 - val_mae: 3.8599\n",
      "Epoch 86/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 447.2784 - mae: 14.3240 - val_loss: 28.5726 - val_mae: 4.0220\n",
      "Epoch 87/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 412.5644 - mae: 14.2281 - val_loss: 27.5345 - val_mae: 3.8598\n",
      "Epoch 88/250\n",
      "122/122 [==============================] - 0s 529us/step - loss: 391.7782 - mae: 13.6837 - val_loss: 26.2475 - val_mae: 3.8469\n",
      "Epoch 89/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 392.9779 - mae: 13.6144 - val_loss: 25.1130 - val_mae: 3.5874\n",
      "Epoch 90/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 386.9117 - mae: 13.7834 - val_loss: 23.7445 - val_mae: 3.3231\n",
      "Epoch 91/250\n",
      "122/122 [==============================] - 0s 567us/step - loss: 408.6510 - mae: 13.9980 - val_loss: 35.0929 - val_mae: 4.0867\n",
      "Epoch 92/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 388.9678 - mae: 13.6869 - val_loss: 25.0329 - val_mae: 3.7387\n",
      "Epoch 93/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 397.2690 - mae: 13.7047 - val_loss: 24.4701 - val_mae: 3.8044\n",
      "Epoch 94/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 384.8010 - mae: 13.5283 - val_loss: 25.6811 - val_mae: 3.5419\n",
      "Epoch 95/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 396.9923 - mae: 13.5436 - val_loss: 23.4189 - val_mae: 3.5567\n",
      "Epoch 96/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 387.9872 - mae: 13.6863 - val_loss: 43.2776 - val_mae: 4.8674\n",
      "Epoch 97/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 407.1982 - mae: 13.5995 - val_loss: 40.5178 - val_mae: 5.1090\n",
      "Epoch 98/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 375.8081 - mae: 13.2214 - val_loss: 31.6407 - val_mae: 4.2743\n",
      "Epoch 99/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 381.3087 - mae: 13.3333 - val_loss: 22.7654 - val_mae: 3.4143\n",
      "Epoch 100/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 394.2396 - mae: 13.4152 - val_loss: 31.7981 - val_mae: 4.3789\n",
      "Epoch 101/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 368.2865 - mae: 13.1193 - val_loss: 25.3386 - val_mae: 3.5727\n",
      "Epoch 102/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 360.6745 - mae: 13.1903 - val_loss: 27.8641 - val_mae: 3.9749\n",
      "Epoch 103/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 399.9404 - mae: 13.5707 - val_loss: 78.7579 - val_mae: 6.5073\n",
      "Epoch 104/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 363.5266 - mae: 13.1728 - val_loss: 34.6294 - val_mae: 4.1806\n",
      "Epoch 105/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 379.3556 - mae: 13.0596 - val_loss: 25.6367 - val_mae: 3.5538\n",
      "Epoch 106/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 397.1162 - mae: 13.6858 - val_loss: 26.5741 - val_mae: 3.6754\n",
      "Epoch 107/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 388.6499 - mae: 13.3904 - val_loss: 24.5506 - val_mae: 3.7437\n",
      "Epoch 108/250\n",
      "122/122 [==============================] - 0s 633us/step - loss: 379.6792 - mae: 13.2368 - val_loss: 26.0695 - val_mae: 3.8025\n",
      "Epoch 109/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 354.4026 - mae: 12.7396 - val_loss: 28.4289 - val_mae: 3.9943\n",
      "Epoch 110/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 356.4089 - mae: 12.9424 - val_loss: 27.0751 - val_mae: 3.5797\n",
      "Epoch 111/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 351.6009 - mae: 13.0125 - val_loss: 36.9549 - val_mae: 4.1092\n",
      "Epoch 112/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 351.3210 - mae: 12.6279 - val_loss: 31.0873 - val_mae: 4.1581\n",
      "Epoch 113/250\n",
      "122/122 [==============================] - 0s 568us/step - loss: 384.2687 - mae: 13.0226 - val_loss: 32.5162 - val_mae: 4.3096\n",
      "Epoch 114/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 350.7197 - mae: 12.8336 - val_loss: 27.2389 - val_mae: 3.9063\n",
      "Epoch 115/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 370.3195 - mae: 13.1256 - val_loss: 22.9074 - val_mae: 3.4133\n",
      "Epoch 116/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 343.0628 - mae: 12.6166 - val_loss: 22.0399 - val_mae: 3.3569\n",
      "Epoch 117/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 331.8719 - mae: 12.4071 - val_loss: 22.6661 - val_mae: 3.3662\n",
      "Epoch 118/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 354.2045 - mae: 12.7591 - val_loss: 34.1435 - val_mae: 4.4574\n",
      "Epoch 119/250\n",
      "122/122 [==============================] - 0s 529us/step - loss: 356.0969 - mae: 12.5754 - val_loss: 26.4035 - val_mae: 3.6123\n",
      "Epoch 120/250\n",
      "122/122 [==============================] - 0s 544us/step - loss: 349.3655 - mae: 12.5132 - val_loss: 24.1310 - val_mae: 3.4353\n",
      "Epoch 121/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 336.1585 - mae: 12.3388 - val_loss: 25.5830 - val_mae: 3.7152\n",
      "Epoch 122/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 343.5194 - mae: 12.4654 - val_loss: 35.3214 - val_mae: 4.4600\n",
      "Epoch 123/250\n",
      "122/122 [==============================] - 0s 551us/step - loss: 330.8185 - mae: 12.1507 - val_loss: 23.6933 - val_mae: 3.6580\n",
      "Epoch 124/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 335.7335 - mae: 12.4258 - val_loss: 21.1245 - val_mae: 3.1940\n",
      "Epoch 125/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 336.5339 - mae: 12.4121 - val_loss: 30.8739 - val_mae: 4.1879\n",
      "Epoch 126/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 332.9933 - mae: 12.2520 - val_loss: 27.7646 - val_mae: 3.7696\n",
      "Epoch 127/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 320.5712 - mae: 12.2017 - val_loss: 27.3416 - val_mae: 3.7350\n",
      "Epoch 128/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 321.1848 - mae: 12.1886 - val_loss: 23.3095 - val_mae: 3.3977\n",
      "Epoch 129/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 330.8586 - mae: 12.1338 - val_loss: 24.1456 - val_mae: 3.4049\n",
      "Epoch 130/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 332.9631 - mae: 12.2357 - val_loss: 22.6257 - val_mae: 3.3388\n",
      "Epoch 131/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 332.5861 - mae: 12.3125 - val_loss: 23.8895 - val_mae: 3.5220\n",
      "Epoch 132/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 317.0288 - mae: 12.0871 - val_loss: 23.9372 - val_mae: 3.3981\n",
      "Epoch 133/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 320.4331 - mae: 11.9856 - val_loss: 29.4303 - val_mae: 3.7766\n",
      "Epoch 134/250\n",
      "122/122 [==============================] - 0s 571us/step - loss: 333.7586 - mae: 12.3041 - val_loss: 26.3613 - val_mae: 3.7564\n",
      "Epoch 135/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 313.6217 - mae: 11.9532 - val_loss: 26.0142 - val_mae: 3.7333\n",
      "Epoch 136/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 305.2148 - mae: 11.7931 - val_loss: 38.1490 - val_mae: 4.7236\n",
      "Epoch 137/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 338.0989 - mae: 12.1569 - val_loss: 22.2292 - val_mae: 3.3446\n",
      "Epoch 138/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 308.0615 - mae: 11.8255 - val_loss: 24.0631 - val_mae: 3.4885\n",
      "Epoch 139/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 305.2520 - mae: 11.8710 - val_loss: 26.7039 - val_mae: 3.7282\n",
      "Epoch 140/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 305.1659 - mae: 11.7973 - val_loss: 27.3971 - val_mae: 3.9306\n",
      "Epoch 141/250\n",
      "122/122 [==============================] - 0s 647us/step - loss: 310.3060 - mae: 11.7217 - val_loss: 34.1589 - val_mae: 4.4370\n",
      "Epoch 142/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 325.9731 - mae: 11.9646 - val_loss: 23.2794 - val_mae: 3.4085\n",
      "Epoch 143/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 278.5354 - mae: 11.3666 - val_loss: 32.9450 - val_mae: 4.3406\n",
      "Epoch 144/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 285.3141 - mae: 11.3695 - val_loss: 24.3000 - val_mae: 3.4388\n",
      "Epoch 145/250\n",
      "122/122 [==============================] - 0s 528us/step - loss: 314.1947 - mae: 11.7984 - val_loss: 21.9336 - val_mae: 3.3108\n",
      "Epoch 146/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 302.2946 - mae: 11.4617 - val_loss: 25.4268 - val_mae: 3.6687\n",
      "Epoch 147/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 290.1716 - mae: 11.4963 - val_loss: 31.0340 - val_mae: 4.1483\n",
      "Epoch 148/250\n",
      "122/122 [==============================] - 0s 528us/step - loss: 278.9803 - mae: 11.2567 - val_loss: 22.7442 - val_mae: 3.4184\n",
      "Epoch 149/250\n",
      "122/122 [==============================] - 0s 531us/step - loss: 316.3084 - mae: 11.8659 - val_loss: 25.1128 - val_mae: 3.6148\n",
      "Epoch 150/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 296.0656 - mae: 11.5310 - val_loss: 33.4494 - val_mae: 4.3536\n",
      "Epoch 151/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 286.7207 - mae: 11.3090 - val_loss: 38.3935 - val_mae: 4.7232\n",
      "Epoch 152/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 296.5536 - mae: 11.4278 - val_loss: 26.2566 - val_mae: 3.7019\n",
      "Epoch 153/250\n",
      "122/122 [==============================] - 0s 530us/step - loss: 305.2996 - mae: 11.6462 - val_loss: 29.2641 - val_mae: 3.6852\n",
      "Epoch 154/250\n",
      "122/122 [==============================] - 0s 563us/step - loss: 300.9056 - mae: 11.4431 - val_loss: 27.1413 - val_mae: 3.7957\n",
      "Epoch 155/250\n",
      "122/122 [==============================] - 0s 536us/step - loss: 292.3065 - mae: 11.3868 - val_loss: 45.0311 - val_mae: 4.9978\n",
      "Epoch 156/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 264.9399 - mae: 10.9365 - val_loss: 26.7669 - val_mae: 3.6729\n",
      "Epoch 157/250\n",
      "122/122 [==============================] - 0s 547us/step - loss: 284.1805 - mae: 11.1166 - val_loss: 21.9985 - val_mae: 3.3265\n",
      "Epoch 158/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 283.3161 - mae: 11.1225 - val_loss: 24.1512 - val_mae: 3.4997\n",
      "Epoch 159/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 296.5373 - mae: 11.3704 - val_loss: 44.5859 - val_mae: 4.9467\n",
      "Epoch 160/250\n",
      "122/122 [==============================] - 0s 546us/step - loss: 269.3427 - mae: 10.7880 - val_loss: 24.1047 - val_mae: 3.5269\n",
      "Epoch 161/250\n",
      "122/122 [==============================] - 0s 547us/step - loss: 266.8655 - mae: 10.7960 - val_loss: 21.2919 - val_mae: 3.2491\n",
      "Epoch 162/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 292.6977 - mae: 11.1789 - val_loss: 33.9403 - val_mae: 4.1632\n",
      "Epoch 163/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 275.8683 - mae: 10.9753 - val_loss: 22.8829 - val_mae: 3.4643\n",
      "Epoch 164/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 270.9843 - mae: 10.7716 - val_loss: 23.4777 - val_mae: 3.3896\n",
      "Epoch 165/250\n",
      "122/122 [==============================] - 0s 546us/step - loss: 273.9001 - mae: 10.8723 - val_loss: 25.9564 - val_mae: 3.5513\n",
      "Epoch 166/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 271.9593 - mae: 10.9357 - val_loss: 31.5864 - val_mae: 4.2963\n",
      "Epoch 167/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 285.9467 - mae: 11.1646 - val_loss: 22.7164 - val_mae: 3.4670\n",
      "Epoch 168/250\n",
      "122/122 [==============================] - 0s 560us/step - loss: 239.6584 - mae: 10.5261 - val_loss: 40.4308 - val_mae: 4.8273\n",
      "Epoch 169/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 263.7720 - mae: 10.7096 - val_loss: 27.0075 - val_mae: 3.8965\n",
      "Epoch 170/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 251.2303 - mae: 10.5234 - val_loss: 24.7225 - val_mae: 3.5119\n",
      "Epoch 171/250\n",
      "122/122 [==============================] - 0s 639us/step - loss: 255.9742 - mae: 10.5219 - val_loss: 30.2783 - val_mae: 4.1513\n",
      "Epoch 172/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 257.0857 - mae: 10.5183 - val_loss: 22.0063 - val_mae: 3.3350\n",
      "Epoch 173/250\n",
      "122/122 [==============================] - 0s 669us/step - loss: 242.3177 - mae: 10.3670 - val_loss: 32.2741 - val_mae: 4.2290\n",
      "Epoch 174/250\n",
      "122/122 [==============================] - 0s 659us/step - loss: 245.7527 - mae: 10.2877 - val_loss: 37.5685 - val_mae: 4.6366\n",
      "Epoch 175/250\n",
      "122/122 [==============================] - 0s 589us/step - loss: 249.1718 - mae: 10.3524 - val_loss: 24.0119 - val_mae: 3.4885\n",
      "Epoch 176/250\n",
      "122/122 [==============================] - 0s 702us/step - loss: 254.9846 - mae: 10.4017 - val_loss: 27.5697 - val_mae: 3.9893\n",
      "Epoch 177/250\n",
      "122/122 [==============================] - 0s 724us/step - loss: 246.0025 - mae: 10.3138 - val_loss: 30.5050 - val_mae: 4.2113\n",
      "Epoch 178/250\n",
      "122/122 [==============================] - 0s 561us/step - loss: 255.8503 - mae: 10.4691 - val_loss: 25.9366 - val_mae: 3.7722\n",
      "Epoch 179/250\n",
      "122/122 [==============================] - 0s 572us/step - loss: 255.9217 - mae: 10.2604 - val_loss: 31.4991 - val_mae: 4.2209\n",
      "Epoch 180/250\n",
      "122/122 [==============================] - 0s 566us/step - loss: 250.1382 - mae: 10.4093 - val_loss: 22.5694 - val_mae: 3.3732\n",
      "Epoch 181/250\n",
      "122/122 [==============================] - 0s 601us/step - loss: 246.1261 - mae: 10.2686 - val_loss: 25.5747 - val_mae: 3.7717\n",
      "Epoch 182/250\n",
      "122/122 [==============================] - 0s 627us/step - loss: 232.0232 - mae: 9.9858 - val_loss: 29.4286 - val_mae: 4.0699\n",
      "Epoch 183/250\n",
      "122/122 [==============================] - 0s 607us/step - loss: 228.0138 - mae: 10.0644 - val_loss: 23.3762 - val_mae: 3.4723\n",
      "Epoch 184/250\n",
      "122/122 [==============================] - 0s 546us/step - loss: 241.2014 - mae: 10.0564 - val_loss: 21.7235 - val_mae: 3.2931\n",
      "Epoch 185/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 239.7988 - mae: 10.1872 - val_loss: 34.3834 - val_mae: 4.4606\n",
      "Epoch 186/250\n",
      "122/122 [==============================] - 0s 553us/step - loss: 240.5672 - mae: 10.1097 - val_loss: 22.3709 - val_mae: 3.3648\n",
      "Epoch 187/250\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 223.6675 - mae: 9.9759 - val_loss: 26.3870 - val_mae: 3.6272\n",
      "Epoch 188/250\n",
      "122/122 [==============================] - 0s 636us/step - loss: 214.3291 - mae: 9.5749 - val_loss: 26.0232 - val_mae: 3.7557\n",
      "Epoch 189/250\n",
      "122/122 [==============================] - 0s 595us/step - loss: 237.9035 - mae: 10.0624 - val_loss: 23.1209 - val_mae: 3.4314\n",
      "Epoch 190/250\n",
      "122/122 [==============================] - 0s 551us/step - loss: 229.6065 - mae: 9.9623 - val_loss: 43.5295 - val_mae: 5.0611\n",
      "Epoch 191/250\n",
      "122/122 [==============================] - 0s 582us/step - loss: 230.3884 - mae: 9.8570 - val_loss: 24.5064 - val_mae: 3.5922\n",
      "Epoch 192/250\n",
      "122/122 [==============================] - 0s 572us/step - loss: 225.6462 - mae: 9.6882 - val_loss: 30.2106 - val_mae: 3.8881\n",
      "Epoch 193/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 237.4110 - mae: 9.9615 - val_loss: 23.7010 - val_mae: 3.5204\n",
      "Epoch 194/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 227.7070 - mae: 9.7897 - val_loss: 21.8542 - val_mae: 3.2979\n",
      "Epoch 195/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 239.9609 - mae: 9.9062 - val_loss: 22.9139 - val_mae: 3.4202\n",
      "Epoch 196/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 224.8566 - mae: 9.7608 - val_loss: 23.8918 - val_mae: 3.5155\n",
      "Epoch 197/250\n",
      "122/122 [==============================] - 0s 648us/step - loss: 215.0303 - mae: 9.5904 - val_loss: 22.6697 - val_mae: 3.4139\n",
      "Epoch 198/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 222.4326 - mae: 9.6621 - val_loss: 22.9806 - val_mae: 3.3931\n",
      "Epoch 199/250\n",
      "122/122 [==============================] - 0s 548us/step - loss: 210.1240 - mae: 9.5023 - val_loss: 36.1257 - val_mae: 4.5476\n",
      "Epoch 200/250\n",
      "122/122 [==============================] - 0s 635us/step - loss: 216.5226 - mae: 9.4336 - val_loss: 29.5541 - val_mae: 3.9698\n",
      "Epoch 201/250\n",
      "122/122 [==============================] - 0s 577us/step - loss: 222.7832 - mae: 9.6705 - val_loss: 21.4141 - val_mae: 3.2519\n",
      "Epoch 202/250\n",
      "122/122 [==============================] - 0s 584us/step - loss: 214.8678 - mae: 9.4923 - val_loss: 27.8088 - val_mae: 3.9244\n",
      "Epoch 203/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 218.7773 - mae: 9.4375 - val_loss: 25.4314 - val_mae: 3.6984\n",
      "Epoch 204/250\n",
      "122/122 [==============================] - 0s 549us/step - loss: 208.9921 - mae: 9.4320 - val_loss: 33.2626 - val_mae: 4.2927\n",
      "Epoch 205/250\n",
      "122/122 [==============================] - 0s 532us/step - loss: 189.1879 - mae: 9.0500 - val_loss: 21.5016 - val_mae: 3.2702\n",
      "Epoch 206/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 209.6623 - mae: 9.3573 - val_loss: 23.2465 - val_mae: 3.4060\n",
      "Epoch 207/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 200.7454 - mae: 9.1903 - val_loss: 39.9686 - val_mae: 4.7663\n",
      "Epoch 208/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 209.7993 - mae: 9.3962 - val_loss: 27.1312 - val_mae: 3.8243\n",
      "Epoch 209/250\n",
      "122/122 [==============================] - 0s 529us/step - loss: 204.7556 - mae: 9.2577 - val_loss: 26.4853 - val_mae: 3.7906\n",
      "Epoch 210/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 194.1251 - mae: 9.1581 - val_loss: 26.3816 - val_mae: 3.8129\n",
      "Epoch 211/250\n",
      "122/122 [==============================] - 0s 558us/step - loss: 195.2011 - mae: 9.1670 - val_loss: 22.8878 - val_mae: 3.3676\n",
      "Epoch 212/250\n",
      "122/122 [==============================] - 0s 537us/step - loss: 192.3423 - mae: 9.0244 - val_loss: 26.5834 - val_mae: 3.8324\n",
      "Epoch 213/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 198.2826 - mae: 9.0420 - val_loss: 22.5928 - val_mae: 3.4485\n",
      "Epoch 214/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 189.3796 - mae: 8.9608 - val_loss: 27.3632 - val_mae: 3.9255\n",
      "Epoch 215/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 189.2804 - mae: 8.9836 - val_loss: 33.4338 - val_mae: 4.2793\n",
      "Epoch 216/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 191.7023 - mae: 8.9697 - val_loss: 22.7614 - val_mae: 3.4567\n",
      "Epoch 217/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 180.6250 - mae: 8.8161 - val_loss: 26.1456 - val_mae: 3.8274\n",
      "Epoch 218/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 189.6488 - mae: 9.0562 - val_loss: 25.7643 - val_mae: 3.6902\n",
      "Epoch 219/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 189.1035 - mae: 8.9733 - val_loss: 44.0495 - val_mae: 5.0568\n",
      "Epoch 220/250\n",
      "122/122 [==============================] - 0s 540us/step - loss: 197.8564 - mae: 9.0234 - val_loss: 24.5188 - val_mae: 3.5647\n",
      "Epoch 221/250\n",
      "122/122 [==============================] - 0s 641us/step - loss: 197.8610 - mae: 9.1225 - val_loss: 27.7497 - val_mae: 3.7441\n",
      "Epoch 222/250\n",
      "122/122 [==============================] - 0s 624us/step - loss: 183.3767 - mae: 8.8352 - val_loss: 23.7946 - val_mae: 3.5103\n",
      "Epoch 223/250\n",
      "122/122 [==============================] - 0s 565us/step - loss: 178.4904 - mae: 8.6717 - val_loss: 30.7914 - val_mae: 4.2115\n",
      "Epoch 224/250\n",
      "122/122 [==============================] - 0s 585us/step - loss: 181.4504 - mae: 8.7791 - val_loss: 28.5223 - val_mae: 3.8965\n",
      "Epoch 225/250\n",
      "122/122 [==============================] - 0s 615us/step - loss: 177.3389 - mae: 8.7213 - val_loss: 27.4831 - val_mae: 3.8686\n",
      "Epoch 226/250\n",
      "122/122 [==============================] - 0s 545us/step - loss: 181.7599 - mae: 8.7359 - val_loss: 27.5523 - val_mae: 3.9308\n",
      "Epoch 227/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 185.9133 - mae: 8.8580 - val_loss: 23.8139 - val_mae: 3.5115\n",
      "Epoch 228/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 184.6077 - mae: 8.8132 - val_loss: 22.9141 - val_mae: 3.3784\n",
      "Epoch 229/250\n",
      "122/122 [==============================] - 0s 534us/step - loss: 172.7608 - mae: 8.5489 - val_loss: 24.0832 - val_mae: 3.5216\n",
      "Epoch 230/250\n",
      "122/122 [==============================] - 0s 538us/step - loss: 178.4840 - mae: 8.6043 - val_loss: 27.3518 - val_mae: 3.8080\n",
      "Epoch 231/250\n",
      "122/122 [==============================] - 0s 539us/step - loss: 168.4775 - mae: 8.6168 - val_loss: 23.3635 - val_mae: 3.4880\n",
      "Epoch 232/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 161.5078 - mae: 8.4200 - val_loss: 29.4418 - val_mae: 3.9312\n",
      "Epoch 233/250\n",
      "122/122 [==============================] - 0s 566us/step - loss: 170.4654 - mae: 8.5257 - val_loss: 25.6307 - val_mae: 3.5886\n",
      "Epoch 234/250\n",
      "122/122 [==============================] - 0s 533us/step - loss: 182.8021 - mae: 8.7534 - val_loss: 24.4368 - val_mae: 3.4395\n",
      "Epoch 235/250\n",
      "122/122 [==============================] - 0s 543us/step - loss: 169.7722 - mae: 8.5501 - val_loss: 28.2558 - val_mae: 3.9075\n",
      "Epoch 236/250\n",
      "122/122 [==============================] - 0s 566us/step - loss: 174.6812 - mae: 8.6054 - val_loss: 29.5450 - val_mae: 4.0826\n",
      "Epoch 237/250\n",
      "122/122 [==============================] - 0s 549us/step - loss: 167.8149 - mae: 8.3641 - val_loss: 27.8372 - val_mae: 3.8725\n",
      "Epoch 238/250\n",
      "122/122 [==============================] - 0s 571us/step - loss: 166.2465 - mae: 8.4731 - val_loss: 25.0116 - val_mae: 3.5786\n",
      "Epoch 239/250\n",
      "122/122 [==============================] - 0s 565us/step - loss: 165.1420 - mae: 8.2758 - val_loss: 22.3435 - val_mae: 3.3746\n",
      "Epoch 240/250\n",
      "122/122 [==============================] - 0s 553us/step - loss: 168.5083 - mae: 8.4797 - val_loss: 25.6214 - val_mae: 3.6092\n",
      "Epoch 241/250\n",
      "122/122 [==============================] - 0s 553us/step - loss: 166.7229 - mae: 8.4605 - val_loss: 26.0843 - val_mae: 3.7370\n",
      "Epoch 242/250\n",
      "122/122 [==============================] - 0s 557us/step - loss: 159.1409 - mae: 8.3231 - val_loss: 23.8544 - val_mae: 3.4995\n",
      "Epoch 243/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 168.0970 - mae: 8.4150 - val_loss: 39.5921 - val_mae: 4.8513\n",
      "Epoch 244/250\n",
      "122/122 [==============================] - 0s 558us/step - loss: 162.5664 - mae: 8.3381 - val_loss: 24.6978 - val_mae: 3.6381\n",
      "Epoch 245/250\n",
      "122/122 [==============================] - 0s 549us/step - loss: 154.6143 - mae: 8.1216 - val_loss: 21.1095 - val_mae: 3.2790\n",
      "Epoch 246/250\n",
      "122/122 [==============================] - 0s 541us/step - loss: 164.8073 - mae: 8.3007 - val_loss: 24.9147 - val_mae: 3.4888\n",
      "Epoch 247/250\n",
      "122/122 [==============================] - 0s 567us/step - loss: 159.7355 - mae: 8.2394 - val_loss: 25.1591 - val_mae: 3.6700\n",
      "Epoch 248/250\n",
      "122/122 [==============================] - 0s 542us/step - loss: 158.2307 - mae: 8.2440 - val_loss: 22.3065 - val_mae: 3.3975\n",
      "Epoch 249/250\n",
      "122/122 [==============================] - 0s 535us/step - loss: 158.0461 - mae: 8.2164 - val_loss: 28.7668 - val_mae: 3.9248\n",
      "Epoch 250/250\n",
      "122/122 [==============================] - 0s 551us/step - loss: 174.4726 - mae: 8.4845 - val_loss: 32.3638 - val_mae: 4.3641\n",
      "31/31 [==============================] - 0s 304us/step\n",
      "Epochs: 250 | MAE: 4.3641277916541465\n",
      "Training model with 300 epochs\n",
      "Epoch 1/300\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 66678.3906 - mae: 223.8096 - val_loss: 55024.6406 - val_mae: 203.0565\n",
      "Epoch 2/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 31536.6094 - mae: 147.5010 - val_loss: 8884.3975 - val_mae: 78.1558\n",
      "Epoch 3/300\n",
      "122/122 [==============================] - 0s 565us/step - loss: 5592.6392 - mae: 56.5223 - val_loss: 3274.3235 - val_mae: 42.3371\n",
      "Epoch 4/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 3398.9468 - mae: 43.0228 - val_loss: 2174.8516 - val_mae: 34.7895\n",
      "Epoch 5/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 2670.0620 - mae: 38.4810 - val_loss: 1679.1299 - val_mae: 30.6264\n",
      "Epoch 6/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 2175.7871 - mae: 34.9429 - val_loss: 1353.9648 - val_mae: 27.3110\n",
      "Epoch 7/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 1952.7128 - mae: 33.7032 - val_loss: 1179.5912 - val_mae: 25.4024\n",
      "Epoch 8/300\n",
      "122/122 [==============================] - 0s 539us/step - loss: 1835.8975 - mae: 32.3648 - val_loss: 1045.5392 - val_mae: 24.0333\n",
      "Epoch 9/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 1624.7627 - mae: 30.5464 - val_loss: 910.3050 - val_mae: 22.4855\n",
      "Epoch 10/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 1553.6941 - mae: 30.2265 - val_loss: 822.0820 - val_mae: 21.3833\n",
      "Epoch 11/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 1371.4623 - mae: 29.1716 - val_loss: 747.0206 - val_mae: 20.4017\n",
      "Epoch 12/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 1403.3121 - mae: 28.6277 - val_loss: 679.5765 - val_mae: 19.4606\n",
      "Epoch 13/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 1214.3065 - mae: 27.3165 - val_loss: 601.2009 - val_mae: 18.2817\n",
      "Epoch 14/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 1219.5190 - mae: 26.9718 - val_loss: 538.5217 - val_mae: 17.4941\n",
      "Epoch 15/300\n",
      "122/122 [==============================] - 0s 539us/step - loss: 1119.1329 - mae: 25.9310 - val_loss: 481.2971 - val_mae: 16.5946\n",
      "Epoch 16/300\n",
      "122/122 [==============================] - 0s 575us/step - loss: 1021.6098 - mae: 24.7383 - val_loss: 438.5306 - val_mae: 15.6133\n",
      "Epoch 17/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 1041.9844 - mae: 25.0513 - val_loss: 395.9029 - val_mae: 14.9544\n",
      "Epoch 18/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 1011.2563 - mae: 24.4828 - val_loss: 363.6926 - val_mae: 14.5193\n",
      "Epoch 19/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 919.7374 - mae: 23.4891 - val_loss: 310.2382 - val_mae: 13.5503\n",
      "Epoch 20/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 877.8199 - mae: 22.9187 - val_loss: 275.9322 - val_mae: 12.4078\n",
      "Epoch 21/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 862.2541 - mae: 22.7168 - val_loss: 258.6461 - val_mae: 12.3291\n",
      "Epoch 22/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 822.5820 - mae: 21.8774 - val_loss: 218.7894 - val_mae: 10.9663\n",
      "Epoch 23/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 753.8409 - mae: 21.0211 - val_loss: 205.0436 - val_mae: 10.7566\n",
      "Epoch 24/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 774.4460 - mae: 21.0069 - val_loss: 174.0050 - val_mae: 9.6728\n",
      "Epoch 25/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 778.0491 - mae: 21.1113 - val_loss: 153.2333 - val_mae: 9.1367\n",
      "Epoch 26/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 723.5891 - mae: 20.4713 - val_loss: 145.1304 - val_mae: 9.0509\n",
      "Epoch 27/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 690.8656 - mae: 19.7037 - val_loss: 128.5499 - val_mae: 8.7128\n",
      "Epoch 28/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 699.5538 - mae: 19.8146 - val_loss: 121.3489 - val_mae: 8.1222\n",
      "Epoch 29/300\n",
      "122/122 [==============================] - 0s 606us/step - loss: 629.3592 - mae: 18.8920 - val_loss: 98.8448 - val_mae: 7.4783\n",
      "Epoch 30/300\n",
      "122/122 [==============================] - 0s 564us/step - loss: 662.3893 - mae: 19.3595 - val_loss: 116.1261 - val_mae: 8.5468\n",
      "Epoch 31/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 625.3570 - mae: 18.6533 - val_loss: 92.1930 - val_mae: 7.1281\n",
      "Epoch 32/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 633.0682 - mae: 18.7571 - val_loss: 92.8370 - val_mae: 7.5088\n",
      "Epoch 33/300\n",
      "122/122 [==============================] - 0s 556us/step - loss: 614.1108 - mae: 18.5975 - val_loss: 81.3582 - val_mae: 6.8622\n",
      "Epoch 34/300\n",
      "122/122 [==============================] - 0s 576us/step - loss: 643.7616 - mae: 18.5626 - val_loss: 82.9458 - val_mae: 7.0036\n",
      "Epoch 35/300\n",
      "122/122 [==============================] - 0s 559us/step - loss: 578.8169 - mae: 17.8038 - val_loss: 60.6865 - val_mae: 5.9381\n",
      "Epoch 36/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 582.4464 - mae: 17.8221 - val_loss: 66.0346 - val_mae: 6.1043\n",
      "Epoch 37/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 609.3379 - mae: 18.0365 - val_loss: 58.3580 - val_mae: 5.8443\n",
      "Epoch 38/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 599.2266 - mae: 17.7763 - val_loss: 54.2359 - val_mae: 5.6279\n",
      "Epoch 39/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 578.6977 - mae: 17.5705 - val_loss: 59.6096 - val_mae: 5.9273\n",
      "Epoch 40/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 567.2268 - mae: 17.3485 - val_loss: 45.8315 - val_mae: 5.1540\n",
      "Epoch 41/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 565.4174 - mae: 17.2811 - val_loss: 40.0834 - val_mae: 4.8307\n",
      "Epoch 42/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 552.9830 - mae: 17.1305 - val_loss: 47.8134 - val_mae: 5.2485\n",
      "Epoch 43/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 519.4392 - mae: 16.4997 - val_loss: 34.8822 - val_mae: 4.5473\n",
      "Epoch 44/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 497.3122 - mae: 16.1590 - val_loss: 37.4966 - val_mae: 4.4518\n",
      "Epoch 45/300\n",
      "122/122 [==============================] - 0s 536us/step - loss: 547.2562 - mae: 16.7878 - val_loss: 39.0955 - val_mae: 4.5911\n",
      "Epoch 46/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 538.4311 - mae: 16.5386 - val_loss: 38.1128 - val_mae: 4.7872\n",
      "Epoch 47/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 515.5009 - mae: 16.2588 - val_loss: 34.6398 - val_mae: 4.3569\n",
      "Epoch 48/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 544.0198 - mae: 16.6656 - val_loss: 36.6709 - val_mae: 4.4437\n",
      "Epoch 49/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 512.3514 - mae: 16.2020 - val_loss: 36.3101 - val_mae: 4.7225\n",
      "Epoch 50/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 532.6380 - mae: 16.2251 - val_loss: 30.3935 - val_mae: 4.1857\n",
      "Epoch 51/300\n",
      "122/122 [==============================] - 0s 581us/step - loss: 526.1359 - mae: 16.1482 - val_loss: 44.3323 - val_mae: 4.8385\n",
      "Epoch 52/300\n",
      "122/122 [==============================] - 0s 540us/step - loss: 487.7618 - mae: 15.7170 - val_loss: 27.4755 - val_mae: 3.7728\n",
      "Epoch 53/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 495.2887 - mae: 15.7862 - val_loss: 36.1253 - val_mae: 4.6518\n",
      "Epoch 54/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 496.1937 - mae: 15.8745 - val_loss: 37.0664 - val_mae: 4.5751\n",
      "Epoch 55/300\n",
      "122/122 [==============================] - 0s 641us/step - loss: 495.6524 - mae: 15.7171 - val_loss: 27.9377 - val_mae: 3.7960\n",
      "Epoch 56/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 481.3947 - mae: 15.6201 - val_loss: 31.4964 - val_mae: 4.1663\n",
      "Epoch 57/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 493.4355 - mae: 15.8503 - val_loss: 27.8289 - val_mae: 3.6835\n",
      "Epoch 58/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 518.2883 - mae: 15.8783 - val_loss: 32.2373 - val_mae: 4.2624\n",
      "Epoch 59/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 491.1475 - mae: 15.5461 - val_loss: 30.0134 - val_mae: 3.9858\n",
      "Epoch 60/300\n",
      "122/122 [==============================] - 0s 568us/step - loss: 457.6829 - mae: 15.1844 - val_loss: 24.5054 - val_mae: 3.5513\n",
      "Epoch 61/300\n",
      "122/122 [==============================] - 0s 538us/step - loss: 471.7704 - mae: 15.2905 - val_loss: 28.3457 - val_mae: 4.0501\n",
      "Epoch 62/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 482.7256 - mae: 15.4388 - val_loss: 28.3430 - val_mae: 3.8534\n",
      "Epoch 63/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 449.0828 - mae: 14.9055 - val_loss: 30.0490 - val_mae: 3.9821\n",
      "Epoch 64/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 457.9423 - mae: 14.8227 - val_loss: 33.1182 - val_mae: 4.1612\n",
      "Epoch 65/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 457.7474 - mae: 15.0046 - val_loss: 24.9589 - val_mae: 3.6760\n",
      "Epoch 66/300\n",
      "122/122 [==============================] - 0s 578us/step - loss: 461.7145 - mae: 14.9515 - val_loss: 30.1297 - val_mae: 4.0767\n",
      "Epoch 67/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 457.8503 - mae: 15.1161 - val_loss: 35.0747 - val_mae: 4.3204\n",
      "Epoch 68/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 445.5000 - mae: 14.8294 - val_loss: 27.7881 - val_mae: 3.9613\n",
      "Epoch 69/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 452.8987 - mae: 14.8572 - val_loss: 30.2360 - val_mae: 4.1211\n",
      "Epoch 70/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 460.8916 - mae: 15.0069 - val_loss: 29.4793 - val_mae: 3.9186\n",
      "Epoch 71/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 429.3776 - mae: 14.4766 - val_loss: 25.4351 - val_mae: 3.7220\n",
      "Epoch 72/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 431.4948 - mae: 14.5294 - val_loss: 29.1813 - val_mae: 4.0528\n",
      "Epoch 73/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 455.2731 - mae: 14.7016 - val_loss: 31.9001 - val_mae: 3.9736\n",
      "Epoch 74/300\n",
      "122/122 [==============================] - 0s 563us/step - loss: 455.8252 - mae: 14.7251 - val_loss: 33.8287 - val_mae: 4.4154\n",
      "Epoch 75/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 453.9953 - mae: 14.6806 - val_loss: 33.8200 - val_mae: 4.4293\n",
      "Epoch 76/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 426.8014 - mae: 14.3194 - val_loss: 30.5658 - val_mae: 4.1384\n",
      "Epoch 77/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 417.8798 - mae: 14.2717 - val_loss: 30.6249 - val_mae: 3.7223\n",
      "Epoch 78/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 423.4921 - mae: 14.2344 - val_loss: 37.4850 - val_mae: 4.5451\n",
      "Epoch 79/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 462.9905 - mae: 14.8076 - val_loss: 27.1005 - val_mae: 3.8377\n",
      "Epoch 80/300\n",
      "122/122 [==============================] - 0s 571us/step - loss: 431.0590 - mae: 14.2013 - val_loss: 30.0241 - val_mae: 3.8640\n",
      "Epoch 81/300\n",
      "122/122 [==============================] - 0s 641us/step - loss: 443.0478 - mae: 14.4592 - val_loss: 36.5039 - val_mae: 4.5682\n",
      "Epoch 82/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 420.8589 - mae: 14.2981 - val_loss: 25.5854 - val_mae: 3.7420\n",
      "Epoch 83/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 402.1209 - mae: 14.0238 - val_loss: 37.6133 - val_mae: 4.6536\n",
      "Epoch 84/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 408.1603 - mae: 14.0022 - val_loss: 32.9011 - val_mae: 4.0049\n",
      "Epoch 85/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 400.0630 - mae: 13.9264 - val_loss: 26.8865 - val_mae: 3.7009\n",
      "Epoch 86/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 424.3330 - mae: 14.1237 - val_loss: 24.0628 - val_mae: 3.4546\n",
      "Epoch 87/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 418.9263 - mae: 14.0361 - val_loss: 31.3251 - val_mae: 4.1873\n",
      "Epoch 88/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 379.7243 - mae: 13.4340 - val_loss: 31.2260 - val_mae: 4.0258\n",
      "Epoch 89/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 423.0180 - mae: 13.9363 - val_loss: 32.3686 - val_mae: 4.2960\n",
      "Epoch 90/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 408.0372 - mae: 13.8092 - val_loss: 28.1309 - val_mae: 3.7129\n",
      "Epoch 91/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 412.8662 - mae: 13.8315 - val_loss: 33.7387 - val_mae: 4.4115\n",
      "Epoch 92/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 369.9508 - mae: 13.3432 - val_loss: 25.5964 - val_mae: 3.6060\n",
      "Epoch 93/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 383.7100 - mae: 13.4814 - val_loss: 49.3189 - val_mae: 5.4486\n",
      "Epoch 94/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 399.5538 - mae: 13.5840 - val_loss: 32.9815 - val_mae: 4.1687\n",
      "Epoch 95/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 413.9327 - mae: 13.8248 - val_loss: 31.2908 - val_mae: 3.9741\n",
      "Epoch 96/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 382.9109 - mae: 13.3402 - val_loss: 28.7203 - val_mae: 3.8235\n",
      "Epoch 97/300\n",
      "122/122 [==============================] - 0s 585us/step - loss: 380.1047 - mae: 13.5110 - val_loss: 30.1523 - val_mae: 4.0571\n",
      "Epoch 98/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 388.4340 - mae: 13.4756 - val_loss: 36.0618 - val_mae: 4.6083\n",
      "Epoch 99/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 394.4487 - mae: 13.4930 - val_loss: 30.0678 - val_mae: 3.8424\n",
      "Epoch 100/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 384.0109 - mae: 13.4919 - val_loss: 28.1010 - val_mae: 3.7940\n",
      "Epoch 101/300\n",
      "122/122 [==============================] - 0s 563us/step - loss: 376.7677 - mae: 13.1082 - val_loss: 28.9060 - val_mae: 3.7494\n",
      "Epoch 102/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 379.4044 - mae: 13.4453 - val_loss: 22.2081 - val_mae: 3.3344\n",
      "Epoch 103/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 375.8161 - mae: 13.2140 - val_loss: 25.3054 - val_mae: 3.6625\n",
      "Epoch 104/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 368.5698 - mae: 13.0478 - val_loss: 25.1194 - val_mae: 3.5195\n",
      "Epoch 105/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 365.3677 - mae: 13.1053 - val_loss: 32.7902 - val_mae: 4.2928\n",
      "Epoch 106/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 370.1763 - mae: 13.0629 - val_loss: 25.1370 - val_mae: 3.4926\n",
      "Epoch 107/300\n",
      "122/122 [==============================] - 0s 603us/step - loss: 360.9691 - mae: 12.9431 - val_loss: 25.2266 - val_mae: 3.4440\n",
      "Epoch 108/300\n",
      "122/122 [==============================] - 0s 574us/step - loss: 371.8525 - mae: 13.0426 - val_loss: 30.2224 - val_mae: 4.1734\n",
      "Epoch 109/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 357.4651 - mae: 12.9222 - val_loss: 33.5828 - val_mae: 4.3394\n",
      "Epoch 110/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 387.1545 - mae: 13.1812 - val_loss: 31.2221 - val_mae: 3.9626\n",
      "Epoch 111/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 382.0683 - mae: 13.0952 - val_loss: 28.7629 - val_mae: 3.9068\n",
      "Epoch 112/300\n",
      "122/122 [==============================] - 0s 575us/step - loss: 348.2897 - mae: 12.6784 - val_loss: 35.1256 - val_mae: 4.3313\n",
      "Epoch 113/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 334.5173 - mae: 12.5015 - val_loss: 47.6317 - val_mae: 5.3781\n",
      "Epoch 114/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 365.2149 - mae: 12.7915 - val_loss: 42.0269 - val_mae: 4.8704\n",
      "Epoch 115/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 328.2180 - mae: 12.4486 - val_loss: 25.0625 - val_mae: 3.4332\n",
      "Epoch 116/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 338.0403 - mae: 12.3542 - val_loss: 28.5561 - val_mae: 3.6961\n",
      "Epoch 117/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 367.2193 - mae: 12.8473 - val_loss: 27.9692 - val_mae: 3.8243\n",
      "Epoch 118/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 353.9575 - mae: 12.5766 - val_loss: 25.4222 - val_mae: 3.7184\n",
      "Epoch 119/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 329.4023 - mae: 12.2071 - val_loss: 32.2335 - val_mae: 3.8756\n",
      "Epoch 120/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 347.1591 - mae: 12.4469 - val_loss: 26.2788 - val_mae: 3.5538\n",
      "Epoch 121/300\n",
      "122/122 [==============================] - 0s 538us/step - loss: 338.1017 - mae: 12.3825 - val_loss: 27.3529 - val_mae: 3.7517\n",
      "Epoch 122/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 358.6042 - mae: 12.5652 - val_loss: 30.3971 - val_mae: 4.0138\n",
      "Epoch 123/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 337.8505 - mae: 12.3485 - val_loss: 24.9653 - val_mae: 3.4285\n",
      "Epoch 124/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 337.7501 - mae: 12.2386 - val_loss: 23.9955 - val_mae: 3.4782\n",
      "Epoch 125/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 332.5650 - mae: 12.1744 - val_loss: 26.3246 - val_mae: 3.6748\n",
      "Epoch 126/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 335.5221 - mae: 12.3065 - val_loss: 27.0326 - val_mae: 3.7731\n",
      "Epoch 127/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 314.3169 - mae: 11.9232 - val_loss: 24.4116 - val_mae: 3.4006\n",
      "Epoch 128/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 329.4503 - mae: 12.2112 - val_loss: 26.1519 - val_mae: 3.5557\n",
      "Epoch 129/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 341.0934 - mae: 12.0740 - val_loss: 31.3881 - val_mae: 4.1708\n",
      "Epoch 130/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 329.9236 - mae: 12.1960 - val_loss: 25.5273 - val_mae: 3.4996\n",
      "Epoch 131/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 328.7934 - mae: 12.1227 - val_loss: 23.6623 - val_mae: 3.4167\n",
      "Epoch 132/300\n",
      "122/122 [==============================] - 0s 607us/step - loss: 329.0788 - mae: 12.2365 - val_loss: 36.5418 - val_mae: 4.5721\n",
      "Epoch 133/300\n",
      "122/122 [==============================] - 0s 583us/step - loss: 294.7277 - mae: 11.6914 - val_loss: 22.9778 - val_mae: 3.3357\n",
      "Epoch 134/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 315.6070 - mae: 11.9106 - val_loss: 31.8690 - val_mae: 4.1376\n",
      "Epoch 135/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 311.0635 - mae: 11.7888 - val_loss: 32.3543 - val_mae: 4.2623\n",
      "Epoch 136/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 300.1679 - mae: 11.7791 - val_loss: 26.4961 - val_mae: 3.7217\n",
      "Epoch 137/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 297.3991 - mae: 11.5562 - val_loss: 23.9767 - val_mae: 3.5643\n",
      "Epoch 138/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 311.4074 - mae: 11.8044 - val_loss: 34.9808 - val_mae: 4.5352\n",
      "Epoch 139/300\n",
      "122/122 [==============================] - 0s 565us/step - loss: 306.5672 - mae: 11.7239 - val_loss: 20.5708 - val_mae: 3.1745\n",
      "Epoch 140/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 316.8647 - mae: 11.8849 - val_loss: 28.1376 - val_mae: 3.7738\n",
      "Epoch 141/300\n",
      "122/122 [==============================] - 0s 559us/step - loss: 313.9272 - mae: 11.7642 - val_loss: 21.9095 - val_mae: 3.3266\n",
      "Epoch 142/300\n",
      "122/122 [==============================] - 0s 585us/step - loss: 299.5053 - mae: 11.5784 - val_loss: 23.2693 - val_mae: 3.4376\n",
      "Epoch 143/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 292.2080 - mae: 11.5078 - val_loss: 27.3055 - val_mae: 3.9160\n",
      "Epoch 144/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 298.8706 - mae: 11.6052 - val_loss: 28.2426 - val_mae: 3.9134\n",
      "Epoch 145/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 306.1815 - mae: 11.6748 - val_loss: 21.9695 - val_mae: 3.2847\n",
      "Epoch 146/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 301.0601 - mae: 11.5498 - val_loss: 22.0804 - val_mae: 3.3283\n",
      "Epoch 147/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 300.9204 - mae: 11.6540 - val_loss: 34.1192 - val_mae: 4.4776\n",
      "Epoch 148/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 279.9845 - mae: 11.2224 - val_loss: 26.6348 - val_mae: 3.8387\n",
      "Epoch 149/300\n",
      "122/122 [==============================] - 0s 556us/step - loss: 278.0232 - mae: 11.1296 - val_loss: 28.2497 - val_mae: 4.0003\n",
      "Epoch 150/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 288.3959 - mae: 11.2137 - val_loss: 24.0510 - val_mae: 3.4443\n",
      "Epoch 151/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 266.2442 - mae: 10.9804 - val_loss: 21.3275 - val_mae: 3.2319\n",
      "Epoch 152/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 273.4963 - mae: 11.0196 - val_loss: 22.2492 - val_mae: 3.3646\n",
      "Epoch 153/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 281.1790 - mae: 11.1759 - val_loss: 33.3101 - val_mae: 4.3627\n",
      "Epoch 154/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 283.0577 - mae: 11.1284 - val_loss: 23.4011 - val_mae: 3.4943\n",
      "Epoch 155/300\n",
      "122/122 [==============================] - 0s 640us/step - loss: 265.1114 - mae: 10.8849 - val_loss: 27.1725 - val_mae: 3.9122\n",
      "Epoch 156/300\n",
      "122/122 [==============================] - 0s 571us/step - loss: 284.1075 - mae: 11.2401 - val_loss: 27.7787 - val_mae: 3.8260\n",
      "Epoch 157/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 275.2010 - mae: 11.0691 - val_loss: 23.6973 - val_mae: 3.5053\n",
      "Epoch 158/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 275.8418 - mae: 10.9607 - val_loss: 25.8968 - val_mae: 3.6969\n",
      "Epoch 159/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 268.5624 - mae: 10.7492 - val_loss: 32.9709 - val_mae: 4.3091\n",
      "Epoch 160/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 257.8398 - mae: 10.7419 - val_loss: 24.1866 - val_mae: 3.6308\n",
      "Epoch 161/300\n",
      "122/122 [==============================] - 0s 538us/step - loss: 261.5604 - mae: 10.8977 - val_loss: 29.8964 - val_mae: 4.0987\n",
      "Epoch 162/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 261.7423 - mae: 10.7056 - val_loss: 26.5868 - val_mae: 3.6221\n",
      "Epoch 163/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 273.8481 - mae: 10.8447 - val_loss: 25.1306 - val_mae: 3.5523\n",
      "Epoch 164/300\n",
      "122/122 [==============================] - 0s 563us/step - loss: 250.5256 - mae: 10.6300 - val_loss: 32.8674 - val_mae: 4.1141\n",
      "Epoch 165/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 260.1956 - mae: 10.5129 - val_loss: 24.4432 - val_mae: 3.5560\n",
      "Epoch 166/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 266.5001 - mae: 10.7561 - val_loss: 25.6920 - val_mae: 3.6979\n",
      "Epoch 167/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 275.3120 - mae: 10.8439 - val_loss: 23.9104 - val_mae: 3.5673\n",
      "Epoch 168/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 257.5439 - mae: 10.6990 - val_loss: 23.8274 - val_mae: 3.5063\n",
      "Epoch 169/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 262.3618 - mae: 10.5633 - val_loss: 25.9294 - val_mae: 3.6618\n",
      "Epoch 170/300\n",
      "122/122 [==============================] - 0s 579us/step - loss: 256.0797 - mae: 10.4806 - val_loss: 26.2450 - val_mae: 3.6212\n",
      "Epoch 171/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 249.2803 - mae: 10.4362 - val_loss: 25.2523 - val_mae: 3.6743\n",
      "Epoch 172/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 254.8822 - mae: 10.4089 - val_loss: 25.4140 - val_mae: 3.7025\n",
      "Epoch 173/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 234.6301 - mae: 10.1954 - val_loss: 23.7548 - val_mae: 3.4889\n",
      "Epoch 174/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 246.1914 - mae: 10.3800 - val_loss: 24.5349 - val_mae: 3.4894\n",
      "Epoch 175/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 235.0777 - mae: 10.1217 - val_loss: 23.8037 - val_mae: 3.5896\n",
      "Epoch 176/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 255.7577 - mae: 10.3020 - val_loss: 22.3087 - val_mae: 3.3511\n",
      "Epoch 177/300\n",
      "122/122 [==============================] - 0s 537us/step - loss: 236.4739 - mae: 10.1172 - val_loss: 32.4281 - val_mae: 4.3169\n",
      "Epoch 178/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 241.2773 - mae: 10.1713 - val_loss: 24.1222 - val_mae: 3.5385\n",
      "Epoch 179/300\n",
      "122/122 [==============================] - 0s 653us/step - loss: 238.6623 - mae: 10.1007 - val_loss: 26.3825 - val_mae: 3.7006\n",
      "Epoch 180/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 243.4577 - mae: 10.2361 - val_loss: 26.4643 - val_mae: 3.7405\n",
      "Epoch 181/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 256.7607 - mae: 10.3412 - val_loss: 24.5804 - val_mae: 3.6322\n",
      "Epoch 182/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 229.6753 - mae: 10.0423 - val_loss: 24.2604 - val_mae: 3.5462\n",
      "Epoch 183/300\n",
      "122/122 [==============================] - 0s 581us/step - loss: 235.6641 - mae: 10.1084 - val_loss: 23.7121 - val_mae: 3.4526\n",
      "Epoch 184/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 212.1725 - mae: 9.6417 - val_loss: 27.9052 - val_mae: 3.9275\n",
      "Epoch 185/300\n",
      "122/122 [==============================] - 0s 567us/step - loss: 229.9036 - mae: 9.9821 - val_loss: 28.0823 - val_mae: 3.9317\n",
      "Epoch 186/300\n",
      "122/122 [==============================] - 0s 556us/step - loss: 224.2216 - mae: 9.7425 - val_loss: 24.6511 - val_mae: 3.5880\n",
      "Epoch 187/300\n",
      "122/122 [==============================] - 0s 557us/step - loss: 218.2997 - mae: 9.6892 - val_loss: 25.4018 - val_mae: 3.7677\n",
      "Epoch 188/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 215.4725 - mae: 9.5810 - val_loss: 26.4492 - val_mae: 3.6292\n",
      "Epoch 189/300\n",
      "122/122 [==============================] - 0s 568us/step - loss: 218.5653 - mae: 9.6312 - val_loss: 24.5830 - val_mae: 3.5442\n",
      "Epoch 190/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 223.3027 - mae: 9.7643 - val_loss: 33.5861 - val_mae: 4.4372\n",
      "Epoch 191/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 205.4928 - mae: 9.4069 - val_loss: 25.8354 - val_mae: 3.7812\n",
      "Epoch 192/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 213.7775 - mae: 9.6500 - val_loss: 23.7592 - val_mae: 3.5336\n",
      "Epoch 193/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 231.0831 - mae: 9.9295 - val_loss: 24.6705 - val_mae: 3.6088\n",
      "Epoch 194/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 230.4341 - mae: 9.8532 - val_loss: 33.1056 - val_mae: 4.4410\n",
      "Epoch 195/300\n",
      "122/122 [==============================] - 0s 562us/step - loss: 229.5376 - mae: 9.7037 - val_loss: 24.5894 - val_mae: 3.6356\n",
      "Epoch 196/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 220.4839 - mae: 9.7947 - val_loss: 26.0307 - val_mae: 3.5274\n",
      "Epoch 197/300\n",
      "122/122 [==============================] - 0s 577us/step - loss: 205.5380 - mae: 9.3962 - val_loss: 22.7752 - val_mae: 3.4567\n",
      "Epoch 198/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 209.9962 - mae: 9.4158 - val_loss: 23.3940 - val_mae: 3.4608\n",
      "Epoch 199/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 206.9400 - mae: 9.4147 - val_loss: 25.4338 - val_mae: 3.7258\n",
      "Epoch 200/300\n",
      "122/122 [==============================] - 0s 648us/step - loss: 205.1919 - mae: 9.4606 - val_loss: 36.1986 - val_mae: 4.6332\n",
      "Epoch 201/300\n",
      "122/122 [==============================] - 0s 558us/step - loss: 199.2760 - mae: 9.3205 - val_loss: 27.4646 - val_mae: 3.9254\n",
      "Epoch 202/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 213.7379 - mae: 9.4878 - val_loss: 29.2515 - val_mae: 4.0889\n",
      "Epoch 203/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 204.8411 - mae: 9.3569 - val_loss: 25.8372 - val_mae: 3.7449\n",
      "Epoch 204/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 193.9510 - mae: 9.1597 - val_loss: 23.1762 - val_mae: 3.4758\n",
      "Epoch 205/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 183.1138 - mae: 9.0381 - val_loss: 24.1468 - val_mae: 3.4331\n",
      "Epoch 206/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 206.8565 - mae: 9.4536 - val_loss: 25.0231 - val_mae: 3.5870\n",
      "Epoch 207/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 184.3439 - mae: 9.0173 - val_loss: 29.8590 - val_mae: 4.1371\n",
      "Epoch 208/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 198.2781 - mae: 9.1460 - val_loss: 25.0798 - val_mae: 3.6540\n",
      "Epoch 209/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 200.9887 - mae: 9.2242 - val_loss: 26.3181 - val_mae: 3.7455\n",
      "Epoch 210/300\n",
      "122/122 [==============================] - 0s 583us/step - loss: 181.4906 - mae: 8.8818 - val_loss: 28.1072 - val_mae: 3.7781\n",
      "Epoch 211/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 184.1640 - mae: 8.9712 - val_loss: 37.9703 - val_mae: 4.7192\n",
      "Epoch 212/300\n",
      "122/122 [==============================] - 0s 559us/step - loss: 190.2431 - mae: 8.9640 - val_loss: 33.1800 - val_mae: 4.3587\n",
      "Epoch 213/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 176.1347 - mae: 8.7599 - val_loss: 41.0347 - val_mae: 4.8182\n",
      "Epoch 214/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 191.2347 - mae: 9.0097 - val_loss: 30.7513 - val_mae: 4.1713\n",
      "Epoch 215/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 194.7402 - mae: 9.0349 - val_loss: 24.7763 - val_mae: 3.5551\n",
      "Epoch 216/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 178.4996 - mae: 8.8080 - val_loss: 24.5449 - val_mae: 3.5816\n",
      "Epoch 217/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 184.2400 - mae: 8.9024 - val_loss: 25.4810 - val_mae: 3.6896\n",
      "Epoch 218/300\n",
      "122/122 [==============================] - 0s 566us/step - loss: 173.0445 - mae: 8.6589 - val_loss: 33.6591 - val_mae: 4.4013\n",
      "Epoch 219/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 187.1717 - mae: 8.8851 - val_loss: 22.6531 - val_mae: 3.3946\n",
      "Epoch 220/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 177.4801 - mae: 8.7847 - val_loss: 37.7796 - val_mae: 4.7221\n",
      "Epoch 221/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 178.8930 - mae: 8.7555 - val_loss: 32.8499 - val_mae: 4.3709\n",
      "Epoch 222/300\n",
      "122/122 [==============================] - 0s 650us/step - loss: 177.6726 - mae: 8.7494 - val_loss: 24.1275 - val_mae: 3.4801\n",
      "Epoch 223/300\n",
      "122/122 [==============================] - 0s 583us/step - loss: 181.0020 - mae: 8.8219 - val_loss: 25.0003 - val_mae: 3.5774\n",
      "Epoch 224/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 182.8655 - mae: 8.8322 - val_loss: 29.2566 - val_mae: 4.0298\n",
      "Epoch 225/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 174.1841 - mae: 8.6628 - val_loss: 31.9168 - val_mae: 4.3098\n",
      "Epoch 226/300\n",
      "122/122 [==============================] - 0s 586us/step - loss: 184.0572 - mae: 8.7911 - val_loss: 23.4373 - val_mae: 3.4336\n",
      "Epoch 227/300\n",
      "122/122 [==============================] - 0s 583us/step - loss: 171.7375 - mae: 8.5558 - val_loss: 31.7629 - val_mae: 4.3066\n",
      "Epoch 228/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 154.9167 - mae: 8.2676 - val_loss: 29.3002 - val_mae: 4.1213\n",
      "Epoch 229/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 171.5085 - mae: 8.5608 - val_loss: 24.7138 - val_mae: 3.5722\n",
      "Epoch 230/300\n",
      "122/122 [==============================] - 0s 556us/step - loss: 165.1515 - mae: 8.4267 - val_loss: 24.4426 - val_mae: 3.6103\n",
      "Epoch 231/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 166.9538 - mae: 8.5304 - val_loss: 32.9857 - val_mae: 4.2851\n",
      "Epoch 232/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 163.3097 - mae: 8.4389 - val_loss: 23.7202 - val_mae: 3.4928\n",
      "Epoch 233/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 157.9075 - mae: 8.3499 - val_loss: 24.8927 - val_mae: 3.5207\n",
      "Epoch 234/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 168.1325 - mae: 8.3735 - val_loss: 24.1659 - val_mae: 3.4403\n",
      "Epoch 235/300\n",
      "122/122 [==============================] - 0s 671us/step - loss: 169.7439 - mae: 8.5209 - val_loss: 22.9511 - val_mae: 3.3927\n",
      "Epoch 236/300\n",
      "122/122 [==============================] - 0s 645us/step - loss: 153.2790 - mae: 8.1934 - val_loss: 27.6006 - val_mae: 3.9462\n",
      "Epoch 237/300\n",
      "122/122 [==============================] - 0s 559us/step - loss: 174.7212 - mae: 8.6290 - val_loss: 23.4589 - val_mae: 3.4313\n",
      "Epoch 238/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 159.5936 - mae: 8.3699 - val_loss: 27.2131 - val_mae: 3.6573\n",
      "Epoch 239/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 155.8024 - mae: 8.3504 - val_loss: 24.2905 - val_mae: 3.4949\n",
      "Epoch 240/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 160.5953 - mae: 8.1952 - val_loss: 24.7781 - val_mae: 3.5575\n",
      "Epoch 241/300\n",
      "122/122 [==============================] - 0s 543us/step - loss: 160.8505 - mae: 8.3750 - val_loss: 28.4252 - val_mae: 4.0246\n",
      "Epoch 242/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 161.8389 - mae: 8.4215 - val_loss: 24.1861 - val_mae: 3.4808\n",
      "Epoch 243/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 150.2496 - mae: 8.2319 - val_loss: 26.6857 - val_mae: 3.8508\n",
      "Epoch 244/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 164.4910 - mae: 8.4556 - val_loss: 35.4149 - val_mae: 4.5342\n",
      "Epoch 245/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 165.6983 - mae: 8.3735 - val_loss: 24.3070 - val_mae: 3.4727\n",
      "Epoch 246/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 154.7086 - mae: 8.1877 - val_loss: 24.2749 - val_mae: 3.5349\n",
      "Epoch 247/300\n",
      "122/122 [==============================] - 0s 582us/step - loss: 157.9500 - mae: 8.1712 - val_loss: 25.5384 - val_mae: 3.6235\n",
      "Epoch 248/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 161.4394 - mae: 8.3527 - val_loss: 24.2198 - val_mae: 3.4818\n",
      "Epoch 249/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 147.9216 - mae: 8.1475 - val_loss: 25.0243 - val_mae: 3.5523\n",
      "Epoch 250/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 157.8556 - mae: 8.0954 - val_loss: 37.7914 - val_mae: 4.7148\n",
      "Epoch 251/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 148.1984 - mae: 8.0982 - val_loss: 23.5719 - val_mae: 3.4515\n",
      "Epoch 252/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 144.7071 - mae: 8.0729 - val_loss: 30.5933 - val_mae: 4.0701\n",
      "Epoch 253/300\n",
      "122/122 [==============================] - 0s 537us/step - loss: 154.2994 - mae: 8.2669 - val_loss: 30.6302 - val_mae: 4.0628\n",
      "Epoch 254/300\n",
      "122/122 [==============================] - 0s 646us/step - loss: 144.4770 - mae: 8.0289 - val_loss: 25.0399 - val_mae: 3.6040\n",
      "Epoch 255/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 144.6409 - mae: 7.9550 - val_loss: 35.5995 - val_mae: 4.4767\n",
      "Epoch 256/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 144.1803 - mae: 7.8833 - val_loss: 25.2118 - val_mae: 3.6533\n",
      "Epoch 257/300\n",
      "122/122 [==============================] - 0s 547us/step - loss: 146.3870 - mae: 7.9401 - val_loss: 24.1933 - val_mae: 3.5912\n",
      "Epoch 258/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 162.5948 - mae: 8.3326 - val_loss: 26.4269 - val_mae: 3.7519\n",
      "Epoch 259/300\n",
      "122/122 [==============================] - 0s 583us/step - loss: 141.8203 - mae: 7.9885 - val_loss: 23.1502 - val_mae: 3.4160\n",
      "Epoch 260/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 146.3828 - mae: 8.0946 - val_loss: 24.3032 - val_mae: 3.5134\n",
      "Epoch 261/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 143.9614 - mae: 7.9836 - val_loss: 24.4094 - val_mae: 3.4321\n",
      "Epoch 262/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 144.4654 - mae: 7.9873 - val_loss: 24.8006 - val_mae: 3.6218\n",
      "Epoch 263/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 147.0928 - mae: 8.0441 - val_loss: 31.2715 - val_mae: 4.2471\n",
      "Epoch 264/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 150.5509 - mae: 8.0260 - val_loss: 40.2840 - val_mae: 4.8151\n",
      "Epoch 265/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 136.1640 - mae: 7.8077 - val_loss: 30.8672 - val_mae: 4.0915\n",
      "Epoch 266/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 133.0596 - mae: 7.8306 - val_loss: 26.0960 - val_mae: 3.6991\n",
      "Epoch 267/300\n",
      "122/122 [==============================] - 0s 539us/step - loss: 134.9852 - mae: 7.7925 - val_loss: 26.2342 - val_mae: 3.6750\n",
      "Epoch 268/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 146.0862 - mae: 7.8308 - val_loss: 25.2154 - val_mae: 3.6144\n",
      "Epoch 269/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 134.5866 - mae: 7.7156 - val_loss: 31.6936 - val_mae: 4.2910\n",
      "Epoch 270/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 124.0228 - mae: 7.4780 - val_loss: 32.2812 - val_mae: 4.2981\n",
      "Epoch 271/300\n",
      "122/122 [==============================] - 0s 542us/step - loss: 133.3896 - mae: 7.7420 - val_loss: 26.9506 - val_mae: 3.7137\n",
      "Epoch 272/300\n",
      "122/122 [==============================] - 0s 571us/step - loss: 136.9288 - mae: 7.7189 - val_loss: 28.2539 - val_mae: 3.8839\n",
      "Epoch 273/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 134.7498 - mae: 7.7600 - val_loss: 23.1576 - val_mae: 3.3963\n",
      "Epoch 274/300\n",
      "122/122 [==============================] - 0s 558us/step - loss: 140.8359 - mae: 7.8363 - val_loss: 42.3871 - val_mae: 4.9043\n",
      "Epoch 275/300\n",
      "122/122 [==============================] - 0s 544us/step - loss: 126.1814 - mae: 7.7215 - val_loss: 27.9371 - val_mae: 3.9822\n",
      "Epoch 276/300\n",
      "122/122 [==============================] - 0s 651us/step - loss: 127.7002 - mae: 7.6036 - val_loss: 29.9452 - val_mae: 3.9258\n",
      "Epoch 277/300\n",
      "122/122 [==============================] - 0s 552us/step - loss: 132.7359 - mae: 7.6843 - val_loss: 27.7359 - val_mae: 3.9207\n",
      "Epoch 278/300\n",
      "122/122 [==============================] - 0s 550us/step - loss: 127.0963 - mae: 7.5846 - val_loss: 25.9260 - val_mae: 3.5740\n",
      "Epoch 279/300\n",
      "122/122 [==============================] - 0s 553us/step - loss: 136.5572 - mae: 7.7716 - val_loss: 28.8161 - val_mae: 3.9908\n",
      "Epoch 280/300\n",
      "122/122 [==============================] - 0s 545us/step - loss: 130.8041 - mae: 7.5977 - val_loss: 25.5981 - val_mae: 3.6181\n",
      "Epoch 281/300\n",
      "122/122 [==============================] - 0s 541us/step - loss: 134.7712 - mae: 7.7971 - val_loss: 30.6795 - val_mae: 3.9757\n",
      "Epoch 282/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 139.1655 - mae: 7.8921 - val_loss: 31.0815 - val_mae: 4.1860\n",
      "Epoch 283/300\n",
      "122/122 [==============================] - 0s 548us/step - loss: 137.2596 - mae: 7.7861 - val_loss: 27.8328 - val_mae: 3.8117\n",
      "Epoch 284/300\n",
      "122/122 [==============================] - 0s 583us/step - loss: 129.1544 - mae: 7.5861 - val_loss: 25.2922 - val_mae: 3.6658\n",
      "Epoch 285/300\n",
      "122/122 [==============================] - 0s 567us/step - loss: 125.8999 - mae: 7.5173 - val_loss: 23.2235 - val_mae: 3.3957\n",
      "Epoch 286/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 135.4640 - mae: 7.8489 - val_loss: 30.6372 - val_mae: 4.1694\n",
      "Epoch 287/300\n",
      "122/122 [==============================] - 0s 570us/step - loss: 131.6111 - mae: 7.6324 - val_loss: 46.6319 - val_mae: 5.2404\n",
      "Epoch 288/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 131.1732 - mae: 7.7260 - val_loss: 27.2043 - val_mae: 3.6589\n",
      "Epoch 289/300\n",
      "122/122 [==============================] - 0s 558us/step - loss: 126.2526 - mae: 7.6359 - val_loss: 30.5266 - val_mae: 4.1218\n",
      "Epoch 290/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 121.7159 - mae: 7.4611 - val_loss: 24.6288 - val_mae: 3.4951\n",
      "Epoch 291/300\n",
      "122/122 [==============================] - 0s 560us/step - loss: 125.9861 - mae: 7.4885 - val_loss: 31.7218 - val_mae: 4.2092\n",
      "Epoch 292/300\n",
      "122/122 [==============================] - 0s 551us/step - loss: 114.3084 - mae: 7.3017 - val_loss: 26.2282 - val_mae: 3.7071\n",
      "Epoch 293/300\n",
      "122/122 [==============================] - 0s 619us/step - loss: 122.0214 - mae: 7.4154 - val_loss: 25.2215 - val_mae: 3.6241\n",
      "Epoch 294/300\n",
      "122/122 [==============================] - 0s 579us/step - loss: 129.1404 - mae: 7.5178 - val_loss: 30.6212 - val_mae: 4.0967\n",
      "Epoch 295/300\n",
      "122/122 [==============================] - 0s 558us/step - loss: 125.9139 - mae: 7.4951 - val_loss: 25.8361 - val_mae: 3.6912\n",
      "Epoch 296/300\n",
      "122/122 [==============================] - 0s 554us/step - loss: 125.0617 - mae: 7.5947 - val_loss: 25.1786 - val_mae: 3.5086\n",
      "Epoch 297/300\n",
      "122/122 [==============================] - 0s 572us/step - loss: 125.4164 - mae: 7.5427 - val_loss: 31.7750 - val_mae: 4.1764\n",
      "Epoch 298/300\n",
      "122/122 [==============================] - 0s 555us/step - loss: 118.8401 - mae: 7.3268 - val_loss: 29.5909 - val_mae: 3.9668\n",
      "Epoch 299/300\n",
      "122/122 [==============================] - 0s 546us/step - loss: 117.9538 - mae: 7.2971 - val_loss: 26.5025 - val_mae: 3.6576\n",
      "Epoch 300/300\n",
      "122/122 [==============================] - 0s 549us/step - loss: 114.7660 - mae: 7.2864 - val_loss: 32.6947 - val_mae: 4.3416\n",
      "31/31 [==============================] - 0s 304us/step\n",
      "Epochs: 300 | MAE: 4.341572904146635\n",
      "Training model with 350 epochs\n",
      "Epoch 1/350\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 67218.0234 - mae: 224.4680 - val_loss: 57095.2383 - val_mae: 205.8580\n",
      "Epoch 2/350\n",
      "122/122 [==============================] - 0s 573us/step - loss: 34886.6250 - mae: 155.2676 - val_loss: 11287.8613 - val_mae: 89.5010\n",
      "Epoch 3/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 5710.4678 - mae: 58.3478 - val_loss: 2946.8994 - val_mae: 40.9192\n",
      "Epoch 4/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 3044.5615 - mae: 41.8222 - val_loss: 1926.5630 - val_mae: 32.8524\n",
      "Epoch 5/350\n",
      "122/122 [==============================] - 0s 581us/step - loss: 2446.6460 - mae: 37.6483 - val_loss: 1556.0953 - val_mae: 29.3978\n",
      "Epoch 6/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 2167.3286 - mae: 35.5966 - val_loss: 1331.7083 - val_mae: 27.1191\n",
      "Epoch 7/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 1912.3267 - mae: 33.4393 - val_loss: 1185.8287 - val_mae: 25.5533\n",
      "Epoch 8/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 1856.7397 - mae: 32.9470 - val_loss: 1066.0596 - val_mae: 24.3138\n",
      "Epoch 9/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 1647.7975 - mae: 31.7496 - val_loss: 962.4871 - val_mae: 23.1983\n",
      "Epoch 10/350\n",
      "122/122 [==============================] - 0s 654us/step - loss: 1544.6803 - mae: 30.2024 - val_loss: 867.7234 - val_mae: 21.9336\n",
      "Epoch 11/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 1473.8384 - mae: 29.8803 - val_loss: 783.6509 - val_mae: 20.8125\n",
      "Epoch 12/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 1368.2329 - mae: 28.9635 - val_loss: 716.4293 - val_mae: 20.0439\n",
      "Epoch 13/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 1305.8198 - mae: 27.8570 - val_loss: 666.2044 - val_mae: 19.3037\n",
      "Epoch 14/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 1243.5894 - mae: 27.5325 - val_loss: 589.1603 - val_mae: 17.9969\n",
      "Epoch 15/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 1165.3250 - mae: 26.5886 - val_loss: 540.1464 - val_mae: 17.2295\n",
      "Epoch 16/350\n",
      "122/122 [==============================] - 0s 581us/step - loss: 1100.1890 - mae: 25.9266 - val_loss: 486.6051 - val_mae: 16.3350\n",
      "Epoch 17/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 1054.8345 - mae: 25.2075 - val_loss: 443.7392 - val_mae: 15.6318\n",
      "Epoch 18/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 987.9822 - mae: 24.4220 - val_loss: 398.0821 - val_mae: 14.6964\n",
      "Epoch 19/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 962.4590 - mae: 24.2121 - val_loss: 357.0298 - val_mae: 13.8220\n",
      "Epoch 20/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 915.3779 - mae: 23.3714 - val_loss: 325.2704 - val_mae: 13.7769\n",
      "Epoch 21/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 876.8521 - mae: 22.8600 - val_loss: 288.3356 - val_mae: 12.5052\n",
      "Epoch 22/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 850.6624 - mae: 22.4556 - val_loss: 268.4464 - val_mae: 12.3991\n",
      "Epoch 23/350\n",
      "122/122 [==============================] - 0s 548us/step - loss: 841.0497 - mae: 22.1933 - val_loss: 236.0296 - val_mae: 11.1625\n",
      "Epoch 24/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 794.2793 - mae: 21.5389 - val_loss: 209.2836 - val_mae: 10.5943\n",
      "Epoch 25/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 737.9996 - mae: 20.9605 - val_loss: 188.1060 - val_mae: 9.9952\n",
      "Epoch 26/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 724.8331 - mae: 20.6582 - val_loss: 171.9170 - val_mae: 9.7006\n",
      "Epoch 27/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 726.7448 - mae: 20.3026 - val_loss: 150.8133 - val_mae: 9.2277\n",
      "Epoch 28/350\n",
      "122/122 [==============================] - 0s 709us/step - loss: 697.0143 - mae: 20.0445 - val_loss: 137.8404 - val_mae: 8.7001\n",
      "Epoch 29/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 665.1454 - mae: 19.6931 - val_loss: 125.6070 - val_mae: 8.4327\n",
      "Epoch 30/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 628.4789 - mae: 18.9645 - val_loss: 121.1088 - val_mae: 8.1713\n",
      "Epoch 31/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 655.8068 - mae: 19.2388 - val_loss: 97.0154 - val_mae: 7.0426\n",
      "Epoch 32/350\n",
      "122/122 [==============================] - 0s 568us/step - loss: 624.7484 - mae: 18.8691 - val_loss: 105.0582 - val_mae: 7.6214\n",
      "Epoch 33/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 601.6597 - mae: 18.2054 - val_loss: 89.4389 - val_mae: 6.9350\n",
      "Epoch 34/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 591.4211 - mae: 18.0421 - val_loss: 96.6878 - val_mae: 7.2115\n",
      "Epoch 35/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 592.0800 - mae: 17.9393 - val_loss: 75.0945 - val_mae: 6.3554\n",
      "Epoch 36/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 571.9591 - mae: 17.5893 - val_loss: 64.8252 - val_mae: 6.0377\n",
      "Epoch 37/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 566.5688 - mae: 17.4659 - val_loss: 60.9424 - val_mae: 5.6819\n",
      "Epoch 38/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 563.0564 - mae: 17.3611 - val_loss: 52.3214 - val_mae: 5.3512\n",
      "Epoch 39/350\n",
      "122/122 [==============================] - 0s 580us/step - loss: 551.6236 - mae: 17.1476 - val_loss: 54.7472 - val_mae: 5.7938\n",
      "Epoch 40/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 537.4767 - mae: 16.8521 - val_loss: 48.6169 - val_mae: 5.1530\n",
      "Epoch 41/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 507.6540 - mae: 16.5614 - val_loss: 65.5734 - val_mae: 6.2393\n",
      "Epoch 42/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 522.8846 - mae: 16.5095 - val_loss: 41.8447 - val_mae: 4.7338\n",
      "Epoch 43/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 520.1646 - mae: 16.4878 - val_loss: 51.8535 - val_mae: 5.2078\n",
      "Epoch 44/350\n",
      "122/122 [==============================] - 0s 644us/step - loss: 550.3399 - mae: 16.8328 - val_loss: 47.3230 - val_mae: 5.3271\n",
      "Epoch 45/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 512.1678 - mae: 16.2116 - val_loss: 42.3441 - val_mae: 4.6699\n",
      "Epoch 46/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 501.1320 - mae: 15.9611 - val_loss: 39.6290 - val_mae: 4.5232\n",
      "Epoch 47/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 517.6176 - mae: 16.1052 - val_loss: 44.8224 - val_mae: 4.9477\n",
      "Epoch 48/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 514.1826 - mae: 16.1448 - val_loss: 37.5739 - val_mae: 4.7024\n",
      "Epoch 49/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 502.7272 - mae: 16.1314 - val_loss: 38.7312 - val_mae: 4.7172\n",
      "Epoch 50/350\n",
      "122/122 [==============================] - 0s 581us/step - loss: 491.9305 - mae: 15.9724 - val_loss: 42.0331 - val_mae: 5.1289\n",
      "Epoch 51/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 482.3274 - mae: 15.5781 - val_loss: 35.0473 - val_mae: 4.2511\n",
      "Epoch 52/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 491.5855 - mae: 15.8034 - val_loss: 34.7228 - val_mae: 4.2748\n",
      "Epoch 53/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 467.9070 - mae: 15.3640 - val_loss: 31.3355 - val_mae: 4.1025\n",
      "Epoch 54/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 479.5610 - mae: 15.4474 - val_loss: 39.9506 - val_mae: 4.8561\n",
      "Epoch 55/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 495.1658 - mae: 15.5714 - val_loss: 33.6416 - val_mae: 4.1316\n",
      "Epoch 56/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 481.9681 - mae: 15.5296 - val_loss: 28.9280 - val_mae: 4.0164\n",
      "Epoch 57/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 475.2009 - mae: 15.4108 - val_loss: 39.7904 - val_mae: 4.7046\n",
      "Epoch 58/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 457.8714 - mae: 14.8057 - val_loss: 30.9717 - val_mae: 4.0388\n",
      "Epoch 59/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 462.4780 - mae: 15.1627 - val_loss: 38.2193 - val_mae: 4.7999\n",
      "Epoch 60/350\n",
      "122/122 [==============================] - 0s 620us/step - loss: 438.1040 - mae: 14.7018 - val_loss: 34.5459 - val_mae: 4.1450\n",
      "Epoch 61/350\n",
      "122/122 [==============================] - 0s 575us/step - loss: 447.8070 - mae: 14.9775 - val_loss: 45.1364 - val_mae: 5.2480\n",
      "Epoch 62/350\n",
      "122/122 [==============================] - 0s 572us/step - loss: 462.0900 - mae: 15.0862 - val_loss: 31.5345 - val_mae: 4.2938\n",
      "Epoch 63/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 465.5598 - mae: 15.0111 - val_loss: 31.4315 - val_mae: 4.1491\n",
      "Epoch 64/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 440.0162 - mae: 14.8615 - val_loss: 36.0863 - val_mae: 4.3675\n",
      "Epoch 65/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 447.4925 - mae: 14.7502 - val_loss: 47.9978 - val_mae: 5.2836\n",
      "Epoch 66/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 448.5538 - mae: 14.6583 - val_loss: 26.9914 - val_mae: 3.8228\n",
      "Epoch 67/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 436.4082 - mae: 14.4777 - val_loss: 28.9634 - val_mae: 3.8911\n",
      "Epoch 68/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 420.0181 - mae: 14.4364 - val_loss: 28.9794 - val_mae: 3.7678\n",
      "Epoch 69/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 444.4473 - mae: 14.5632 - val_loss: 43.3004 - val_mae: 5.1034\n",
      "Epoch 70/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 434.7822 - mae: 14.3911 - val_loss: 30.4657 - val_mae: 3.8430\n",
      "Epoch 71/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 423.4502 - mae: 14.2613 - val_loss: 35.2777 - val_mae: 4.5299\n",
      "Epoch 72/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 415.0431 - mae: 14.1162 - val_loss: 39.5601 - val_mae: 4.7970\n",
      "Epoch 73/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 421.5327 - mae: 14.2540 - val_loss: 33.6170 - val_mae: 4.0718\n",
      "Epoch 74/350\n",
      "122/122 [==============================] - 0s 576us/step - loss: 413.2800 - mae: 14.1487 - val_loss: 32.6051 - val_mae: 4.1286\n",
      "Epoch 75/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 424.4409 - mae: 14.1292 - val_loss: 36.8835 - val_mae: 4.5728\n",
      "Epoch 76/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 413.4146 - mae: 14.0134 - val_loss: 38.7180 - val_mae: 4.8105\n",
      "Epoch 77/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 421.1379 - mae: 14.0430 - val_loss: 23.8305 - val_mae: 3.4779\n",
      "Epoch 78/350\n",
      "122/122 [==============================] - 0s 566us/step - loss: 424.2481 - mae: 13.9095 - val_loss: 30.2222 - val_mae: 4.0079\n",
      "Epoch 79/350\n",
      "122/122 [==============================] - 0s 567us/step - loss: 400.4408 - mae: 13.7286 - val_loss: 26.3252 - val_mae: 3.6818\n",
      "Epoch 80/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 405.7953 - mae: 14.0338 - val_loss: 45.4553 - val_mae: 5.1376\n",
      "Epoch 81/350\n",
      "122/122 [==============================] - 0s 655us/step - loss: 390.8622 - mae: 13.6528 - val_loss: 24.9375 - val_mae: 3.6476\n",
      "Epoch 82/350\n",
      "122/122 [==============================] - 0s 574us/step - loss: 402.2362 - mae: 13.6757 - val_loss: 26.1261 - val_mae: 3.5212\n",
      "Epoch 83/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 374.1619 - mae: 13.4028 - val_loss: 23.7755 - val_mae: 3.5350\n",
      "Epoch 84/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 389.1382 - mae: 13.5288 - val_loss: 32.1164 - val_mae: 4.4022\n",
      "Epoch 85/350\n",
      "122/122 [==============================] - 0s 584us/step - loss: 396.1968 - mae: 13.6190 - val_loss: 41.6186 - val_mae: 5.0069\n",
      "Epoch 86/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 398.3179 - mae: 13.4942 - val_loss: 29.8974 - val_mae: 3.9104\n",
      "Epoch 87/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 383.6434 - mae: 13.4076 - val_loss: 24.1821 - val_mae: 3.5182\n",
      "Epoch 88/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 397.5363 - mae: 13.5151 - val_loss: 34.3081 - val_mae: 4.3696\n",
      "Epoch 89/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 348.4253 - mae: 12.8564 - val_loss: 26.1238 - val_mae: 3.7597\n",
      "Epoch 90/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 355.7127 - mae: 13.0027 - val_loss: 23.9805 - val_mae: 3.4664\n",
      "Epoch 91/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 357.1353 - mae: 13.0622 - val_loss: 24.7795 - val_mae: 3.6287\n",
      "Epoch 92/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 351.8371 - mae: 12.7088 - val_loss: 26.8765 - val_mae: 3.6704\n",
      "Epoch 93/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 366.0457 - mae: 13.0444 - val_loss: 26.5971 - val_mae: 3.5519\n",
      "Epoch 94/350\n",
      "122/122 [==============================] - 0s 570us/step - loss: 362.5559 - mae: 12.8759 - val_loss: 24.8835 - val_mae: 3.5997\n",
      "Epoch 95/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 367.4273 - mae: 12.8892 - val_loss: 23.0395 - val_mae: 3.4086\n",
      "Epoch 96/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 342.6264 - mae: 12.7188 - val_loss: 37.0596 - val_mae: 4.7159\n",
      "Epoch 97/350\n",
      "122/122 [==============================] - 0s 699us/step - loss: 360.5818 - mae: 12.9429 - val_loss: 28.4526 - val_mae: 3.8256\n",
      "Epoch 98/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 352.8852 - mae: 12.6328 - val_loss: 22.9509 - val_mae: 3.3491\n",
      "Epoch 99/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 348.9830 - mae: 12.7044 - val_loss: 23.8104 - val_mae: 3.3898\n",
      "Epoch 100/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 349.0761 - mae: 12.4328 - val_loss: 27.1115 - val_mae: 3.8183\n",
      "Epoch 101/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 347.1351 - mae: 12.5735 - val_loss: 27.0462 - val_mae: 3.7487\n",
      "Epoch 102/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 332.1051 - mae: 12.3958 - val_loss: 38.0540 - val_mae: 4.8091\n",
      "Epoch 103/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 336.1193 - mae: 12.5022 - val_loss: 23.3616 - val_mae: 3.3223\n",
      "Epoch 104/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 355.5223 - mae: 12.7377 - val_loss: 30.7007 - val_mae: 4.1200\n",
      "Epoch 105/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 342.8225 - mae: 12.4191 - val_loss: 29.7200 - val_mae: 4.1131\n",
      "Epoch 106/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 349.2019 - mae: 12.4720 - val_loss: 23.1159 - val_mae: 3.3596\n",
      "Epoch 107/350\n",
      "122/122 [==============================] - 0s 613us/step - loss: 322.2314 - mae: 12.1712 - val_loss: 23.4408 - val_mae: 3.4397\n",
      "Epoch 108/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 348.9059 - mae: 12.3986 - val_loss: 57.4690 - val_mae: 5.8787\n",
      "Epoch 109/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 316.2833 - mae: 11.9484 - val_loss: 44.3795 - val_mae: 5.1755\n",
      "Epoch 110/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 335.0069 - mae: 12.3903 - val_loss: 25.9774 - val_mae: 3.6561\n",
      "Epoch 111/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 327.8524 - mae: 12.1660 - val_loss: 28.2467 - val_mae: 4.0297\n",
      "Epoch 112/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 334.8969 - mae: 12.2878 - val_loss: 38.1280 - val_mae: 4.7195\n",
      "Epoch 113/350\n",
      "122/122 [==============================] - 0s 659us/step - loss: 302.1863 - mae: 11.7039 - val_loss: 27.7546 - val_mae: 3.8947\n",
      "Epoch 114/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 322.4408 - mae: 12.0654 - val_loss: 26.5564 - val_mae: 3.6969\n",
      "Epoch 115/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 301.7499 - mae: 11.5847 - val_loss: 25.3496 - val_mae: 3.7423\n",
      "Epoch 116/350\n",
      "122/122 [==============================] - 0s 580us/step - loss: 314.4374 - mae: 11.8407 - val_loss: 23.0970 - val_mae: 3.4013\n",
      "Epoch 117/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 294.5873 - mae: 11.5674 - val_loss: 37.0910 - val_mae: 4.6050\n",
      "Epoch 118/350\n",
      "122/122 [==============================] - 0s 574us/step - loss: 311.8246 - mae: 11.9499 - val_loss: 26.8309 - val_mae: 3.8726\n",
      "Epoch 119/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 301.2011 - mae: 11.7523 - val_loss: 23.2645 - val_mae: 3.4665\n",
      "Epoch 120/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 300.0508 - mae: 11.6652 - val_loss: 22.1339 - val_mae: 3.3403\n",
      "Epoch 121/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 308.6708 - mae: 11.7512 - val_loss: 27.5461 - val_mae: 3.9147\n",
      "Epoch 122/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 318.2719 - mae: 11.7155 - val_loss: 24.2614 - val_mae: 3.5205\n",
      "Epoch 123/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 309.7847 - mae: 11.6848 - val_loss: 23.4663 - val_mae: 3.3972\n",
      "Epoch 124/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 295.6219 - mae: 11.4569 - val_loss: 28.2004 - val_mae: 3.8691\n",
      "Epoch 125/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 285.1493 - mae: 11.1936 - val_loss: 33.1899 - val_mae: 4.4607\n",
      "Epoch 126/350\n",
      "122/122 [==============================] - 0s 574us/step - loss: 305.7962 - mae: 11.4689 - val_loss: 25.6208 - val_mae: 3.5180\n",
      "Epoch 127/350\n",
      "122/122 [==============================] - 0s 609us/step - loss: 280.9432 - mae: 11.2824 - val_loss: 28.4784 - val_mae: 3.9445\n",
      "Epoch 128/350\n",
      "122/122 [==============================] - 0s 578us/step - loss: 290.2575 - mae: 11.3693 - val_loss: 21.3735 - val_mae: 3.2541\n",
      "Epoch 129/350\n",
      "122/122 [==============================] - 0s 663us/step - loss: 290.6644 - mae: 11.2440 - val_loss: 25.0295 - val_mae: 3.5853\n",
      "Epoch 130/350\n",
      "122/122 [==============================] - 0s 568us/step - loss: 298.9218 - mae: 11.4826 - val_loss: 21.2129 - val_mae: 3.2616\n",
      "Epoch 131/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 273.4162 - mae: 11.1761 - val_loss: 22.6595 - val_mae: 3.3877\n",
      "Epoch 132/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 282.6695 - mae: 11.1833 - val_loss: 54.0147 - val_mae: 5.5810\n",
      "Epoch 133/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 271.5560 - mae: 11.1216 - val_loss: 23.4955 - val_mae: 3.4768\n",
      "Epoch 134/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 267.1758 - mae: 11.0052 - val_loss: 25.1801 - val_mae: 3.6069\n",
      "Epoch 135/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 297.3104 - mae: 11.3424 - val_loss: 25.4785 - val_mae: 3.7423\n",
      "Epoch 136/350\n",
      "122/122 [==============================] - 0s 582us/step - loss: 278.1139 - mae: 11.0636 - val_loss: 28.2118 - val_mae: 3.9841\n",
      "Epoch 137/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 254.6739 - mae: 10.6842 - val_loss: 22.7624 - val_mae: 3.4767\n",
      "Epoch 138/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 266.7876 - mae: 10.8356 - val_loss: 21.8670 - val_mae: 3.2951\n",
      "Epoch 139/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 270.5502 - mae: 10.7879 - val_loss: 31.8476 - val_mae: 4.2118\n",
      "Epoch 140/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 255.4609 - mae: 10.6314 - val_loss: 25.1909 - val_mae: 3.7334\n",
      "Epoch 141/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 267.1284 - mae: 10.7643 - val_loss: 32.0730 - val_mae: 4.3151\n",
      "Epoch 142/350\n",
      "122/122 [==============================] - 0s 568us/step - loss: 261.3371 - mae: 10.6178 - val_loss: 24.9709 - val_mae: 3.5888\n",
      "Epoch 143/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 271.9103 - mae: 10.8424 - val_loss: 25.2079 - val_mae: 3.7530\n",
      "Epoch 144/350\n",
      "122/122 [==============================] - 0s 661us/step - loss: 250.9727 - mae: 10.5895 - val_loss: 30.0014 - val_mae: 4.2449\n",
      "Epoch 145/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 240.8060 - mae: 10.3136 - val_loss: 21.9721 - val_mae: 3.3614\n",
      "Epoch 146/350\n",
      "122/122 [==============================] - 0s 575us/step - loss: 244.5955 - mae: 10.3363 - val_loss: 23.6437 - val_mae: 3.4725\n",
      "Epoch 147/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 234.1456 - mae: 10.3580 - val_loss: 22.0864 - val_mae: 3.3509\n",
      "Epoch 148/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 248.9569 - mae: 10.3974 - val_loss: 26.7449 - val_mae: 3.9297\n",
      "Epoch 149/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 265.1104 - mae: 10.7137 - val_loss: 24.0437 - val_mae: 3.6209\n",
      "Epoch 150/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 244.8092 - mae: 10.4738 - val_loss: 21.1985 - val_mae: 3.3257\n",
      "Epoch 151/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 246.3379 - mae: 10.1947 - val_loss: 21.7230 - val_mae: 3.2978\n",
      "Epoch 152/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 240.4101 - mae: 10.2709 - val_loss: 23.1104 - val_mae: 3.4857\n",
      "Epoch 153/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 274.0304 - mae: 10.6332 - val_loss: 34.0868 - val_mae: 4.4188\n",
      "Epoch 154/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 267.3671 - mae: 10.6684 - val_loss: 22.7797 - val_mae: 3.5290\n",
      "Epoch 155/350\n",
      "122/122 [==============================] - 0s 548us/step - loss: 232.0607 - mae: 9.9934 - val_loss: 37.0247 - val_mae: 4.6823\n",
      "Epoch 156/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 233.7015 - mae: 10.0262 - val_loss: 29.7216 - val_mae: 4.0875\n",
      "Epoch 157/350\n",
      "122/122 [==============================] - 0s 582us/step - loss: 233.9852 - mae: 10.1326 - val_loss: 23.0873 - val_mae: 3.4931\n",
      "Epoch 158/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 247.9729 - mae: 10.1907 - val_loss: 22.7255 - val_mae: 3.4041\n",
      "Epoch 159/350\n",
      "122/122 [==============================] - 0s 599us/step - loss: 232.8276 - mae: 10.0263 - val_loss: 32.4872 - val_mae: 4.3743\n",
      "Epoch 160/350\n",
      "122/122 [==============================] - 0s 583us/step - loss: 236.3122 - mae: 9.9744 - val_loss: 25.1426 - val_mae: 3.7337\n",
      "Epoch 161/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 219.0087 - mae: 9.7027 - val_loss: 26.5462 - val_mae: 3.8157\n",
      "Epoch 162/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 224.1205 - mae: 9.9307 - val_loss: 36.2218 - val_mae: 4.5629\n",
      "Epoch 163/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 242.2352 - mae: 10.0336 - val_loss: 23.3265 - val_mae: 3.5101\n",
      "Epoch 164/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 227.6806 - mae: 9.8525 - val_loss: 22.7792 - val_mae: 3.4805\n",
      "Epoch 165/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 239.8318 - mae: 9.9693 - val_loss: 22.2500 - val_mae: 3.4067\n",
      "Epoch 166/350\n",
      "122/122 [==============================] - 0s 547us/step - loss: 207.7343 - mae: 9.4309 - val_loss: 25.0318 - val_mae: 3.6538\n",
      "Epoch 167/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 212.9260 - mae: 9.6072 - val_loss: 24.4605 - val_mae: 3.5852\n",
      "Epoch 168/350\n",
      "122/122 [==============================] - 0s 578us/step - loss: 195.2658 - mae: 9.4108 - val_loss: 22.8002 - val_mae: 3.4163\n",
      "Epoch 169/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 229.5666 - mae: 9.8014 - val_loss: 27.8309 - val_mae: 3.9662\n",
      "Epoch 170/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 232.8044 - mae: 9.8143 - val_loss: 25.9540 - val_mae: 3.6803\n",
      "Epoch 171/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 229.9048 - mae: 9.8077 - val_loss: 36.6899 - val_mae: 4.6407\n",
      "Epoch 172/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 210.6990 - mae: 9.4118 - val_loss: 22.8268 - val_mae: 3.4331\n",
      "Epoch 173/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 197.5570 - mae: 9.2026 - val_loss: 31.9109 - val_mae: 4.2729\n",
      "Epoch 174/350\n",
      "122/122 [==============================] - 0s 653us/step - loss: 209.1351 - mae: 9.3546 - val_loss: 25.2636 - val_mae: 3.6091\n",
      "Epoch 175/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 205.9214 - mae: 9.3637 - val_loss: 27.2877 - val_mae: 3.9353\n",
      "Epoch 176/350\n",
      "122/122 [==============================] - 0s 573us/step - loss: 201.9215 - mae: 9.2363 - val_loss: 23.8962 - val_mae: 3.5794\n",
      "Epoch 177/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 208.3601 - mae: 9.3465 - val_loss: 22.0205 - val_mae: 3.3103\n",
      "Epoch 178/350\n",
      "122/122 [==============================] - 0s 622us/step - loss: 212.5671 - mae: 9.4158 - val_loss: 23.2765 - val_mae: 3.4305\n",
      "Epoch 179/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 197.4194 - mae: 9.1570 - val_loss: 25.7928 - val_mae: 3.7706\n",
      "Epoch 180/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 203.1438 - mae: 9.1183 - val_loss: 28.7471 - val_mae: 4.0579\n",
      "Epoch 181/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 184.5180 - mae: 8.9389 - val_loss: 24.5230 - val_mae: 3.6090\n",
      "Epoch 182/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 202.2815 - mae: 9.2587 - val_loss: 22.8746 - val_mae: 3.4559\n",
      "Epoch 183/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 192.9180 - mae: 9.0031 - val_loss: 25.0086 - val_mae: 3.5698\n",
      "Epoch 184/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 194.2557 - mae: 9.0843 - val_loss: 26.9543 - val_mae: 3.8742\n",
      "Epoch 185/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 211.2519 - mae: 9.2402 - val_loss: 26.3749 - val_mae: 3.7599\n",
      "Epoch 186/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 182.5864 - mae: 8.8646 - val_loss: 23.7863 - val_mae: 3.4963\n",
      "Epoch 187/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 190.5469 - mae: 9.0234 - val_loss: 22.6910 - val_mae: 3.4167\n",
      "Epoch 188/350\n",
      "122/122 [==============================] - 0s 612us/step - loss: 187.9305 - mae: 8.8729 - val_loss: 21.9305 - val_mae: 3.3356\n",
      "Epoch 189/350\n",
      "122/122 [==============================] - 0s 639us/step - loss: 198.8712 - mae: 9.1422 - val_loss: 22.6712 - val_mae: 3.3531\n",
      "Epoch 190/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 186.9445 - mae: 8.9238 - val_loss: 31.1484 - val_mae: 4.1930\n",
      "Epoch 191/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 188.0786 - mae: 8.8513 - val_loss: 32.6882 - val_mae: 4.2222\n",
      "Epoch 192/350\n",
      "122/122 [==============================] - 0s 550us/step - loss: 186.7321 - mae: 8.8727 - val_loss: 23.7653 - val_mae: 3.4664\n",
      "Epoch 193/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 174.8827 - mae: 8.6450 - val_loss: 22.2373 - val_mae: 3.3817\n",
      "Epoch 194/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 171.9927 - mae: 8.5463 - val_loss: 24.6237 - val_mae: 3.6281\n",
      "Epoch 195/350\n",
      "122/122 [==============================] - 0s 570us/step - loss: 193.6664 - mae: 8.9630 - val_loss: 25.2325 - val_mae: 3.6514\n",
      "Epoch 196/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 194.2310 - mae: 8.9601 - val_loss: 23.5155 - val_mae: 3.4951\n",
      "Epoch 197/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 179.3074 - mae: 8.7171 - val_loss: 24.4586 - val_mae: 3.6253\n",
      "Epoch 198/350\n",
      "122/122 [==============================] - 0s 575us/step - loss: 183.8010 - mae: 8.7654 - val_loss: 23.4614 - val_mae: 3.4844\n",
      "Epoch 199/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 193.0653 - mae: 8.8397 - val_loss: 23.5440 - val_mae: 3.4783\n",
      "Epoch 200/350\n",
      "122/122 [==============================] - 0s 589us/step - loss: 181.4994 - mae: 8.6440 - val_loss: 32.5749 - val_mae: 4.3107\n",
      "Epoch 201/350\n",
      "122/122 [==============================] - 0s 568us/step - loss: 184.7425 - mae: 8.7692 - val_loss: 27.0687 - val_mae: 3.8846\n",
      "Epoch 202/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 182.3503 - mae: 8.7431 - val_loss: 23.9930 - val_mae: 3.4735\n",
      "Epoch 203/350\n",
      "122/122 [==============================] - 0s 651us/step - loss: 161.2662 - mae: 8.3023 - val_loss: 30.9731 - val_mae: 4.1959\n",
      "Epoch 204/350\n",
      "122/122 [==============================] - 0s 566us/step - loss: 174.9080 - mae: 8.5899 - val_loss: 24.4807 - val_mae: 3.5816\n",
      "Epoch 205/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 169.3342 - mae: 8.4545 - val_loss: 29.8381 - val_mae: 4.0324\n",
      "Epoch 206/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 164.2876 - mae: 8.3257 - val_loss: 23.8164 - val_mae: 3.4674\n",
      "Epoch 207/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 165.8341 - mae: 8.3859 - val_loss: 23.1828 - val_mae: 3.4344\n",
      "Epoch 208/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 175.6603 - mae: 8.6046 - val_loss: 24.6459 - val_mae: 3.5969\n",
      "Epoch 209/350\n",
      "122/122 [==============================] - 0s 571us/step - loss: 174.9754 - mae: 8.5393 - val_loss: 26.2612 - val_mae: 3.6616\n",
      "Epoch 210/350\n",
      "122/122 [==============================] - 0s 586us/step - loss: 162.2320 - mae: 8.2905 - val_loss: 32.7295 - val_mae: 4.3009\n",
      "Epoch 211/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 160.8330 - mae: 8.2745 - val_loss: 23.4720 - val_mae: 3.4631\n",
      "Epoch 212/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 162.8801 - mae: 8.3362 - val_loss: 22.7109 - val_mae: 3.4044\n",
      "Epoch 213/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 163.6883 - mae: 8.3562 - val_loss: 24.2159 - val_mae: 3.5013\n",
      "Epoch 214/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 152.6317 - mae: 8.0584 - val_loss: 23.3860 - val_mae: 3.4468\n",
      "Epoch 215/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 157.7585 - mae: 8.2257 - val_loss: 24.4860 - val_mae: 3.5763\n",
      "Epoch 216/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 164.5808 - mae: 8.2902 - val_loss: 26.4628 - val_mae: 3.7745\n",
      "Epoch 217/350\n",
      "122/122 [==============================] - 0s 619us/step - loss: 165.7669 - mae: 8.3374 - val_loss: 25.8467 - val_mae: 3.6752\n",
      "Epoch 218/350\n",
      "122/122 [==============================] - 0s 585us/step - loss: 159.7676 - mae: 8.2893 - val_loss: 34.8696 - val_mae: 4.5073\n",
      "Epoch 219/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 167.0385 - mae: 8.3749 - val_loss: 26.6163 - val_mae: 3.7238\n",
      "Epoch 220/350\n",
      "122/122 [==============================] - 0s 579us/step - loss: 163.3843 - mae: 8.3256 - val_loss: 24.0891 - val_mae: 3.4669\n",
      "Epoch 221/350\n",
      "122/122 [==============================] - 0s 567us/step - loss: 148.0390 - mae: 7.9621 - val_loss: 43.5500 - val_mae: 5.0883\n",
      "Epoch 222/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 150.3358 - mae: 8.1247 - val_loss: 22.2299 - val_mae: 3.3365\n",
      "Epoch 223/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 151.1548 - mae: 7.9451 - val_loss: 23.9679 - val_mae: 3.4744\n",
      "Epoch 224/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 171.3929 - mae: 8.3669 - val_loss: 26.1504 - val_mae: 3.6727\n",
      "Epoch 225/350\n",
      "122/122 [==============================] - 0s 572us/step - loss: 161.9788 - mae: 8.2477 - val_loss: 25.5444 - val_mae: 3.6138\n",
      "Epoch 226/350\n",
      "122/122 [==============================] - 0s 573us/step - loss: 158.2154 - mae: 8.0350 - val_loss: 27.6681 - val_mae: 3.8013\n",
      "Epoch 227/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 157.4889 - mae: 8.1286 - val_loss: 24.8098 - val_mae: 3.5805\n",
      "Epoch 228/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 170.1770 - mae: 8.2991 - val_loss: 25.7965 - val_mae: 3.6628\n",
      "Epoch 229/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 153.3948 - mae: 8.1617 - val_loss: 41.3395 - val_mae: 4.8671\n",
      "Epoch 230/350\n",
      "122/122 [==============================] - 0s 588us/step - loss: 152.6714 - mae: 8.1112 - val_loss: 23.1298 - val_mae: 3.4422\n",
      "Epoch 231/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 149.7200 - mae: 7.9293 - val_loss: 23.6563 - val_mae: 3.5035\n",
      "Epoch 232/350\n",
      "122/122 [==============================] - 0s 676us/step - loss: 156.1983 - mae: 8.0916 - val_loss: 32.2467 - val_mae: 4.2384\n",
      "Epoch 233/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 149.6956 - mae: 7.9217 - val_loss: 23.7537 - val_mae: 3.4700\n",
      "Epoch 234/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 153.1179 - mae: 8.1717 - val_loss: 30.1976 - val_mae: 4.0609\n",
      "Epoch 235/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 148.5793 - mae: 8.0189 - val_loss: 24.1142 - val_mae: 3.5453\n",
      "Epoch 236/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 143.6071 - mae: 7.8445 - val_loss: 23.7724 - val_mae: 3.4832\n",
      "Epoch 237/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 150.2650 - mae: 8.0285 - val_loss: 29.6544 - val_mae: 4.0683\n",
      "Epoch 238/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 147.1643 - mae: 7.8543 - val_loss: 22.2705 - val_mae: 3.4075\n",
      "Epoch 239/350\n",
      "122/122 [==============================] - 0s 581us/step - loss: 145.9163 - mae: 7.8822 - val_loss: 24.2524 - val_mae: 3.5220\n",
      "Epoch 240/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 142.7386 - mae: 7.8557 - val_loss: 24.9242 - val_mae: 3.5336\n",
      "Epoch 241/350\n",
      "122/122 [==============================] - 0s 576us/step - loss: 160.8380 - mae: 8.1631 - val_loss: 30.9220 - val_mae: 4.1671\n",
      "Epoch 242/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 130.3676 - mae: 7.6257 - val_loss: 25.9428 - val_mae: 3.8010\n",
      "Epoch 243/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 145.1616 - mae: 7.8516 - val_loss: 23.9715 - val_mae: 3.4621\n",
      "Epoch 244/350\n",
      "122/122 [==============================] - 0s 545us/step - loss: 146.9967 - mae: 7.9079 - val_loss: 24.9893 - val_mae: 3.5749\n",
      "Epoch 245/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 143.5232 - mae: 7.8014 - val_loss: 29.2172 - val_mae: 4.1032\n",
      "Epoch 246/350\n",
      "122/122 [==============================] - 0s 650us/step - loss: 136.5545 - mae: 7.6849 - val_loss: 25.5075 - val_mae: 3.6975\n",
      "Epoch 247/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 144.3972 - mae: 7.7852 - val_loss: 24.2892 - val_mae: 3.6142\n",
      "Epoch 248/350\n",
      "122/122 [==============================] - 0s 572us/step - loss: 144.9873 - mae: 7.8341 - val_loss: 23.8267 - val_mae: 3.4497\n",
      "Epoch 249/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 133.9195 - mae: 7.7697 - val_loss: 24.8955 - val_mae: 3.5897\n",
      "Epoch 250/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 137.0368 - mae: 7.6506 - val_loss: 27.5290 - val_mae: 3.8810\n",
      "Epoch 251/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 144.9914 - mae: 7.7812 - val_loss: 39.8771 - val_mae: 4.7642\n",
      "Epoch 252/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 131.8154 - mae: 7.6087 - val_loss: 25.4813 - val_mae: 3.6240\n",
      "Epoch 253/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 143.9104 - mae: 7.8900 - val_loss: 30.7437 - val_mae: 4.1243\n",
      "Epoch 254/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 138.5711 - mae: 7.6759 - val_loss: 55.9469 - val_mae: 5.6104\n",
      "Epoch 255/350\n",
      "122/122 [==============================] - 0s 582us/step - loss: 142.1212 - mae: 7.7564 - val_loss: 28.6691 - val_mae: 4.0180\n",
      "Epoch 256/350\n",
      "122/122 [==============================] - 0s 581us/step - loss: 138.3624 - mae: 7.6759 - val_loss: 28.1201 - val_mae: 3.8453\n",
      "Epoch 257/350\n",
      "122/122 [==============================] - 0s 551us/step - loss: 140.8483 - mae: 7.7982 - val_loss: 25.1806 - val_mae: 3.5990\n",
      "Epoch 258/350\n",
      "122/122 [==============================] - 0s 585us/step - loss: 122.4925 - mae: 7.4151 - val_loss: 23.6413 - val_mae: 3.5264\n",
      "Epoch 259/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 128.9492 - mae: 7.5284 - val_loss: 23.2001 - val_mae: 3.4845\n",
      "Epoch 260/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 135.6297 - mae: 7.5158 - val_loss: 24.4252 - val_mae: 3.6004\n",
      "Epoch 261/350\n",
      "122/122 [==============================] - 0s 656us/step - loss: 132.5865 - mae: 7.6055 - val_loss: 30.0333 - val_mae: 4.1494\n",
      "Epoch 262/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 128.4590 - mae: 7.5438 - val_loss: 23.5167 - val_mae: 3.4697\n",
      "Epoch 263/350\n",
      "122/122 [==============================] - 0s 584us/step - loss: 130.2516 - mae: 7.5283 - val_loss: 33.1648 - val_mae: 4.4141\n",
      "Epoch 264/350\n",
      "122/122 [==============================] - 0s 555us/step - loss: 136.9942 - mae: 7.7782 - val_loss: 72.8474 - val_mae: 6.5489\n",
      "Epoch 265/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 134.3389 - mae: 7.7216 - val_loss: 24.8456 - val_mae: 3.5444\n",
      "Epoch 266/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 127.1029 - mae: 7.4831 - val_loss: 31.9790 - val_mae: 4.2783\n",
      "Epoch 267/350\n",
      "122/122 [==============================] - 0s 583us/step - loss: 130.6272 - mae: 7.4725 - val_loss: 25.1405 - val_mae: 3.6414\n",
      "Epoch 268/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 119.0626 - mae: 7.2767 - val_loss: 24.0432 - val_mae: 3.4718\n",
      "Epoch 269/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 127.9562 - mae: 7.4391 - val_loss: 25.3796 - val_mae: 3.5762\n",
      "Epoch 270/350\n",
      "122/122 [==============================] - 0s 548us/step - loss: 121.8483 - mae: 7.4377 - val_loss: 24.5609 - val_mae: 3.5619\n",
      "Epoch 271/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 122.6702 - mae: 7.4924 - val_loss: 24.2682 - val_mae: 3.4795\n",
      "Epoch 272/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 132.2627 - mae: 7.5108 - val_loss: 31.0418 - val_mae: 4.2316\n",
      "Epoch 273/350\n",
      "122/122 [==============================] - 0s 546us/step - loss: 123.8987 - mae: 7.4664 - val_loss: 29.2938 - val_mae: 4.0289\n",
      "Epoch 274/350\n",
      "122/122 [==============================] - 0s 590us/step - loss: 127.3408 - mae: 7.4489 - val_loss: 23.7916 - val_mae: 3.5450\n",
      "Epoch 275/350\n",
      "122/122 [==============================] - 0s 647us/step - loss: 122.3070 - mae: 7.4010 - val_loss: 25.1030 - val_mae: 3.5730\n",
      "Epoch 276/350\n",
      "122/122 [==============================] - 0s 587us/step - loss: 127.8955 - mae: 7.5260 - val_loss: 28.2844 - val_mae: 3.7881\n",
      "Epoch 277/350\n",
      "122/122 [==============================] - 0s 569us/step - loss: 126.7610 - mae: 7.5862 - val_loss: 23.3705 - val_mae: 3.5312\n",
      "Epoch 278/350\n",
      "122/122 [==============================] - 0s 582us/step - loss: 122.2287 - mae: 7.3999 - val_loss: 28.3794 - val_mae: 3.9336\n",
      "Epoch 279/350\n",
      "122/122 [==============================] - 0s 566us/step - loss: 128.9514 - mae: 7.5268 - val_loss: 35.7613 - val_mae: 4.6175\n",
      "Epoch 280/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 128.5888 - mae: 7.4702 - val_loss: 25.8294 - val_mae: 3.7119\n",
      "Epoch 281/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 125.5831 - mae: 7.5145 - val_loss: 27.6831 - val_mae: 3.8859\n",
      "Epoch 282/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 127.4266 - mae: 7.5653 - val_loss: 26.9260 - val_mae: 3.8482\n",
      "Epoch 283/350\n",
      "122/122 [==============================] - 0s 568us/step - loss: 115.3329 - mae: 7.3079 - val_loss: 31.8652 - val_mae: 4.1502\n",
      "Epoch 284/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 123.9696 - mae: 7.4428 - val_loss: 23.7968 - val_mae: 3.4934\n",
      "Epoch 285/350\n",
      "122/122 [==============================] - 0s 566us/step - loss: 124.2521 - mae: 7.4674 - val_loss: 27.0290 - val_mae: 3.8472\n",
      "Epoch 286/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 111.9138 - mae: 7.2221 - val_loss: 24.7607 - val_mae: 3.6361\n",
      "Epoch 287/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 121.1767 - mae: 7.4001 - val_loss: 23.9787 - val_mae: 3.5785\n",
      "Epoch 288/350\n",
      "122/122 [==============================] - 0s 591us/step - loss: 125.7489 - mae: 7.4390 - val_loss: 26.5642 - val_mae: 3.7457\n",
      "Epoch 289/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 119.9186 - mae: 7.2872 - val_loss: 23.2601 - val_mae: 3.4707\n",
      "Epoch 290/350\n",
      "122/122 [==============================] - 0s 649us/step - loss: 123.9772 - mae: 7.4194 - val_loss: 69.7958 - val_mae: 6.4577\n",
      "Epoch 291/350\n",
      "122/122 [==============================] - 0s 586us/step - loss: 121.2127 - mae: 7.3094 - val_loss: 25.3088 - val_mae: 3.6866\n",
      "Epoch 292/350\n",
      "122/122 [==============================] - 0s 573us/step - loss: 117.7150 - mae: 7.3556 - val_loss: 27.6290 - val_mae: 3.9077\n",
      "Epoch 293/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 118.5484 - mae: 7.3592 - val_loss: 26.2609 - val_mae: 3.7178\n",
      "Epoch 294/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 118.8944 - mae: 7.3420 - val_loss: 35.6157 - val_mae: 4.5993\n",
      "Epoch 295/350\n",
      "122/122 [==============================] - 0s 610us/step - loss: 113.1057 - mae: 7.2103 - val_loss: 32.4018 - val_mae: 4.3439\n",
      "Epoch 296/350\n",
      "122/122 [==============================] - 0s 612us/step - loss: 106.7337 - mae: 7.0086 - val_loss: 23.8241 - val_mae: 3.5641\n",
      "Epoch 297/350\n",
      "122/122 [==============================] - 0s 566us/step - loss: 113.5546 - mae: 7.2682 - val_loss: 26.3842 - val_mae: 3.7747\n",
      "Epoch 298/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 116.0156 - mae: 7.3059 - val_loss: 28.2652 - val_mae: 3.9812\n",
      "Epoch 299/350\n",
      "122/122 [==============================] - 0s 548us/step - loss: 108.7800 - mae: 7.0818 - val_loss: 33.0283 - val_mae: 4.3487\n",
      "Epoch 300/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 118.0219 - mae: 7.3356 - val_loss: 24.7589 - val_mae: 3.6892\n",
      "Epoch 301/350\n",
      "122/122 [==============================] - 0s 572us/step - loss: 117.5512 - mae: 7.2835 - val_loss: 29.2335 - val_mae: 4.0207\n",
      "Epoch 302/350\n",
      "122/122 [==============================] - 0s 573us/step - loss: 111.1176 - mae: 7.1499 - val_loss: 24.4712 - val_mae: 3.5545\n",
      "Epoch 303/350\n",
      "122/122 [==============================] - 0s 566us/step - loss: 109.6762 - mae: 7.1094 - val_loss: 37.4837 - val_mae: 4.5626\n",
      "Epoch 304/350\n",
      "122/122 [==============================] - 0s 653us/step - loss: 109.5560 - mae: 7.1139 - val_loss: 33.6373 - val_mae: 4.3494\n",
      "Epoch 305/350\n",
      "122/122 [==============================] - 0s 585us/step - loss: 107.6473 - mae: 7.0482 - val_loss: 25.9720 - val_mae: 3.7300\n",
      "Epoch 306/350\n",
      "122/122 [==============================] - 0s 570us/step - loss: 106.0101 - mae: 7.1204 - val_loss: 27.4207 - val_mae: 3.8543\n",
      "Epoch 307/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 110.0528 - mae: 7.1094 - val_loss: 27.3425 - val_mae: 3.8513\n",
      "Epoch 308/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 110.5352 - mae: 7.1602 - val_loss: 29.7540 - val_mae: 4.0917\n",
      "Epoch 309/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 102.1881 - mae: 6.9861 - val_loss: 43.9896 - val_mae: 5.1037\n",
      "Epoch 310/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 106.8620 - mae: 7.0985 - val_loss: 34.7899 - val_mae: 4.3889\n",
      "Epoch 311/350\n",
      "122/122 [==============================] - 0s 552us/step - loss: 103.9236 - mae: 7.0042 - val_loss: 41.4136 - val_mae: 4.9445\n",
      "Epoch 312/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 104.5016 - mae: 7.0664 - val_loss: 31.9529 - val_mae: 4.3269\n",
      "Epoch 313/350\n",
      "122/122 [==============================] - 0s 558us/step - loss: 106.3507 - mae: 6.9219 - val_loss: 22.9891 - val_mae: 3.5017\n",
      "Epoch 314/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 104.8927 - mae: 7.0344 - val_loss: 28.6355 - val_mae: 4.0614\n",
      "Epoch 315/350\n",
      "122/122 [==============================] - 0s 580us/step - loss: 101.3182 - mae: 6.9931 - val_loss: 42.1313 - val_mae: 5.0883\n",
      "Epoch 316/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 108.2655 - mae: 7.0264 - val_loss: 34.5142 - val_mae: 4.5255\n",
      "Epoch 317/350\n",
      "122/122 [==============================] - 0s 619us/step - loss: 108.2728 - mae: 7.0393 - val_loss: 23.9905 - val_mae: 3.5291\n",
      "Epoch 318/350\n",
      "122/122 [==============================] - 0s 584us/step - loss: 112.7386 - mae: 7.2722 - val_loss: 30.1533 - val_mae: 4.0498\n",
      "Epoch 319/350\n",
      "122/122 [==============================] - 0s 561us/step - loss: 102.0946 - mae: 6.8063 - val_loss: 35.0572 - val_mae: 4.5235\n",
      "Epoch 320/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 110.3384 - mae: 7.1577 - val_loss: 26.4402 - val_mae: 3.8311\n",
      "Epoch 321/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 100.6962 - mae: 6.9053 - val_loss: 30.0585 - val_mae: 4.1805\n",
      "Epoch 322/350\n",
      "122/122 [==============================] - 0s 571us/step - loss: 102.8622 - mae: 6.9256 - val_loss: 24.5234 - val_mae: 3.5889\n",
      "Epoch 323/350\n",
      "122/122 [==============================] - 0s 572us/step - loss: 100.0227 - mae: 6.9190 - val_loss: 25.9694 - val_mae: 3.7528\n",
      "Epoch 324/350\n",
      "122/122 [==============================] - 0s 560us/step - loss: 105.4643 - mae: 7.0452 - val_loss: 28.9331 - val_mae: 4.0689\n",
      "Epoch 325/350\n",
      "122/122 [==============================] - 0s 637us/step - loss: 100.7569 - mae: 6.8437 - val_loss: 26.1534 - val_mae: 3.7384\n",
      "Epoch 326/350\n",
      "122/122 [==============================] - 0s 581us/step - loss: 100.6826 - mae: 6.8552 - val_loss: 26.5467 - val_mae: 3.7969\n",
      "Epoch 327/350\n",
      "122/122 [==============================] - 0s 570us/step - loss: 104.3211 - mae: 6.9028 - val_loss: 28.0299 - val_mae: 3.7907\n",
      "Epoch 328/350\n",
      "122/122 [==============================] - 0s 635us/step - loss: 101.7250 - mae: 6.8823 - val_loss: 32.9262 - val_mae: 4.3301\n",
      "Epoch 329/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 92.4277 - mae: 6.7072 - val_loss: 26.9038 - val_mae: 3.7073\n",
      "Epoch 330/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 96.9673 - mae: 6.8612 - val_loss: 26.9941 - val_mae: 3.8366\n",
      "Epoch 331/350\n",
      "122/122 [==============================] - 0s 563us/step - loss: 99.6668 - mae: 6.9156 - val_loss: 28.8875 - val_mae: 3.9971\n",
      "Epoch 332/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 102.0679 - mae: 7.0278 - val_loss: 37.6005 - val_mae: 4.8074\n",
      "Epoch 333/350\n",
      "122/122 [==============================] - 0s 584us/step - loss: 106.8442 - mae: 7.0517 - val_loss: 30.5296 - val_mae: 4.2368\n",
      "Epoch 334/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 98.7397 - mae: 6.8438 - val_loss: 36.5470 - val_mae: 4.7170\n",
      "Epoch 335/350\n",
      "122/122 [==============================] - 0s 565us/step - loss: 103.5116 - mae: 6.9854 - val_loss: 26.3159 - val_mae: 3.6662\n",
      "Epoch 336/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 94.3429 - mae: 6.7473 - val_loss: 27.2604 - val_mae: 3.7950\n",
      "Epoch 337/350\n",
      "122/122 [==============================] - 0s 576us/step - loss: 98.0652 - mae: 6.7975 - val_loss: 26.8705 - val_mae: 3.6982\n",
      "Epoch 338/350\n",
      "122/122 [==============================] - 0s 548us/step - loss: 98.0204 - mae: 6.8983 - val_loss: 57.6433 - val_mae: 5.9502\n",
      "Epoch 339/350\n",
      "122/122 [==============================] - 0s 549us/step - loss: 98.1105 - mae: 6.8294 - val_loss: 25.7757 - val_mae: 3.6312\n",
      "Epoch 340/350\n",
      "122/122 [==============================] - 0s 557us/step - loss: 98.1381 - mae: 6.8952 - val_loss: 28.3259 - val_mae: 3.9960\n",
      "Epoch 341/350\n",
      "122/122 [==============================] - 0s 649us/step - loss: 107.2748 - mae: 7.0048 - val_loss: 56.0715 - val_mae: 5.6998\n",
      "Epoch 342/350\n",
      "122/122 [==============================] - 0s 601us/step - loss: 96.7479 - mae: 6.7802 - val_loss: 32.6353 - val_mae: 4.4026\n",
      "Epoch 343/350\n",
      "122/122 [==============================] - 0s 562us/step - loss: 93.9263 - mae: 6.6462 - val_loss: 36.9327 - val_mae: 4.7274\n",
      "Epoch 344/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 91.5707 - mae: 6.6837 - val_loss: 27.7137 - val_mae: 3.8309\n",
      "Epoch 345/350\n",
      "122/122 [==============================] - 0s 544us/step - loss: 98.6163 - mae: 6.9088 - val_loss: 37.9720 - val_mae: 4.7479\n",
      "Epoch 346/350\n",
      "122/122 [==============================] - 0s 564us/step - loss: 96.6917 - mae: 6.8697 - val_loss: 38.3094 - val_mae: 4.7804\n",
      "Epoch 347/350\n",
      "122/122 [==============================] - 0s 554us/step - loss: 108.8083 - mae: 7.0560 - val_loss: 34.9440 - val_mae: 4.5762\n",
      "Epoch 348/350\n",
      "122/122 [==============================] - 0s 559us/step - loss: 100.8373 - mae: 6.9476 - val_loss: 27.7325 - val_mae: 3.9164\n",
      "Epoch 349/350\n",
      "122/122 [==============================] - 0s 556us/step - loss: 93.9309 - mae: 6.8071 - val_loss: 32.5632 - val_mae: 4.4236\n",
      "Epoch 350/350\n",
      "122/122 [==============================] - 0s 553us/step - loss: 91.8293 - mae: 6.6689 - val_loss: 24.5626 - val_mae: 3.6741\n",
      "31/31 [==============================] - 0s 306us/step\n",
      "Epochs: 350 | MAE: 3.674060824506711\n",
      "Training model with 400 epochs\n",
      "Epoch 1/400\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 65284.7773 - mae: 220.5200 - val_loss: 51078.7539 - val_mae: 194.0041\n",
      "Epoch 2/400\n",
      "122/122 [==============================] - 0s 667us/step - loss: 26084.5684 - mae: 131.1040 - val_loss: 6114.3872 - val_mae: 62.2375\n",
      "Epoch 3/400\n",
      "122/122 [==============================] - 0s 574us/step - loss: 4383.3447 - mae: 50.2723 - val_loss: 2696.5320 - val_mae: 39.1794\n",
      "Epoch 4/400\n",
      "122/122 [==============================] - 0s 611us/step - loss: 3053.7480 - mae: 41.0203 - val_loss: 1879.7544 - val_mae: 32.7292\n",
      "Epoch 5/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 2348.3198 - mae: 36.8482 - val_loss: 1514.4060 - val_mae: 29.4445\n",
      "Epoch 6/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 2107.3765 - mae: 34.7753 - val_loss: 1281.0125 - val_mae: 27.0517\n",
      "Epoch 7/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 1974.2799 - mae: 34.2799 - val_loss: 1155.1906 - val_mae: 25.6785\n",
      "Epoch 8/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 1789.5431 - mae: 32.6233 - val_loss: 1002.1050 - val_mae: 24.1076\n",
      "Epoch 9/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 1620.3468 - mae: 31.5230 - val_loss: 935.6102 - val_mae: 23.2294\n",
      "Epoch 10/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 1519.6892 - mae: 30.3359 - val_loss: 824.9772 - val_mae: 21.8043\n",
      "Epoch 11/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 1440.3381 - mae: 29.4128 - val_loss: 762.5831 - val_mae: 20.9787\n",
      "Epoch 12/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 1427.2936 - mae: 29.1583 - val_loss: 682.7425 - val_mae: 19.8088\n",
      "Epoch 13/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 1278.6829 - mae: 27.8404 - val_loss: 592.9335 - val_mae: 18.4464\n",
      "Epoch 14/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 1252.5076 - mae: 27.6972 - val_loss: 542.3546 - val_mae: 17.7083\n",
      "Epoch 15/400\n",
      "122/122 [==============================] - 0s 653us/step - loss: 1138.7529 - mae: 26.4675 - val_loss: 477.1624 - val_mae: 16.4567\n",
      "Epoch 16/400\n",
      "122/122 [==============================] - 0s 579us/step - loss: 1115.6372 - mae: 25.8550 - val_loss: 438.7550 - val_mae: 15.7656\n",
      "Epoch 17/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 1083.3109 - mae: 25.3792 - val_loss: 393.6115 - val_mae: 14.7096\n",
      "Epoch 18/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 959.0758 - mae: 23.9027 - val_loss: 368.5382 - val_mae: 14.5122\n",
      "Epoch 19/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 943.0048 - mae: 23.8149 - val_loss: 317.5656 - val_mae: 13.3046\n",
      "Epoch 20/400\n",
      "122/122 [==============================] - 0s 591us/step - loss: 877.8731 - mae: 23.0116 - val_loss: 285.9520 - val_mae: 12.6528\n",
      "Epoch 21/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 869.6531 - mae: 22.5638 - val_loss: 274.1514 - val_mae: 12.2991\n",
      "Epoch 22/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 859.0656 - mae: 22.5364 - val_loss: 230.0462 - val_mae: 11.2481\n",
      "Epoch 23/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 840.3005 - mae: 22.2968 - val_loss: 206.9041 - val_mae: 10.9380\n",
      "Epoch 24/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 753.7437 - mae: 21.0819 - val_loss: 188.4801 - val_mae: 10.2183\n",
      "Epoch 25/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 757.7182 - mae: 20.8760 - val_loss: 166.2800 - val_mae: 9.5674\n",
      "Epoch 26/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 732.2256 - mae: 20.5807 - val_loss: 151.9608 - val_mae: 9.0851\n",
      "Epoch 27/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 710.8195 - mae: 20.2358 - val_loss: 132.0364 - val_mae: 8.6130\n",
      "Epoch 28/400\n",
      "122/122 [==============================] - 0s 679us/step - loss: 699.7090 - mae: 19.9037 - val_loss: 117.2593 - val_mae: 7.9870\n",
      "Epoch 29/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 661.1317 - mae: 19.1872 - val_loss: 116.7819 - val_mae: 8.3021\n",
      "Epoch 30/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 651.4061 - mae: 19.1144 - val_loss: 102.6677 - val_mae: 7.4443\n",
      "Epoch 31/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 627.9866 - mae: 18.6822 - val_loss: 102.5477 - val_mae: 7.9091\n",
      "Epoch 32/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 617.1796 - mae: 18.4105 - val_loss: 77.8581 - val_mae: 6.6754\n",
      "Epoch 33/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 614.8101 - mae: 18.4221 - val_loss: 72.5623 - val_mae: 6.4519\n",
      "Epoch 34/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 573.7298 - mae: 17.8310 - val_loss: 67.4500 - val_mae: 6.3756\n",
      "Epoch 35/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 595.6218 - mae: 17.8447 - val_loss: 62.1799 - val_mae: 6.1134\n",
      "Epoch 36/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 579.5503 - mae: 17.5874 - val_loss: 53.2304 - val_mae: 5.3102\n",
      "Epoch 37/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 576.0984 - mae: 17.6678 - val_loss: 64.4425 - val_mae: 6.1913\n",
      "Epoch 38/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 592.5498 - mae: 17.6737 - val_loss: 61.4112 - val_mae: 6.0198\n",
      "Epoch 39/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 546.5934 - mae: 17.0436 - val_loss: 48.4905 - val_mae: 4.9919\n",
      "Epoch 40/400\n",
      "122/122 [==============================] - 0s 650us/step - loss: 550.2484 - mae: 17.0990 - val_loss: 50.7918 - val_mae: 5.2241\n",
      "Epoch 41/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 555.0200 - mae: 16.9827 - val_loss: 47.5489 - val_mae: 5.1936\n",
      "Epoch 42/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 536.7473 - mae: 16.7464 - val_loss: 70.1103 - val_mae: 6.5368\n",
      "Epoch 43/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 535.9444 - mae: 16.6060 - val_loss: 51.9196 - val_mae: 5.6099\n",
      "Epoch 44/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 513.3795 - mae: 16.4926 - val_loss: 39.8988 - val_mae: 4.5147\n",
      "Epoch 45/400\n",
      "122/122 [==============================] - 0s 601us/step - loss: 547.4297 - mae: 16.5617 - val_loss: 42.7252 - val_mae: 4.8411\n",
      "Epoch 46/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 519.2780 - mae: 16.3156 - val_loss: 29.9565 - val_mae: 3.9430\n",
      "Epoch 47/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 495.6602 - mae: 15.8953 - val_loss: 53.9680 - val_mae: 5.7504\n",
      "Epoch 48/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 516.4616 - mae: 16.1848 - val_loss: 37.2177 - val_mae: 4.7196\n",
      "Epoch 49/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 506.5650 - mae: 16.0237 - val_loss: 30.9681 - val_mae: 4.0364\n",
      "Epoch 50/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 502.8924 - mae: 15.9649 - val_loss: 32.6731 - val_mae: 4.4021\n",
      "Epoch 51/400\n",
      "122/122 [==============================] - 0s 579us/step - loss: 514.8524 - mae: 15.9372 - val_loss: 28.8606 - val_mae: 3.8660\n",
      "Epoch 52/400\n",
      "122/122 [==============================] - 0s 627us/step - loss: 488.1979 - mae: 15.6673 - val_loss: 26.5213 - val_mae: 3.6512\n",
      "Epoch 53/400\n",
      "122/122 [==============================] - 0s 620us/step - loss: 489.1909 - mae: 15.6176 - val_loss: 28.0718 - val_mae: 3.8032\n",
      "Epoch 54/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 479.5859 - mae: 15.4791 - val_loss: 33.8605 - val_mae: 4.3229\n",
      "Epoch 55/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 473.7802 - mae: 15.4112 - val_loss: 32.7197 - val_mae: 4.2519\n",
      "Epoch 56/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 486.9895 - mae: 15.4475 - val_loss: 25.4107 - val_mae: 3.4448\n",
      "Epoch 57/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 461.7910 - mae: 15.2354 - val_loss: 27.5778 - val_mae: 3.9299\n",
      "Epoch 58/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 465.8239 - mae: 15.2333 - val_loss: 40.5180 - val_mae: 4.6930\n",
      "Epoch 59/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 470.8676 - mae: 15.2292 - val_loss: 22.8550 - val_mae: 3.3601\n",
      "Epoch 60/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 455.7692 - mae: 14.9787 - val_loss: 27.8040 - val_mae: 3.7070\n",
      "Epoch 61/400\n",
      "122/122 [==============================] - 0s 614us/step - loss: 461.6599 - mae: 14.9698 - val_loss: 27.9540 - val_mae: 3.7717\n",
      "Epoch 62/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 441.6718 - mae: 14.8500 - val_loss: 23.1322 - val_mae: 3.4090\n",
      "Epoch 63/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 478.1023 - mae: 15.2720 - val_loss: 29.8844 - val_mae: 4.0929\n",
      "Epoch 64/400\n",
      "122/122 [==============================] - 0s 648us/step - loss: 456.3260 - mae: 14.8961 - val_loss: 28.5965 - val_mae: 3.7929\n",
      "Epoch 65/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 437.0773 - mae: 14.7097 - val_loss: 25.1293 - val_mae: 3.7622\n",
      "Epoch 66/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 451.6568 - mae: 14.6194 - val_loss: 33.1961 - val_mae: 4.1462\n",
      "Epoch 67/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 450.1397 - mae: 14.5024 - val_loss: 30.5705 - val_mae: 4.1333\n",
      "Epoch 68/400\n",
      "122/122 [==============================] - 0s 605us/step - loss: 433.0779 - mae: 14.6768 - val_loss: 24.3049 - val_mae: 3.5856\n",
      "Epoch 69/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 424.1875 - mae: 14.4185 - val_loss: 30.3707 - val_mae: 3.9248\n",
      "Epoch 70/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 414.5683 - mae: 14.3019 - val_loss: 24.6530 - val_mae: 3.6145\n",
      "Epoch 71/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 434.6022 - mae: 14.4084 - val_loss: 29.8723 - val_mae: 4.0439\n",
      "Epoch 72/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 413.5510 - mae: 14.0486 - val_loss: 28.1772 - val_mae: 3.9236\n",
      "Epoch 73/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 409.4859 - mae: 13.9971 - val_loss: 31.0033 - val_mae: 4.1872\n",
      "Epoch 74/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 442.8947 - mae: 14.5501 - val_loss: 28.6481 - val_mae: 3.7230\n",
      "Epoch 75/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 427.2667 - mae: 14.1162 - val_loss: 22.9843 - val_mae: 3.4899\n",
      "Epoch 76/400\n",
      "122/122 [==============================] - 0s 687us/step - loss: 410.9268 - mae: 14.1220 - val_loss: 32.0394 - val_mae: 4.2009\n",
      "Epoch 77/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 395.4875 - mae: 13.7714 - val_loss: 33.8036 - val_mae: 4.4143\n",
      "Epoch 78/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 407.3749 - mae: 14.0065 - val_loss: 22.9747 - val_mae: 3.4626\n",
      "Epoch 79/400\n",
      "122/122 [==============================] - 0s 551us/step - loss: 383.8008 - mae: 13.5199 - val_loss: 25.0972 - val_mae: 3.6180\n",
      "Epoch 80/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 400.3347 - mae: 13.9079 - val_loss: 23.1554 - val_mae: 3.3982\n",
      "Epoch 81/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 397.8097 - mae: 13.8888 - val_loss: 23.4586 - val_mae: 3.5941\n",
      "Epoch 82/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 411.9782 - mae: 14.0337 - val_loss: 43.1519 - val_mae: 4.9642\n",
      "Epoch 83/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 410.8242 - mae: 13.9304 - val_loss: 25.7422 - val_mae: 3.7199\n",
      "Epoch 84/400\n",
      "122/122 [==============================] - 0s 585us/step - loss: 405.0647 - mae: 13.7890 - val_loss: 22.8008 - val_mae: 3.4629\n",
      "Epoch 85/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 378.3795 - mae: 13.2975 - val_loss: 30.2799 - val_mae: 4.1204\n",
      "Epoch 86/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 387.9310 - mae: 13.5274 - val_loss: 24.8596 - val_mae: 3.5613\n",
      "Epoch 87/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 389.8959 - mae: 13.5647 - val_loss: 26.4455 - val_mae: 3.9515\n",
      "Epoch 88/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 370.4941 - mae: 13.2465 - val_loss: 39.8944 - val_mae: 4.7526\n",
      "Epoch 89/400\n",
      "122/122 [==============================] - 0s 553us/step - loss: 398.5957 - mae: 13.6896 - val_loss: 42.0497 - val_mae: 5.0190\n",
      "Epoch 90/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 385.3362 - mae: 13.3945 - val_loss: 28.3270 - val_mae: 3.8715\n",
      "Epoch 91/400\n",
      "122/122 [==============================] - 0s 778us/step - loss: 383.4686 - mae: 13.4357 - val_loss: 39.5433 - val_mae: 4.8231\n",
      "Epoch 92/400\n",
      "122/122 [==============================] - 0s 742us/step - loss: 339.5201 - mae: 12.7903 - val_loss: 39.6584 - val_mae: 4.7887\n",
      "Epoch 93/400\n",
      "122/122 [==============================] - 0s 766us/step - loss: 396.1042 - mae: 13.3469 - val_loss: 28.9766 - val_mae: 4.0854\n",
      "Epoch 94/400\n",
      "122/122 [==============================] - 0s 785us/step - loss: 371.5160 - mae: 13.2066 - val_loss: 41.0942 - val_mae: 4.8953\n",
      "Epoch 95/400\n",
      "122/122 [==============================] - 0s 816us/step - loss: 375.1948 - mae: 13.1715 - val_loss: 22.8445 - val_mae: 3.4701\n",
      "Epoch 96/400\n",
      "122/122 [==============================] - 0s 829us/step - loss: 340.7969 - mae: 12.7450 - val_loss: 27.4468 - val_mae: 3.9075\n",
      "Epoch 97/400\n",
      "122/122 [==============================] - 0s 676us/step - loss: 376.9473 - mae: 13.2157 - val_loss: 26.3967 - val_mae: 3.9056\n",
      "Epoch 98/400\n",
      "122/122 [==============================] - 0s 586us/step - loss: 359.0237 - mae: 13.0069 - val_loss: 26.4286 - val_mae: 3.7575\n",
      "Epoch 99/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 381.5102 - mae: 13.3260 - val_loss: 26.9449 - val_mae: 3.7971\n",
      "Epoch 100/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 357.7843 - mae: 12.8868 - val_loss: 22.0087 - val_mae: 3.3779\n",
      "Epoch 101/400\n",
      "122/122 [==============================] - 0s 655us/step - loss: 388.7489 - mae: 13.4317 - val_loss: 24.9283 - val_mae: 3.6557\n",
      "Epoch 102/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 351.1369 - mae: 12.8704 - val_loss: 26.3701 - val_mae: 3.7415\n",
      "Epoch 103/400\n",
      "122/122 [==============================] - 0s 594us/step - loss: 343.0589 - mae: 12.6435 - val_loss: 38.1157 - val_mae: 4.4874\n",
      "Epoch 104/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 348.8996 - mae: 12.5700 - val_loss: 27.3022 - val_mae: 3.9961\n",
      "Epoch 105/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 361.0490 - mae: 12.6382 - val_loss: 27.6260 - val_mae: 3.9133\n",
      "Epoch 106/400\n",
      "122/122 [==============================] - 0s 578us/step - loss: 339.5929 - mae: 12.6177 - val_loss: 28.8863 - val_mae: 3.9539\n",
      "Epoch 107/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 333.9346 - mae: 12.1932 - val_loss: 27.5396 - val_mae: 3.7891\n",
      "Epoch 108/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 352.8210 - mae: 12.8083 - val_loss: 35.3265 - val_mae: 4.4974\n",
      "Epoch 109/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 335.8956 - mae: 12.5378 - val_loss: 24.9985 - val_mae: 3.7164\n",
      "Epoch 110/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 332.7011 - mae: 12.4045 - val_loss: 29.3224 - val_mae: 4.0268\n",
      "Epoch 111/400\n",
      "122/122 [==============================] - 0s 714us/step - loss: 332.2404 - mae: 12.4371 - val_loss: 27.0273 - val_mae: 3.8775\n",
      "Epoch 112/400\n",
      "122/122 [==============================] - 0s 620us/step - loss: 330.2581 - mae: 12.2461 - val_loss: 31.6924 - val_mae: 4.2806\n",
      "Epoch 113/400\n",
      "122/122 [==============================] - 0s 713us/step - loss: 361.5983 - mae: 12.7932 - val_loss: 28.9275 - val_mae: 4.0668\n",
      "Epoch 114/400\n",
      "122/122 [==============================] - 0s 666us/step - loss: 342.1644 - mae: 12.4049 - val_loss: 23.4423 - val_mae: 3.5594\n",
      "Epoch 115/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 328.9640 - mae: 12.2562 - val_loss: 34.7371 - val_mae: 4.4512\n",
      "Epoch 116/400\n",
      "122/122 [==============================] - 0s 608us/step - loss: 328.7115 - mae: 12.2493 - val_loss: 26.0501 - val_mae: 3.7033\n",
      "Epoch 117/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 316.9662 - mae: 12.0219 - val_loss: 30.4309 - val_mae: 4.2817\n",
      "Epoch 118/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 331.9026 - mae: 12.1131 - val_loss: 23.8039 - val_mae: 3.5750\n",
      "Epoch 119/400\n",
      "122/122 [==============================] - 0s 589us/step - loss: 308.8966 - mae: 11.8474 - val_loss: 24.5988 - val_mae: 3.5958\n",
      "Epoch 120/400\n",
      "122/122 [==============================] - 0s 578us/step - loss: 304.8372 - mae: 11.8447 - val_loss: 23.1655 - val_mae: 3.3954\n",
      "Epoch 121/400\n",
      "122/122 [==============================] - 0s 586us/step - loss: 298.7387 - mae: 11.8837 - val_loss: 25.6737 - val_mae: 3.7236\n",
      "Epoch 122/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 301.7041 - mae: 11.7694 - val_loss: 27.5145 - val_mae: 3.8597\n",
      "Epoch 123/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 328.8397 - mae: 12.2048 - val_loss: 29.2120 - val_mae: 3.9589\n",
      "Epoch 124/400\n",
      "122/122 [==============================] - 0s 618us/step - loss: 325.6679 - mae: 11.9783 - val_loss: 23.6535 - val_mae: 3.4112\n",
      "Epoch 125/400\n",
      "122/122 [==============================] - 0s 668us/step - loss: 311.9809 - mae: 11.7811 - val_loss: 25.4876 - val_mae: 3.6780\n",
      "Epoch 126/400\n",
      "122/122 [==============================] - 0s 615us/step - loss: 310.4226 - mae: 11.8684 - val_loss: 28.4334 - val_mae: 3.8716\n",
      "Epoch 127/400\n",
      "122/122 [==============================] - 0s 604us/step - loss: 308.3883 - mae: 11.8565 - val_loss: 32.7115 - val_mae: 4.3583\n",
      "Epoch 128/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 312.5131 - mae: 11.8342 - val_loss: 23.7220 - val_mae: 3.4724\n",
      "Epoch 129/400\n",
      "122/122 [==============================] - 0s 654us/step - loss: 291.9399 - mae: 11.6597 - val_loss: 26.8118 - val_mae: 3.6600\n",
      "Epoch 130/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 285.6266 - mae: 11.4159 - val_loss: 25.6178 - val_mae: 3.5988\n",
      "Epoch 131/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 306.5576 - mae: 11.8250 - val_loss: 29.1198 - val_mae: 4.0056\n",
      "Epoch 132/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 308.8877 - mae: 11.6740 - val_loss: 28.4761 - val_mae: 4.0402\n",
      "Epoch 133/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 293.3928 - mae: 11.3611 - val_loss: 25.9825 - val_mae: 3.6639\n",
      "Epoch 134/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 306.3098 - mae: 11.6936 - val_loss: 28.9255 - val_mae: 4.1651\n",
      "Epoch 135/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 295.2548 - mae: 11.6579 - val_loss: 23.8399 - val_mae: 3.4782\n",
      "Epoch 136/400\n",
      "122/122 [==============================] - 0s 699us/step - loss: 300.4953 - mae: 11.6705 - val_loss: 36.5896 - val_mae: 4.7358\n",
      "Epoch 137/400\n",
      "122/122 [==============================] - 0s 578us/step - loss: 308.3431 - mae: 11.6928 - val_loss: 30.0892 - val_mae: 3.8456\n",
      "Epoch 138/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 301.8927 - mae: 11.5526 - val_loss: 24.8590 - val_mae: 3.5054\n",
      "Epoch 139/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 285.8255 - mae: 11.2396 - val_loss: 33.0913 - val_mae: 4.5290\n",
      "Epoch 140/400\n",
      "122/122 [==============================] - 0s 574us/step - loss: 263.1099 - mae: 10.9679 - val_loss: 27.3217 - val_mae: 3.7160\n",
      "Epoch 141/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 264.6319 - mae: 10.9659 - val_loss: 29.7112 - val_mae: 4.0197\n",
      "Epoch 142/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 276.4511 - mae: 11.1839 - val_loss: 29.3224 - val_mae: 4.1136\n",
      "Epoch 143/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 263.1681 - mae: 10.9455 - val_loss: 23.1801 - val_mae: 3.4223\n",
      "Epoch 144/400\n",
      "122/122 [==============================] - 0s 580us/step - loss: 274.4759 - mae: 10.9970 - val_loss: 27.6146 - val_mae: 3.8126\n",
      "Epoch 145/400\n",
      "122/122 [==============================] - 0s 613us/step - loss: 279.9732 - mae: 11.2303 - val_loss: 28.5012 - val_mae: 3.8927\n",
      "Epoch 146/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 280.7945 - mae: 11.0953 - val_loss: 24.5173 - val_mae: 3.5084\n",
      "Epoch 147/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 281.8434 - mae: 11.1000 - val_loss: 32.1406 - val_mae: 4.3479\n",
      "Epoch 148/400\n",
      "122/122 [==============================] - 0s 689us/step - loss: 275.2538 - mae: 11.0445 - val_loss: 26.1775 - val_mae: 3.6419\n",
      "Epoch 149/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 287.4451 - mae: 11.3656 - val_loss: 31.7790 - val_mae: 4.3515\n",
      "Epoch 150/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 261.4877 - mae: 10.9085 - val_loss: 34.8503 - val_mae: 4.5111\n",
      "Epoch 151/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 281.9535 - mae: 11.2438 - val_loss: 23.4513 - val_mae: 3.4205\n",
      "Epoch 152/400\n",
      "122/122 [==============================] - 0s 605us/step - loss: 276.9463 - mae: 11.1500 - val_loss: 42.0780 - val_mae: 4.9221\n",
      "Epoch 153/400\n",
      "122/122 [==============================] - 0s 584us/step - loss: 256.6629 - mae: 10.8463 - val_loss: 25.5159 - val_mae: 3.6278\n",
      "Epoch 154/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 267.1012 - mae: 10.8998 - val_loss: 32.2602 - val_mae: 4.3011\n",
      "Epoch 155/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 249.0435 - mae: 10.7982 - val_loss: 23.2320 - val_mae: 3.3798\n",
      "Epoch 156/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 249.7977 - mae: 10.5712 - val_loss: 28.8721 - val_mae: 3.9677\n",
      "Epoch 157/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 251.3378 - mae: 10.7012 - val_loss: 27.8707 - val_mae: 3.7124\n",
      "Epoch 158/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 263.9785 - mae: 10.7622 - val_loss: 27.1449 - val_mae: 3.7614\n",
      "Epoch 159/400\n",
      "122/122 [==============================] - 0s 679us/step - loss: 246.7074 - mae: 10.5747 - val_loss: 23.6274 - val_mae: 3.4490\n",
      "Epoch 160/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 245.7502 - mae: 10.5671 - val_loss: 29.8661 - val_mae: 3.9428\n",
      "Epoch 161/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 242.5549 - mae: 10.4659 - val_loss: 39.1584 - val_mae: 4.8139\n",
      "Epoch 162/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 267.9913 - mae: 10.8091 - val_loss: 24.0083 - val_mae: 3.4685\n",
      "Epoch 163/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 245.7045 - mae: 10.5139 - val_loss: 30.6288 - val_mae: 4.0571\n",
      "Epoch 164/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 237.3556 - mae: 10.2896 - val_loss: 26.9199 - val_mae: 3.6827\n",
      "Epoch 165/400\n",
      "122/122 [==============================] - 0s 593us/step - loss: 241.6819 - mae: 10.2776 - val_loss: 24.3387 - val_mae: 3.5647\n",
      "Epoch 166/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 244.0261 - mae: 10.5419 - val_loss: 25.8400 - val_mae: 3.5917\n",
      "Epoch 167/400\n",
      "122/122 [==============================] - 0s 585us/step - loss: 244.5141 - mae: 10.4406 - val_loss: 28.3331 - val_mae: 3.8657\n",
      "Epoch 168/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 220.6455 - mae: 9.9406 - val_loss: 25.7564 - val_mae: 3.5897\n",
      "Epoch 169/400\n",
      "122/122 [==============================] - 0s 615us/step - loss: 223.5953 - mae: 10.0919 - val_loss: 26.1809 - val_mae: 3.7240\n",
      "Epoch 170/400\n",
      "122/122 [==============================] - 0s 675us/step - loss: 224.1675 - mae: 10.1623 - val_loss: 43.4077 - val_mae: 5.1391\n",
      "Epoch 171/400\n",
      "122/122 [==============================] - 0s 585us/step - loss: 219.3670 - mae: 9.9557 - val_loss: 32.4990 - val_mae: 4.3276\n",
      "Epoch 172/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 233.9693 - mae: 10.2300 - val_loss: 36.3181 - val_mae: 4.5814\n",
      "Epoch 173/400\n",
      "122/122 [==============================] - 0s 580us/step - loss: 210.7777 - mae: 9.7578 - val_loss: 33.6841 - val_mae: 4.3470\n",
      "Epoch 174/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 229.3569 - mae: 10.0157 - val_loss: 26.6034 - val_mae: 3.7169\n",
      "Epoch 175/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 229.8606 - mae: 10.2606 - val_loss: 25.0275 - val_mae: 3.5372\n",
      "Epoch 176/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 212.4255 - mae: 9.8233 - val_loss: 36.4611 - val_mae: 4.5195\n",
      "Epoch 177/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 215.9670 - mae: 10.0232 - val_loss: 35.0263 - val_mae: 4.4999\n",
      "Epoch 178/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 220.3593 - mae: 9.8573 - val_loss: 38.2116 - val_mae: 4.7218\n",
      "Epoch 179/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 217.8439 - mae: 9.8325 - val_loss: 65.2245 - val_mae: 6.2360\n",
      "Epoch 180/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 218.1539 - mae: 9.9308 - val_loss: 28.2839 - val_mae: 3.9551\n",
      "Epoch 181/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 191.7824 - mae: 9.3833 - val_loss: 24.7528 - val_mae: 3.6060\n",
      "Epoch 182/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 207.7511 - mae: 9.6478 - val_loss: 58.0200 - val_mae: 5.7088\n",
      "Epoch 183/400\n",
      "122/122 [==============================] - 0s 589us/step - loss: 209.5791 - mae: 9.6807 - val_loss: 25.5748 - val_mae: 3.7734\n",
      "Epoch 184/400\n",
      "122/122 [==============================] - 0s 653us/step - loss: 190.0199 - mae: 9.3426 - val_loss: 41.3607 - val_mae: 4.8935\n",
      "Epoch 185/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 201.1759 - mae: 9.6103 - val_loss: 33.0040 - val_mae: 4.3996\n",
      "Epoch 186/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 209.1448 - mae: 9.5344 - val_loss: 50.1370 - val_mae: 5.2780\n",
      "Epoch 187/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 199.5898 - mae: 9.4792 - val_loss: 47.5297 - val_mae: 5.1470\n",
      "Epoch 188/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 189.8899 - mae: 9.3350 - val_loss: 37.4755 - val_mae: 4.7195\n",
      "Epoch 189/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 191.5777 - mae: 9.2353 - val_loss: 43.9907 - val_mae: 5.0283\n",
      "Epoch 190/400\n",
      "122/122 [==============================] - 0s 586us/step - loss: 190.0717 - mae: 9.3047 - val_loss: 90.0539 - val_mae: 6.8205\n",
      "Epoch 191/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 195.8569 - mae: 9.2560 - val_loss: 37.9982 - val_mae: 4.7020\n",
      "Epoch 192/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 190.1323 - mae: 9.1885 - val_loss: 44.6175 - val_mae: 5.1296\n",
      "Epoch 193/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 181.1006 - mae: 9.1457 - val_loss: 36.7035 - val_mae: 4.6331\n",
      "Epoch 194/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 184.9204 - mae: 9.1168 - val_loss: 72.8071 - val_mae: 6.2635\n",
      "Epoch 195/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 188.2842 - mae: 9.1576 - val_loss: 42.8308 - val_mae: 4.9324\n",
      "Epoch 196/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 178.0107 - mae: 8.8723 - val_loss: 59.9556 - val_mae: 5.7551\n",
      "Epoch 197/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 164.8610 - mae: 8.6106 - val_loss: 31.7909 - val_mae: 4.3180\n",
      "Epoch 198/400\n",
      "122/122 [==============================] - 0s 682us/step - loss: 179.3245 - mae: 8.8868 - val_loss: 52.8120 - val_mae: 5.4841\n",
      "Epoch 199/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 187.8439 - mae: 9.0110 - val_loss: 110.8414 - val_mae: 7.3785\n",
      "Epoch 200/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 174.4402 - mae: 8.8725 - val_loss: 41.7282 - val_mae: 4.9794\n",
      "Epoch 201/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 171.6952 - mae: 8.8660 - val_loss: 53.9382 - val_mae: 5.5734\n",
      "Epoch 202/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 165.7346 - mae: 8.7028 - val_loss: 91.0945 - val_mae: 7.1902\n",
      "Epoch 203/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 170.8610 - mae: 8.6699 - val_loss: 86.2926 - val_mae: 6.7084\n",
      "Epoch 204/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 159.8743 - mae: 8.7164 - val_loss: 45.7526 - val_mae: 5.1365\n",
      "Epoch 205/400\n",
      "122/122 [==============================] - 0s 598us/step - loss: 149.2749 - mae: 8.3016 - val_loss: 60.2837 - val_mae: 5.8296\n",
      "Epoch 206/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 157.6176 - mae: 8.5256 - val_loss: 71.0681 - val_mae: 6.2076\n",
      "Epoch 207/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 156.5982 - mae: 8.4982 - val_loss: 126.3392 - val_mae: 7.9205\n",
      "Epoch 208/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 148.3238 - mae: 8.4042 - val_loss: 67.9498 - val_mae: 6.0474\n",
      "Epoch 209/400\n",
      "122/122 [==============================] - 0s 615us/step - loss: 151.8089 - mae: 8.3230 - val_loss: 58.3972 - val_mae: 5.7586\n",
      "Epoch 210/400\n",
      "122/122 [==============================] - 0s 611us/step - loss: 148.3287 - mae: 8.2064 - val_loss: 84.0090 - val_mae: 6.6141\n",
      "Epoch 211/400\n",
      "122/122 [==============================] - 0s 598us/step - loss: 158.8279 - mae: 8.4247 - val_loss: 76.3612 - val_mae: 6.3360\n",
      "Epoch 212/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 141.6761 - mae: 8.1274 - val_loss: 67.5645 - val_mae: 6.0664\n",
      "Epoch 213/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 160.6949 - mae: 8.4607 - val_loss: 100.4790 - val_mae: 7.1370\n",
      "Epoch 214/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 151.0875 - mae: 8.2803 - val_loss: 135.8371 - val_mae: 8.0665\n",
      "Epoch 215/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 144.6997 - mae: 8.1955 - val_loss: 93.9796 - val_mae: 6.8738\n",
      "Epoch 216/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 135.2577 - mae: 7.8492 - val_loss: 81.4990 - val_mae: 6.4487\n",
      "Epoch 217/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 137.4172 - mae: 7.9692 - val_loss: 92.8843 - val_mae: 7.0032\n",
      "Epoch 218/400\n",
      "122/122 [==============================] - 0s 610us/step - loss: 143.0146 - mae: 8.0775 - val_loss: 106.2498 - val_mae: 7.2260\n",
      "Epoch 219/400\n",
      "122/122 [==============================] - 0s 651us/step - loss: 135.0085 - mae: 7.8680 - val_loss: 120.6840 - val_mae: 7.6311\n",
      "Epoch 220/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 130.8273 - mae: 7.8094 - val_loss: 74.1755 - val_mae: 6.3794\n",
      "Epoch 221/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 127.6060 - mae: 7.7301 - val_loss: 114.4526 - val_mae: 7.4659\n",
      "Epoch 222/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 148.0452 - mae: 8.0862 - val_loss: 103.9930 - val_mae: 7.1839\n",
      "Epoch 223/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 147.4743 - mae: 7.9668 - val_loss: 130.5936 - val_mae: 7.8397\n",
      "Epoch 224/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 126.1344 - mae: 7.7572 - val_loss: 104.5381 - val_mae: 7.1986\n",
      "Epoch 225/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 142.3902 - mae: 7.9529 - val_loss: 96.9171 - val_mae: 7.0003\n",
      "Epoch 226/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 131.5811 - mae: 7.7593 - val_loss: 110.1698 - val_mae: 7.4854\n",
      "Epoch 227/400\n",
      "122/122 [==============================] - 0s 549us/step - loss: 118.0035 - mae: 7.4923 - val_loss: 155.6907 - val_mae: 8.3394\n",
      "Epoch 228/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 129.4192 - mae: 7.6838 - val_loss: 73.2148 - val_mae: 6.1979\n",
      "Epoch 229/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 127.3992 - mae: 7.7496 - val_loss: 203.3326 - val_mae: 9.2976\n",
      "Epoch 230/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 136.9493 - mae: 7.9798 - val_loss: 124.8188 - val_mae: 7.8664\n",
      "Epoch 231/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 122.7443 - mae: 7.6147 - val_loss: 146.2448 - val_mae: 8.2033\n",
      "Epoch 232/400\n",
      "122/122 [==============================] - 0s 667us/step - loss: 124.6702 - mae: 7.6006 - val_loss: 112.4182 - val_mae: 7.3768\n",
      "Epoch 233/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 118.9394 - mae: 7.5341 - val_loss: 175.7182 - val_mae: 8.9450\n",
      "Epoch 234/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 110.7925 - mae: 7.2253 - val_loss: 114.9223 - val_mae: 7.4798\n",
      "Epoch 235/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 123.7081 - mae: 7.4949 - val_loss: 107.1366 - val_mae: 7.1929\n",
      "Epoch 236/400\n",
      "122/122 [==============================] - 0s 548us/step - loss: 122.8406 - mae: 7.4931 - val_loss: 174.0751 - val_mae: 8.8092\n",
      "Epoch 237/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 120.1234 - mae: 7.4283 - val_loss: 179.8647 - val_mae: 8.8701\n",
      "Epoch 238/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 126.8060 - mae: 7.5897 - val_loss: 97.1917 - val_mae: 6.9932\n",
      "Epoch 239/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 124.4070 - mae: 7.5892 - val_loss: 99.4848 - val_mae: 7.1293\n",
      "Epoch 240/400\n",
      "122/122 [==============================] - 0s 582us/step - loss: 119.2839 - mae: 7.4231 - val_loss: 137.1946 - val_mae: 8.1372\n",
      "Epoch 241/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 117.0191 - mae: 7.3572 - val_loss: 85.8470 - val_mae: 6.6914\n",
      "Epoch 242/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 118.0208 - mae: 7.3088 - val_loss: 136.2118 - val_mae: 7.9782\n",
      "Epoch 243/400\n",
      "122/122 [==============================] - 0s 550us/step - loss: 108.3961 - mae: 7.1759 - val_loss: 124.9383 - val_mae: 7.7898\n",
      "Epoch 244/400\n",
      "122/122 [==============================] - 0s 665us/step - loss: 111.4562 - mae: 7.2148 - val_loss: 111.7852 - val_mae: 7.4444\n",
      "Epoch 245/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 112.5333 - mae: 7.2320 - val_loss: 127.9411 - val_mae: 7.7447\n",
      "Epoch 246/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 113.9695 - mae: 7.1495 - val_loss: 104.4792 - val_mae: 7.2883\n",
      "Epoch 247/400\n",
      "122/122 [==============================] - 0s 575us/step - loss: 114.9801 - mae: 7.2568 - val_loss: 129.9420 - val_mae: 7.7945\n",
      "Epoch 248/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 108.9213 - mae: 6.9651 - val_loss: 130.8729 - val_mae: 7.9024\n",
      "Epoch 249/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 104.6569 - mae: 6.9751 - val_loss: 93.0787 - val_mae: 6.9102\n",
      "Epoch 250/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 116.6022 - mae: 7.2924 - val_loss: 111.0540 - val_mae: 7.3017\n",
      "Epoch 251/400\n",
      "122/122 [==============================] - 0s 556us/step - loss: 115.2244 - mae: 7.2752 - val_loss: 150.2330 - val_mae: 8.2772\n",
      "Epoch 252/400\n",
      "122/122 [==============================] - 0s 550us/step - loss: 111.2261 - mae: 7.0877 - val_loss: 165.2571 - val_mae: 8.5281\n",
      "Epoch 253/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 103.1654 - mae: 6.9318 - val_loss: 115.2098 - val_mae: 7.4727\n",
      "Epoch 254/400\n",
      "122/122 [==============================] - 0s 553us/step - loss: 103.6163 - mae: 6.9000 - val_loss: 159.4666 - val_mae: 8.4778\n",
      "Epoch 255/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 109.2545 - mae: 7.1340 - val_loss: 185.8632 - val_mae: 9.1559\n",
      "Epoch 256/400\n",
      "122/122 [==============================] - 0s 658us/step - loss: 108.6527 - mae: 7.0713 - val_loss: 162.0508 - val_mae: 8.5986\n",
      "Epoch 257/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 109.7066 - mae: 7.1341 - val_loss: 198.1376 - val_mae: 9.3367\n",
      "Epoch 258/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 106.1983 - mae: 6.9758 - val_loss: 245.2130 - val_mae: 10.0679\n",
      "Epoch 259/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 96.1252 - mae: 6.7550 - val_loss: 198.0489 - val_mae: 9.2440\n",
      "Epoch 260/400\n",
      "122/122 [==============================] - 0s 552us/step - loss: 105.2029 - mae: 7.0395 - val_loss: 212.9828 - val_mae: 9.5322\n",
      "Epoch 261/400\n",
      "122/122 [==============================] - 0s 557us/step - loss: 98.8243 - mae: 6.7948 - val_loss: 144.3667 - val_mae: 8.1632\n",
      "Epoch 262/400\n",
      "122/122 [==============================] - 0s 580us/step - loss: 107.0629 - mae: 6.9765 - val_loss: 192.4830 - val_mae: 9.1110\n",
      "Epoch 263/400\n",
      "122/122 [==============================] - 0s 603us/step - loss: 106.7238 - mae: 6.8992 - val_loss: 162.2534 - val_mae: 8.4636\n",
      "Epoch 264/400\n",
      "122/122 [==============================] - 0s 574us/step - loss: 102.7299 - mae: 6.8852 - val_loss: 171.9373 - val_mae: 8.6644\n",
      "Epoch 265/400\n",
      "122/122 [==============================] - 0s 579us/step - loss: 105.0140 - mae: 6.9367 - val_loss: 130.5224 - val_mae: 7.8376\n",
      "Epoch 266/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 100.7010 - mae: 6.6795 - val_loss: 198.5136 - val_mae: 9.2617\n",
      "Epoch 267/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 102.8785 - mae: 6.8815 - val_loss: 131.0923 - val_mae: 7.8594\n",
      "Epoch 268/400\n",
      "122/122 [==============================] - 0s 579us/step - loss: 103.6333 - mae: 6.9464 - val_loss: 158.8671 - val_mae: 8.5311\n",
      "Epoch 269/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 95.8224 - mae: 6.7310 - val_loss: 134.7832 - val_mae: 8.0181\n",
      "Epoch 270/400\n",
      "122/122 [==============================] - 0s 716us/step - loss: 100.6049 - mae: 6.8783 - val_loss: 147.7808 - val_mae: 8.3016\n",
      "Epoch 271/400\n",
      "122/122 [==============================] - 0s 802us/step - loss: 107.1539 - mae: 7.0240 - val_loss: 175.7273 - val_mae: 8.8952\n",
      "Epoch 272/400\n",
      "122/122 [==============================] - 0s 627us/step - loss: 108.8867 - mae: 6.8739 - val_loss: 170.4176 - val_mae: 8.7873\n",
      "Epoch 273/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 102.5468 - mae: 6.9617 - val_loss: 133.7289 - val_mae: 7.9707\n",
      "Epoch 274/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 93.6973 - mae: 6.6535 - val_loss: 215.9005 - val_mae: 9.5432\n",
      "Epoch 275/400\n",
      "122/122 [==============================] - 0s 628us/step - loss: 96.8124 - mae: 6.7453 - val_loss: 160.4696 - val_mae: 8.5918\n",
      "Epoch 276/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 93.1856 - mae: 6.7217 - val_loss: 131.8476 - val_mae: 7.8793\n",
      "Epoch 277/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 95.1223 - mae: 6.7281 - val_loss: 115.1978 - val_mae: 7.5738\n",
      "Epoch 278/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 97.1439 - mae: 6.7852 - val_loss: 110.1849 - val_mae: 7.3433\n",
      "Epoch 279/400\n",
      "122/122 [==============================] - 0s 606us/step - loss: 89.5202 - mae: 6.5789 - val_loss: 148.5779 - val_mae: 8.2137\n",
      "Epoch 280/400\n",
      "122/122 [==============================] - 0s 604us/step - loss: 89.6603 - mae: 6.4874 - val_loss: 147.8984 - val_mae: 8.2753\n",
      "Epoch 281/400\n",
      "122/122 [==============================] - 0s 620us/step - loss: 93.1213 - mae: 6.5447 - val_loss: 198.6298 - val_mae: 9.3953\n",
      "Epoch 282/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 91.7261 - mae: 6.5636 - val_loss: 160.7496 - val_mae: 8.4487\n",
      "Epoch 283/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 88.7170 - mae: 6.6107 - val_loss: 234.8380 - val_mae: 9.8418\n",
      "Epoch 284/400\n",
      "122/122 [==============================] - 0s 617us/step - loss: 91.1087 - mae: 6.5219 - val_loss: 184.7134 - val_mae: 8.9224\n",
      "Epoch 285/400\n",
      "122/122 [==============================] - 0s 674us/step - loss: 93.5176 - mae: 6.4496 - val_loss: 216.1730 - val_mae: 9.5778\n",
      "Epoch 286/400\n",
      "122/122 [==============================] - 0s 700us/step - loss: 100.6117 - mae: 6.8080 - val_loss: 170.0525 - val_mae: 8.6593\n",
      "Epoch 287/400\n",
      "122/122 [==============================] - 0s 613us/step - loss: 97.1340 - mae: 6.6296 - val_loss: 174.6204 - val_mae: 8.7214\n",
      "Epoch 288/400\n",
      "122/122 [==============================] - 0s 584us/step - loss: 89.5117 - mae: 6.4602 - val_loss: 146.7700 - val_mae: 8.1931\n",
      "Epoch 289/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 109.5308 - mae: 7.0273 - val_loss: 196.0376 - val_mae: 9.2805\n",
      "Epoch 290/400\n",
      "122/122 [==============================] - 0s 654us/step - loss: 89.0393 - mae: 6.5010 - val_loss: 212.4864 - val_mae: 9.5577\n",
      "Epoch 291/400\n",
      "122/122 [==============================] - 0s 620us/step - loss: 82.9812 - mae: 6.3206 - val_loss: 200.7065 - val_mae: 9.3424\n",
      "Epoch 292/400\n",
      "122/122 [==============================] - 0s 684us/step - loss: 85.6010 - mae: 6.3507 - val_loss: 112.7874 - val_mae: 7.4221\n",
      "Epoch 293/400\n",
      "122/122 [==============================] - 0s 653us/step - loss: 95.9961 - mae: 6.6159 - val_loss: 229.6799 - val_mae: 9.8405\n",
      "Epoch 294/400\n",
      "122/122 [==============================] - 0s 637us/step - loss: 94.3707 - mae: 6.6897 - val_loss: 158.8750 - val_mae: 8.5288\n",
      "Epoch 295/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 83.3449 - mae: 6.3262 - val_loss: 229.5848 - val_mae: 9.7875\n",
      "Epoch 296/400\n",
      "122/122 [==============================] - 0s 615us/step - loss: 91.2034 - mae: 6.5735 - val_loss: 163.8928 - val_mae: 8.8071\n",
      "Epoch 297/400\n",
      "122/122 [==============================] - 0s 869us/step - loss: 83.2944 - mae: 6.4573 - val_loss: 135.3264 - val_mae: 8.1011\n",
      "Epoch 298/400\n",
      "122/122 [==============================] - 0s 772us/step - loss: 91.3101 - mae: 6.5154 - val_loss: 179.0684 - val_mae: 8.8975\n",
      "Epoch 299/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 85.8725 - mae: 6.4021 - val_loss: 246.9965 - val_mae: 10.0463\n",
      "Epoch 300/400\n",
      "122/122 [==============================] - 0s 593us/step - loss: 91.8523 - mae: 6.4904 - val_loss: 189.4414 - val_mae: 9.1482\n",
      "Epoch 301/400\n",
      "122/122 [==============================] - 0s 623us/step - loss: 81.3722 - mae: 6.2826 - val_loss: 168.6095 - val_mae: 8.5848\n",
      "Epoch 302/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 98.2501 - mae: 6.6180 - val_loss: 159.2307 - val_mae: 8.4411\n",
      "Epoch 303/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 88.3111 - mae: 6.4133 - val_loss: 210.3366 - val_mae: 9.3801\n",
      "Epoch 304/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 81.5491 - mae: 6.3039 - val_loss: 103.1530 - val_mae: 7.3831\n",
      "Epoch 305/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 95.2561 - mae: 6.6283 - val_loss: 199.9626 - val_mae: 9.2098\n",
      "Epoch 306/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 92.2015 - mae: 6.5139 - val_loss: 247.1823 - val_mae: 10.1269\n",
      "Epoch 307/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 82.9497 - mae: 6.3055 - val_loss: 233.0100 - val_mae: 9.8791\n",
      "Epoch 308/400\n",
      "122/122 [==============================] - 0s 688us/step - loss: 85.5210 - mae: 6.3460 - val_loss: 173.9464 - val_mae: 8.7309\n",
      "Epoch 309/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 87.4025 - mae: 6.3979 - val_loss: 145.1353 - val_mae: 8.0951\n",
      "Epoch 310/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 89.4457 - mae: 6.4430 - val_loss: 207.3101 - val_mae: 9.3358\n",
      "Epoch 311/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 84.4772 - mae: 6.3835 - val_loss: 223.0081 - val_mae: 9.6386\n",
      "Epoch 312/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 83.9699 - mae: 6.2825 - val_loss: 244.5053 - val_mae: 9.9817\n",
      "Epoch 313/400\n",
      "122/122 [==============================] - 0s 555us/step - loss: 85.8951 - mae: 6.3497 - val_loss: 174.4282 - val_mae: 8.7903\n",
      "Epoch 314/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 89.1897 - mae: 6.5003 - val_loss: 206.8042 - val_mae: 9.3300\n",
      "Epoch 315/400\n",
      "122/122 [==============================] - 0s 603us/step - loss: 81.8762 - mae: 6.2065 - val_loss: 198.8400 - val_mae: 9.2801\n",
      "Epoch 316/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 84.0432 - mae: 6.3975 - val_loss: 239.2917 - val_mae: 9.8686\n",
      "Epoch 317/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 92.2191 - mae: 6.3802 - val_loss: 190.1269 - val_mae: 9.1223\n",
      "Epoch 318/400\n",
      "122/122 [==============================] - 0s 669us/step - loss: 87.2117 - mae: 6.3043 - val_loss: 161.4209 - val_mae: 8.5382\n",
      "Epoch 319/400\n",
      "122/122 [==============================] - 0s 577us/step - loss: 81.5371 - mae: 6.1663 - val_loss: 164.8024 - val_mae: 8.5980\n",
      "Epoch 320/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 87.4098 - mae: 6.3760 - val_loss: 231.2455 - val_mae: 9.7208\n",
      "Epoch 321/400\n",
      "122/122 [==============================] - 0s 580us/step - loss: 87.5325 - mae: 6.2998 - val_loss: 193.4468 - val_mae: 9.0724\n",
      "Epoch 322/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 82.1591 - mae: 6.2628 - val_loss: 210.4143 - val_mae: 9.4841\n",
      "Epoch 323/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 82.4536 - mae: 6.1788 - val_loss: 163.7491 - val_mae: 8.7112\n",
      "Epoch 324/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 92.3044 - mae: 6.4287 - val_loss: 144.4716 - val_mae: 8.2180\n",
      "Epoch 325/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 76.4037 - mae: 6.0842 - val_loss: 168.0303 - val_mae: 8.6303\n",
      "Epoch 326/400\n",
      "122/122 [==============================] - 0s 659us/step - loss: 91.1324 - mae: 6.3315 - val_loss: 152.0566 - val_mae: 8.3412\n",
      "Epoch 327/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 81.6766 - mae: 6.1743 - val_loss: 200.2364 - val_mae: 9.2187\n",
      "Epoch 328/400\n",
      "122/122 [==============================] - 0s 683us/step - loss: 77.4928 - mae: 6.1430 - val_loss: 204.1192 - val_mae: 9.3046\n",
      "Epoch 329/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 78.3698 - mae: 6.1754 - val_loss: 230.7412 - val_mae: 9.6603\n",
      "Epoch 330/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 82.8025 - mae: 6.2104 - val_loss: 181.1783 - val_mae: 8.8391\n",
      "Epoch 331/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 82.4271 - mae: 6.2668 - val_loss: 172.2091 - val_mae: 8.7484\n",
      "Epoch 332/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 82.6604 - mae: 6.2743 - val_loss: 232.7503 - val_mae: 9.7715\n",
      "Epoch 333/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 84.3721 - mae: 6.2295 - val_loss: 157.3386 - val_mae: 8.5084\n",
      "Epoch 334/400\n",
      "122/122 [==============================] - 0s 588us/step - loss: 85.6817 - mae: 6.2387 - val_loss: 196.1110 - val_mae: 9.2283\n",
      "Epoch 335/400\n",
      "122/122 [==============================] - 0s 604us/step - loss: 77.8697 - mae: 6.0876 - val_loss: 177.7028 - val_mae: 8.8504\n",
      "Epoch 336/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 82.5756 - mae: 6.1669 - val_loss: 178.6631 - val_mae: 8.8268\n",
      "Epoch 337/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 76.4182 - mae: 6.0502 - val_loss: 195.4603 - val_mae: 9.1431\n",
      "Epoch 338/400\n",
      "122/122 [==============================] - 0s 667us/step - loss: 77.0102 - mae: 6.0566 - val_loss: 119.4111 - val_mae: 7.4941\n",
      "Epoch 339/400\n",
      "122/122 [==============================] - 0s 585us/step - loss: 84.4617 - mae: 6.2859 - val_loss: 278.5291 - val_mae: 10.6455\n",
      "Epoch 340/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 82.1565 - mae: 6.1762 - val_loss: 169.6640 - val_mae: 8.7478\n",
      "Epoch 341/400\n",
      "122/122 [==============================] - 0s 579us/step - loss: 76.4693 - mae: 5.9814 - val_loss: 160.3923 - val_mae: 8.5605\n",
      "Epoch 342/400\n",
      "122/122 [==============================] - 0s 604us/step - loss: 80.1148 - mae: 6.0786 - val_loss: 211.7897 - val_mae: 9.3825\n",
      "Epoch 343/400\n",
      "122/122 [==============================] - 0s 581us/step - loss: 77.5444 - mae: 6.1314 - val_loss: 122.8914 - val_mae: 7.8612\n",
      "Epoch 344/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 86.1380 - mae: 6.3373 - val_loss: 176.1021 - val_mae: 8.8039\n",
      "Epoch 345/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 77.1206 - mae: 5.9961 - val_loss: 187.9534 - val_mae: 8.9844\n",
      "Epoch 346/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 80.6840 - mae: 6.1527 - val_loss: 201.1235 - val_mae: 9.2462\n",
      "Epoch 347/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 76.6105 - mae: 5.9690 - val_loss: 169.1727 - val_mae: 8.7065\n",
      "Epoch 348/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 85.8797 - mae: 6.2440 - val_loss: 163.1881 - val_mae: 8.5260\n",
      "Epoch 349/400\n",
      "122/122 [==============================] - 0s 608us/step - loss: 78.6217 - mae: 6.0766 - val_loss: 226.6483 - val_mae: 9.7295\n",
      "Epoch 350/400\n",
      "122/122 [==============================] - 0s 574us/step - loss: 75.9774 - mae: 5.9887 - val_loss: 158.1560 - val_mae: 8.4526\n",
      "Epoch 351/400\n",
      "122/122 [==============================] - 0s 654us/step - loss: 76.5287 - mae: 6.0340 - val_loss: 229.1665 - val_mae: 9.7895\n",
      "Epoch 352/400\n",
      "122/122 [==============================] - 0s 576us/step - loss: 69.0161 - mae: 5.7839 - val_loss: 188.6143 - val_mae: 9.0871\n",
      "Epoch 353/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 77.8666 - mae: 5.9987 - val_loss: 141.5498 - val_mae: 8.1254\n",
      "Epoch 354/400\n",
      "122/122 [==============================] - 0s 570us/step - loss: 76.7219 - mae: 5.9710 - val_loss: 128.5708 - val_mae: 7.8284\n",
      "Epoch 355/400\n",
      "122/122 [==============================] - 0s 608us/step - loss: 79.2795 - mae: 6.0181 - val_loss: 180.6796 - val_mae: 9.0152\n",
      "Epoch 356/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 72.3895 - mae: 5.9368 - val_loss: 185.3301 - val_mae: 8.9746\n",
      "Epoch 357/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 76.9048 - mae: 6.0522 - val_loss: 324.9254 - val_mae: 11.1530\n",
      "Epoch 358/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 76.8464 - mae: 6.0842 - val_loss: 199.2521 - val_mae: 9.2428\n",
      "Epoch 359/400\n",
      "122/122 [==============================] - 0s 558us/step - loss: 79.8185 - mae: 6.1529 - val_loss: 192.5889 - val_mae: 9.0559\n",
      "Epoch 360/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 77.3390 - mae: 5.9988 - val_loss: 207.7434 - val_mae: 9.3671\n",
      "Epoch 361/400\n",
      "122/122 [==============================] - 0s 675us/step - loss: 72.9300 - mae: 5.8304 - val_loss: 161.8023 - val_mae: 8.5351\n",
      "Epoch 362/400\n",
      "122/122 [==============================] - 0s 585us/step - loss: 73.1622 - mae: 5.8755 - val_loss: 169.7627 - val_mae: 8.9034\n",
      "Epoch 363/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 81.7559 - mae: 6.0778 - val_loss: 172.2786 - val_mae: 8.7882\n",
      "Epoch 364/400\n",
      "122/122 [==============================] - 0s 574us/step - loss: 77.5320 - mae: 6.0118 - val_loss: 240.5170 - val_mae: 10.0061\n",
      "Epoch 365/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 81.2319 - mae: 6.0123 - val_loss: 167.0968 - val_mae: 8.6403\n",
      "Epoch 366/400\n",
      "122/122 [==============================] - 0s 565us/step - loss: 71.5804 - mae: 5.8918 - val_loss: 105.2979 - val_mae: 7.3035\n",
      "Epoch 367/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 72.0980 - mae: 5.9237 - val_loss: 153.8342 - val_mae: 8.3132\n",
      "Epoch 368/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 69.1250 - mae: 5.8078 - val_loss: 195.9686 - val_mae: 9.2638\n",
      "Epoch 369/400\n",
      "122/122 [==============================] - 0s 610us/step - loss: 71.5772 - mae: 5.8582 - val_loss: 180.5946 - val_mae: 9.0537\n",
      "Epoch 370/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 81.8780 - mae: 6.0985 - val_loss: 230.5156 - val_mae: 9.7904\n",
      "Epoch 371/400\n",
      "122/122 [==============================] - 0s 664us/step - loss: 72.6031 - mae: 5.8975 - val_loss: 205.0558 - val_mae: 9.3436\n",
      "Epoch 372/400\n",
      "122/122 [==============================] - 0s 583us/step - loss: 77.2447 - mae: 5.9769 - val_loss: 228.7250 - val_mae: 9.8250\n",
      "Epoch 373/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 73.6370 - mae: 5.8533 - val_loss: 139.8293 - val_mae: 8.1124\n",
      "Epoch 374/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 75.0653 - mae: 5.9482 - val_loss: 220.2341 - val_mae: 9.6456\n",
      "Epoch 375/400\n",
      "122/122 [==============================] - 0s 578us/step - loss: 74.2374 - mae: 5.8415 - val_loss: 174.5594 - val_mae: 8.7299\n",
      "Epoch 376/400\n",
      "122/122 [==============================] - 0s 590us/step - loss: 71.8809 - mae: 5.8964 - val_loss: 95.1700 - val_mae: 6.9399\n",
      "Epoch 377/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 78.7250 - mae: 6.0269 - val_loss: 175.2964 - val_mae: 8.8118\n",
      "Epoch 378/400\n",
      "122/122 [==============================] - 0s 560us/step - loss: 75.9109 - mae: 5.8730 - val_loss: 182.3728 - val_mae: 8.8832\n",
      "Epoch 379/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 72.1287 - mae: 5.7127 - val_loss: 231.8259 - val_mae: 9.7141\n",
      "Epoch 380/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 75.9977 - mae: 5.8643 - val_loss: 117.2936 - val_mae: 7.5145\n",
      "Epoch 381/400\n",
      "122/122 [==============================] - 0s 667us/step - loss: 74.3076 - mae: 5.8900 - val_loss: 230.1924 - val_mae: 9.8410\n",
      "Epoch 382/400\n",
      "122/122 [==============================] - 0s 571us/step - loss: 62.6914 - mae: 5.5300 - val_loss: 179.0104 - val_mae: 8.8285\n",
      "Epoch 383/400\n",
      "122/122 [==============================] - 0s 590us/step - loss: 71.6493 - mae: 5.7915 - val_loss: 185.9329 - val_mae: 8.9233\n",
      "Epoch 384/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 74.5805 - mae: 5.8730 - val_loss: 127.2087 - val_mae: 7.8251\n",
      "Epoch 385/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 68.0126 - mae: 5.7382 - val_loss: 168.0561 - val_mae: 8.6195\n",
      "Epoch 386/400\n",
      "122/122 [==============================] - 0s 568us/step - loss: 72.1546 - mae: 5.8694 - val_loss: 199.4358 - val_mae: 9.3001\n",
      "Epoch 387/400\n",
      "122/122 [==============================] - 0s 563us/step - loss: 88.5430 - mae: 5.9815 - val_loss: 159.1822 - val_mae: 8.5525\n",
      "Epoch 388/400\n",
      "122/122 [==============================] - 0s 554us/step - loss: 74.0048 - mae: 5.8519 - val_loss: 185.9095 - val_mae: 8.9420\n",
      "Epoch 389/400\n",
      "122/122 [==============================] - 0s 569us/step - loss: 79.0041 - mae: 6.0235 - val_loss: 164.2448 - val_mae: 8.5104\n",
      "Epoch 390/400\n",
      "122/122 [==============================] - 0s 562us/step - loss: 71.8903 - mae: 5.7903 - val_loss: 121.2097 - val_mae: 7.5636\n",
      "Epoch 391/400\n",
      "122/122 [==============================] - 0s 685us/step - loss: 70.7319 - mae: 5.7089 - val_loss: 230.3311 - val_mae: 9.7623\n",
      "Epoch 392/400\n",
      "122/122 [==============================] - 0s 572us/step - loss: 70.4475 - mae: 5.7907 - val_loss: 189.7655 - val_mae: 8.9969\n",
      "Epoch 393/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 68.5083 - mae: 5.6935 - val_loss: 135.1794 - val_mae: 7.8338\n",
      "Epoch 394/400\n",
      "122/122 [==============================] - 0s 566us/step - loss: 71.5669 - mae: 5.7551 - val_loss: 171.5917 - val_mae: 8.6625\n",
      "Epoch 395/400\n",
      "122/122 [==============================] - 0s 559us/step - loss: 71.6509 - mae: 5.8839 - val_loss: 150.4149 - val_mae: 8.2458\n",
      "Epoch 396/400\n",
      "122/122 [==============================] - 0s 561us/step - loss: 69.5682 - mae: 5.8512 - val_loss: 200.1455 - val_mae: 9.2340\n",
      "Epoch 397/400\n",
      "122/122 [==============================] - 0s 567us/step - loss: 69.6963 - mae: 5.8508 - val_loss: 184.5594 - val_mae: 9.0601\n",
      "Epoch 398/400\n",
      "122/122 [==============================] - 0s 599us/step - loss: 68.7594 - mae: 5.6933 - val_loss: 161.9116 - val_mae: 8.6240\n",
      "Epoch 399/400\n",
      "122/122 [==============================] - 0s 564us/step - loss: 74.2793 - mae: 5.9374 - val_loss: 219.2424 - val_mae: 9.5384\n",
      "Epoch 400/400\n",
      "122/122 [==============================] - 0s 573us/step - loss: 80.3132 - mae: 5.9751 - val_loss: 199.4578 - val_mae: 9.2232\n",
      "31/31 [==============================] - 0s 361us/step\n",
      "Epochs: 400 | MAE: 9.223212161646135\n",
      "Training model with 450 epochs\n",
      "Epoch 1/450\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 66741.7969 - mae: 223.5440 - val_loss: 55575.5117 - val_mae: 203.3120\n",
      "Epoch 2/450\n",
      "122/122 [==============================] - 0s 575us/step - loss: 33198.7734 - mae: 151.3982 - val_loss: 9645.6055 - val_mae: 83.6642\n",
      "Epoch 3/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 5098.5562 - mae: 55.2965 - val_loss: 2757.0449 - val_mae: 39.4672\n",
      "Epoch 4/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 2916.8044 - mae: 40.9461 - val_loss: 1903.7328 - val_mae: 32.9090\n",
      "Epoch 5/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 2466.7341 - mae: 37.0091 - val_loss: 1601.3586 - val_mae: 30.1322\n",
      "Epoch 6/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 2127.7715 - mae: 35.2990 - val_loss: 1330.5879 - val_mae: 27.3859\n",
      "Epoch 7/450\n",
      "122/122 [==============================] - 0s 585us/step - loss: 1955.0463 - mae: 34.1271 - val_loss: 1191.0341 - val_mae: 26.0070\n",
      "Epoch 8/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 1754.8757 - mae: 32.4785 - val_loss: 1091.1720 - val_mae: 24.8370\n",
      "Epoch 9/450\n",
      "122/122 [==============================] - 0s 612us/step - loss: 1593.8804 - mae: 31.2683 - val_loss: 961.8180 - val_mae: 23.3576\n",
      "Epoch 10/450\n",
      "122/122 [==============================] - 0s 601us/step - loss: 1527.8712 - mae: 30.4791 - val_loss: 869.2076 - val_mae: 22.2464\n",
      "Epoch 11/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 1507.3341 - mae: 30.3188 - val_loss: 833.6253 - val_mae: 22.1785\n",
      "Epoch 12/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 1357.1151 - mae: 28.7897 - val_loss: 737.0673 - val_mae: 20.5231\n",
      "Epoch 13/450\n",
      "122/122 [==============================] - 0s 549us/step - loss: 1351.2085 - mae: 28.6427 - val_loss: 678.2725 - val_mae: 19.7352\n",
      "Epoch 14/450\n",
      "122/122 [==============================] - 0s 619us/step - loss: 1187.7549 - mae: 26.9783 - val_loss: 607.4587 - val_mae: 18.6364\n",
      "Epoch 15/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 1137.4681 - mae: 26.3183 - val_loss: 542.5080 - val_mae: 17.6314\n",
      "Epoch 16/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 1100.2388 - mae: 25.9532 - val_loss: 504.9047 - val_mae: 17.1177\n",
      "Epoch 17/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 1051.5388 - mae: 25.2222 - val_loss: 457.6149 - val_mae: 16.1569\n",
      "Epoch 18/450\n",
      "122/122 [==============================] - 0s 662us/step - loss: 990.6944 - mae: 24.5464 - val_loss: 400.2797 - val_mae: 14.9351\n",
      "Epoch 19/450\n",
      "122/122 [==============================] - 0s 601us/step - loss: 970.9464 - mae: 24.3943 - val_loss: 375.8725 - val_mae: 14.7096\n",
      "Epoch 20/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 928.9573 - mae: 23.6355 - val_loss: 329.7583 - val_mae: 13.3861\n",
      "Epoch 21/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 874.3464 - mae: 22.8893 - val_loss: 320.6613 - val_mae: 13.2561\n",
      "Epoch 22/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 858.8961 - mae: 22.3995 - val_loss: 291.9521 - val_mae: 12.6264\n",
      "Epoch 23/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 843.3028 - mae: 22.3321 - val_loss: 244.0070 - val_mae: 11.5341\n",
      "Epoch 24/450\n",
      "122/122 [==============================] - 0s 577us/step - loss: 808.4345 - mae: 21.8554 - val_loss: 230.7933 - val_mae: 11.0366\n",
      "Epoch 25/450\n",
      "122/122 [==============================] - 0s 606us/step - loss: 761.1328 - mae: 21.2864 - val_loss: 197.4709 - val_mae: 10.2672\n",
      "Epoch 26/450\n",
      "122/122 [==============================] - 0s 605us/step - loss: 730.8922 - mae: 20.6838 - val_loss: 185.3771 - val_mae: 9.9412\n",
      "Epoch 27/450\n",
      "122/122 [==============================] - 0s 600us/step - loss: 738.6692 - mae: 20.6707 - val_loss: 167.5383 - val_mae: 9.4040\n",
      "Epoch 28/450\n",
      "122/122 [==============================] - 0s 572us/step - loss: 718.4904 - mae: 20.5563 - val_loss: 156.0869 - val_mae: 9.1609\n",
      "Epoch 29/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 696.3481 - mae: 19.9765 - val_loss: 147.7337 - val_mae: 8.9238\n",
      "Epoch 30/450\n",
      "122/122 [==============================] - 0s 587us/step - loss: 673.0123 - mae: 19.7048 - val_loss: 126.0013 - val_mae: 8.5300\n",
      "Epoch 31/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 681.6296 - mae: 19.6488 - val_loss: 145.9283 - val_mae: 8.9682\n",
      "Epoch 32/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 653.4145 - mae: 19.0285 - val_loss: 118.2975 - val_mae: 7.9687\n",
      "Epoch 33/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 612.6191 - mae: 18.4995 - val_loss: 97.4436 - val_mae: 7.7039\n",
      "Epoch 34/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 642.9121 - mae: 18.8049 - val_loss: 95.1184 - val_mae: 7.2001\n",
      "Epoch 35/450\n",
      "122/122 [==============================] - 0s 658us/step - loss: 615.6638 - mae: 18.4647 - val_loss: 82.0417 - val_mae: 6.6755\n",
      "Epoch 36/450\n",
      "122/122 [==============================] - 0s 607us/step - loss: 608.9603 - mae: 18.0204 - val_loss: 70.6326 - val_mae: 6.4588\n",
      "Epoch 37/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 584.0283 - mae: 17.9959 - val_loss: 67.8380 - val_mae: 6.2788\n",
      "Epoch 38/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 578.5103 - mae: 17.6295 - val_loss: 83.6959 - val_mae: 6.8584\n",
      "Epoch 39/450\n",
      "122/122 [==============================] - 0s 553us/step - loss: 560.6598 - mae: 17.2747 - val_loss: 66.1026 - val_mae: 6.3156\n",
      "Epoch 40/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 561.4727 - mae: 17.4423 - val_loss: 63.2416 - val_mae: 5.8568\n",
      "Epoch 41/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 523.3876 - mae: 16.7149 - val_loss: 48.0641 - val_mae: 5.2181\n",
      "Epoch 42/450\n",
      "122/122 [==============================] - 0s 577us/step - loss: 549.3156 - mae: 17.0208 - val_loss: 43.7052 - val_mae: 4.8892\n",
      "Epoch 43/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 564.4395 - mae: 16.9682 - val_loss: 43.3839 - val_mae: 5.1308\n",
      "Epoch 44/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 532.4703 - mae: 16.6769 - val_loss: 46.0352 - val_mae: 5.0961\n",
      "Epoch 45/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 550.5828 - mae: 16.9018 - val_loss: 62.8810 - val_mae: 5.9842\n",
      "Epoch 46/450\n",
      "122/122 [==============================] - 0s 605us/step - loss: 524.3473 - mae: 16.3838 - val_loss: 39.0024 - val_mae: 4.7637\n",
      "Epoch 47/450\n",
      "122/122 [==============================] - 0s 610us/step - loss: 549.3406 - mae: 16.7765 - val_loss: 68.9653 - val_mae: 6.5106\n",
      "Epoch 48/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 517.4589 - mae: 16.2498 - val_loss: 45.6591 - val_mae: 4.9710\n",
      "Epoch 49/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 489.4676 - mae: 15.8404 - val_loss: 43.2737 - val_mae: 4.8693\n",
      "Epoch 50/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 491.8047 - mae: 15.8811 - val_loss: 37.6458 - val_mae: 4.4020\n",
      "Epoch 51/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 488.4332 - mae: 15.7198 - val_loss: 31.3841 - val_mae: 4.1516\n",
      "Epoch 52/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 461.6752 - mae: 15.3322 - val_loss: 42.5747 - val_mae: 4.6176\n",
      "Epoch 53/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 474.9020 - mae: 15.5387 - val_loss: 37.3334 - val_mae: 4.5569\n",
      "Epoch 54/450\n",
      "122/122 [==============================] - 0s 585us/step - loss: 469.0328 - mae: 15.2237 - val_loss: 34.6402 - val_mae: 4.3260\n",
      "Epoch 55/450\n",
      "122/122 [==============================] - 0s 589us/step - loss: 423.7739 - mae: 14.5636 - val_loss: 29.4995 - val_mae: 3.9136\n",
      "Epoch 56/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 484.5194 - mae: 15.4773 - val_loss: 50.7498 - val_mae: 5.4161\n",
      "Epoch 57/450\n",
      "122/122 [==============================] - 0s 624us/step - loss: 480.2373 - mae: 15.4152 - val_loss: 49.2433 - val_mae: 5.5583\n",
      "Epoch 58/450\n",
      "122/122 [==============================] - 0s 581us/step - loss: 455.1838 - mae: 14.9878 - val_loss: 37.7833 - val_mae: 4.7986\n",
      "Epoch 59/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 452.4546 - mae: 14.9185 - val_loss: 28.3557 - val_mae: 3.9197\n",
      "Epoch 60/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 436.2807 - mae: 14.8185 - val_loss: 42.1603 - val_mae: 4.8451\n",
      "Epoch 61/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 451.4206 - mae: 14.7997 - val_loss: 41.8140 - val_mae: 4.5321\n",
      "Epoch 62/450\n",
      "122/122 [==============================] - 0s 611us/step - loss: 450.4160 - mae: 14.7597 - val_loss: 30.6590 - val_mae: 4.1205\n",
      "Epoch 63/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 426.8676 - mae: 14.5911 - val_loss: 27.3506 - val_mae: 3.7382\n",
      "Epoch 64/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 419.5106 - mae: 14.3392 - val_loss: 33.0772 - val_mae: 4.3904\n",
      "Epoch 65/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 456.0395 - mae: 14.7497 - val_loss: 45.5593 - val_mae: 5.3507\n",
      "Epoch 66/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 425.5966 - mae: 14.3896 - val_loss: 25.6394 - val_mae: 3.6674\n",
      "Epoch 67/450\n",
      "122/122 [==============================] - 0s 698us/step - loss: 446.5208 - mae: 14.6432 - val_loss: 26.8345 - val_mae: 3.7213\n",
      "Epoch 68/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 420.2643 - mae: 14.2389 - val_loss: 27.3358 - val_mae: 3.8366\n",
      "Epoch 69/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 419.2632 - mae: 14.2708 - val_loss: 64.9581 - val_mae: 6.2370\n",
      "Epoch 70/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 408.2724 - mae: 14.0855 - val_loss: 30.5519 - val_mae: 3.8634\n",
      "Epoch 71/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 411.8488 - mae: 14.2924 - val_loss: 31.2361 - val_mae: 4.1956\n",
      "Epoch 72/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 416.3922 - mae: 14.1029 - val_loss: 30.0728 - val_mae: 4.0108\n",
      "Epoch 73/450\n",
      "122/122 [==============================] - 0s 586us/step - loss: 396.5995 - mae: 13.7989 - val_loss: 31.9442 - val_mae: 4.0864\n",
      "Epoch 74/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 415.0862 - mae: 13.9472 - val_loss: 27.2787 - val_mae: 3.7463\n",
      "Epoch 75/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 368.5779 - mae: 13.4834 - val_loss: 28.8875 - val_mae: 3.7840\n",
      "Epoch 76/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 405.2874 - mae: 13.8447 - val_loss: 26.3333 - val_mae: 3.6948\n",
      "Epoch 77/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 391.2376 - mae: 13.5131 - val_loss: 25.2626 - val_mae: 3.6205\n",
      "Epoch 78/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 376.8860 - mae: 13.3929 - val_loss: 32.1802 - val_mae: 4.3528\n",
      "Epoch 79/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 376.1456 - mae: 13.3346 - val_loss: 28.6967 - val_mae: 3.8708\n",
      "Epoch 80/450\n",
      "122/122 [==============================] - 0s 658us/step - loss: 387.7631 - mae: 13.5333 - val_loss: 24.9904 - val_mae: 3.6328\n",
      "Epoch 81/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 371.9879 - mae: 13.4077 - val_loss: 34.4153 - val_mae: 4.3271\n",
      "Epoch 82/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 378.8343 - mae: 13.3403 - val_loss: 33.9735 - val_mae: 4.4696\n",
      "Epoch 83/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 363.8892 - mae: 13.1952 - val_loss: 32.0018 - val_mae: 4.1801\n",
      "Epoch 84/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 382.6435 - mae: 13.2624 - val_loss: 29.7364 - val_mae: 4.1418\n",
      "Epoch 85/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 376.6836 - mae: 13.1854 - val_loss: 30.9588 - val_mae: 4.0912\n",
      "Epoch 86/450\n",
      "122/122 [==============================] - 0s 589us/step - loss: 383.4525 - mae: 13.2937 - val_loss: 40.7440 - val_mae: 4.7274\n",
      "Epoch 87/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 364.0908 - mae: 12.9692 - val_loss: 37.4068 - val_mae: 4.5257\n",
      "Epoch 88/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 358.5822 - mae: 12.9735 - val_loss: 36.0968 - val_mae: 4.6802\n",
      "Epoch 89/450\n",
      "122/122 [==============================] - 0s 651us/step - loss: 357.9409 - mae: 12.8293 - val_loss: 27.7823 - val_mae: 3.7548\n",
      "Epoch 90/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 372.7857 - mae: 13.2174 - val_loss: 30.7327 - val_mae: 3.9610\n",
      "Epoch 91/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 359.0731 - mae: 12.9121 - val_loss: 27.0804 - val_mae: 3.8732\n",
      "Epoch 92/450\n",
      "122/122 [==============================] - 0s 582us/step - loss: 338.9664 - mae: 12.6421 - val_loss: 26.2103 - val_mae: 3.7070\n",
      "Epoch 93/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 345.2086 - mae: 12.6446 - val_loss: 26.5939 - val_mae: 3.7615\n",
      "Epoch 94/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 361.7285 - mae: 12.8930 - val_loss: 29.8592 - val_mae: 3.9269\n",
      "Epoch 95/450\n",
      "122/122 [==============================] - 0s 578us/step - loss: 370.6368 - mae: 13.0443 - val_loss: 24.4573 - val_mae: 3.4721\n",
      "Epoch 96/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 354.4618 - mae: 12.6367 - val_loss: 25.1118 - val_mae: 3.6231\n",
      "Epoch 97/450\n",
      "122/122 [==============================] - 0s 576us/step - loss: 345.1741 - mae: 12.6428 - val_loss: 34.2923 - val_mae: 4.3974\n",
      "Epoch 98/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 329.4948 - mae: 12.3350 - val_loss: 24.1367 - val_mae: 3.6088\n",
      "Epoch 99/450\n",
      "122/122 [==============================] - 0s 704us/step - loss: 340.1208 - mae: 12.5033 - val_loss: 42.2522 - val_mae: 5.0999\n",
      "Epoch 100/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 341.7225 - mae: 12.4772 - val_loss: 26.8366 - val_mae: 3.7237\n",
      "Epoch 101/450\n",
      "122/122 [==============================] - 0s 652us/step - loss: 315.6364 - mae: 12.1725 - val_loss: 22.5693 - val_mae: 3.4091\n",
      "Epoch 102/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 326.6970 - mae: 12.1713 - val_loss: 25.4635 - val_mae: 3.5532\n",
      "Epoch 103/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 312.5194 - mae: 12.1674 - val_loss: 22.4490 - val_mae: 3.3978\n",
      "Epoch 104/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 321.0424 - mae: 12.1690 - val_loss: 23.8532 - val_mae: 3.5290\n",
      "Epoch 105/450\n",
      "122/122 [==============================] - 0s 607us/step - loss: 327.6871 - mae: 12.1227 - val_loss: 29.4906 - val_mae: 4.0951\n",
      "Epoch 106/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 295.7798 - mae: 11.7803 - val_loss: 60.1697 - val_mae: 6.0382\n",
      "Epoch 107/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 316.2228 - mae: 11.9885 - val_loss: 26.5006 - val_mae: 3.6481\n",
      "Epoch 108/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 329.1554 - mae: 12.3470 - val_loss: 24.0723 - val_mae: 3.5233\n",
      "Epoch 109/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 290.9681 - mae: 11.5577 - val_loss: 34.4840 - val_mae: 4.2646\n",
      "Epoch 110/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 308.6528 - mae: 11.8145 - val_loss: 22.4659 - val_mae: 3.3713\n",
      "Epoch 111/450\n",
      "122/122 [==============================] - 0s 581us/step - loss: 310.9591 - mae: 11.8290 - val_loss: 39.3630 - val_mae: 4.7472\n",
      "Epoch 112/450\n",
      "122/122 [==============================] - 0s 684us/step - loss: 301.0151 - mae: 11.7314 - val_loss: 25.2167 - val_mae: 3.5503\n",
      "Epoch 113/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 303.3698 - mae: 11.7330 - val_loss: 46.8360 - val_mae: 5.2684\n",
      "Epoch 114/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 298.0565 - mae: 11.8142 - val_loss: 36.7848 - val_mae: 4.5294\n",
      "Epoch 115/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 288.1455 - mae: 11.3588 - val_loss: 27.2305 - val_mae: 3.7064\n",
      "Epoch 116/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 273.4557 - mae: 11.2240 - val_loss: 30.6811 - val_mae: 4.0774\n",
      "Epoch 117/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 301.1972 - mae: 11.6543 - val_loss: 31.8325 - val_mae: 4.1970\n",
      "Epoch 118/450\n",
      "122/122 [==============================] - 0s 603us/step - loss: 283.9394 - mae: 11.2603 - val_loss: 29.1978 - val_mae: 4.0448\n",
      "Epoch 119/450\n",
      "122/122 [==============================] - 0s 581us/step - loss: 284.1126 - mae: 11.4343 - val_loss: 32.0942 - val_mae: 4.3411\n",
      "Epoch 120/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 296.1806 - mae: 11.5048 - val_loss: 29.7046 - val_mae: 4.1290\n",
      "Epoch 121/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 273.9583 - mae: 11.3008 - val_loss: 23.9784 - val_mae: 3.5075\n",
      "Epoch 122/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 281.3250 - mae: 11.3411 - val_loss: 25.4293 - val_mae: 3.5053\n",
      "Epoch 123/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 276.4543 - mae: 11.0355 - val_loss: 25.3258 - val_mae: 3.7278\n",
      "Epoch 124/450\n",
      "122/122 [==============================] - 0s 703us/step - loss: 275.5623 - mae: 11.0903 - val_loss: 25.4838 - val_mae: 3.5846\n",
      "Epoch 125/450\n",
      "122/122 [==============================] - 0s 577us/step - loss: 272.6653 - mae: 11.2386 - val_loss: 29.2930 - val_mae: 3.9229\n",
      "Epoch 126/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 267.3566 - mae: 11.1161 - val_loss: 25.6542 - val_mae: 3.5023\n",
      "Epoch 127/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 286.6674 - mae: 11.2312 - val_loss: 41.1823 - val_mae: 4.9122\n",
      "Epoch 128/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 264.0383 - mae: 10.8178 - val_loss: 31.2821 - val_mae: 4.1589\n",
      "Epoch 129/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 258.3468 - mae: 10.8189 - val_loss: 28.9458 - val_mae: 4.0513\n",
      "Epoch 130/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 259.5703 - mae: 10.8312 - val_loss: 25.1707 - val_mae: 3.6938\n",
      "Epoch 131/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 257.9254 - mae: 10.6604 - val_loss: 23.7851 - val_mae: 3.4669\n",
      "Epoch 132/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 266.6021 - mae: 10.8341 - val_loss: 28.5123 - val_mae: 3.8539\n",
      "Epoch 133/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 263.8384 - mae: 10.9262 - val_loss: 28.7535 - val_mae: 4.0192\n",
      "Epoch 134/450\n",
      "122/122 [==============================] - 0s 662us/step - loss: 265.0720 - mae: 10.9282 - val_loss: 24.5823 - val_mae: 3.5808\n",
      "Epoch 135/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 247.4742 - mae: 10.4302 - val_loss: 25.8653 - val_mae: 3.5892\n",
      "Epoch 136/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 247.5706 - mae: 10.4566 - val_loss: 51.7564 - val_mae: 5.5301\n",
      "Epoch 137/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 259.7304 - mae: 10.6645 - val_loss: 22.9915 - val_mae: 3.4606\n",
      "Epoch 138/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 265.7310 - mae: 10.7521 - val_loss: 26.0576 - val_mae: 3.5858\n",
      "Epoch 139/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 257.9098 - mae: 10.8288 - val_loss: 53.7163 - val_mae: 5.5764\n",
      "Epoch 140/450\n",
      "122/122 [==============================] - 0s 572us/step - loss: 253.8777 - mae: 10.5578 - val_loss: 29.4955 - val_mae: 4.1387\n",
      "Epoch 141/450\n",
      "122/122 [==============================] - 0s 571us/step - loss: 248.9256 - mae: 10.4555 - val_loss: 25.9509 - val_mae: 3.6580\n",
      "Epoch 142/450\n",
      "122/122 [==============================] - 0s 549us/step - loss: 240.4417 - mae: 10.3025 - val_loss: 24.0103 - val_mae: 3.5452\n",
      "Epoch 143/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 249.5225 - mae: 10.4170 - val_loss: 25.1335 - val_mae: 3.7161\n",
      "Epoch 144/450\n",
      "122/122 [==============================] - 0s 688us/step - loss: 254.3067 - mae: 10.4806 - val_loss: 29.6786 - val_mae: 3.9755\n",
      "Epoch 145/450\n",
      "122/122 [==============================] - 0s 583us/step - loss: 246.3502 - mae: 10.3634 - val_loss: 23.1306 - val_mae: 3.4460\n",
      "Epoch 146/450\n",
      "122/122 [==============================] - 0s 548us/step - loss: 238.2169 - mae: 10.2644 - val_loss: 27.9673 - val_mae: 3.8667\n",
      "Epoch 147/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 238.5662 - mae: 10.1887 - val_loss: 43.4031 - val_mae: 4.9961\n",
      "Epoch 148/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 238.5013 - mae: 10.2251 - val_loss: 24.2099 - val_mae: 3.5178\n",
      "Epoch 149/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 214.8730 - mae: 9.8470 - val_loss: 23.4742 - val_mae: 3.4493\n",
      "Epoch 150/450\n",
      "122/122 [==============================] - 0s 581us/step - loss: 222.9749 - mae: 9.9604 - val_loss: 22.8888 - val_mae: 3.4280\n",
      "Epoch 151/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 235.7526 - mae: 10.1496 - val_loss: 35.2499 - val_mae: 4.3643\n",
      "Epoch 152/450\n",
      "122/122 [==============================] - 0s 553us/step - loss: 234.6682 - mae: 10.1173 - val_loss: 25.4533 - val_mae: 3.6669\n",
      "Epoch 153/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 228.5963 - mae: 9.9107 - val_loss: 26.2516 - val_mae: 3.7787\n",
      "Epoch 154/450\n",
      "122/122 [==============================] - 0s 655us/step - loss: 236.3262 - mae: 10.1008 - val_loss: 26.9071 - val_mae: 3.8167\n",
      "Epoch 155/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 222.9073 - mae: 9.8063 - val_loss: 28.1516 - val_mae: 3.9397\n",
      "Epoch 156/450\n",
      "122/122 [==============================] - 0s 598us/step - loss: 208.1115 - mae: 9.6621 - val_loss: 25.9890 - val_mae: 3.6660\n",
      "Epoch 157/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 218.3517 - mae: 9.7867 - val_loss: 25.7198 - val_mae: 3.5102\n",
      "Epoch 158/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 219.0536 - mae: 9.7700 - val_loss: 40.4489 - val_mae: 4.8640\n",
      "Epoch 159/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 218.6702 - mae: 9.7754 - val_loss: 23.2319 - val_mae: 3.4905\n",
      "Epoch 160/450\n",
      "122/122 [==============================] - 0s 548us/step - loss: 210.7493 - mae: 9.5441 - val_loss: 27.5001 - val_mae: 3.9056\n",
      "Epoch 161/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 222.0385 - mae: 9.8505 - val_loss: 27.1949 - val_mae: 3.7461\n",
      "Epoch 162/450\n",
      "122/122 [==============================] - 0s 584us/step - loss: 220.4626 - mae: 9.6467 - val_loss: 36.9446 - val_mae: 4.6127\n",
      "Epoch 163/450\n",
      "122/122 [==============================] - 0s 553us/step - loss: 220.8454 - mae: 9.6315 - val_loss: 29.0040 - val_mae: 3.9271\n",
      "Epoch 164/450\n",
      "122/122 [==============================] - 0s 642us/step - loss: 229.3154 - mae: 9.6058 - val_loss: 26.3427 - val_mae: 3.6941\n",
      "Epoch 165/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 217.6813 - mae: 9.6632 - val_loss: 37.1952 - val_mae: 4.5972\n",
      "Epoch 166/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 216.3147 - mae: 9.7443 - val_loss: 47.8870 - val_mae: 5.1865\n",
      "Epoch 167/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 196.2064 - mae: 9.1451 - val_loss: 42.1248 - val_mae: 4.8888\n",
      "Epoch 168/450\n",
      "122/122 [==============================] - 0s 599us/step - loss: 200.0422 - mae: 9.3218 - val_loss: 25.6951 - val_mae: 3.6444\n",
      "Epoch 169/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 194.7738 - mae: 9.1731 - val_loss: 23.5905 - val_mae: 3.5271\n",
      "Epoch 170/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 204.1561 - mae: 9.4160 - val_loss: 33.9381 - val_mae: 4.3956\n",
      "Epoch 171/450\n",
      "122/122 [==============================] - 0s 549us/step - loss: 194.7012 - mae: 9.2229 - val_loss: 25.2485 - val_mae: 3.6605\n",
      "Epoch 172/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 198.6357 - mae: 9.3177 - val_loss: 23.8791 - val_mae: 3.5422\n",
      "Epoch 173/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 206.8920 - mae: 9.4094 - val_loss: 38.6482 - val_mae: 4.6656\n",
      "Epoch 174/450\n",
      "122/122 [==============================] - 0s 587us/step - loss: 190.5939 - mae: 9.1538 - val_loss: 29.8085 - val_mae: 4.0809\n",
      "Epoch 175/450\n",
      "122/122 [==============================] - 0s 637us/step - loss: 196.2077 - mae: 9.1767 - val_loss: 29.6266 - val_mae: 4.0229\n",
      "Epoch 176/450\n",
      "122/122 [==============================] - 0s 584us/step - loss: 183.2506 - mae: 9.0190 - val_loss: 24.2048 - val_mae: 3.6089\n",
      "Epoch 177/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 203.1398 - mae: 9.2712 - val_loss: 39.1007 - val_mae: 4.7166\n",
      "Epoch 178/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 191.2477 - mae: 9.0813 - val_loss: 27.6816 - val_mae: 3.8671\n",
      "Epoch 179/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 197.3419 - mae: 9.1689 - val_loss: 34.7522 - val_mae: 4.4293\n",
      "Epoch 180/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 177.4689 - mae: 8.8763 - val_loss: 24.9719 - val_mae: 3.6881\n",
      "Epoch 181/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 182.2249 - mae: 8.9117 - val_loss: 27.6022 - val_mae: 3.9060\n",
      "Epoch 182/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 188.5349 - mae: 8.9729 - val_loss: 28.9643 - val_mae: 4.0319\n",
      "Epoch 183/450\n",
      "122/122 [==============================] - 0s 607us/step - loss: 188.7272 - mae: 8.9389 - val_loss: 26.1427 - val_mae: 3.7903\n",
      "Epoch 184/450\n",
      "122/122 [==============================] - 0s 617us/step - loss: 182.0370 - mae: 9.0040 - val_loss: 24.6646 - val_mae: 3.5589\n",
      "Epoch 185/450\n",
      "122/122 [==============================] - 0s 607us/step - loss: 175.0340 - mae: 8.7561 - val_loss: 26.2777 - val_mae: 3.6436\n",
      "Epoch 186/450\n",
      "122/122 [==============================] - 0s 633us/step - loss: 179.5691 - mae: 8.7690 - val_loss: 26.1946 - val_mae: 3.7624\n",
      "Epoch 187/450\n",
      "122/122 [==============================] - 0s 606us/step - loss: 178.0340 - mae: 8.8351 - val_loss: 28.6053 - val_mae: 3.8988\n",
      "Epoch 188/450\n",
      "122/122 [==============================] - 0s 586us/step - loss: 174.1176 - mae: 8.7725 - val_loss: 27.5210 - val_mae: 3.8812\n",
      "Epoch 189/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 173.4374 - mae: 8.7422 - val_loss: 52.6737 - val_mae: 5.4257\n",
      "Epoch 190/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 185.8719 - mae: 8.9023 - val_loss: 25.8543 - val_mae: 3.7560\n",
      "Epoch 191/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 165.3680 - mae: 8.6819 - val_loss: 31.6536 - val_mae: 4.2264\n",
      "Epoch 192/450\n",
      "122/122 [==============================] - 0s 553us/step - loss: 178.3210 - mae: 8.7318 - val_loss: 34.7737 - val_mae: 4.4165\n",
      "Epoch 193/450\n",
      "122/122 [==============================] - 0s 694us/step - loss: 160.3317 - mae: 8.3703 - val_loss: 30.4867 - val_mae: 4.1630\n",
      "Epoch 194/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 170.3678 - mae: 8.7119 - val_loss: 44.8186 - val_mae: 4.9186\n",
      "Epoch 195/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 169.9784 - mae: 8.6283 - val_loss: 34.0379 - val_mae: 4.2935\n",
      "Epoch 196/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 158.5982 - mae: 8.4054 - val_loss: 49.4483 - val_mae: 5.1778\n",
      "Epoch 197/450\n",
      "122/122 [==============================] - 0s 553us/step - loss: 169.8841 - mae: 8.6119 - val_loss: 42.6547 - val_mae: 4.8240\n",
      "Epoch 198/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 174.0480 - mae: 8.6544 - val_loss: 39.5588 - val_mae: 4.6666\n",
      "Epoch 199/450\n",
      "122/122 [==============================] - 0s 603us/step - loss: 158.2056 - mae: 8.3398 - val_loss: 37.1614 - val_mae: 4.5692\n",
      "Epoch 200/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 160.1652 - mae: 8.2830 - val_loss: 32.4014 - val_mae: 4.2126\n",
      "Epoch 201/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 176.4480 - mae: 8.6843 - val_loss: 28.6511 - val_mae: 3.9656\n",
      "Epoch 202/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 158.8246 - mae: 8.3228 - val_loss: 34.1859 - val_mae: 4.4133\n",
      "Epoch 203/450\n",
      "122/122 [==============================] - 0s 653us/step - loss: 161.7890 - mae: 8.3845 - val_loss: 61.3259 - val_mae: 5.7364\n",
      "Epoch 204/450\n",
      "122/122 [==============================] - 0s 595us/step - loss: 155.1996 - mae: 8.1710 - val_loss: 25.8040 - val_mae: 3.6936\n",
      "Epoch 205/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 158.2466 - mae: 8.3571 - val_loss: 54.3121 - val_mae: 5.4199\n",
      "Epoch 206/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 144.0691 - mae: 8.0735 - val_loss: 37.3321 - val_mae: 4.5925\n",
      "Epoch 207/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 170.0159 - mae: 8.5213 - val_loss: 62.5074 - val_mae: 5.8047\n",
      "Epoch 208/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 159.3957 - mae: 8.3258 - val_loss: 63.5977 - val_mae: 5.8041\n",
      "Epoch 209/450\n",
      "122/122 [==============================] - 0s 578us/step - loss: 140.0774 - mae: 7.8727 - val_loss: 53.2783 - val_mae: 5.5002\n",
      "Epoch 210/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 142.1145 - mae: 7.9447 - val_loss: 40.2213 - val_mae: 4.7833\n",
      "Epoch 211/450\n",
      "122/122 [==============================] - 0s 604us/step - loss: 136.0597 - mae: 7.8841 - val_loss: 65.0258 - val_mae: 5.8588\n",
      "Epoch 212/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 144.0365 - mae: 7.9541 - val_loss: 33.8755 - val_mae: 4.3911\n",
      "Epoch 213/450\n",
      "122/122 [==============================] - 0s 657us/step - loss: 150.3212 - mae: 8.2024 - val_loss: 68.4720 - val_mae: 6.1132\n",
      "Epoch 214/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 143.1658 - mae: 8.0000 - val_loss: 36.7544 - val_mae: 4.4628\n",
      "Epoch 215/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 137.5993 - mae: 7.7985 - val_loss: 54.4678 - val_mae: 5.5341\n",
      "Epoch 216/450\n",
      "122/122 [==============================] - 0s 603us/step - loss: 145.3339 - mae: 8.0058 - val_loss: 81.1325 - val_mae: 6.3744\n",
      "Epoch 217/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 151.5034 - mae: 8.0848 - val_loss: 59.4754 - val_mae: 5.6209\n",
      "Epoch 218/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 134.1012 - mae: 7.8330 - val_loss: 44.9132 - val_mae: 5.0977\n",
      "Epoch 219/450\n",
      "122/122 [==============================] - 0s 576us/step - loss: 136.3633 - mae: 7.8980 - val_loss: 66.7066 - val_mae: 6.0015\n",
      "Epoch 220/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 127.3224 - mae: 7.6195 - val_loss: 61.7672 - val_mae: 5.8308\n",
      "Epoch 221/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 125.4513 - mae: 7.6202 - val_loss: 66.8587 - val_mae: 5.9901\n",
      "Epoch 222/450\n",
      "122/122 [==============================] - 0s 643us/step - loss: 128.5913 - mae: 7.8004 - val_loss: 40.3751 - val_mae: 4.8570\n",
      "Epoch 223/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 129.0844 - mae: 7.6468 - val_loss: 83.5856 - val_mae: 6.5824\n",
      "Epoch 224/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 121.0562 - mae: 7.5075 - val_loss: 80.0301 - val_mae: 6.4121\n",
      "Epoch 225/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 133.2660 - mae: 7.8032 - val_loss: 64.2986 - val_mae: 5.9218\n",
      "Epoch 226/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 124.1516 - mae: 7.5467 - val_loss: 35.1995 - val_mae: 4.5252\n",
      "Epoch 227/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 125.9819 - mae: 7.5402 - val_loss: 64.8213 - val_mae: 6.0123\n",
      "Epoch 228/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 129.7120 - mae: 7.7062 - val_loss: 58.1720 - val_mae: 5.6709\n",
      "Epoch 229/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 124.7839 - mae: 7.5426 - val_loss: 81.0826 - val_mae: 6.4779\n",
      "Epoch 230/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 123.8581 - mae: 7.5659 - val_loss: 90.4641 - val_mae: 6.9021\n",
      "Epoch 231/450\n",
      "122/122 [==============================] - 0s 632us/step - loss: 119.1540 - mae: 7.5252 - val_loss: 40.5793 - val_mae: 4.9068\n",
      "Epoch 232/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 117.5323 - mae: 7.3724 - val_loss: 65.1304 - val_mae: 5.9385\n",
      "Epoch 233/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 114.7727 - mae: 7.3661 - val_loss: 58.2771 - val_mae: 5.7074\n",
      "Epoch 234/450\n",
      "122/122 [==============================] - 0s 584us/step - loss: 120.0753 - mae: 7.4983 - val_loss: 54.2087 - val_mae: 5.4899\n",
      "Epoch 235/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 112.8406 - mae: 7.3263 - val_loss: 42.4459 - val_mae: 4.9625\n",
      "Epoch 236/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 109.1705 - mae: 7.0576 - val_loss: 76.3356 - val_mae: 6.3236\n",
      "Epoch 237/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 106.3805 - mae: 7.1501 - val_loss: 55.3163 - val_mae: 5.6816\n",
      "Epoch 238/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 110.4083 - mae: 7.2277 - val_loss: 108.8474 - val_mae: 7.3406\n",
      "Epoch 239/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 111.1087 - mae: 7.1735 - val_loss: 147.2883 - val_mae: 8.1995\n",
      "Epoch 240/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 119.8669 - mae: 7.2542 - val_loss: 89.8224 - val_mae: 6.7724\n",
      "Epoch 241/450\n",
      "122/122 [==============================] - 0s 696us/step - loss: 113.7648 - mae: 7.3087 - val_loss: 78.4502 - val_mae: 6.4852\n",
      "Epoch 242/450\n",
      "122/122 [==============================] - 0s 575us/step - loss: 109.4318 - mae: 7.1545 - val_loss: 111.2646 - val_mae: 7.3107\n",
      "Epoch 243/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 107.1302 - mae: 7.1407 - val_loss: 81.2292 - val_mae: 6.5143\n",
      "Epoch 244/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 108.2245 - mae: 7.1159 - val_loss: 78.0024 - val_mae: 6.3301\n",
      "Epoch 245/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 112.6451 - mae: 7.1640 - val_loss: 46.1894 - val_mae: 5.1913\n",
      "Epoch 246/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 109.7598 - mae: 7.1748 - val_loss: 99.8146 - val_mae: 7.0182\n",
      "Epoch 247/450\n",
      "122/122 [==============================] - 0s 608us/step - loss: 107.5910 - mae: 7.0991 - val_loss: 77.5903 - val_mae: 6.2809\n",
      "Epoch 248/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 104.2201 - mae: 7.1420 - val_loss: 85.0848 - val_mae: 6.6147\n",
      "Epoch 249/450\n",
      "122/122 [==============================] - 0s 632us/step - loss: 106.4374 - mae: 7.0507 - val_loss: 83.5804 - val_mae: 6.6576\n",
      "Epoch 250/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 111.2657 - mae: 7.1240 - val_loss: 98.3373 - val_mae: 6.9290\n",
      "Epoch 251/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 109.4550 - mae: 7.1370 - val_loss: 64.3381 - val_mae: 6.1561\n",
      "Epoch 252/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 105.2179 - mae: 6.9640 - val_loss: 103.1162 - val_mae: 7.1048\n",
      "Epoch 253/450\n",
      "122/122 [==============================] - 0s 592us/step - loss: 102.2485 - mae: 6.9488 - val_loss: 125.7054 - val_mae: 7.6878\n",
      "Epoch 254/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 106.0173 - mae: 7.0055 - val_loss: 140.0376 - val_mae: 8.0090\n",
      "Epoch 255/450\n",
      "122/122 [==============================] - 0s 575us/step - loss: 106.1913 - mae: 7.0717 - val_loss: 101.4582 - val_mae: 7.2202\n",
      "Epoch 256/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 107.9467 - mae: 7.1237 - val_loss: 149.1575 - val_mae: 8.2827\n",
      "Epoch 257/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 98.1431 - mae: 6.9125 - val_loss: 77.1986 - val_mae: 6.2838\n",
      "Epoch 258/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 105.2695 - mae: 7.0806 - val_loss: 92.7608 - val_mae: 6.8944\n",
      "Epoch 259/450\n",
      "122/122 [==============================] - 0s 675us/step - loss: 98.4147 - mae: 6.9578 - val_loss: 83.5642 - val_mae: 6.4807\n",
      "Epoch 260/450\n",
      "122/122 [==============================] - 0s 604us/step - loss: 95.9162 - mae: 6.8480 - val_loss: 79.4916 - val_mae: 6.3460\n",
      "Epoch 261/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 95.9961 - mae: 6.8452 - val_loss: 101.4436 - val_mae: 7.3530\n",
      "Epoch 262/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 98.8895 - mae: 6.8214 - val_loss: 149.5424 - val_mae: 8.1580\n",
      "Epoch 263/450\n",
      "122/122 [==============================] - 0s 571us/step - loss: 90.0218 - mae: 6.6059 - val_loss: 91.3733 - val_mae: 6.8845\n",
      "Epoch 264/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 95.6148 - mae: 6.7434 - val_loss: 92.2851 - val_mae: 6.6927\n",
      "Epoch 265/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 90.7089 - mae: 6.6988 - val_loss: 118.5110 - val_mae: 7.5073\n",
      "Epoch 266/450\n",
      "122/122 [==============================] - 0s 592us/step - loss: 94.8704 - mae: 6.7212 - val_loss: 116.6406 - val_mae: 7.4183\n",
      "Epoch 267/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 91.4288 - mae: 6.7176 - val_loss: 92.9253 - val_mae: 6.9416\n",
      "Epoch 268/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 93.5965 - mae: 6.7322 - val_loss: 161.5113 - val_mae: 8.4717\n",
      "Epoch 269/450\n",
      "122/122 [==============================] - 0s 605us/step - loss: 102.1417 - mae: 7.0071 - val_loss: 117.2411 - val_mae: 7.5347\n",
      "Epoch 270/450\n",
      "122/122 [==============================] - 0s 611us/step - loss: 84.1938 - mae: 6.4475 - val_loss: 120.4576 - val_mae: 7.5004\n",
      "Epoch 271/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 91.9703 - mae: 6.6632 - val_loss: 122.4427 - val_mae: 7.5724\n",
      "Epoch 272/450\n",
      "122/122 [==============================] - 0s 582us/step - loss: 90.7916 - mae: 6.6217 - val_loss: 87.2493 - val_mae: 6.6686\n",
      "Epoch 273/450\n",
      "122/122 [==============================] - 0s 571us/step - loss: 92.1829 - mae: 6.6149 - val_loss: 105.1781 - val_mae: 7.1721\n",
      "Epoch 274/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 89.2514 - mae: 6.5429 - val_loss: 146.5242 - val_mae: 8.1452\n",
      "Epoch 275/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 88.2409 - mae: 6.5541 - val_loss: 101.0978 - val_mae: 7.0440\n",
      "Epoch 276/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 88.1662 - mae: 6.4722 - val_loss: 147.9293 - val_mae: 8.1449\n",
      "Epoch 277/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 84.4989 - mae: 6.4829 - val_loss: 99.8701 - val_mae: 7.1126\n",
      "Epoch 278/450\n",
      "122/122 [==============================] - 0s 581us/step - loss: 91.0013 - mae: 6.5921 - val_loss: 161.6504 - val_mae: 8.5369\n",
      "Epoch 279/450\n",
      "122/122 [==============================] - 0s 698us/step - loss: 84.0289 - mae: 6.3800 - val_loss: 108.3779 - val_mae: 7.3912\n",
      "Epoch 280/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 89.7101 - mae: 6.5478 - val_loss: 148.5892 - val_mae: 8.3001\n",
      "Epoch 281/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 89.8380 - mae: 6.6503 - val_loss: 160.8623 - val_mae: 8.3716\n",
      "Epoch 282/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 86.0011 - mae: 6.5176 - val_loss: 125.5273 - val_mae: 7.9489\n",
      "Epoch 283/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 81.9454 - mae: 6.3270 - val_loss: 173.1116 - val_mae: 8.8359\n",
      "Epoch 284/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 80.0497 - mae: 6.3070 - val_loss: 127.9066 - val_mae: 7.7213\n",
      "Epoch 285/450\n",
      "122/122 [==============================] - 0s 586us/step - loss: 84.0413 - mae: 6.4253 - val_loss: 170.9172 - val_mae: 8.8331\n",
      "Epoch 286/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 83.0512 - mae: 6.2750 - val_loss: 159.6808 - val_mae: 8.6313\n",
      "Epoch 287/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 80.5886 - mae: 6.2998 - val_loss: 145.7345 - val_mae: 8.3824\n",
      "Epoch 288/450\n",
      "122/122 [==============================] - 0s 674us/step - loss: 84.1425 - mae: 6.4026 - val_loss: 216.1811 - val_mae: 9.5850\n",
      "Epoch 289/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 79.7214 - mae: 6.2067 - val_loss: 149.8012 - val_mae: 8.3035\n",
      "Epoch 290/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 83.7891 - mae: 6.3908 - val_loss: 216.3102 - val_mae: 9.5508\n",
      "Epoch 291/450\n",
      "122/122 [==============================] - 0s 584us/step - loss: 81.1720 - mae: 6.2723 - val_loss: 148.9512 - val_mae: 8.3836\n",
      "Epoch 292/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 79.6115 - mae: 6.1747 - val_loss: 166.2812 - val_mae: 8.6468\n",
      "Epoch 293/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 78.4990 - mae: 6.1098 - val_loss: 167.9346 - val_mae: 8.5751\n",
      "Epoch 294/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 76.3842 - mae: 6.1895 - val_loss: 267.9622 - val_mae: 10.3130\n",
      "Epoch 295/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 83.7904 - mae: 6.3981 - val_loss: 164.8427 - val_mae: 8.7935\n",
      "Epoch 296/450\n",
      "122/122 [==============================] - 0s 595us/step - loss: 79.0655 - mae: 6.1789 - val_loss: 194.5251 - val_mae: 9.1409\n",
      "Epoch 297/450\n",
      "122/122 [==============================] - 0s 634us/step - loss: 79.7043 - mae: 6.1804 - val_loss: 166.2808 - val_mae: 8.5949\n",
      "Epoch 298/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 69.5516 - mae: 5.9542 - val_loss: 191.4529 - val_mae: 9.1176\n",
      "Epoch 299/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 72.1021 - mae: 6.0609 - val_loss: 216.4617 - val_mae: 9.4979\n",
      "Epoch 300/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 77.2293 - mae: 6.1869 - val_loss: 206.3258 - val_mae: 9.5721\n",
      "Epoch 301/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 72.6958 - mae: 6.1138 - val_loss: 202.9114 - val_mae: 9.3168\n",
      "Epoch 302/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 74.4185 - mae: 6.0740 - val_loss: 214.8523 - val_mae: 9.5538\n",
      "Epoch 303/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 71.2430 - mae: 6.0227 - val_loss: 187.8020 - val_mae: 9.0867\n",
      "Epoch 304/450\n",
      "122/122 [==============================] - 0s 607us/step - loss: 73.4133 - mae: 5.9805 - val_loss: 150.8035 - val_mae: 8.3258\n",
      "Epoch 305/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 74.5119 - mae: 6.1010 - val_loss: 152.0318 - val_mae: 8.3923\n",
      "Epoch 306/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 71.6152 - mae: 6.0233 - val_loss: 199.1649 - val_mae: 9.3497\n",
      "Epoch 307/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 74.1015 - mae: 5.9940 - val_loss: 160.7387 - val_mae: 8.7193\n",
      "Epoch 308/450\n",
      "122/122 [==============================] - 0s 688us/step - loss: 71.4390 - mae: 5.9592 - val_loss: 174.2743 - val_mae: 8.8262\n",
      "Epoch 309/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 69.8698 - mae: 5.9166 - val_loss: 205.3141 - val_mae: 9.3884\n",
      "Epoch 310/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 72.1779 - mae: 6.0165 - val_loss: 253.1060 - val_mae: 10.1603\n",
      "Epoch 311/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 69.1576 - mae: 5.8778 - val_loss: 194.7533 - val_mae: 9.1965\n",
      "Epoch 312/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 66.5225 - mae: 5.7805 - val_loss: 208.9561 - val_mae: 9.3843\n",
      "Epoch 313/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 73.9475 - mae: 6.0602 - val_loss: 184.3374 - val_mae: 9.0620\n",
      "Epoch 314/450\n",
      "122/122 [==============================] - 0s 598us/step - loss: 70.5242 - mae: 5.9116 - val_loss: 236.5873 - val_mae: 10.0494\n",
      "Epoch 315/450\n",
      "122/122 [==============================] - 0s 577us/step - loss: 71.8806 - mae: 5.9355 - val_loss: 181.1915 - val_mae: 8.9372\n",
      "Epoch 316/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 70.5219 - mae: 5.8521 - val_loss: 228.7236 - val_mae: 9.8320\n",
      "Epoch 317/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 72.8405 - mae: 5.9062 - val_loss: 229.6973 - val_mae: 9.9224\n",
      "Epoch 318/450\n",
      "122/122 [==============================] - 0s 626us/step - loss: 73.1574 - mae: 6.0439 - val_loss: 228.1022 - val_mae: 9.7512\n",
      "Epoch 319/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 71.8555 - mae: 5.9119 - val_loss: 188.4652 - val_mae: 9.1001\n",
      "Epoch 320/450\n",
      "122/122 [==============================] - 0s 556us/step - loss: 63.8408 - mae: 5.7773 - val_loss: 173.0608 - val_mae: 8.8551\n",
      "Epoch 321/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 67.2876 - mae: 5.8445 - val_loss: 209.1171 - val_mae: 9.3694\n",
      "Epoch 322/450\n",
      "122/122 [==============================] - 0s 577us/step - loss: 66.3061 - mae: 5.7000 - val_loss: 244.5658 - val_mae: 10.0118\n",
      "Epoch 323/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 72.1665 - mae: 5.9175 - val_loss: 200.1817 - val_mae: 9.3232\n",
      "Epoch 324/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 67.3699 - mae: 5.8315 - val_loss: 290.7261 - val_mae: 10.6488\n",
      "Epoch 325/450\n",
      "122/122 [==============================] - 0s 600us/step - loss: 63.4646 - mae: 5.7481 - val_loss: 239.5437 - val_mae: 9.9094\n",
      "Epoch 326/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 63.2524 - mae: 5.6595 - val_loss: 229.4573 - val_mae: 9.8030\n",
      "Epoch 327/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 70.3761 - mae: 5.9202 - val_loss: 211.6528 - val_mae: 9.6284\n",
      "Epoch 328/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 71.9755 - mae: 5.9272 - val_loss: 245.5360 - val_mae: 10.0227\n",
      "Epoch 329/450\n",
      "122/122 [==============================] - 0s 620us/step - loss: 66.4099 - mae: 5.7837 - val_loss: 251.6396 - val_mae: 10.1872\n",
      "Epoch 330/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 60.2971 - mae: 5.5750 - val_loss: 212.4848 - val_mae: 9.6371\n",
      "Epoch 331/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 64.9980 - mae: 5.6525 - val_loss: 236.3034 - val_mae: 9.9451\n",
      "Epoch 332/450\n",
      "122/122 [==============================] - 0s 575us/step - loss: 65.1096 - mae: 5.8052 - val_loss: 338.2389 - val_mae: 11.4961\n",
      "Epoch 333/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 61.5542 - mae: 5.6238 - val_loss: 200.9099 - val_mae: 9.4761\n",
      "Epoch 334/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 65.0563 - mae: 5.8008 - val_loss: 248.5337 - val_mae: 10.2213\n",
      "Epoch 335/450\n",
      "122/122 [==============================] - 0s 587us/step - loss: 67.3384 - mae: 5.8391 - val_loss: 202.9737 - val_mae: 9.3625\n",
      "Epoch 336/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 63.1469 - mae: 5.6842 - val_loss: 287.7804 - val_mae: 10.6316\n",
      "Epoch 337/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 66.2578 - mae: 5.7595 - val_loss: 224.6159 - val_mae: 9.8652\n",
      "Epoch 338/450\n",
      "122/122 [==============================] - 0s 668us/step - loss: 63.6805 - mae: 5.6850 - val_loss: 252.2230 - val_mae: 10.0474\n",
      "Epoch 339/450\n",
      "122/122 [==============================] - 0s 572us/step - loss: 64.0482 - mae: 5.6612 - val_loss: 290.3019 - val_mae: 10.7432\n",
      "Epoch 340/450\n",
      "122/122 [==============================] - 0s 587us/step - loss: 66.1827 - mae: 5.7858 - val_loss: 331.7312 - val_mae: 11.3928\n",
      "Epoch 341/450\n",
      "122/122 [==============================] - 0s 572us/step - loss: 67.5111 - mae: 5.7503 - val_loss: 191.7939 - val_mae: 9.2290\n",
      "Epoch 342/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 64.1185 - mae: 5.7585 - val_loss: 285.7858 - val_mae: 10.6160\n",
      "Epoch 343/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 64.0378 - mae: 5.6710 - val_loss: 254.9992 - val_mae: 10.2720\n",
      "Epoch 344/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 65.0531 - mae: 5.6957 - val_loss: 296.4428 - val_mae: 10.7982\n",
      "Epoch 345/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 62.5138 - mae: 5.6037 - val_loss: 223.8090 - val_mae: 9.8952\n",
      "Epoch 346/450\n",
      "122/122 [==============================] - 0s 610us/step - loss: 62.0936 - mae: 5.6218 - val_loss: 205.4936 - val_mae: 9.2901\n",
      "Epoch 347/450\n",
      "122/122 [==============================] - 0s 589us/step - loss: 60.2095 - mae: 5.5676 - val_loss: 262.9915 - val_mae: 10.2956\n",
      "Epoch 348/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 61.2082 - mae: 5.6047 - val_loss: 263.4863 - val_mae: 10.2895\n",
      "Epoch 349/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 63.4684 - mae: 5.5827 - val_loss: 277.6222 - val_mae: 10.6591\n",
      "Epoch 350/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 58.2982 - mae: 5.4963 - val_loss: 226.1483 - val_mae: 9.6262\n",
      "Epoch 351/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 59.7579 - mae: 5.4982 - val_loss: 264.9113 - val_mae: 10.3510\n",
      "Epoch 352/450\n",
      "122/122 [==============================] - 0s 601us/step - loss: 62.9456 - mae: 5.5868 - val_loss: 202.6090 - val_mae: 9.4446\n",
      "Epoch 353/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 65.7453 - mae: 5.7666 - val_loss: 236.8220 - val_mae: 9.9225\n",
      "Epoch 354/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 59.6918 - mae: 5.6334 - val_loss: 218.3765 - val_mae: 9.7267\n",
      "Epoch 355/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 61.0611 - mae: 5.6482 - val_loss: 274.3531 - val_mae: 10.6224\n",
      "Epoch 356/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 63.7499 - mae: 5.6330 - val_loss: 246.9740 - val_mae: 10.0683\n",
      "Epoch 357/450\n",
      "122/122 [==============================] - 0s 654us/step - loss: 60.1118 - mae: 5.5725 - val_loss: 238.0272 - val_mae: 9.9669\n",
      "Epoch 358/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 62.5122 - mae: 5.5164 - val_loss: 281.9693 - val_mae: 10.5250\n",
      "Epoch 359/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 57.4232 - mae: 5.4970 - val_loss: 260.6464 - val_mae: 10.2531\n",
      "Epoch 360/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 58.0153 - mae: 5.4794 - val_loss: 195.6389 - val_mae: 9.1061\n",
      "Epoch 361/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 59.1521 - mae: 5.5724 - val_loss: 231.8625 - val_mae: 9.7622\n",
      "Epoch 362/450\n",
      "122/122 [==============================] - 0s 585us/step - loss: 60.8660 - mae: 5.4749 - val_loss: 159.2233 - val_mae: 8.5761\n",
      "Epoch 363/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 63.9541 - mae: 5.6428 - val_loss: 234.3848 - val_mae: 9.8552\n",
      "Epoch 364/450\n",
      "122/122 [==============================] - 0s 555us/step - loss: 56.8715 - mae: 5.4634 - val_loss: 209.3514 - val_mae: 9.6570\n",
      "Epoch 365/450\n",
      "122/122 [==============================] - 0s 639us/step - loss: 58.0392 - mae: 5.5173 - val_loss: 229.5921 - val_mae: 9.7586\n",
      "Epoch 366/450\n",
      "122/122 [==============================] - 0s 581us/step - loss: 63.0230 - mae: 5.5019 - val_loss: 243.0568 - val_mae: 10.1289\n",
      "Epoch 367/450\n",
      "122/122 [==============================] - 0s 598us/step - loss: 65.4776 - mae: 5.6512 - val_loss: 221.9111 - val_mae: 9.6039\n",
      "Epoch 368/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 61.2826 - mae: 5.5665 - val_loss: 327.2592 - val_mae: 11.2597\n",
      "Epoch 369/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 55.7836 - mae: 5.3530 - val_loss: 247.5757 - val_mae: 9.9882\n",
      "Epoch 370/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 58.7751 - mae: 5.4898 - val_loss: 307.5688 - val_mae: 10.9323\n",
      "Epoch 371/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 58.5074 - mae: 5.4724 - val_loss: 309.1801 - val_mae: 10.9523\n",
      "Epoch 372/450\n",
      "122/122 [==============================] - 0s 769us/step - loss: 59.2114 - mae: 5.5041 - val_loss: 235.1889 - val_mae: 9.7858\n",
      "Epoch 373/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 62.4094 - mae: 5.5730 - val_loss: 273.1705 - val_mae: 10.4037\n",
      "Epoch 374/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 59.2324 - mae: 5.4549 - val_loss: 278.5658 - val_mae: 10.5210\n",
      "Epoch 375/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 60.7864 - mae: 5.5920 - val_loss: 267.4243 - val_mae: 10.3634\n",
      "Epoch 376/450\n",
      "122/122 [==============================] - 0s 575us/step - loss: 58.7522 - mae: 5.4975 - val_loss: 263.1864 - val_mae: 10.3334\n",
      "Epoch 377/450\n",
      "122/122 [==============================] - 0s 586us/step - loss: 54.8211 - mae: 5.4001 - val_loss: 370.1061 - val_mae: 11.8242\n",
      "Epoch 378/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 59.2165 - mae: 5.4449 - val_loss: 285.3802 - val_mae: 10.5270\n",
      "Epoch 379/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 60.9465 - mae: 5.5572 - val_loss: 315.0211 - val_mae: 11.0489\n",
      "Epoch 380/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 60.5646 - mae: 5.5032 - val_loss: 211.6335 - val_mae: 9.5375\n",
      "Epoch 381/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 56.6485 - mae: 5.3162 - val_loss: 291.9963 - val_mae: 10.6204\n",
      "Epoch 382/450\n",
      "122/122 [==============================] - 0s 687us/step - loss: 58.8864 - mae: 5.5169 - val_loss: 302.4246 - val_mae: 10.8612\n",
      "Epoch 383/450\n",
      "122/122 [==============================] - 0s 573us/step - loss: 63.9173 - mae: 5.5912 - val_loss: 315.9799 - val_mae: 11.0660\n",
      "Epoch 384/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 55.4121 - mae: 5.2708 - val_loss: 271.0489 - val_mae: 10.3943\n",
      "Epoch 385/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 55.8517 - mae: 5.3478 - val_loss: 262.4197 - val_mae: 10.3637\n",
      "Epoch 386/450\n",
      "122/122 [==============================] - 0s 576us/step - loss: 55.4777 - mae: 5.3356 - val_loss: 262.5338 - val_mae: 10.2811\n",
      "Epoch 387/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 53.8401 - mae: 5.2672 - val_loss: 288.7271 - val_mae: 10.7662\n",
      "Epoch 388/450\n",
      "122/122 [==============================] - 0s 587us/step - loss: 57.5043 - mae: 5.3804 - val_loss: 273.9450 - val_mae: 10.3530\n",
      "Epoch 389/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 57.9535 - mae: 5.3961 - val_loss: 336.8659 - val_mae: 11.3230\n",
      "Epoch 390/450\n",
      "122/122 [==============================] - 0s 630us/step - loss: 54.4076 - mae: 5.3089 - val_loss: 242.8739 - val_mae: 9.9227\n",
      "Epoch 391/450\n",
      "122/122 [==============================] - 0s 575us/step - loss: 58.0976 - mae: 5.4184 - val_loss: 314.6601 - val_mae: 11.0077\n",
      "Epoch 392/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 53.2025 - mae: 5.2393 - val_loss: 289.2759 - val_mae: 10.7617\n",
      "Epoch 393/450\n",
      "122/122 [==============================] - 0s 583us/step - loss: 50.7938 - mae: 5.1879 - val_loss: 401.6203 - val_mae: 12.1687\n",
      "Epoch 394/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 58.0079 - mae: 5.5099 - val_loss: 279.5908 - val_mae: 10.5099\n",
      "Epoch 395/450\n",
      "122/122 [==============================] - 0s 571us/step - loss: 53.1582 - mae: 5.2774 - val_loss: 319.2684 - val_mae: 11.0867\n",
      "Epoch 396/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 57.6384 - mae: 5.3998 - val_loss: 319.7483 - val_mae: 11.1979\n",
      "Epoch 397/450\n",
      "122/122 [==============================] - 0s 566us/step - loss: 60.1801 - mae: 5.5097 - val_loss: 277.5503 - val_mae: 10.5418\n",
      "Epoch 398/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 53.7753 - mae: 5.3099 - val_loss: 358.6847 - val_mae: 11.6713\n",
      "Epoch 399/450\n",
      "122/122 [==============================] - 0s 705us/step - loss: 57.0364 - mae: 5.3997 - val_loss: 378.6611 - val_mae: 11.8310\n",
      "Epoch 400/450\n",
      "122/122 [==============================] - 0s 572us/step - loss: 54.7144 - mae: 5.3252 - val_loss: 291.7344 - val_mae: 10.6929\n",
      "Epoch 401/450\n",
      "122/122 [==============================] - 0s 653us/step - loss: 51.3957 - mae: 5.1674 - val_loss: 327.9443 - val_mae: 11.2550\n",
      "Epoch 402/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 56.2234 - mae: 5.3476 - val_loss: 291.6816 - val_mae: 10.6773\n",
      "Epoch 403/450\n",
      "122/122 [==============================] - 0s 586us/step - loss: 53.6809 - mae: 5.2447 - val_loss: 239.2319 - val_mae: 9.9452\n",
      "Epoch 404/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 57.4540 - mae: 5.4032 - val_loss: 256.5052 - val_mae: 10.2775\n",
      "Epoch 405/450\n",
      "122/122 [==============================] - 0s 564us/step - loss: 59.3306 - mae: 5.4512 - val_loss: 296.6369 - val_mae: 10.7816\n",
      "Epoch 406/450\n",
      "122/122 [==============================] - 0s 553us/step - loss: 51.3968 - mae: 5.1817 - val_loss: 235.4348 - val_mae: 9.9863\n",
      "Epoch 407/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 55.2874 - mae: 5.4257 - val_loss: 241.4594 - val_mae: 9.9176\n",
      "Epoch 408/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 56.9426 - mae: 5.3223 - val_loss: 279.0016 - val_mae: 10.4842\n",
      "Epoch 409/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 54.2018 - mae: 5.2613 - val_loss: 346.2325 - val_mae: 11.5175\n",
      "Epoch 410/450\n",
      "122/122 [==============================] - 0s 569us/step - loss: 50.3005 - mae: 5.1499 - val_loss: 347.4850 - val_mae: 11.6240\n",
      "Epoch 411/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 56.5434 - mae: 5.3641 - val_loss: 402.4020 - val_mae: 12.1995\n",
      "Epoch 412/450\n",
      "122/122 [==============================] - 0s 649us/step - loss: 55.8703 - mae: 5.3231 - val_loss: 351.4210 - val_mae: 11.5448\n",
      "Epoch 413/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 53.6813 - mae: 5.2807 - val_loss: 363.5460 - val_mae: 11.6396\n",
      "Epoch 414/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 55.8766 - mae: 5.2947 - val_loss: 242.2297 - val_mae: 10.0430\n",
      "Epoch 415/450\n",
      "122/122 [==============================] - 0s 582us/step - loss: 53.8903 - mae: 5.3559 - val_loss: 324.4952 - val_mae: 11.3181\n",
      "Epoch 416/450\n",
      "122/122 [==============================] - 0s 574us/step - loss: 50.4990 - mae: 5.1269 - val_loss: 298.7186 - val_mae: 10.8523\n",
      "Epoch 417/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 59.6025 - mae: 5.4322 - val_loss: 360.3154 - val_mae: 11.6871\n",
      "Epoch 418/450\n",
      "122/122 [==============================] - 0s 567us/step - loss: 51.9928 - mae: 5.1979 - val_loss: 360.9162 - val_mae: 11.7432\n",
      "Epoch 419/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 55.0290 - mae: 5.3217 - val_loss: 314.7434 - val_mae: 11.2043\n",
      "Epoch 420/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 52.9524 - mae: 5.3151 - val_loss: 290.4555 - val_mae: 10.7509\n",
      "Epoch 421/450\n",
      "122/122 [==============================] - 0s 588us/step - loss: 54.6064 - mae: 5.2282 - val_loss: 180.2147 - val_mae: 8.8909\n",
      "Epoch 422/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 54.1986 - mae: 5.3240 - val_loss: 242.9465 - val_mae: 9.8214\n",
      "Epoch 423/450\n",
      "122/122 [==============================] - 0s 644us/step - loss: 51.0985 - mae: 5.1939 - val_loss: 318.7135 - val_mae: 11.0733\n",
      "Epoch 424/450\n",
      "122/122 [==============================] - 0s 586us/step - loss: 57.9239 - mae: 5.3982 - val_loss: 359.8684 - val_mae: 11.5643\n",
      "Epoch 425/450\n",
      "122/122 [==============================] - 0s 571us/step - loss: 54.5180 - mae: 5.2071 - val_loss: 208.4857 - val_mae: 9.5150\n",
      "Epoch 426/450\n",
      "122/122 [==============================] - 0s 589us/step - loss: 52.1260 - mae: 5.2149 - val_loss: 279.8044 - val_mae: 10.4903\n",
      "Epoch 427/450\n",
      "122/122 [==============================] - 0s 559us/step - loss: 54.3719 - mae: 5.1885 - val_loss: 290.9798 - val_mae: 10.7215\n",
      "Epoch 428/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 53.3410 - mae: 5.2657 - val_loss: 353.2915 - val_mae: 11.5678\n",
      "Epoch 429/450\n",
      "122/122 [==============================] - 0s 557us/step - loss: 54.8222 - mae: 5.3331 - val_loss: 280.1142 - val_mae: 10.6908\n",
      "Epoch 430/450\n",
      "122/122 [==============================] - 0s 554us/step - loss: 52.2025 - mae: 5.1224 - val_loss: 322.4340 - val_mae: 11.1097\n",
      "Epoch 431/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 51.9048 - mae: 5.2387 - val_loss: 357.8386 - val_mae: 11.5877\n",
      "Epoch 432/450\n",
      "122/122 [==============================] - 0s 584us/step - loss: 50.5071 - mae: 5.1653 - val_loss: 338.6828 - val_mae: 11.2247\n",
      "Epoch 433/450\n",
      "122/122 [==============================] - 0s 568us/step - loss: 58.7599 - mae: 5.3850 - val_loss: 369.3414 - val_mae: 11.7530\n",
      "Epoch 434/450\n",
      "122/122 [==============================] - 0s 639us/step - loss: 54.7314 - mae: 5.2344 - val_loss: 277.8167 - val_mae: 10.5489\n",
      "Epoch 435/450\n",
      "122/122 [==============================] - 0s 579us/step - loss: 54.6612 - mae: 5.2904 - val_loss: 300.3916 - val_mae: 10.8725\n",
      "Epoch 436/450\n",
      "122/122 [==============================] - 0s 562us/step - loss: 53.9837 - mae: 5.2521 - val_loss: 353.6333 - val_mae: 11.5339\n",
      "Epoch 437/450\n",
      "122/122 [==============================] - 0s 590us/step - loss: 52.7089 - mae: 5.2378 - val_loss: 313.2113 - val_mae: 10.9501\n",
      "Epoch 438/450\n",
      "122/122 [==============================] - 0s 565us/step - loss: 49.3626 - mae: 5.0588 - val_loss: 324.5064 - val_mae: 11.0483\n",
      "Epoch 439/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 55.9871 - mae: 5.3060 - val_loss: 270.7908 - val_mae: 10.5531\n",
      "Epoch 440/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 54.9804 - mae: 5.2965 - val_loss: 345.3907 - val_mae: 11.5798\n",
      "Epoch 441/450\n",
      "122/122 [==============================] - 0s 560us/step - loss: 55.4381 - mae: 5.2980 - val_loss: 277.9880 - val_mae: 10.5467\n",
      "Epoch 442/450\n",
      "122/122 [==============================] - 0s 552us/step - loss: 55.4677 - mae: 5.2787 - val_loss: 378.1296 - val_mae: 11.8578\n",
      "Epoch 443/450\n",
      "122/122 [==============================] - 0s 705us/step - loss: 52.1740 - mae: 5.1320 - val_loss: 314.1851 - val_mae: 11.0212\n",
      "Epoch 444/450\n",
      "122/122 [==============================] - 0s 577us/step - loss: 53.2930 - mae: 5.1824 - val_loss: 361.0748 - val_mae: 11.6276\n",
      "Epoch 445/450\n",
      "122/122 [==============================] - 0s 558us/step - loss: 54.2443 - mae: 5.2682 - val_loss: 184.0759 - val_mae: 8.9877\n",
      "Epoch 446/450\n",
      "122/122 [==============================] - 0s 561us/step - loss: 53.0255 - mae: 5.2015 - val_loss: 308.2821 - val_mae: 10.9007\n",
      "Epoch 447/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 50.3916 - mae: 5.1095 - val_loss: 321.4868 - val_mae: 11.1074\n",
      "Epoch 448/450\n",
      "122/122 [==============================] - 0s 585us/step - loss: 50.6874 - mae: 5.1434 - val_loss: 313.3219 - val_mae: 10.9346\n",
      "Epoch 449/450\n",
      "122/122 [==============================] - 0s 570us/step - loss: 53.2243 - mae: 5.1977 - val_loss: 354.1693 - val_mae: 11.4777\n",
      "Epoch 450/450\n",
      "122/122 [==============================] - 0s 563us/step - loss: 50.4610 - mae: 5.1333 - val_loss: 362.0897 - val_mae: 11.6994\n",
      "31/31 [==============================] - 0s 379us/step\n",
      "Epochs: 450 | MAE: 11.699360898844402\n",
      "Training model with 500 epochs\n",
      "Epoch 1/500\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68229.8984 - mae: 226.2827 - val_loss: 59930.4727 - val_mae: 211.0684\n",
      "Epoch 2/500\n",
      "122/122 [==============================] - 0s 597us/step - loss: 38863.7930 - mae: 163.8839 - val_loss: 13849.3301 - val_mae: 98.8154\n",
      "Epoch 3/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 6362.9136 - mae: 61.8912 - val_loss: 3083.6272 - val_mae: 41.4570\n",
      "Epoch 4/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 3132.7935 - mae: 42.5995 - val_loss: 2140.9395 - val_mae: 34.4352\n",
      "Epoch 5/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 2446.9917 - mae: 37.7269 - val_loss: 1708.4430 - val_mae: 30.7792\n",
      "Epoch 6/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 2135.8462 - mae: 35.6329 - val_loss: 1491.3726 - val_mae: 28.7175\n",
      "Epoch 7/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 1992.0256 - mae: 34.6659 - val_loss: 1279.9133 - val_mae: 26.5893\n",
      "Epoch 8/500\n",
      "122/122 [==============================] - 0s 651us/step - loss: 1814.7566 - mae: 32.9212 - val_loss: 1159.8667 - val_mae: 25.4355\n",
      "Epoch 9/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 1731.2515 - mae: 32.4063 - val_loss: 1023.4173 - val_mae: 23.7961\n",
      "Epoch 10/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 1665.5363 - mae: 31.6135 - val_loss: 925.4605 - val_mae: 22.4978\n",
      "Epoch 11/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 1535.8448 - mae: 30.7253 - val_loss: 817.8400 - val_mae: 21.1764\n",
      "Epoch 12/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 1431.5785 - mae: 29.2244 - val_loss: 757.2232 - val_mae: 20.2850\n",
      "Epoch 13/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 1371.6580 - mae: 28.7046 - val_loss: 686.3116 - val_mae: 19.3128\n",
      "Epoch 14/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 1253.0432 - mae: 27.5083 - val_loss: 590.1889 - val_mae: 17.9183\n",
      "Epoch 15/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 1193.9382 - mae: 26.7296 - val_loss: 525.1284 - val_mae: 16.9735\n",
      "Epoch 16/500\n",
      "122/122 [==============================] - 0s 709us/step - loss: 1114.7263 - mae: 25.9619 - val_loss: 480.6737 - val_mae: 16.5765\n",
      "Epoch 17/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 1075.6399 - mae: 25.6121 - val_loss: 427.3865 - val_mae: 15.2213\n",
      "Epoch 18/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 1013.8290 - mae: 24.6807 - val_loss: 364.9118 - val_mae: 14.1010\n",
      "Epoch 19/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 966.4724 - mae: 24.0439 - val_loss: 335.5279 - val_mae: 13.4892\n",
      "Epoch 20/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 892.3195 - mae: 22.8208 - val_loss: 295.1656 - val_mae: 12.7618\n",
      "Epoch 21/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 846.8456 - mae: 22.3751 - val_loss: 256.2476 - val_mae: 11.9379\n",
      "Epoch 22/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 830.4632 - mae: 22.2382 - val_loss: 233.6543 - val_mae: 11.0533\n",
      "Epoch 23/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 789.3392 - mae: 21.5435 - val_loss: 212.0439 - val_mae: 11.2268\n",
      "Epoch 24/500\n",
      "122/122 [==============================] - 0s 629us/step - loss: 775.4351 - mae: 21.3554 - val_loss: 182.7356 - val_mae: 9.8853\n",
      "Epoch 25/500\n",
      "122/122 [==============================] - 0s 593us/step - loss: 737.8557 - mae: 20.7223 - val_loss: 168.4588 - val_mae: 9.6848\n",
      "Epoch 26/500\n",
      "122/122 [==============================] - 0s 602us/step - loss: 714.3214 - mae: 20.1966 - val_loss: 149.6640 - val_mae: 9.4705\n",
      "Epoch 27/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 673.0352 - mae: 19.8084 - val_loss: 134.4991 - val_mae: 8.8992\n",
      "Epoch 28/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 681.9777 - mae: 19.7835 - val_loss: 112.5583 - val_mae: 7.7886\n",
      "Epoch 29/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 669.5207 - mae: 19.5454 - val_loss: 107.2998 - val_mae: 8.1026\n",
      "Epoch 30/500\n",
      "122/122 [==============================] - 0s 558us/step - loss: 648.4720 - mae: 19.1136 - val_loss: 96.1791 - val_mae: 7.6144\n",
      "Epoch 31/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 619.1289 - mae: 18.4850 - val_loss: 104.3210 - val_mae: 7.8339\n",
      "Epoch 32/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 605.7662 - mae: 18.5690 - val_loss: 79.7399 - val_mae: 6.6240\n",
      "Epoch 33/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 634.4883 - mae: 18.7235 - val_loss: 63.5000 - val_mae: 5.9407\n",
      "Epoch 34/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 579.2004 - mae: 17.6926 - val_loss: 64.3270 - val_mae: 6.0912\n",
      "Epoch 35/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 588.6898 - mae: 17.9699 - val_loss: 85.3337 - val_mae: 7.4237\n",
      "Epoch 36/500\n",
      "122/122 [==============================] - 0s 681us/step - loss: 600.6963 - mae: 17.9838 - val_loss: 71.1980 - val_mae: 6.8022\n",
      "Epoch 37/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 599.2450 - mae: 17.6147 - val_loss: 48.0799 - val_mae: 5.2741\n",
      "Epoch 38/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 558.4557 - mae: 17.2081 - val_loss: 62.8067 - val_mae: 6.4911\n",
      "Epoch 39/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 600.3103 - mae: 17.6746 - val_loss: 62.1718 - val_mae: 6.0651\n",
      "Epoch 40/500\n",
      "122/122 [==============================] - 0s 595us/step - loss: 558.2341 - mae: 17.0363 - val_loss: 57.3641 - val_mae: 6.0327\n",
      "Epoch 41/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 540.4865 - mae: 16.7973 - val_loss: 50.9465 - val_mae: 5.5528\n",
      "Epoch 42/500\n",
      "122/122 [==============================] - 0s 632us/step - loss: 567.8633 - mae: 17.1733 - val_loss: 43.4836 - val_mae: 5.1711\n",
      "Epoch 43/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 545.8261 - mae: 16.7962 - val_loss: 32.0256 - val_mae: 4.1624\n",
      "Epoch 44/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 535.9548 - mae: 16.5833 - val_loss: 34.4841 - val_mae: 4.3285\n",
      "Epoch 45/500\n",
      "122/122 [==============================] - 0s 652us/step - loss: 521.9064 - mae: 16.5246 - val_loss: 33.9905 - val_mae: 4.3347\n",
      "Epoch 46/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 506.5890 - mae: 16.2420 - val_loss: 29.8609 - val_mae: 4.1220\n",
      "Epoch 47/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 531.5486 - mae: 16.5248 - val_loss: 30.3986 - val_mae: 4.0549\n",
      "Epoch 48/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 512.9292 - mae: 16.2606 - val_loss: 39.3663 - val_mae: 4.8202\n",
      "Epoch 49/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 497.6468 - mae: 15.9554 - val_loss: 30.2163 - val_mae: 4.0550\n",
      "Epoch 50/500\n",
      "122/122 [==============================] - 0s 611us/step - loss: 508.2274 - mae: 15.9624 - val_loss: 36.7877 - val_mae: 4.6838\n",
      "Epoch 51/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 512.1797 - mae: 15.9913 - val_loss: 26.2988 - val_mae: 3.6965\n",
      "Epoch 52/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 526.9497 - mae: 16.2748 - val_loss: 31.7910 - val_mae: 4.3121\n",
      "Epoch 53/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 496.7632 - mae: 15.8361 - val_loss: 28.8383 - val_mae: 4.0052\n",
      "Epoch 54/500\n",
      "122/122 [==============================] - 0s 651us/step - loss: 490.2929 - mae: 15.6716 - val_loss: 42.3439 - val_mae: 5.0590\n",
      "Epoch 55/500\n",
      "122/122 [==============================] - 0s 607us/step - loss: 479.1134 - mae: 15.6203 - val_loss: 27.9571 - val_mae: 3.7928\n",
      "Epoch 56/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 505.3614 - mae: 15.8162 - val_loss: 27.0118 - val_mae: 3.9293\n",
      "Epoch 57/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 530.2465 - mae: 16.1852 - val_loss: 41.8314 - val_mae: 5.2538\n",
      "Epoch 58/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 482.8678 - mae: 15.6440 - val_loss: 33.1802 - val_mae: 4.4098\n",
      "Epoch 59/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 488.7050 - mae: 15.4359 - val_loss: 27.8116 - val_mae: 3.7836\n",
      "Epoch 60/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 480.6756 - mae: 15.4270 - val_loss: 41.4627 - val_mae: 5.0238\n",
      "Epoch 61/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 483.1400 - mae: 15.6121 - val_loss: 24.3068 - val_mae: 3.4858\n",
      "Epoch 62/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 461.5963 - mae: 15.1792 - val_loss: 27.6777 - val_mae: 3.9316\n",
      "Epoch 63/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 452.1654 - mae: 15.1256 - val_loss: 25.1332 - val_mae: 3.5201\n",
      "Epoch 64/500\n",
      "122/122 [==============================] - 0s 645us/step - loss: 466.3665 - mae: 15.1223 - val_loss: 33.5852 - val_mae: 4.4420\n",
      "Epoch 65/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 458.3938 - mae: 14.9819 - val_loss: 39.5103 - val_mae: 4.8445\n",
      "Epoch 66/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 441.6710 - mae: 14.9106 - val_loss: 28.9788 - val_mae: 3.9843\n",
      "Epoch 67/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 459.0016 - mae: 14.9338 - val_loss: 25.5644 - val_mae: 3.7515\n",
      "Epoch 68/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 481.7159 - mae: 15.2330 - val_loss: 40.0574 - val_mae: 4.5315\n",
      "Epoch 69/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 477.2934 - mae: 15.0536 - val_loss: 36.5228 - val_mae: 4.5147\n",
      "Epoch 70/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 462.0193 - mae: 14.9948 - val_loss: 24.2227 - val_mae: 3.6335\n",
      "Epoch 71/500\n",
      "122/122 [==============================] - 0s 659us/step - loss: 442.3869 - mae: 14.6966 - val_loss: 25.3361 - val_mae: 3.6840\n",
      "Epoch 72/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 471.4481 - mae: 15.1932 - val_loss: 21.9772 - val_mae: 3.3234\n",
      "Epoch 73/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 473.5403 - mae: 14.9589 - val_loss: 23.9874 - val_mae: 3.5100\n",
      "Epoch 74/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 445.9279 - mae: 14.8756 - val_loss: 39.9067 - val_mae: 4.8595\n",
      "Epoch 75/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 442.8199 - mae: 14.3752 - val_loss: 22.0651 - val_mae: 3.3066\n",
      "Epoch 76/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 452.8633 - mae: 14.6576 - val_loss: 27.0957 - val_mae: 3.9656\n",
      "Epoch 77/500\n",
      "122/122 [==============================] - 0s 612us/step - loss: 446.5650 - mae: 14.7176 - val_loss: 49.9461 - val_mae: 5.2259\n",
      "Epoch 78/500\n",
      "122/122 [==============================] - 0s 657us/step - loss: 440.0579 - mae: 14.4942 - val_loss: 29.2644 - val_mae: 3.9663\n",
      "Epoch 79/500\n",
      "122/122 [==============================] - 0s 597us/step - loss: 440.6684 - mae: 14.4142 - val_loss: 29.6592 - val_mae: 4.0869\n",
      "Epoch 80/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 408.7294 - mae: 13.8399 - val_loss: 30.0512 - val_mae: 4.0275\n",
      "Epoch 81/500\n",
      "122/122 [==============================] - 0s 598us/step - loss: 429.6974 - mae: 14.3123 - val_loss: 25.9115 - val_mae: 3.5473\n",
      "Epoch 82/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 435.4784 - mae: 14.3459 - val_loss: 26.1626 - val_mae: 3.7033\n",
      "Epoch 83/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 441.8225 - mae: 14.2428 - val_loss: 23.3518 - val_mae: 3.3977\n",
      "Epoch 84/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 429.8905 - mae: 14.1098 - val_loss: 29.4673 - val_mae: 4.0482\n",
      "Epoch 85/500\n",
      "122/122 [==============================] - 0s 706us/step - loss: 398.6449 - mae: 13.7317 - val_loss: 29.8949 - val_mae: 4.1249\n",
      "Epoch 86/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 431.6796 - mae: 14.1151 - val_loss: 37.7810 - val_mae: 4.7006\n",
      "Epoch 87/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 427.2231 - mae: 14.2968 - val_loss: 24.9458 - val_mae: 3.5634\n",
      "Epoch 88/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 408.7016 - mae: 13.9504 - val_loss: 30.8653 - val_mae: 4.1739\n",
      "Epoch 89/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 428.6936 - mae: 14.0319 - val_loss: 23.1191 - val_mae: 3.3842\n",
      "Epoch 90/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 416.1162 - mae: 13.9894 - val_loss: 30.2789 - val_mae: 4.0634\n",
      "Epoch 91/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 402.4083 - mae: 13.7472 - val_loss: 28.7376 - val_mae: 3.8912\n",
      "Epoch 92/500\n",
      "122/122 [==============================] - 0s 615us/step - loss: 413.2523 - mae: 13.9244 - val_loss: 25.9547 - val_mae: 3.5646\n",
      "Epoch 93/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 415.9628 - mae: 13.8279 - val_loss: 24.4007 - val_mae: 3.5613\n",
      "Epoch 94/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 407.8407 - mae: 13.9449 - val_loss: 32.2969 - val_mae: 3.9259\n",
      "Epoch 95/500\n",
      "122/122 [==============================] - 0s 683us/step - loss: 416.5703 - mae: 13.9854 - val_loss: 27.6817 - val_mae: 3.8918\n",
      "Epoch 96/500\n",
      "122/122 [==============================] - 0s 611us/step - loss: 400.7094 - mae: 13.5686 - val_loss: 25.3213 - val_mae: 3.6596\n",
      "Epoch 97/500\n",
      "122/122 [==============================] - 0s 585us/step - loss: 390.3646 - mae: 13.4122 - val_loss: 23.3917 - val_mae: 3.3969\n",
      "Epoch 98/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 418.0040 - mae: 13.6495 - val_loss: 24.4452 - val_mae: 3.6124\n",
      "Epoch 99/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 402.7322 - mae: 13.7805 - val_loss: 36.8983 - val_mae: 4.5614\n",
      "Epoch 100/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 388.9003 - mae: 13.5472 - val_loss: 23.6240 - val_mae: 3.3796\n",
      "Epoch 101/500\n",
      "122/122 [==============================] - 0s 561us/step - loss: 379.7928 - mae: 13.3068 - val_loss: 24.5790 - val_mae: 3.5910\n",
      "Epoch 102/500\n",
      "122/122 [==============================] - 0s 605us/step - loss: 374.3044 - mae: 13.2770 - val_loss: 30.8475 - val_mae: 4.1843\n",
      "Epoch 103/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 377.5313 - mae: 13.0886 - val_loss: 31.5954 - val_mae: 4.1684\n",
      "Epoch 104/500\n",
      "122/122 [==============================] - 0s 678us/step - loss: 385.8026 - mae: 13.3389 - val_loss: 25.7986 - val_mae: 3.5902\n",
      "Epoch 105/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 414.5018 - mae: 13.6693 - val_loss: 23.6609 - val_mae: 3.4533\n",
      "Epoch 106/500\n",
      "122/122 [==============================] - 0s 584us/step - loss: 378.4541 - mae: 13.0936 - val_loss: 24.3178 - val_mae: 3.5559\n",
      "Epoch 107/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 369.1852 - mae: 13.0314 - val_loss: 26.6324 - val_mae: 3.6782\n",
      "Epoch 108/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 384.3068 - mae: 13.2708 - val_loss: 25.0156 - val_mae: 3.6259\n",
      "Epoch 109/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 382.3557 - mae: 13.1048 - val_loss: 22.0732 - val_mae: 3.3505\n",
      "Epoch 110/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 394.1596 - mae: 13.4226 - val_loss: 23.7975 - val_mae: 3.5811\n",
      "Epoch 111/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 378.4059 - mae: 13.1900 - val_loss: 34.5763 - val_mae: 4.4443\n",
      "Epoch 112/500\n",
      "122/122 [==============================] - 0s 591us/step - loss: 351.8840 - mae: 12.8408 - val_loss: 24.0642 - val_mae: 3.4974\n",
      "Epoch 113/500\n",
      "122/122 [==============================] - 0s 660us/step - loss: 373.6210 - mae: 12.8598 - val_loss: 27.6537 - val_mae: 3.8413\n",
      "Epoch 114/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 347.5653 - mae: 12.6318 - val_loss: 31.3204 - val_mae: 4.2772\n",
      "Epoch 115/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 373.5379 - mae: 13.1173 - val_loss: 24.4932 - val_mae: 3.6431\n",
      "Epoch 116/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 365.8309 - mae: 12.8800 - val_loss: 24.6443 - val_mae: 3.5439\n",
      "Epoch 117/500\n",
      "122/122 [==============================] - 0s 652us/step - loss: 380.4086 - mae: 13.0086 - val_loss: 24.6488 - val_mae: 3.6650\n",
      "Epoch 118/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 363.0066 - mae: 12.7659 - val_loss: 29.1892 - val_mae: 3.9809\n",
      "Epoch 119/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 347.8093 - mae: 12.5124 - val_loss: 25.4691 - val_mae: 3.6914\n",
      "Epoch 120/500\n",
      "122/122 [==============================] - 0s 627us/step - loss: 354.4821 - mae: 12.6388 - val_loss: 23.9896 - val_mae: 3.4490\n",
      "Epoch 121/500\n",
      "122/122 [==============================] - 0s 615us/step - loss: 356.2945 - mae: 12.5845 - val_loss: 25.9749 - val_mae: 3.6399\n",
      "Epoch 122/500\n",
      "122/122 [==============================] - 0s 611us/step - loss: 350.0983 - mae: 12.6583 - val_loss: 37.6922 - val_mae: 4.6418\n",
      "Epoch 123/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 361.4317 - mae: 12.6229 - val_loss: 25.7073 - val_mae: 3.7664\n",
      "Epoch 124/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 355.7396 - mae: 12.6071 - val_loss: 27.6788 - val_mae: 3.8967\n",
      "Epoch 125/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 352.8385 - mae: 12.6222 - val_loss: 23.9940 - val_mae: 3.5190\n",
      "Epoch 126/500\n",
      "122/122 [==============================] - 0s 600us/step - loss: 332.1236 - mae: 12.2543 - val_loss: 29.7518 - val_mae: 4.0974\n",
      "Epoch 127/500\n",
      "122/122 [==============================] - 0s 592us/step - loss: 335.5909 - mae: 12.3177 - val_loss: 35.8972 - val_mae: 4.6267\n",
      "Epoch 128/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 327.3032 - mae: 12.1042 - val_loss: 26.0975 - val_mae: 3.7275\n",
      "Epoch 129/500\n",
      "122/122 [==============================] - 0s 657us/step - loss: 349.8879 - mae: 12.6141 - val_loss: 33.5703 - val_mae: 4.3698\n",
      "Epoch 130/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 325.4651 - mae: 12.1743 - val_loss: 30.3579 - val_mae: 4.1753\n",
      "Epoch 131/500\n",
      "122/122 [==============================] - 0s 602us/step - loss: 324.3138 - mae: 12.1701 - val_loss: 29.1205 - val_mae: 4.0756\n",
      "Epoch 132/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 319.8376 - mae: 12.0841 - val_loss: 32.3996 - val_mae: 4.3152\n",
      "Epoch 133/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 318.2985 - mae: 12.1316 - val_loss: 30.8761 - val_mae: 4.2021\n",
      "Epoch 134/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 325.4564 - mae: 12.0425 - val_loss: 25.4078 - val_mae: 3.7584\n",
      "Epoch 135/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 330.1485 - mae: 12.2140 - val_loss: 22.7860 - val_mae: 3.4025\n",
      "Epoch 136/500\n",
      "122/122 [==============================] - 0s 613us/step - loss: 320.0682 - mae: 11.9013 - val_loss: 30.6205 - val_mae: 4.0080\n",
      "Epoch 137/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 301.3912 - mae: 11.7146 - val_loss: 33.5412 - val_mae: 4.3931\n",
      "Epoch 138/500\n",
      "122/122 [==============================] - 0s 648us/step - loss: 341.0305 - mae: 12.2767 - val_loss: 35.2074 - val_mae: 4.5038\n",
      "Epoch 139/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 317.7814 - mae: 12.0221 - val_loss: 27.4723 - val_mae: 3.7690\n",
      "Epoch 140/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 341.9196 - mae: 12.1458 - val_loss: 29.8916 - val_mae: 4.0358\n",
      "Epoch 141/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 324.3460 - mae: 11.9419 - val_loss: 28.8573 - val_mae: 4.0121\n",
      "Epoch 142/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 314.9084 - mae: 11.9159 - val_loss: 25.6361 - val_mae: 3.6984\n",
      "Epoch 143/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 325.3320 - mae: 11.9880 - val_loss: 26.1587 - val_mae: 3.5981\n",
      "Epoch 144/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 310.8335 - mae: 11.7794 - val_loss: 23.7660 - val_mae: 3.5301\n",
      "Epoch 145/500\n",
      "122/122 [==============================] - 0s 594us/step - loss: 304.6259 - mae: 11.8253 - val_loss: 23.7034 - val_mae: 3.5555\n",
      "Epoch 146/500\n",
      "122/122 [==============================] - 0s 645us/step - loss: 299.1505 - mae: 11.5846 - val_loss: 35.3134 - val_mae: 4.5372\n",
      "Epoch 147/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 325.7859 - mae: 11.9235 - val_loss: 34.0327 - val_mae: 4.4721\n",
      "Epoch 148/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 320.3813 - mae: 11.7154 - val_loss: 24.9731 - val_mae: 3.6018\n",
      "Epoch 149/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 307.9801 - mae: 11.6271 - val_loss: 29.4550 - val_mae: 4.0130\n",
      "Epoch 150/500\n",
      "122/122 [==============================] - 0s 607us/step - loss: 299.6682 - mae: 11.4829 - val_loss: 24.9546 - val_mae: 3.5798\n",
      "Epoch 151/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 286.9238 - mae: 11.3279 - val_loss: 37.1821 - val_mae: 4.6839\n",
      "Epoch 152/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 291.4218 - mae: 11.5168 - val_loss: 29.6726 - val_mae: 4.0817\n",
      "Epoch 153/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 294.2132 - mae: 11.5490 - val_loss: 32.0416 - val_mae: 4.1752\n",
      "Epoch 154/500\n",
      "122/122 [==============================] - 0s 648us/step - loss: 296.3682 - mae: 11.4275 - val_loss: 45.7262 - val_mae: 5.1641\n",
      "Epoch 155/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 297.2718 - mae: 11.3902 - val_loss: 22.7164 - val_mae: 3.3862\n",
      "Epoch 156/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 291.5588 - mae: 11.4837 - val_loss: 30.2675 - val_mae: 4.1691\n",
      "Epoch 157/500\n",
      "122/122 [==============================] - 0s 585us/step - loss: 297.7468 - mae: 11.3794 - val_loss: 23.2796 - val_mae: 3.4710\n",
      "Epoch 158/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 286.6085 - mae: 11.3015 - val_loss: 23.9700 - val_mae: 3.5457\n",
      "Epoch 159/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 312.0860 - mae: 11.5446 - val_loss: 24.6680 - val_mae: 3.6558\n",
      "Epoch 160/500\n",
      "122/122 [==============================] - 0s 612us/step - loss: 293.6643 - mae: 11.3945 - val_loss: 28.5784 - val_mae: 4.0182\n",
      "Epoch 161/500\n",
      "122/122 [==============================] - 0s 642us/step - loss: 294.3513 - mae: 11.2414 - val_loss: 28.5031 - val_mae: 3.7937\n",
      "Epoch 162/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 272.6252 - mae: 11.0020 - val_loss: 34.1408 - val_mae: 4.5719\n",
      "Epoch 163/500\n",
      "122/122 [==============================] - 0s 619us/step - loss: 301.1227 - mae: 11.3944 - val_loss: 27.5357 - val_mae: 3.7715\n",
      "Epoch 164/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 282.4966 - mae: 11.2162 - val_loss: 29.6761 - val_mae: 4.0576\n",
      "Epoch 165/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 279.4751 - mae: 11.0611 - val_loss: 26.6410 - val_mae: 3.7165\n",
      "Epoch 166/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 277.1544 - mae: 10.9213 - val_loss: 22.9618 - val_mae: 3.4002\n",
      "Epoch 167/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 262.2975 - mae: 10.7945 - val_loss: 24.0524 - val_mae: 3.5206\n",
      "Epoch 168/500\n",
      "122/122 [==============================] - 0s 695us/step - loss: 257.8483 - mae: 10.8201 - val_loss: 26.2460 - val_mae: 3.6392\n",
      "Epoch 169/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 275.1299 - mae: 10.8227 - val_loss: 28.8073 - val_mae: 3.9917\n",
      "Epoch 170/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 265.6877 - mae: 10.6936 - val_loss: 25.5325 - val_mae: 3.6007\n",
      "Epoch 171/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 266.2358 - mae: 10.7375 - val_loss: 25.6067 - val_mae: 3.5181\n",
      "Epoch 172/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 281.2717 - mae: 10.9228 - val_loss: 24.0888 - val_mae: 3.4711\n",
      "Epoch 173/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 260.0100 - mae: 10.6792 - val_loss: 34.5888 - val_mae: 4.5145\n",
      "Epoch 174/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 262.5275 - mae: 10.7300 - val_loss: 36.1083 - val_mae: 4.5853\n",
      "Epoch 175/500\n",
      "122/122 [==============================] - 0s 638us/step - loss: 247.5265 - mae: 10.4068 - val_loss: 25.1940 - val_mae: 3.6481\n",
      "Epoch 176/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 266.1867 - mae: 10.8362 - val_loss: 29.0798 - val_mae: 4.1058\n",
      "Epoch 177/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 263.7251 - mae: 10.5788 - val_loss: 29.6987 - val_mae: 3.7812\n",
      "Epoch 178/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 273.2410 - mae: 10.7555 - val_loss: 27.5479 - val_mae: 3.9329\n",
      "Epoch 179/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 260.0431 - mae: 10.6603 - val_loss: 27.0339 - val_mae: 3.7587\n",
      "Epoch 180/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 239.3693 - mae: 10.2140 - val_loss: 23.5283 - val_mae: 3.4884\n",
      "Epoch 181/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 259.6052 - mae: 10.5246 - val_loss: 24.0625 - val_mae: 3.5193\n",
      "Epoch 182/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 255.7938 - mae: 10.3863 - val_loss: 23.8070 - val_mae: 3.4534\n",
      "Epoch 183/500\n",
      "122/122 [==============================] - 0s 681us/step - loss: 247.2289 - mae: 10.3154 - val_loss: 33.5745 - val_mae: 4.4564\n",
      "Epoch 184/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 259.6424 - mae: 10.3539 - val_loss: 26.0024 - val_mae: 3.7207\n",
      "Epoch 185/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 254.5509 - mae: 10.4484 - val_loss: 31.7096 - val_mae: 4.0890\n",
      "Epoch 186/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 251.7299 - mae: 10.4099 - val_loss: 29.6494 - val_mae: 3.9448\n",
      "Epoch 187/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 233.7072 - mae: 10.2119 - val_loss: 33.8880 - val_mae: 4.4739\n",
      "Epoch 188/500\n",
      "122/122 [==============================] - 0s 609us/step - loss: 238.6519 - mae: 10.0718 - val_loss: 25.4685 - val_mae: 3.6642\n",
      "Epoch 189/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 248.9209 - mae: 10.2603 - val_loss: 24.4971 - val_mae: 3.5379\n",
      "Epoch 190/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 234.2740 - mae: 9.9011 - val_loss: 53.2583 - val_mae: 5.5865\n",
      "Epoch 191/500\n",
      "122/122 [==============================] - 0s 667us/step - loss: 239.1455 - mae: 10.0156 - val_loss: 26.6191 - val_mae: 3.6770\n",
      "Epoch 192/500\n",
      "122/122 [==============================] - 0s 612us/step - loss: 243.3222 - mae: 10.0080 - val_loss: 25.6102 - val_mae: 3.6030\n",
      "Epoch 193/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 235.6787 - mae: 10.1430 - val_loss: 27.4222 - val_mae: 3.7408\n",
      "Epoch 194/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 232.9958 - mae: 10.0998 - val_loss: 32.4428 - val_mae: 4.1411\n",
      "Epoch 195/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 227.6085 - mae: 9.8471 - val_loss: 27.5853 - val_mae: 3.8370\n",
      "Epoch 196/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 237.2919 - mae: 10.1034 - val_loss: 24.9106 - val_mae: 3.6054\n",
      "Epoch 197/500\n",
      "122/122 [==============================] - 0s 646us/step - loss: 227.7969 - mae: 9.7517 - val_loss: 33.1271 - val_mae: 4.3781\n",
      "Epoch 198/500\n",
      "122/122 [==============================] - 0s 675us/step - loss: 236.4078 - mae: 10.0172 - val_loss: 25.9521 - val_mae: 3.6125\n",
      "Epoch 199/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 229.6207 - mae: 9.7915 - val_loss: 25.4252 - val_mae: 3.5728\n",
      "Epoch 200/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 225.5464 - mae: 9.8439 - val_loss: 26.7434 - val_mae: 3.7205\n",
      "Epoch 201/500\n",
      "122/122 [==============================] - 0s 616us/step - loss: 221.9092 - mae: 9.7155 - val_loss: 23.4571 - val_mae: 3.4398\n",
      "Epoch 202/500\n",
      "122/122 [==============================] - 0s 613us/step - loss: 230.3598 - mae: 9.8232 - val_loss: 29.3223 - val_mae: 4.0932\n",
      "Epoch 203/500\n",
      "122/122 [==============================] - 0s 591us/step - loss: 223.4574 - mae: 9.6306 - val_loss: 24.2524 - val_mae: 3.5061\n",
      "Epoch 204/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 243.8372 - mae: 9.9543 - val_loss: 27.6486 - val_mae: 3.8394\n",
      "Epoch 205/500\n",
      "122/122 [==============================] - 0s 683us/step - loss: 226.4753 - mae: 9.5949 - val_loss: 27.1861 - val_mae: 3.6692\n",
      "Epoch 206/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 228.1276 - mae: 9.7660 - val_loss: 25.1426 - val_mae: 3.5973\n",
      "Epoch 207/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 213.9150 - mae: 9.5760 - val_loss: 26.8937 - val_mae: 3.7989\n",
      "Epoch 208/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 206.2086 - mae: 9.3981 - val_loss: 29.4257 - val_mae: 4.0488\n",
      "Epoch 209/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 215.2544 - mae: 9.5197 - val_loss: 24.5243 - val_mae: 3.5665\n",
      "Epoch 210/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 214.7626 - mae: 9.4765 - val_loss: 29.0469 - val_mae: 3.8111\n",
      "Epoch 211/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 221.5313 - mae: 9.5872 - val_loss: 27.7751 - val_mae: 3.6875\n",
      "Epoch 212/500\n",
      "122/122 [==============================] - 0s 644us/step - loss: 226.9880 - mae: 9.7423 - val_loss: 29.8464 - val_mae: 4.0098\n",
      "Epoch 213/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 210.2697 - mae: 9.2554 - val_loss: 36.3679 - val_mae: 4.4043\n",
      "Epoch 214/500\n",
      "122/122 [==============================] - 0s 600us/step - loss: 216.3594 - mae: 9.3688 - val_loss: 37.3706 - val_mae: 4.6187\n",
      "Epoch 215/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 205.9627 - mae: 9.4057 - val_loss: 55.4820 - val_mae: 5.7465\n",
      "Epoch 216/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 213.3154 - mae: 9.5272 - val_loss: 27.7159 - val_mae: 3.7300\n",
      "Epoch 217/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 207.8563 - mae: 9.3704 - val_loss: 39.4448 - val_mae: 4.8339\n",
      "Epoch 218/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 216.6953 - mae: 9.4960 - val_loss: 32.9958 - val_mae: 4.1761\n",
      "Epoch 219/500\n",
      "122/122 [==============================] - 0s 601us/step - loss: 200.1185 - mae: 9.2205 - val_loss: 26.6989 - val_mae: 3.6128\n",
      "Epoch 220/500\n",
      "122/122 [==============================] - 0s 684us/step - loss: 205.6345 - mae: 9.3016 - val_loss: 32.5233 - val_mae: 4.3256\n",
      "Epoch 221/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 210.0363 - mae: 9.4808 - val_loss: 31.0110 - val_mae: 4.0134\n",
      "Epoch 222/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 201.8707 - mae: 9.2224 - val_loss: 41.2017 - val_mae: 5.0053\n",
      "Epoch 223/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 217.8857 - mae: 9.3663 - val_loss: 29.3406 - val_mae: 4.0797\n",
      "Epoch 224/500\n",
      "122/122 [==============================] - 0s 602us/step - loss: 200.8658 - mae: 9.1931 - val_loss: 29.6809 - val_mae: 4.1111\n",
      "Epoch 225/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 205.1617 - mae: 9.0976 - val_loss: 29.2215 - val_mae: 3.8456\n",
      "Epoch 226/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 199.0799 - mae: 9.0916 - val_loss: 31.3016 - val_mae: 4.0749\n",
      "Epoch 227/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 206.0223 - mae: 9.2836 - val_loss: 25.6170 - val_mae: 3.6856\n",
      "Epoch 228/500\n",
      "122/122 [==============================] - 0s 687us/step - loss: 209.8299 - mae: 9.2646 - val_loss: 30.0849 - val_mae: 4.0497\n",
      "Epoch 229/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 201.3260 - mae: 9.1668 - val_loss: 28.0842 - val_mae: 3.8722\n",
      "Epoch 230/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 188.3024 - mae: 8.8792 - val_loss: 24.4816 - val_mae: 3.5337\n",
      "Epoch 231/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 203.4792 - mae: 9.0885 - val_loss: 25.0513 - val_mae: 3.6065\n",
      "Epoch 232/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 200.0153 - mae: 8.9760 - val_loss: 26.4197 - val_mae: 3.7387\n",
      "Epoch 233/500\n",
      "122/122 [==============================] - 0s 584us/step - loss: 202.2960 - mae: 9.1808 - val_loss: 26.4714 - val_mae: 3.7785\n",
      "Epoch 234/500\n",
      "122/122 [==============================] - 0s 603us/step - loss: 182.6910 - mae: 8.7997 - val_loss: 26.1798 - val_mae: 3.6815\n",
      "Epoch 235/500\n",
      "122/122 [==============================] - 0s 666us/step - loss: 193.4469 - mae: 8.9593 - val_loss: 28.8395 - val_mae: 3.8018\n",
      "Epoch 236/500\n",
      "122/122 [==============================] - 0s 591us/step - loss: 202.9198 - mae: 9.0258 - val_loss: 25.7619 - val_mae: 3.6948\n",
      "Epoch 237/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 189.7351 - mae: 8.9463 - val_loss: 29.3803 - val_mae: 3.9560\n",
      "Epoch 238/500\n",
      "122/122 [==============================] - 0s 606us/step - loss: 186.5833 - mae: 8.9220 - val_loss: 27.6337 - val_mae: 3.7227\n",
      "Epoch 239/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 203.0759 - mae: 9.1081 - val_loss: 33.5631 - val_mae: 4.4518\n",
      "Epoch 240/500\n",
      "122/122 [==============================] - 0s 559us/step - loss: 191.9800 - mae: 8.9777 - val_loss: 23.7119 - val_mae: 3.4790\n",
      "Epoch 241/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 187.0156 - mae: 8.8754 - val_loss: 30.9460 - val_mae: 4.0212\n",
      "Epoch 242/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 175.5969 - mae: 8.6361 - val_loss: 27.6473 - val_mae: 3.8544\n",
      "Epoch 243/500\n",
      "122/122 [==============================] - 0s 591us/step - loss: 175.0380 - mae: 8.5873 - val_loss: 27.7933 - val_mae: 3.9123\n",
      "Epoch 244/500\n",
      "122/122 [==============================] - 0s 662us/step - loss: 185.8550 - mae: 8.8319 - val_loss: 31.1855 - val_mae: 4.1181\n",
      "Epoch 245/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 192.3244 - mae: 8.9157 - val_loss: 29.7947 - val_mae: 4.0687\n",
      "Epoch 246/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 182.4402 - mae: 8.7332 - val_loss: 38.7948 - val_mae: 4.8614\n",
      "Epoch 247/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 176.8439 - mae: 8.7004 - val_loss: 34.7869 - val_mae: 4.3034\n",
      "Epoch 248/500\n",
      "122/122 [==============================] - 0s 615us/step - loss: 176.4822 - mae: 8.5821 - val_loss: 33.8427 - val_mae: 4.2496\n",
      "Epoch 249/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 178.4050 - mae: 8.6670 - val_loss: 41.6790 - val_mae: 5.0788\n",
      "Epoch 250/500\n",
      "122/122 [==============================] - 0s 629us/step - loss: 163.6100 - mae: 8.3491 - val_loss: 29.6745 - val_mae: 4.0582\n",
      "Epoch 251/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 182.8049 - mae: 8.5796 - val_loss: 31.7528 - val_mae: 4.2825\n",
      "Epoch 252/500\n",
      "122/122 [==============================] - 0s 691us/step - loss: 174.9588 - mae: 8.6281 - val_loss: 26.0788 - val_mae: 3.6874\n",
      "Epoch 253/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 190.6214 - mae: 8.8747 - val_loss: 26.1168 - val_mae: 3.7073\n",
      "Epoch 254/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 188.8866 - mae: 8.7303 - val_loss: 32.7655 - val_mae: 4.3906\n",
      "Epoch 255/500\n",
      "122/122 [==============================] - 0s 561us/step - loss: 176.0865 - mae: 8.7580 - val_loss: 30.1911 - val_mae: 3.9009\n",
      "Epoch 256/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 180.4023 - mae: 8.6190 - val_loss: 37.0961 - val_mae: 4.7438\n",
      "Epoch 257/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 168.8063 - mae: 8.4961 - val_loss: 33.6884 - val_mae: 4.5208\n",
      "Epoch 258/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 167.9358 - mae: 8.4523 - val_loss: 32.8277 - val_mae: 4.3016\n",
      "Epoch 259/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 172.2501 - mae: 8.4870 - val_loss: 29.0536 - val_mae: 3.9270\n",
      "Epoch 260/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 165.7587 - mae: 8.3212 - val_loss: 25.1983 - val_mae: 3.6563\n",
      "Epoch 261/500\n",
      "122/122 [==============================] - 0s 656us/step - loss: 181.6992 - mae: 8.5324 - val_loss: 31.1631 - val_mae: 4.1990\n",
      "Epoch 262/500\n",
      "122/122 [==============================] - 0s 591us/step - loss: 172.6493 - mae: 8.5941 - val_loss: 35.4032 - val_mae: 4.6204\n",
      "Epoch 263/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 175.1166 - mae: 8.5398 - val_loss: 30.6534 - val_mae: 3.9947\n",
      "Epoch 264/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 165.8119 - mae: 8.4252 - val_loss: 28.8334 - val_mae: 3.9258\n",
      "Epoch 265/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 163.1835 - mae: 8.2553 - val_loss: 27.0807 - val_mae: 3.8221\n",
      "Epoch 266/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 167.8599 - mae: 8.4937 - val_loss: 29.5486 - val_mae: 3.9129\n",
      "Epoch 267/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 183.2022 - mae: 8.5093 - val_loss: 31.3493 - val_mae: 4.1868\n",
      "Epoch 268/500\n",
      "122/122 [==============================] - 0s 602us/step - loss: 172.1229 - mae: 8.4669 - val_loss: 30.9558 - val_mae: 4.0868\n",
      "Epoch 269/500\n",
      "122/122 [==============================] - 0s 661us/step - loss: 168.8622 - mae: 8.2657 - val_loss: 26.0591 - val_mae: 3.7171\n",
      "Epoch 270/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 150.0427 - mae: 8.0876 - val_loss: 34.5233 - val_mae: 4.3024\n",
      "Epoch 271/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 173.3444 - mae: 8.4181 - val_loss: 34.4429 - val_mae: 4.5705\n",
      "Epoch 272/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 152.9308 - mae: 8.1087 - val_loss: 31.7484 - val_mae: 4.1625\n",
      "Epoch 273/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 158.5885 - mae: 8.1648 - val_loss: 29.2887 - val_mae: 4.0900\n",
      "Epoch 274/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 159.1987 - mae: 8.1529 - val_loss: 31.2865 - val_mae: 3.9927\n",
      "Epoch 275/500\n",
      "122/122 [==============================] - 0s 592us/step - loss: 152.9007 - mae: 8.0000 - val_loss: 31.6746 - val_mae: 4.2196\n",
      "Epoch 276/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 157.5352 - mae: 8.2551 - val_loss: 35.5397 - val_mae: 4.5931\n",
      "Epoch 277/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 168.6353 - mae: 8.3520 - val_loss: 35.5839 - val_mae: 4.6092\n",
      "Epoch 278/500\n",
      "122/122 [==============================] - 0s 727us/step - loss: 161.4673 - mae: 8.2036 - val_loss: 27.6143 - val_mae: 3.7323\n",
      "Epoch 279/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 144.5130 - mae: 7.9412 - val_loss: 28.1759 - val_mae: 3.8390\n",
      "Epoch 280/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 156.2422 - mae: 8.2155 - val_loss: 29.7106 - val_mae: 3.8694\n",
      "Epoch 281/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 157.9537 - mae: 8.0981 - val_loss: 26.1098 - val_mae: 3.7800\n",
      "Epoch 282/500\n",
      "122/122 [==============================] - 0s 614us/step - loss: 157.4022 - mae: 8.1499 - val_loss: 32.5006 - val_mae: 4.3656\n",
      "Epoch 283/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 148.0245 - mae: 7.8781 - val_loss: 26.5152 - val_mae: 3.7576\n",
      "Epoch 284/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 143.7295 - mae: 7.9626 - val_loss: 26.2311 - val_mae: 3.8043\n",
      "Epoch 285/500\n",
      "122/122 [==============================] - 0s 572us/step - loss: 155.9411 - mae: 8.1718 - val_loss: 28.9229 - val_mae: 3.9428\n",
      "Epoch 286/500\n",
      "122/122 [==============================] - 0s 698us/step - loss: 148.9560 - mae: 8.0380 - val_loss: 28.8562 - val_mae: 3.9028\n",
      "Epoch 287/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 153.2144 - mae: 8.0913 - val_loss: 27.4714 - val_mae: 3.8420\n",
      "Epoch 288/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 148.1912 - mae: 8.0832 - val_loss: 26.5999 - val_mae: 3.7115\n",
      "Epoch 289/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 146.0092 - mae: 7.9320 - val_loss: 46.2139 - val_mae: 5.2982\n",
      "Epoch 290/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 157.1385 - mae: 8.1399 - val_loss: 26.3621 - val_mae: 3.7297\n",
      "Epoch 291/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 148.7628 - mae: 7.9748 - val_loss: 24.8089 - val_mae: 3.5864\n",
      "Epoch 292/500\n",
      "122/122 [==============================] - 0s 601us/step - loss: 139.5997 - mae: 7.8865 - val_loss: 31.1941 - val_mae: 4.1312\n",
      "Epoch 293/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 136.2778 - mae: 7.7186 - val_loss: 32.6547 - val_mae: 4.2208\n",
      "Epoch 294/500\n",
      "122/122 [==============================] - 0s 665us/step - loss: 150.0549 - mae: 7.9257 - val_loss: 27.9327 - val_mae: 3.7733\n",
      "Epoch 295/500\n",
      "122/122 [==============================] - 0s 615us/step - loss: 141.2916 - mae: 7.8190 - val_loss: 32.7406 - val_mae: 4.2682\n",
      "Epoch 296/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 148.9794 - mae: 7.9298 - val_loss: 35.8348 - val_mae: 4.5525\n",
      "Epoch 297/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 144.7159 - mae: 8.0205 - val_loss: 32.0506 - val_mae: 4.2917\n",
      "Epoch 298/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 142.6513 - mae: 7.8318 - val_loss: 30.9319 - val_mae: 4.0882\n",
      "Epoch 299/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 146.8549 - mae: 7.9790 - val_loss: 30.2093 - val_mae: 4.0105\n",
      "Epoch 300/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 137.1974 - mae: 7.7839 - val_loss: 25.5718 - val_mae: 3.6207\n",
      "Epoch 301/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 142.0420 - mae: 7.8542 - val_loss: 40.3345 - val_mae: 5.0125\n",
      "Epoch 302/500\n",
      "122/122 [==============================] - 0s 645us/step - loss: 137.3786 - mae: 7.7781 - val_loss: 27.0546 - val_mae: 3.7529\n",
      "Epoch 303/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 134.8136 - mae: 7.7572 - val_loss: 33.3307 - val_mae: 4.2451\n",
      "Epoch 304/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 141.6181 - mae: 7.8846 - val_loss: 29.7083 - val_mae: 3.9293\n",
      "Epoch 305/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 146.3649 - mae: 8.0067 - val_loss: 29.3494 - val_mae: 3.9152\n",
      "Epoch 306/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 145.0659 - mae: 7.9472 - val_loss: 34.6519 - val_mae: 4.2859\n",
      "Epoch 307/500\n",
      "122/122 [==============================] - 0s 613us/step - loss: 141.3560 - mae: 7.9374 - val_loss: 33.0714 - val_mae: 4.2540\n",
      "Epoch 308/500\n",
      "122/122 [==============================] - 0s 639us/step - loss: 137.5368 - mae: 7.7208 - val_loss: 28.6712 - val_mae: 3.8416\n",
      "Epoch 309/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 146.7829 - mae: 7.9096 - val_loss: 38.2482 - val_mae: 4.5834\n",
      "Epoch 310/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 141.4259 - mae: 7.7822 - val_loss: 38.5659 - val_mae: 4.6752\n",
      "Epoch 311/500\n",
      "122/122 [==============================] - 0s 606us/step - loss: 136.9067 - mae: 7.8232 - val_loss: 36.9097 - val_mae: 4.3512\n",
      "Epoch 312/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 143.4891 - mae: 7.9705 - val_loss: 31.6773 - val_mae: 4.1132\n",
      "Epoch 313/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 136.2155 - mae: 7.8117 - val_loss: 31.9602 - val_mae: 4.1450\n",
      "Epoch 314/500\n",
      "122/122 [==============================] - 0s 595us/step - loss: 132.4829 - mae: 7.7321 - val_loss: 30.0722 - val_mae: 3.9345\n",
      "Epoch 315/500\n",
      "122/122 [==============================] - 0s 597us/step - loss: 131.6160 - mae: 7.7339 - val_loss: 31.8934 - val_mae: 4.1322\n",
      "Epoch 316/500\n",
      "122/122 [==============================] - 0s 649us/step - loss: 136.6698 - mae: 7.7895 - val_loss: 31.4403 - val_mae: 4.0695\n",
      "Epoch 317/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 124.1451 - mae: 7.4878 - val_loss: 30.6280 - val_mae: 3.9989\n",
      "Epoch 318/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 125.4029 - mae: 7.4996 - val_loss: 32.0579 - val_mae: 4.0143\n",
      "Epoch 319/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 127.9220 - mae: 7.5523 - val_loss: 36.7470 - val_mae: 4.5596\n",
      "Epoch 320/500\n",
      "122/122 [==============================] - 0s 605us/step - loss: 128.8217 - mae: 7.7023 - val_loss: 30.9666 - val_mae: 3.9345\n",
      "Epoch 321/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 122.3940 - mae: 7.5768 - val_loss: 36.8053 - val_mae: 4.5317\n",
      "Epoch 322/500\n",
      "122/122 [==============================] - 0s 598us/step - loss: 125.1631 - mae: 7.5238 - val_loss: 30.9791 - val_mae: 4.0935\n",
      "Epoch 323/500\n",
      "122/122 [==============================] - 0s 621us/step - loss: 130.1449 - mae: 7.6051 - val_loss: 34.9972 - val_mae: 4.2777\n",
      "Epoch 324/500\n",
      "122/122 [==============================] - 0s 605us/step - loss: 131.0994 - mae: 7.7027 - val_loss: 32.2294 - val_mae: 4.2607\n",
      "Epoch 325/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 131.0140 - mae: 7.6123 - val_loss: 32.1899 - val_mae: 4.0642\n",
      "Epoch 326/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 128.1238 - mae: 7.6407 - val_loss: 38.9054 - val_mae: 4.8587\n",
      "Epoch 327/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 127.7843 - mae: 7.6354 - val_loss: 26.8133 - val_mae: 3.7113\n",
      "Epoch 328/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 127.4664 - mae: 7.5154 - val_loss: 29.9822 - val_mae: 3.9176\n",
      "Epoch 329/500\n",
      "122/122 [==============================] - 0s 584us/step - loss: 120.0414 - mae: 7.4640 - val_loss: 30.4611 - val_mae: 4.0076\n",
      "Epoch 330/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 122.5778 - mae: 7.4330 - val_loss: 37.7062 - val_mae: 4.6884\n",
      "Epoch 331/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 126.9111 - mae: 7.5584 - val_loss: 25.1299 - val_mae: 3.5497\n",
      "Epoch 332/500\n",
      "122/122 [==============================] - 0s 646us/step - loss: 118.9024 - mae: 7.3980 - val_loss: 33.0060 - val_mae: 4.1979\n",
      "Epoch 333/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 137.1585 - mae: 7.7324 - val_loss: 30.2145 - val_mae: 3.8859\n",
      "Epoch 334/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 122.8801 - mae: 7.5065 - val_loss: 32.8884 - val_mae: 4.3526\n",
      "Epoch 335/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 122.0317 - mae: 7.3949 - val_loss: 35.5258 - val_mae: 4.3575\n",
      "Epoch 336/500\n",
      "122/122 [==============================] - 0s 630us/step - loss: 123.7131 - mae: 7.5082 - val_loss: 30.2060 - val_mae: 3.8411\n",
      "Epoch 337/500\n",
      "122/122 [==============================] - 0s 624us/step - loss: 127.6742 - mae: 7.5825 - val_loss: 32.0462 - val_mae: 4.2800\n",
      "Epoch 338/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 122.2809 - mae: 7.3748 - val_loss: 30.3447 - val_mae: 4.0830\n",
      "Epoch 339/500\n",
      "122/122 [==============================] - 0s 662us/step - loss: 113.0889 - mae: 7.2438 - val_loss: 25.4565 - val_mae: 3.6084\n",
      "Epoch 340/500\n",
      "122/122 [==============================] - 0s 621us/step - loss: 123.1994 - mae: 7.4791 - val_loss: 26.6384 - val_mae: 3.7184\n",
      "Epoch 341/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 117.1055 - mae: 7.3530 - val_loss: 28.4387 - val_mae: 3.8540\n",
      "Epoch 342/500\n",
      "122/122 [==============================] - 0s 586us/step - loss: 122.0923 - mae: 7.3773 - val_loss: 32.0597 - val_mae: 4.1697\n",
      "Epoch 343/500\n",
      "122/122 [==============================] - 0s 641us/step - loss: 118.8239 - mae: 7.3840 - val_loss: 27.6725 - val_mae: 3.8000\n",
      "Epoch 344/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 124.0260 - mae: 7.4496 - val_loss: 33.3279 - val_mae: 4.3046\n",
      "Epoch 345/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 115.6954 - mae: 7.3208 - val_loss: 36.2260 - val_mae: 4.5539\n",
      "Epoch 346/500\n",
      "122/122 [==============================] - 0s 709us/step - loss: 119.5208 - mae: 7.3637 - val_loss: 35.1044 - val_mae: 4.3810\n",
      "Epoch 347/500\n",
      "122/122 [==============================] - 0s 635us/step - loss: 114.5171 - mae: 7.2669 - val_loss: 31.0773 - val_mae: 4.1037\n",
      "Epoch 348/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 117.9308 - mae: 7.3037 - val_loss: 29.3798 - val_mae: 3.9215\n",
      "Epoch 349/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 121.2852 - mae: 7.3605 - val_loss: 36.5594 - val_mae: 4.4234\n",
      "Epoch 350/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 125.7331 - mae: 7.4110 - val_loss: 28.2285 - val_mae: 3.7422\n",
      "Epoch 351/500\n",
      "122/122 [==============================] - 0s 611us/step - loss: 118.2307 - mae: 7.3988 - val_loss: 33.4617 - val_mae: 4.0789\n",
      "Epoch 352/500\n",
      "122/122 [==============================] - 0s 666us/step - loss: 114.0746 - mae: 7.2707 - val_loss: 31.8605 - val_mae: 4.0613\n",
      "Epoch 353/500\n",
      "122/122 [==============================] - 0s 583us/step - loss: 118.9381 - mae: 7.3968 - val_loss: 30.0904 - val_mae: 3.8309\n",
      "Epoch 354/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 117.8131 - mae: 7.3593 - val_loss: 28.3541 - val_mae: 3.7943\n",
      "Epoch 355/500\n",
      "122/122 [==============================] - 0s 614us/step - loss: 114.2877 - mae: 7.2831 - val_loss: 35.8206 - val_mae: 4.5273\n",
      "Epoch 356/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 111.6980 - mae: 7.2336 - val_loss: 36.4906 - val_mae: 4.5422\n",
      "Epoch 357/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 109.4495 - mae: 7.0765 - val_loss: 33.0059 - val_mae: 4.0398\n",
      "Epoch 358/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 120.1869 - mae: 7.3550 - val_loss: 54.4247 - val_mae: 5.8990\n",
      "Epoch 359/500\n",
      "122/122 [==============================] - 0s 666us/step - loss: 108.2635 - mae: 7.1684 - val_loss: 30.5845 - val_mae: 3.9592\n",
      "Epoch 360/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 108.7975 - mae: 7.2661 - val_loss: 33.1547 - val_mae: 4.1981\n",
      "Epoch 361/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 119.1754 - mae: 7.3774 - val_loss: 27.9105 - val_mae: 3.8754\n",
      "Epoch 362/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 117.4925 - mae: 7.3575 - val_loss: 32.5479 - val_mae: 4.0328\n",
      "Epoch 363/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 109.4115 - mae: 7.1633 - val_loss: 39.1233 - val_mae: 4.6226\n",
      "Epoch 364/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 113.6674 - mae: 7.1316 - val_loss: 31.8483 - val_mae: 4.0953\n",
      "Epoch 365/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 106.5568 - mae: 7.0568 - val_loss: 51.7482 - val_mae: 5.4313\n",
      "Epoch 366/500\n",
      "122/122 [==============================] - 0s 657us/step - loss: 114.9093 - mae: 7.3914 - val_loss: 31.8601 - val_mae: 4.2049\n",
      "Epoch 367/500\n",
      "122/122 [==============================] - 0s 607us/step - loss: 106.4293 - mae: 7.1795 - val_loss: 35.7585 - val_mae: 4.4051\n",
      "Epoch 368/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 108.4514 - mae: 7.0931 - val_loss: 31.4734 - val_mae: 4.0862\n",
      "Epoch 369/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 109.2924 - mae: 7.2701 - val_loss: 34.3364 - val_mae: 4.2586\n",
      "Epoch 370/500\n",
      "122/122 [==============================] - 0s 608us/step - loss: 110.0230 - mae: 7.0924 - val_loss: 32.9308 - val_mae: 4.1255\n",
      "Epoch 371/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 106.8694 - mae: 7.1398 - val_loss: 33.8758 - val_mae: 4.1972\n",
      "Epoch 372/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 115.7722 - mae: 7.3462 - val_loss: 30.9051 - val_mae: 4.0481\n",
      "Epoch 373/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 108.6589 - mae: 7.1362 - val_loss: 28.7803 - val_mae: 3.8185\n",
      "Epoch 374/500\n",
      "122/122 [==============================] - 0s 641us/step - loss: 109.0868 - mae: 7.1615 - val_loss: 26.6955 - val_mae: 3.6892\n",
      "Epoch 375/500\n",
      "122/122 [==============================] - 0s 605us/step - loss: 114.8000 - mae: 7.2979 - val_loss: 34.4192 - val_mae: 3.9787\n",
      "Epoch 376/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 101.1322 - mae: 6.9237 - val_loss: 29.8937 - val_mae: 3.8607\n",
      "Epoch 377/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 112.9789 - mae: 7.1263 - val_loss: 32.7157 - val_mae: 4.1353\n",
      "Epoch 378/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 103.8604 - mae: 7.0642 - val_loss: 32.9187 - val_mae: 4.1832\n",
      "Epoch 379/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 110.3658 - mae: 7.1806 - val_loss: 34.8468 - val_mae: 4.3411\n",
      "Epoch 380/500\n",
      "122/122 [==============================] - 0s 657us/step - loss: 101.4608 - mae: 6.9302 - val_loss: 35.7516 - val_mae: 4.4240\n",
      "Epoch 381/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 106.2647 - mae: 7.0376 - val_loss: 29.5490 - val_mae: 3.7821\n",
      "Epoch 382/500\n",
      "122/122 [==============================] - 0s 606us/step - loss: 110.1771 - mae: 7.1809 - val_loss: 30.8880 - val_mae: 3.8271\n",
      "Epoch 383/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 106.0281 - mae: 7.0927 - val_loss: 26.1620 - val_mae: 3.5601\n",
      "Epoch 384/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 105.0337 - mae: 7.0122 - val_loss: 27.3416 - val_mae: 3.7556\n",
      "Epoch 385/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 104.3539 - mae: 6.9433 - val_loss: 35.4771 - val_mae: 4.1546\n",
      "Epoch 386/500\n",
      "122/122 [==============================] - 0s 592us/step - loss: 100.9000 - mae: 7.0025 - val_loss: 34.6829 - val_mae: 4.1736\n",
      "Epoch 387/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 102.5390 - mae: 6.9649 - val_loss: 44.8823 - val_mae: 5.1825\n",
      "Epoch 388/500\n",
      "122/122 [==============================] - 0s 644us/step - loss: 97.9603 - mae: 6.8244 - val_loss: 32.0284 - val_mae: 4.0584\n",
      "Epoch 389/500\n",
      "122/122 [==============================] - 0s 602us/step - loss: 104.2560 - mae: 7.1152 - val_loss: 57.3528 - val_mae: 5.9293\n",
      "Epoch 390/500\n",
      "122/122 [==============================] - 0s 629us/step - loss: 108.3117 - mae: 6.9372 - val_loss: 34.8527 - val_mae: 4.3706\n",
      "Epoch 391/500\n",
      "122/122 [==============================] - 0s 603us/step - loss: 101.7478 - mae: 7.0841 - val_loss: 48.3175 - val_mae: 5.4301\n",
      "Epoch 392/500\n",
      "122/122 [==============================] - 0s 594us/step - loss: 106.6814 - mae: 7.1504 - val_loss: 28.3852 - val_mae: 3.8316\n",
      "Epoch 393/500\n",
      "122/122 [==============================] - 0s 602us/step - loss: 97.3963 - mae: 6.8530 - val_loss: 45.1451 - val_mae: 5.1695\n",
      "Epoch 394/500\n",
      "122/122 [==============================] - 0s 650us/step - loss: 104.0907 - mae: 6.9970 - val_loss: 40.0125 - val_mae: 4.7935\n",
      "Epoch 395/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 99.2579 - mae: 6.9132 - val_loss: 37.2654 - val_mae: 4.3759\n",
      "Epoch 396/500\n",
      "122/122 [==============================] - 0s 584us/step - loss: 100.9203 - mae: 6.9607 - val_loss: 33.6763 - val_mae: 4.0884\n",
      "Epoch 397/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 98.9911 - mae: 6.9208 - val_loss: 54.8275 - val_mae: 5.7344\n",
      "Epoch 398/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 98.6352 - mae: 6.8566 - val_loss: 44.2035 - val_mae: 5.0608\n",
      "Epoch 399/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 100.1909 - mae: 7.0224 - val_loss: 38.8103 - val_mae: 4.6422\n",
      "Epoch 400/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 103.6737 - mae: 6.9006 - val_loss: 45.6083 - val_mae: 5.0667\n",
      "Epoch 401/500\n",
      "122/122 [==============================] - 0s 710us/step - loss: 90.4366 - mae: 6.6880 - val_loss: 41.5182 - val_mae: 4.8707\n",
      "Epoch 402/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 92.6132 - mae: 6.7908 - val_loss: 35.3810 - val_mae: 4.3585\n",
      "Epoch 403/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 105.7345 - mae: 7.0053 - val_loss: 55.7524 - val_mae: 5.7604\n",
      "Epoch 404/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 100.4673 - mae: 6.9505 - val_loss: 54.6339 - val_mae: 5.7289\n",
      "Epoch 405/500\n",
      "122/122 [==============================] - 0s 610us/step - loss: 98.5757 - mae: 6.9331 - val_loss: 32.5398 - val_mae: 3.9499\n",
      "Epoch 406/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 93.1694 - mae: 6.7276 - val_loss: 40.0185 - val_mae: 4.6733\n",
      "Epoch 407/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 95.9707 - mae: 6.8379 - val_loss: 29.5589 - val_mae: 3.8266\n",
      "Epoch 408/500\n",
      "122/122 [==============================] - 0s 647us/step - loss: 99.7909 - mae: 6.8996 - val_loss: 52.5649 - val_mae: 5.6682\n",
      "Epoch 409/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 93.3235 - mae: 6.6953 - val_loss: 38.5080 - val_mae: 4.4725\n",
      "Epoch 410/500\n",
      "122/122 [==============================] - 0s 565us/step - loss: 96.7780 - mae: 6.8131 - val_loss: 43.2454 - val_mae: 4.8661\n",
      "Epoch 411/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 98.4895 - mae: 6.8919 - val_loss: 46.4150 - val_mae: 5.1846\n",
      "Epoch 412/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 85.7451 - mae: 6.5905 - val_loss: 35.5719 - val_mae: 4.2908\n",
      "Epoch 413/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 98.5474 - mae: 6.9066 - val_loss: 53.6700 - val_mae: 5.6132\n",
      "Epoch 414/500\n",
      "122/122 [==============================] - 0s 593us/step - loss: 86.7798 - mae: 6.5909 - val_loss: 30.1050 - val_mae: 3.8753\n",
      "Epoch 415/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 92.1593 - mae: 6.6884 - val_loss: 46.0316 - val_mae: 4.8972\n",
      "Epoch 416/500\n",
      "122/122 [==============================] - 0s 691us/step - loss: 88.0734 - mae: 6.5999 - val_loss: 38.2369 - val_mae: 4.5978\n",
      "Epoch 417/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 93.2599 - mae: 6.8224 - val_loss: 39.0861 - val_mae: 4.6948\n",
      "Epoch 418/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 89.5811 - mae: 6.6487 - val_loss: 31.4289 - val_mae: 4.0128\n",
      "Epoch 419/500\n",
      "122/122 [==============================] - 0s 590us/step - loss: 97.7695 - mae: 6.8455 - val_loss: 42.4800 - val_mae: 4.9366\n",
      "Epoch 420/500\n",
      "122/122 [==============================] - 0s 577us/step - loss: 96.4855 - mae: 6.8572 - val_loss: 31.5941 - val_mae: 4.0642\n",
      "Epoch 421/500\n",
      "122/122 [==============================] - 0s 562us/step - loss: 92.7914 - mae: 6.7487 - val_loss: 28.8281 - val_mae: 3.8898\n",
      "Epoch 422/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 92.8714 - mae: 6.6581 - val_loss: 33.9659 - val_mae: 4.3376\n",
      "Epoch 423/500\n",
      "122/122 [==============================] - 0s 656us/step - loss: 89.1718 - mae: 6.6845 - val_loss: 40.1286 - val_mae: 4.7930\n",
      "Epoch 424/500\n",
      "122/122 [==============================] - 0s 637us/step - loss: 96.5237 - mae: 6.8042 - val_loss: 31.9389 - val_mae: 4.0378\n",
      "Epoch 425/500\n",
      "122/122 [==============================] - 0s 594us/step - loss: 85.7127 - mae: 6.5714 - val_loss: 32.3040 - val_mae: 4.0055\n",
      "Epoch 426/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 91.4902 - mae: 6.7001 - val_loss: 33.4964 - val_mae: 4.1140\n",
      "Epoch 427/500\n",
      "122/122 [==============================] - 0s 595us/step - loss: 90.7178 - mae: 6.6137 - val_loss: 30.4678 - val_mae: 3.8949\n",
      "Epoch 428/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 90.6615 - mae: 6.6332 - val_loss: 29.5530 - val_mae: 3.8618\n",
      "Epoch 429/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 96.0823 - mae: 6.7925 - val_loss: 29.9929 - val_mae: 3.9231\n",
      "Epoch 430/500\n",
      "122/122 [==============================] - 0s 665us/step - loss: 88.7075 - mae: 6.5870 - val_loss: 31.9059 - val_mae: 4.0784\n",
      "Epoch 431/500\n",
      "122/122 [==============================] - 0s 622us/step - loss: 91.4182 - mae: 6.8171 - val_loss: 29.1831 - val_mae: 3.8812\n",
      "Epoch 432/500\n",
      "122/122 [==============================] - 0s 585us/step - loss: 94.4088 - mae: 6.7163 - val_loss: 41.4567 - val_mae: 4.8138\n",
      "Epoch 433/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 96.3954 - mae: 6.7309 - val_loss: 36.0694 - val_mae: 4.4436\n",
      "Epoch 434/500\n",
      "122/122 [==============================] - 0s 576us/step - loss: 86.5418 - mae: 6.5935 - val_loss: 29.1749 - val_mae: 3.8498\n",
      "Epoch 435/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 92.5398 - mae: 6.7049 - val_loss: 30.8875 - val_mae: 3.9961\n",
      "Epoch 436/500\n",
      "122/122 [==============================] - 0s 609us/step - loss: 90.5895 - mae: 6.6159 - val_loss: 33.4314 - val_mae: 4.1689\n",
      "Epoch 437/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 89.5278 - mae: 6.5770 - val_loss: 34.3383 - val_mae: 4.2102\n",
      "Epoch 438/500\n",
      "122/122 [==============================] - 0s 606us/step - loss: 91.9701 - mae: 6.7078 - val_loss: 47.3202 - val_mae: 5.1986\n",
      "Epoch 439/500\n",
      "122/122 [==============================] - 0s 618us/step - loss: 91.2336 - mae: 6.6433 - val_loss: 49.8482 - val_mae: 5.3247\n",
      "Epoch 440/500\n",
      "122/122 [==============================] - 0s 613us/step - loss: 82.8911 - mae: 6.5250 - val_loss: 32.7353 - val_mae: 4.1046\n",
      "Epoch 441/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 90.3109 - mae: 6.6197 - val_loss: 37.9990 - val_mae: 4.2876\n",
      "Epoch 442/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 85.0075 - mae: 6.5339 - val_loss: 34.4387 - val_mae: 4.1231\n",
      "Epoch 443/500\n",
      "122/122 [==============================] - 0s 591us/step - loss: 88.6330 - mae: 6.6255 - val_loss: 34.0836 - val_mae: 4.1231\n",
      "Epoch 444/500\n",
      "122/122 [==============================] - 0s 656us/step - loss: 78.9419 - mae: 6.2977 - val_loss: 27.6111 - val_mae: 3.7266\n",
      "Epoch 445/500\n",
      "122/122 [==============================] - 0s 603us/step - loss: 84.2600 - mae: 6.5742 - val_loss: 31.2495 - val_mae: 4.0027\n",
      "Epoch 446/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 97.4067 - mae: 6.6543 - val_loss: 33.4615 - val_mae: 4.0672\n",
      "Epoch 447/500\n",
      "122/122 [==============================] - 0s 579us/step - loss: 86.4240 - mae: 6.5213 - val_loss: 34.2414 - val_mae: 4.1165\n",
      "Epoch 448/500\n",
      "122/122 [==============================] - 0s 601us/step - loss: 84.5063 - mae: 6.5485 - val_loss: 37.9082 - val_mae: 4.4009\n",
      "Epoch 449/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 87.0846 - mae: 6.6098 - val_loss: 48.1714 - val_mae: 5.2023\n",
      "Epoch 450/500\n",
      "122/122 [==============================] - 0s 646us/step - loss: 85.9367 - mae: 6.5298 - val_loss: 35.8151 - val_mae: 4.2134\n",
      "Epoch 451/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 91.1259 - mae: 6.6564 - val_loss: 42.8541 - val_mae: 4.8710\n",
      "Epoch 452/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 89.2845 - mae: 6.6062 - val_loss: 31.9245 - val_mae: 4.1421\n",
      "Epoch 453/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 81.1126 - mae: 6.4603 - val_loss: 34.9122 - val_mae: 4.1538\n",
      "Epoch 454/500\n",
      "122/122 [==============================] - 0s 574us/step - loss: 84.0751 - mae: 6.5143 - val_loss: 31.7944 - val_mae: 3.9480\n",
      "Epoch 455/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 86.6860 - mae: 6.5508 - val_loss: 53.0096 - val_mae: 5.3511\n",
      "Epoch 456/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 81.6266 - mae: 6.4967 - val_loss: 33.7766 - val_mae: 4.1054\n",
      "Epoch 457/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 88.0724 - mae: 6.5559 - val_loss: 35.5189 - val_mae: 4.2349\n",
      "Epoch 458/500\n",
      "122/122 [==============================] - 0s 609us/step - loss: 83.4180 - mae: 6.4351 - val_loss: 32.7902 - val_mae: 3.9855\n",
      "Epoch 459/500\n",
      "122/122 [==============================] - 0s 619us/step - loss: 82.2164 - mae: 6.4739 - val_loss: 34.5355 - val_mae: 4.2220\n",
      "Epoch 460/500\n",
      "122/122 [==============================] - 0s 613us/step - loss: 87.4736 - mae: 6.5869 - val_loss: 42.8581 - val_mae: 4.7490\n",
      "Epoch 461/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 85.2673 - mae: 6.5564 - val_loss: 43.3243 - val_mae: 4.9655\n",
      "Epoch 462/500\n",
      "122/122 [==============================] - 0s 596us/step - loss: 76.5142 - mae: 6.3290 - val_loss: 30.7924 - val_mae: 3.9604\n",
      "Epoch 463/500\n",
      "122/122 [==============================] - 0s 587us/step - loss: 79.5307 - mae: 6.3896 - val_loss: 49.1451 - val_mae: 5.2177\n",
      "Epoch 464/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 81.9758 - mae: 6.3731 - val_loss: 32.7446 - val_mae: 4.0770\n",
      "Epoch 465/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 81.1358 - mae: 6.3391 - val_loss: 30.6551 - val_mae: 3.9146\n",
      "Epoch 466/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 85.5449 - mae: 6.5204 - val_loss: 42.7517 - val_mae: 4.7751\n",
      "Epoch 467/500\n",
      "122/122 [==============================] - 0s 581us/step - loss: 91.3274 - mae: 6.6704 - val_loss: 47.3499 - val_mae: 5.1320\n",
      "Epoch 468/500\n",
      "122/122 [==============================] - 0s 622us/step - loss: 84.9604 - mae: 6.5819 - val_loss: 35.7066 - val_mae: 4.1704\n",
      "Epoch 469/500\n",
      "122/122 [==============================] - 0s 622us/step - loss: 83.0290 - mae: 6.4405 - val_loss: 34.8888 - val_mae: 4.2733\n",
      "Epoch 470/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 81.4427 - mae: 6.4012 - val_loss: 32.0076 - val_mae: 4.0749\n",
      "Epoch 471/500\n",
      "122/122 [==============================] - 0s 573us/step - loss: 82.5221 - mae: 6.3380 - val_loss: 34.9167 - val_mae: 4.1669\n",
      "Epoch 472/500\n",
      "122/122 [==============================] - 0s 606us/step - loss: 79.5359 - mae: 6.3136 - val_loss: 32.8895 - val_mae: 4.0069\n",
      "Epoch 473/500\n",
      "122/122 [==============================] - 0s 582us/step - loss: 85.3434 - mae: 6.5164 - val_loss: 43.1066 - val_mae: 4.8699\n",
      "Epoch 474/500\n",
      "122/122 [==============================] - 0s 564us/step - loss: 80.8404 - mae: 6.2866 - val_loss: 37.5015 - val_mae: 4.4285\n",
      "Epoch 475/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 79.9951 - mae: 6.3831 - val_loss: 30.2947 - val_mae: 3.8826\n",
      "Epoch 476/500\n",
      "122/122 [==============================] - 0s 658us/step - loss: 78.6590 - mae: 6.2997 - val_loss: 34.7884 - val_mae: 4.3412\n",
      "Epoch 477/500\n",
      "122/122 [==============================] - 0s 604us/step - loss: 86.1977 - mae: 6.5132 - val_loss: 45.9289 - val_mae: 4.9585\n",
      "Epoch 478/500\n",
      "122/122 [==============================] - 0s 578us/step - loss: 80.8055 - mae: 6.4961 - val_loss: 37.1321 - val_mae: 4.4226\n",
      "Epoch 479/500\n",
      "122/122 [==============================] - 0s 571us/step - loss: 83.7937 - mae: 6.4592 - val_loss: 56.6305 - val_mae: 5.8143\n",
      "Epoch 480/500\n",
      "122/122 [==============================] - 0s 592us/step - loss: 75.7856 - mae: 6.2972 - val_loss: 27.9731 - val_mae: 3.7549\n",
      "Epoch 481/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 74.7048 - mae: 6.3014 - val_loss: 40.1814 - val_mae: 4.6649\n",
      "Epoch 482/500\n",
      "122/122 [==============================] - 0s 647us/step - loss: 79.7113 - mae: 6.3049 - val_loss: 42.2397 - val_mae: 4.6868\n",
      "Epoch 483/500\n",
      "122/122 [==============================] - 0s 575us/step - loss: 78.7364 - mae: 6.3479 - val_loss: 35.2778 - val_mae: 4.2562\n",
      "Epoch 484/500\n",
      "122/122 [==============================] - 0s 580us/step - loss: 75.1203 - mae: 6.2545 - val_loss: 37.1291 - val_mae: 4.3041\n",
      "Epoch 485/500\n",
      "122/122 [==============================] - 0s 589us/step - loss: 75.4843 - mae: 6.2935 - val_loss: 62.1375 - val_mae: 6.0169\n",
      "Epoch 486/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 78.7876 - mae: 6.2916 - val_loss: 35.5475 - val_mae: 4.2381\n",
      "Epoch 487/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 76.7454 - mae: 6.2426 - val_loss: 30.7211 - val_mae: 3.9499\n",
      "Epoch 488/500\n",
      "122/122 [==============================] - 0s 563us/step - loss: 78.2013 - mae: 6.3409 - val_loss: 31.5301 - val_mae: 4.0117\n",
      "Epoch 489/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 76.5128 - mae: 6.2259 - val_loss: 46.7653 - val_mae: 5.0367\n",
      "Epoch 490/500\n",
      "122/122 [==============================] - 0s 594us/step - loss: 84.2163 - mae: 6.4684 - val_loss: 30.4639 - val_mae: 3.9349\n",
      "Epoch 491/500\n",
      "122/122 [==============================] - 0s 645us/step - loss: 75.1198 - mae: 6.2109 - val_loss: 42.6429 - val_mae: 4.7542\n",
      "Epoch 492/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 73.1091 - mae: 6.1845 - val_loss: 35.8150 - val_mae: 4.1998\n",
      "Epoch 493/500\n",
      "122/122 [==============================] - 0s 569us/step - loss: 82.0477 - mae: 6.3774 - val_loss: 32.3541 - val_mae: 4.1691\n",
      "Epoch 494/500\n",
      "122/122 [==============================] - 0s 588us/step - loss: 79.6348 - mae: 6.3609 - val_loss: 38.2261 - val_mae: 4.2931\n",
      "Epoch 495/500\n",
      "122/122 [==============================] - 0s 568us/step - loss: 70.7627 - mae: 6.0689 - val_loss: 33.2625 - val_mae: 4.0647\n",
      "Epoch 496/500\n",
      "122/122 [==============================] - 0s 570us/step - loss: 76.1367 - mae: 6.2070 - val_loss: 38.1778 - val_mae: 4.4824\n",
      "Epoch 497/500\n",
      "122/122 [==============================] - 0s 567us/step - loss: 73.1593 - mae: 6.1953 - val_loss: 29.9255 - val_mae: 3.8421\n",
      "Epoch 498/500\n",
      "122/122 [==============================] - 0s 566us/step - loss: 76.8045 - mae: 6.3368 - val_loss: 30.8804 - val_mae: 3.9747\n",
      "Epoch 499/500\n",
      "122/122 [==============================] - 0s 584us/step - loss: 74.1410 - mae: 6.2305 - val_loss: 35.8447 - val_mae: 4.2321\n",
      "Epoch 500/500\n",
      "122/122 [==============================] - 0s 685us/step - loss: 74.0320 - mae: 6.2038 - val_loss: 34.6990 - val_mae: 4.1940\n",
      "31/31 [==============================] - 0s 310us/step\n",
      "Epochs: 500 | MAE: 4.194033297181741\n",
      "Training model with 550 epochs\n",
      "Epoch 1/550\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 66566.6172 - mae: 223.5761 - val_loss: 55242.5742 - val_mae: 203.1353\n",
      "Epoch 2/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 32398.4082 - mae: 149.5045 - val_loss: 9097.2676 - val_mae: 79.9287\n",
      "Epoch 3/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 5225.4546 - mae: 55.3952 - val_loss: 2975.1729 - val_mae: 40.8221\n",
      "Epoch 4/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 2997.3865 - mae: 40.9022 - val_loss: 2020.2305 - val_mae: 33.8325\n",
      "Epoch 5/550\n",
      "122/122 [==============================] - 0s 624us/step - loss: 2477.9111 - mae: 38.1366 - val_loss: 1597.0496 - val_mae: 30.1570\n",
      "Epoch 6/550\n",
      "122/122 [==============================] - 0s 654us/step - loss: 2049.6636 - mae: 34.9558 - val_loss: 1332.4102 - val_mae: 27.4446\n",
      "Epoch 7/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 1880.5802 - mae: 33.4149 - val_loss: 1179.0414 - val_mae: 25.6049\n",
      "Epoch 8/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 1786.6816 - mae: 32.2121 - val_loss: 1073.4535 - val_mae: 24.4206\n",
      "Epoch 9/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 1596.7842 - mae: 30.9019 - val_loss: 985.8226 - val_mae: 23.6745\n",
      "Epoch 10/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 1497.0999 - mae: 29.8948 - val_loss: 852.4441 - val_mae: 21.8320\n",
      "Epoch 11/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 1415.6418 - mae: 29.0858 - val_loss: 789.4736 - val_mae: 20.8465\n",
      "Epoch 12/550\n",
      "122/122 [==============================] - 0s 648us/step - loss: 1364.9839 - mae: 28.9787 - val_loss: 719.2947 - val_mae: 20.0836\n",
      "Epoch 13/550\n",
      "122/122 [==============================] - 0s 608us/step - loss: 1281.8767 - mae: 28.0794 - val_loss: 677.2803 - val_mae: 19.6390\n",
      "Epoch 14/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 1285.5569 - mae: 27.7365 - val_loss: 616.4692 - val_mae: 18.4592\n",
      "Epoch 15/550\n",
      "122/122 [==============================] - 0s 563us/step - loss: 1205.3241 - mae: 27.1610 - val_loss: 579.2900 - val_mae: 18.4459\n",
      "Epoch 16/550\n",
      "122/122 [==============================] - 0s 588us/step - loss: 1158.4009 - mae: 26.4678 - val_loss: 529.5033 - val_mae: 17.7197\n",
      "Epoch 17/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 1053.4998 - mae: 25.4185 - val_loss: 450.0425 - val_mae: 16.1096\n",
      "Epoch 18/550\n",
      "122/122 [==============================] - 0s 651us/step - loss: 999.0954 - mae: 24.7298 - val_loss: 412.2126 - val_mae: 15.1747\n",
      "Epoch 19/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 950.2278 - mae: 23.9878 - val_loss: 394.1690 - val_mae: 14.9552\n",
      "Epoch 20/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 949.3714 - mae: 23.9499 - val_loss: 332.0932 - val_mae: 13.4466\n",
      "Epoch 21/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 927.2133 - mae: 23.4885 - val_loss: 313.5244 - val_mae: 13.4651\n",
      "Epoch 22/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 878.9155 - mae: 22.8806 - val_loss: 291.2822 - val_mae: 13.1934\n",
      "Epoch 23/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 798.5136 - mae: 21.8239 - val_loss: 247.3624 - val_mae: 11.7977\n",
      "Epoch 24/550\n",
      "122/122 [==============================] - 0s 641us/step - loss: 824.0529 - mae: 22.0781 - val_loss: 218.9450 - val_mae: 11.2108\n",
      "Epoch 25/550\n",
      "122/122 [==============================] - 0s 612us/step - loss: 768.4277 - mae: 21.2229 - val_loss: 220.2915 - val_mae: 11.6450\n",
      "Epoch 26/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 730.0599 - mae: 20.8072 - val_loss: 181.0721 - val_mae: 10.0241\n",
      "Epoch 27/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 734.5110 - mae: 20.6023 - val_loss: 171.1546 - val_mae: 9.6218\n",
      "Epoch 28/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 742.7538 - mae: 20.7658 - val_loss: 152.5064 - val_mae: 9.0639\n",
      "Epoch 29/550\n",
      "122/122 [==============================] - 0s 615us/step - loss: 719.3303 - mae: 20.4096 - val_loss: 146.1842 - val_mae: 9.5194\n",
      "Epoch 30/550\n",
      "122/122 [==============================] - 0s 611us/step - loss: 666.5178 - mae: 19.6524 - val_loss: 123.2974 - val_mae: 8.4306\n",
      "Epoch 31/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 671.4573 - mae: 19.5001 - val_loss: 115.2028 - val_mae: 8.2838\n",
      "Epoch 32/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 666.4102 - mae: 19.2660 - val_loss: 96.6677 - val_mae: 7.4590\n",
      "Epoch 33/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 598.3468 - mae: 18.3410 - val_loss: 118.2858 - val_mae: 8.2758\n",
      "Epoch 34/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 633.8583 - mae: 18.7541 - val_loss: 89.9661 - val_mae: 7.3556\n",
      "Epoch 35/550\n",
      "122/122 [==============================] - 0s 603us/step - loss: 619.3200 - mae: 18.4784 - val_loss: 78.1453 - val_mae: 6.8478\n",
      "Epoch 36/550\n",
      "122/122 [==============================] - 0s 708us/step - loss: 567.6653 - mae: 17.7948 - val_loss: 66.1461 - val_mae: 6.2885\n",
      "Epoch 37/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 576.5089 - mae: 17.6400 - val_loss: 60.8851 - val_mae: 6.0031\n",
      "Epoch 38/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 604.2376 - mae: 18.1265 - val_loss: 60.7467 - val_mae: 5.7627\n",
      "Epoch 39/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 569.4167 - mae: 17.5627 - val_loss: 53.4930 - val_mae: 5.5596\n",
      "Epoch 40/550\n",
      "122/122 [==============================] - 0s 594us/step - loss: 590.4118 - mae: 17.5809 - val_loss: 62.2719 - val_mae: 6.0573\n",
      "Epoch 41/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 590.7448 - mae: 17.3452 - val_loss: 51.1183 - val_mae: 5.6007\n",
      "Epoch 42/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 552.6843 - mae: 17.1957 - val_loss: 51.1718 - val_mae: 5.3481\n",
      "Epoch 43/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 528.1620 - mae: 16.5891 - val_loss: 39.3699 - val_mae: 4.7396\n",
      "Epoch 44/550\n",
      "122/122 [==============================] - 0s 678us/step - loss: 553.4343 - mae: 16.8639 - val_loss: 41.6727 - val_mae: 4.9880\n",
      "Epoch 45/550\n",
      "122/122 [==============================] - 0s 610us/step - loss: 567.6909 - mae: 16.9646 - val_loss: 31.8770 - val_mae: 4.1704\n",
      "Epoch 46/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 509.0579 - mae: 16.2164 - val_loss: 35.9835 - val_mae: 4.5680\n",
      "Epoch 47/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 542.5278 - mae: 16.7291 - val_loss: 48.2523 - val_mae: 5.4336\n",
      "Epoch 48/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 537.0421 - mae: 16.5696 - val_loss: 41.4245 - val_mae: 4.7227\n",
      "Epoch 49/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 493.4112 - mae: 15.8440 - val_loss: 29.3757 - val_mae: 3.8863\n",
      "Epoch 50/550\n",
      "122/122 [==============================] - 0s 593us/step - loss: 504.4756 - mae: 15.9549 - val_loss: 34.5690 - val_mae: 4.2785\n",
      "Epoch 51/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 532.6606 - mae: 16.3911 - val_loss: 31.7950 - val_mae: 4.1837\n",
      "Epoch 52/550\n",
      "122/122 [==============================] - 0s 629us/step - loss: 522.3325 - mae: 16.0891 - val_loss: 33.1992 - val_mae: 4.3891\n",
      "Epoch 53/550\n",
      "122/122 [==============================] - 0s 613us/step - loss: 482.1518 - mae: 15.5832 - val_loss: 27.7223 - val_mae: 3.7055\n",
      "Epoch 54/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 501.1428 - mae: 15.9574 - val_loss: 42.7276 - val_mae: 5.0277\n",
      "Epoch 55/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 488.0605 - mae: 15.5666 - val_loss: 29.2216 - val_mae: 3.9548\n",
      "Epoch 56/550\n",
      "122/122 [==============================] - 0s 610us/step - loss: 496.5119 - mae: 15.7524 - val_loss: 35.3578 - val_mae: 4.5512\n",
      "Epoch 57/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 461.0984 - mae: 15.3336 - val_loss: 26.9305 - val_mae: 3.7027\n",
      "Epoch 58/550\n",
      "122/122 [==============================] - 0s 622us/step - loss: 459.2653 - mae: 15.2235 - val_loss: 37.1785 - val_mae: 4.5690\n",
      "Epoch 59/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 475.0841 - mae: 15.3228 - val_loss: 30.4562 - val_mae: 4.1278\n",
      "Epoch 60/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 492.0374 - mae: 15.6455 - val_loss: 29.8369 - val_mae: 4.1073\n",
      "Epoch 61/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 477.3022 - mae: 15.3225 - val_loss: 24.4659 - val_mae: 3.6707\n",
      "Epoch 62/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 489.9943 - mae: 15.4277 - val_loss: 28.0733 - val_mae: 3.8153\n",
      "Epoch 63/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 457.8361 - mae: 15.0952 - val_loss: 28.8212 - val_mae: 3.9146\n",
      "Epoch 64/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 470.4158 - mae: 15.2647 - val_loss: 33.2390 - val_mae: 4.2008\n",
      "Epoch 65/550\n",
      "122/122 [==============================] - 0s 654us/step - loss: 466.5603 - mae: 15.1649 - val_loss: 34.7943 - val_mae: 4.3222\n",
      "Epoch 66/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 452.6738 - mae: 14.8648 - val_loss: 31.0642 - val_mae: 4.1334\n",
      "Epoch 67/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 471.4828 - mae: 15.3254 - val_loss: 26.1110 - val_mae: 3.8433\n",
      "Epoch 68/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 486.5142 - mae: 15.3852 - val_loss: 24.2011 - val_mae: 3.6093\n",
      "Epoch 69/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 422.1976 - mae: 14.3847 - val_loss: 32.9263 - val_mae: 4.3507\n",
      "Epoch 70/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 471.3041 - mae: 15.0919 - val_loss: 31.6025 - val_mae: 4.1139\n",
      "Epoch 71/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 439.8457 - mae: 14.6646 - val_loss: 28.4855 - val_mae: 3.7709\n",
      "Epoch 72/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 465.6792 - mae: 15.0169 - val_loss: 34.4875 - val_mae: 4.5152\n",
      "Epoch 73/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 456.2206 - mae: 14.9768 - val_loss: 25.8007 - val_mae: 3.8377\n",
      "Epoch 74/550\n",
      "122/122 [==============================] - 0s 623us/step - loss: 441.3356 - mae: 14.7208 - val_loss: 24.9158 - val_mae: 3.6141\n",
      "Epoch 75/550\n",
      "122/122 [==============================] - 0s 630us/step - loss: 427.9619 - mae: 14.3650 - val_loss: 32.3067 - val_mae: 4.2983\n",
      "Epoch 76/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 437.5611 - mae: 14.4994 - val_loss: 27.8892 - val_mae: 3.9950\n",
      "Epoch 77/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 441.5546 - mae: 14.5750 - val_loss: 47.4481 - val_mae: 5.6642\n",
      "Epoch 78/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 422.4611 - mae: 14.3613 - val_loss: 25.3384 - val_mae: 3.7643\n",
      "Epoch 79/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 438.1978 - mae: 14.5343 - val_loss: 28.6833 - val_mae: 3.9773\n",
      "Epoch 80/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 428.8164 - mae: 14.3253 - val_loss: 25.8547 - val_mae: 3.6804\n",
      "Epoch 81/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 423.5225 - mae: 14.1567 - val_loss: 22.3492 - val_mae: 3.4658\n",
      "Epoch 82/550\n",
      "122/122 [==============================] - 0s 564us/step - loss: 437.7154 - mae: 14.5145 - val_loss: 27.7911 - val_mae: 3.9197\n",
      "Epoch 83/550\n",
      "122/122 [==============================] - 0s 708us/step - loss: 431.5258 - mae: 14.2342 - val_loss: 34.6218 - val_mae: 4.4429\n",
      "Epoch 84/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 438.1071 - mae: 14.2664 - val_loss: 26.6955 - val_mae: 3.7307\n",
      "Epoch 85/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 435.9725 - mae: 14.3577 - val_loss: 30.1573 - val_mae: 3.9837\n",
      "Epoch 86/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 414.0165 - mae: 13.8883 - val_loss: 44.8524 - val_mae: 5.2392\n",
      "Epoch 87/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 396.0349 - mae: 13.9091 - val_loss: 24.0958 - val_mae: 3.5781\n",
      "Epoch 88/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 407.9978 - mae: 14.0640 - val_loss: 30.9264 - val_mae: 4.1639\n",
      "Epoch 89/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 384.0627 - mae: 13.7071 - val_loss: 23.3148 - val_mae: 3.4554\n",
      "Epoch 90/550\n",
      "122/122 [==============================] - 0s 563us/step - loss: 402.3116 - mae: 13.7808 - val_loss: 30.2560 - val_mae: 4.1135\n",
      "Epoch 91/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 437.7463 - mae: 14.1346 - val_loss: 26.5220 - val_mae: 3.7547\n",
      "Epoch 92/550\n",
      "122/122 [==============================] - 0s 670us/step - loss: 390.3900 - mae: 13.7087 - val_loss: 25.5214 - val_mae: 3.5851\n",
      "Epoch 93/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 406.9042 - mae: 13.9495 - val_loss: 34.7211 - val_mae: 4.4021\n",
      "Epoch 94/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 395.6473 - mae: 13.8275 - val_loss: 23.6373 - val_mae: 3.3636\n",
      "Epoch 95/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 414.2504 - mae: 13.8789 - val_loss: 27.3205 - val_mae: 3.8359\n",
      "Epoch 96/550\n",
      "122/122 [==============================] - 0s 607us/step - loss: 388.3664 - mae: 13.5141 - val_loss: 28.0318 - val_mae: 3.7239\n",
      "Epoch 97/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 377.4739 - mae: 13.4098 - val_loss: 24.9060 - val_mae: 3.5601\n",
      "Epoch 98/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 383.7527 - mae: 13.2904 - val_loss: 34.7911 - val_mae: 4.3974\n",
      "Epoch 99/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 372.2971 - mae: 13.1831 - val_loss: 29.2517 - val_mae: 3.7570\n",
      "Epoch 100/550\n",
      "122/122 [==============================] - 0s 686us/step - loss: 409.1701 - mae: 13.6546 - val_loss: 33.6727 - val_mae: 3.8568\n",
      "Epoch 101/550\n",
      "122/122 [==============================] - 0s 593us/step - loss: 382.5950 - mae: 13.2784 - val_loss: 26.7435 - val_mae: 3.8736\n",
      "Epoch 102/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 400.9478 - mae: 13.7512 - val_loss: 25.7768 - val_mae: 3.6583\n",
      "Epoch 103/550\n",
      "122/122 [==============================] - 0s 563us/step - loss: 375.7486 - mae: 13.2049 - val_loss: 26.6827 - val_mae: 3.4954\n",
      "Epoch 104/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 366.0779 - mae: 12.9227 - val_loss: 30.2866 - val_mae: 4.1769\n",
      "Epoch 105/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 340.8863 - mae: 12.6866 - val_loss: 23.3559 - val_mae: 3.3981\n",
      "Epoch 106/550\n",
      "122/122 [==============================] - 0s 643us/step - loss: 367.7373 - mae: 13.0148 - val_loss: 28.6531 - val_mae: 3.9901\n",
      "Epoch 107/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 379.7154 - mae: 13.2505 - val_loss: 25.8648 - val_mae: 3.5981\n",
      "Epoch 108/550\n",
      "122/122 [==============================] - 0s 617us/step - loss: 375.0630 - mae: 13.1428 - val_loss: 38.7512 - val_mae: 4.8286\n",
      "Epoch 109/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 376.9452 - mae: 13.1366 - val_loss: 26.1353 - val_mae: 3.6311\n",
      "Epoch 110/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 371.6546 - mae: 12.9791 - val_loss: 24.3056 - val_mae: 3.4576\n",
      "Epoch 111/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 340.0001 - mae: 12.5744 - val_loss: 26.5324 - val_mae: 3.7085\n",
      "Epoch 112/550\n",
      "122/122 [==============================] - 0s 610us/step - loss: 349.7941 - mae: 12.8177 - val_loss: 48.4457 - val_mae: 5.6170\n",
      "Epoch 113/550\n",
      "122/122 [==============================] - 0s 612us/step - loss: 351.7622 - mae: 12.7021 - val_loss: 27.0714 - val_mae: 3.6170\n",
      "Epoch 114/550\n",
      "122/122 [==============================] - 0s 663us/step - loss: 348.8373 - mae: 12.7160 - val_loss: 23.9561 - val_mae: 3.4319\n",
      "Epoch 115/550\n",
      "122/122 [==============================] - 0s 608us/step - loss: 348.1010 - mae: 12.5223 - val_loss: 22.9171 - val_mae: 3.4107\n",
      "Epoch 116/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 345.4664 - mae: 12.4386 - val_loss: 26.5538 - val_mae: 3.6581\n",
      "Epoch 117/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 347.8419 - mae: 12.5909 - val_loss: 26.5695 - val_mae: 3.4482\n",
      "Epoch 118/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 334.0735 - mae: 12.3524 - val_loss: 23.4238 - val_mae: 3.3555\n",
      "Epoch 119/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 355.9215 - mae: 12.7681 - val_loss: 33.0322 - val_mae: 3.9461\n",
      "Epoch 120/550\n",
      "122/122 [==============================] - 0s 612us/step - loss: 331.9894 - mae: 12.4138 - val_loss: 46.6243 - val_mae: 5.2680\n",
      "Epoch 121/550\n",
      "122/122 [==============================] - 0s 663us/step - loss: 344.0789 - mae: 12.4852 - val_loss: 31.8972 - val_mae: 4.1122\n",
      "Epoch 122/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 314.0490 - mae: 12.1193 - val_loss: 29.5343 - val_mae: 4.0119\n",
      "Epoch 123/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 348.7096 - mae: 12.5531 - val_loss: 25.4050 - val_mae: 3.5731\n",
      "Epoch 124/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 345.0004 - mae: 12.3471 - val_loss: 22.7649 - val_mae: 3.3050\n",
      "Epoch 125/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 344.6630 - mae: 12.3768 - val_loss: 23.3215 - val_mae: 3.4695\n",
      "Epoch 126/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 344.4110 - mae: 12.3897 - val_loss: 27.1695 - val_mae: 3.7991\n",
      "Epoch 127/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 349.6235 - mae: 12.4253 - val_loss: 24.4974 - val_mae: 3.5819\n",
      "Epoch 128/550\n",
      "122/122 [==============================] - 0s 646us/step - loss: 349.4156 - mae: 12.5411 - val_loss: 27.2295 - val_mae: 3.7769\n",
      "Epoch 129/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 337.1630 - mae: 12.1995 - val_loss: 23.3307 - val_mae: 3.4280\n",
      "Epoch 130/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 320.8567 - mae: 12.1315 - val_loss: 27.5000 - val_mae: 3.8673\n",
      "Epoch 131/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 309.9898 - mae: 11.9329 - val_loss: 46.0313 - val_mae: 5.1510\n",
      "Epoch 132/550\n",
      "122/122 [==============================] - 0s 605us/step - loss: 334.6833 - mae: 12.2065 - val_loss: 22.3915 - val_mae: 3.3561\n",
      "Epoch 133/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 313.0542 - mae: 11.9715 - val_loss: 21.6046 - val_mae: 3.2742\n",
      "Epoch 134/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 307.0984 - mae: 11.7995 - val_loss: 35.8079 - val_mae: 4.3914\n",
      "Epoch 135/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 321.8135 - mae: 11.9595 - val_loss: 26.6903 - val_mae: 3.7770\n",
      "Epoch 136/550\n",
      "122/122 [==============================] - 0s 691us/step - loss: 306.5893 - mae: 11.6798 - val_loss: 57.9095 - val_mae: 5.7410\n",
      "Epoch 137/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 299.0785 - mae: 11.6864 - val_loss: 24.1887 - val_mae: 3.6276\n",
      "Epoch 138/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 312.0442 - mae: 11.7894 - val_loss: 22.1366 - val_mae: 3.2901\n",
      "Epoch 139/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 325.8713 - mae: 12.1870 - val_loss: 28.3773 - val_mae: 3.9748\n",
      "Epoch 140/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 310.7870 - mae: 11.7527 - val_loss: 22.4705 - val_mae: 3.2713\n",
      "Epoch 141/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 286.2577 - mae: 11.4204 - val_loss: 25.1327 - val_mae: 3.4989\n",
      "Epoch 142/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 274.0948 - mae: 11.1357 - val_loss: 23.9476 - val_mae: 3.4623\n",
      "Epoch 143/550\n",
      "122/122 [==============================] - 0s 670us/step - loss: 292.3598 - mae: 11.5345 - val_loss: 31.5741 - val_mae: 4.0977\n",
      "Epoch 144/550\n",
      "122/122 [==============================] - 0s 623us/step - loss: 304.0565 - mae: 11.6250 - val_loss: 23.0555 - val_mae: 3.3761\n",
      "Epoch 145/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 293.6082 - mae: 11.4868 - val_loss: 32.0391 - val_mae: 4.1644\n",
      "Epoch 146/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 294.3620 - mae: 11.3962 - val_loss: 30.7132 - val_mae: 4.2061\n",
      "Epoch 147/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 293.9585 - mae: 11.4491 - val_loss: 26.1324 - val_mae: 3.5611\n",
      "Epoch 148/550\n",
      "122/122 [==============================] - 0s 615us/step - loss: 292.1593 - mae: 11.4329 - val_loss: 29.5411 - val_mae: 3.9600\n",
      "Epoch 149/550\n",
      "122/122 [==============================] - 0s 653us/step - loss: 277.5079 - mae: 11.2663 - val_loss: 26.2989 - val_mae: 3.7385\n",
      "Epoch 150/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 276.8723 - mae: 11.1463 - val_loss: 23.4859 - val_mae: 3.3513\n",
      "Epoch 151/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 286.6764 - mae: 11.2515 - val_loss: 29.3054 - val_mae: 4.0504\n",
      "Epoch 152/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 281.8766 - mae: 11.1104 - val_loss: 23.4999 - val_mae: 3.3389\n",
      "Epoch 153/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 280.0569 - mae: 11.0362 - val_loss: 40.7148 - val_mae: 4.7972\n",
      "Epoch 154/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 282.0003 - mae: 11.1823 - val_loss: 23.8720 - val_mae: 3.4228\n",
      "Epoch 155/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 281.8094 - mae: 11.0606 - val_loss: 23.7936 - val_mae: 3.3917\n",
      "Epoch 156/550\n",
      "122/122 [==============================] - 0s 708us/step - loss: 285.3662 - mae: 11.0949 - val_loss: 27.5656 - val_mae: 3.7478\n",
      "Epoch 157/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 273.2752 - mae: 11.0445 - val_loss: 27.7451 - val_mae: 3.7972\n",
      "Epoch 158/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 273.1693 - mae: 10.9980 - val_loss: 25.7744 - val_mae: 3.6983\n",
      "Epoch 159/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 264.4710 - mae: 10.9091 - val_loss: 47.2682 - val_mae: 5.2632\n",
      "Epoch 160/550\n",
      "122/122 [==============================] - 0s 611us/step - loss: 258.0797 - mae: 10.7136 - val_loss: 23.3541 - val_mae: 3.4406\n",
      "Epoch 161/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 289.0345 - mae: 11.1843 - val_loss: 25.2463 - val_mae: 3.6377\n",
      "Epoch 162/550\n",
      "122/122 [==============================] - 0s 598us/step - loss: 259.0114 - mae: 10.5828 - val_loss: 23.3221 - val_mae: 3.3595\n",
      "Epoch 163/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 259.1630 - mae: 10.6624 - val_loss: 31.4427 - val_mae: 4.1816\n",
      "Epoch 164/550\n",
      "122/122 [==============================] - 0s 699us/step - loss: 250.9565 - mae: 10.4856 - val_loss: 27.1577 - val_mae: 3.8172\n",
      "Epoch 165/550\n",
      "122/122 [==============================] - 0s 599us/step - loss: 249.1656 - mae: 10.3444 - val_loss: 26.7699 - val_mae: 3.7890\n",
      "Epoch 166/550\n",
      "122/122 [==============================] - 0s 588us/step - loss: 263.7599 - mae: 10.6582 - val_loss: 24.1985 - val_mae: 3.4820\n",
      "Epoch 167/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 273.7024 - mae: 10.7511 - val_loss: 23.3758 - val_mae: 3.3733\n",
      "Epoch 168/550\n",
      "122/122 [==============================] - 0s 633us/step - loss: 251.7622 - mae: 10.4214 - val_loss: 22.7656 - val_mae: 3.3458\n",
      "Epoch 169/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 245.8899 - mae: 10.3799 - val_loss: 31.8655 - val_mae: 4.1108\n",
      "Epoch 170/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 255.2466 - mae: 10.4602 - val_loss: 40.3927 - val_mae: 4.8564\n",
      "Epoch 171/550\n",
      "122/122 [==============================] - 0s 654us/step - loss: 248.9802 - mae: 10.3361 - val_loss: 22.4186 - val_mae: 3.3566\n",
      "Epoch 172/550\n",
      "122/122 [==============================] - 0s 633us/step - loss: 236.9708 - mae: 10.2217 - val_loss: 24.8334 - val_mae: 3.7224\n",
      "Epoch 173/550\n",
      "122/122 [==============================] - 0s 600us/step - loss: 252.3456 - mae: 10.3028 - val_loss: 24.3026 - val_mae: 3.5706\n",
      "Epoch 174/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 232.4307 - mae: 10.1037 - val_loss: 23.4167 - val_mae: 3.4531\n",
      "Epoch 175/550\n",
      "122/122 [==============================] - 0s 615us/step - loss: 227.9375 - mae: 9.9282 - val_loss: 22.0746 - val_mae: 3.2797\n",
      "Epoch 176/550\n",
      "122/122 [==============================] - 0s 610us/step - loss: 239.9819 - mae: 10.1818 - val_loss: 24.2156 - val_mae: 3.4518\n",
      "Epoch 177/550\n",
      "122/122 [==============================] - 0s 683us/step - loss: 236.9153 - mae: 10.1245 - val_loss: 30.7391 - val_mae: 4.1477\n",
      "Epoch 178/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 208.4476 - mae: 9.6194 - val_loss: 26.0165 - val_mae: 3.6191\n",
      "Epoch 179/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 237.7450 - mae: 10.1031 - val_loss: 23.1869 - val_mae: 3.4229\n",
      "Epoch 180/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 236.3336 - mae: 10.0303 - val_loss: 28.3398 - val_mae: 3.9246\n",
      "Epoch 181/550\n",
      "122/122 [==============================] - 0s 608us/step - loss: 240.1767 - mae: 10.1843 - val_loss: 25.3428 - val_mae: 3.7468\n",
      "Epoch 182/550\n",
      "122/122 [==============================] - 0s 619us/step - loss: 230.2420 - mae: 9.8576 - val_loss: 23.1806 - val_mae: 3.4125\n",
      "Epoch 183/550\n",
      "122/122 [==============================] - 0s 618us/step - loss: 229.9066 - mae: 9.8898 - val_loss: 26.9348 - val_mae: 3.6858\n",
      "Epoch 184/550\n",
      "122/122 [==============================] - 0s 629us/step - loss: 233.4408 - mae: 9.8772 - val_loss: 25.1654 - val_mae: 3.4781\n",
      "Epoch 185/550\n",
      "122/122 [==============================] - 0s 621us/step - loss: 214.1423 - mae: 9.6599 - val_loss: 24.4110 - val_mae: 3.5643\n",
      "Epoch 186/550\n",
      "122/122 [==============================] - 0s 599us/step - loss: 225.8224 - mae: 9.7860 - val_loss: 24.7858 - val_mae: 3.5808\n",
      "Epoch 187/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 227.8935 - mae: 9.7165 - val_loss: 25.1615 - val_mae: 3.5837\n",
      "Epoch 188/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 222.6116 - mae: 9.8431 - val_loss: 24.1577 - val_mae: 3.4425\n",
      "Epoch 189/550\n",
      "122/122 [==============================] - 0s 712us/step - loss: 225.5022 - mae: 9.6618 - val_loss: 26.9041 - val_mae: 3.8444\n",
      "Epoch 190/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 216.5324 - mae: 9.6120 - val_loss: 23.6762 - val_mae: 3.3962\n",
      "Epoch 191/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 221.9017 - mae: 9.5387 - val_loss: 25.3197 - val_mae: 3.6256\n",
      "Epoch 192/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 226.6493 - mae: 9.7254 - val_loss: 27.7891 - val_mae: 3.9073\n",
      "Epoch 193/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 219.5355 - mae: 9.5421 - val_loss: 26.8751 - val_mae: 3.7488\n",
      "Epoch 194/550\n",
      "122/122 [==============================] - 0s 564us/step - loss: 210.6245 - mae: 9.4756 - val_loss: 26.2473 - val_mae: 3.6132\n",
      "Epoch 195/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 227.5413 - mae: 9.7676 - val_loss: 24.3136 - val_mae: 3.5471\n",
      "Epoch 196/550\n",
      "122/122 [==============================] - 0s 672us/step - loss: 210.8674 - mae: 9.3868 - val_loss: 28.0511 - val_mae: 3.8352\n",
      "Epoch 197/550\n",
      "122/122 [==============================] - 0s 632us/step - loss: 201.9739 - mae: 9.3052 - val_loss: 26.4087 - val_mae: 3.5771\n",
      "Epoch 198/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 217.7429 - mae: 9.6919 - val_loss: 23.8652 - val_mae: 3.4393\n",
      "Epoch 199/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 217.4917 - mae: 9.5374 - val_loss: 37.0714 - val_mae: 4.6758\n",
      "Epoch 200/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 217.2768 - mae: 9.5726 - val_loss: 27.2655 - val_mae: 3.7053\n",
      "Epoch 201/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 217.9138 - mae: 9.2633 - val_loss: 28.2202 - val_mae: 3.9538\n",
      "Epoch 202/550\n",
      "122/122 [==============================] - 0s 614us/step - loss: 187.4716 - mae: 8.9196 - val_loss: 28.4075 - val_mae: 3.9532\n",
      "Epoch 203/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 208.5182 - mae: 9.3230 - val_loss: 24.1763 - val_mae: 3.5322\n",
      "Epoch 204/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 199.9620 - mae: 9.2231 - val_loss: 26.6141 - val_mae: 3.7399\n",
      "Epoch 205/550\n",
      "122/122 [==============================] - 0s 564us/step - loss: 195.5747 - mae: 9.1030 - val_loss: 31.4994 - val_mae: 4.1154\n",
      "Epoch 206/550\n",
      "122/122 [==============================] - 0s 766us/step - loss: 196.3651 - mae: 9.1028 - val_loss: 38.3324 - val_mae: 4.6658\n",
      "Epoch 207/550\n",
      "122/122 [==============================] - 0s 598us/step - loss: 217.9551 - mae: 9.3019 - val_loss: 25.1813 - val_mae: 3.5311\n",
      "Epoch 208/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 195.0296 - mae: 9.0432 - val_loss: 31.3182 - val_mae: 4.1826\n",
      "Epoch 209/550\n",
      "122/122 [==============================] - 0s 588us/step - loss: 189.6500 - mae: 8.8959 - val_loss: 30.4916 - val_mae: 4.0480\n",
      "Epoch 210/550\n",
      "122/122 [==============================] - 0s 605us/step - loss: 183.7558 - mae: 8.9085 - val_loss: 54.5706 - val_mae: 5.7855\n",
      "Epoch 211/550\n",
      "122/122 [==============================] - 0s 593us/step - loss: 190.8568 - mae: 9.0743 - val_loss: 26.9434 - val_mae: 3.7322\n",
      "Epoch 212/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 198.9172 - mae: 9.1230 - val_loss: 46.5481 - val_mae: 5.1544\n",
      "Epoch 213/550\n",
      "122/122 [==============================] - 0s 640us/step - loss: 189.6602 - mae: 8.9650 - val_loss: 25.4227 - val_mae: 3.5889\n",
      "Epoch 214/550\n",
      "122/122 [==============================] - 0s 612us/step - loss: 185.4212 - mae: 8.9053 - val_loss: 31.6151 - val_mae: 4.2507\n",
      "Epoch 215/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 185.3002 - mae: 8.7915 - val_loss: 26.6129 - val_mae: 3.7630\n",
      "Epoch 216/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 200.9030 - mae: 9.0191 - val_loss: 27.2888 - val_mae: 3.8372\n",
      "Epoch 217/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 193.7717 - mae: 8.9044 - val_loss: 33.2031 - val_mae: 4.3586\n",
      "Epoch 218/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 181.4842 - mae: 8.8376 - val_loss: 31.3113 - val_mae: 4.1524\n",
      "Epoch 219/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 179.2952 - mae: 8.7110 - val_loss: 23.9662 - val_mae: 3.4683\n",
      "Epoch 220/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 169.1168 - mae: 8.6581 - val_loss: 30.5888 - val_mae: 4.0136\n",
      "Epoch 221/550\n",
      "122/122 [==============================] - 0s 632us/step - loss: 174.5964 - mae: 8.7189 - val_loss: 24.6651 - val_mae: 3.5354\n",
      "Epoch 222/550\n",
      "122/122 [==============================] - 0s 617us/step - loss: 186.3417 - mae: 8.7573 - val_loss: 27.7120 - val_mae: 3.8172\n",
      "Epoch 223/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 178.6177 - mae: 8.6885 - val_loss: 36.4841 - val_mae: 4.6459\n",
      "Epoch 224/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 177.8004 - mae: 8.6634 - val_loss: 26.7270 - val_mae: 3.6799\n",
      "Epoch 225/550\n",
      "122/122 [==============================] - 0s 603us/step - loss: 168.0101 - mae: 8.5426 - val_loss: 37.3636 - val_mae: 4.7708\n",
      "Epoch 226/550\n",
      "122/122 [==============================] - 0s 617us/step - loss: 186.5264 - mae: 8.8397 - val_loss: 46.2719 - val_mae: 5.1972\n",
      "Epoch 227/550\n",
      "122/122 [==============================] - 0s 616us/step - loss: 174.2293 - mae: 8.5299 - val_loss: 25.2972 - val_mae: 3.6509\n",
      "Epoch 228/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 174.3385 - mae: 8.5267 - val_loss: 25.5190 - val_mae: 3.5876\n",
      "Epoch 229/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 169.8225 - mae: 8.4462 - val_loss: 23.4406 - val_mae: 3.4161\n",
      "Epoch 230/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 180.5823 - mae: 8.6332 - val_loss: 24.6679 - val_mae: 3.5336\n",
      "Epoch 231/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 164.9163 - mae: 8.3690 - val_loss: 26.3191 - val_mae: 3.6960\n",
      "Epoch 232/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 159.8006 - mae: 8.3045 - val_loss: 31.6812 - val_mae: 4.2184\n",
      "Epoch 233/550\n",
      "122/122 [==============================] - 0s 653us/step - loss: 167.9356 - mae: 8.3894 - val_loss: 24.5238 - val_mae: 3.6016\n",
      "Epoch 234/550\n",
      "122/122 [==============================] - 0s 607us/step - loss: 157.4411 - mae: 8.2628 - val_loss: 25.4257 - val_mae: 3.6412\n",
      "Epoch 235/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 176.9635 - mae: 8.6853 - val_loss: 24.1767 - val_mae: 3.5394\n",
      "Epoch 236/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 171.3118 - mae: 8.4123 - val_loss: 24.8424 - val_mae: 3.5657\n",
      "Epoch 237/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 163.2523 - mae: 8.3993 - val_loss: 25.6517 - val_mae: 3.5919\n",
      "Epoch 238/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 163.9775 - mae: 8.4015 - val_loss: 32.3546 - val_mae: 4.2643\n",
      "Epoch 239/550\n",
      "122/122 [==============================] - 0s 648us/step - loss: 166.2767 - mae: 8.3122 - val_loss: 23.9231 - val_mae: 3.5951\n",
      "Epoch 240/550\n",
      "122/122 [==============================] - 0s 612us/step - loss: 163.4030 - mae: 8.3927 - val_loss: 30.7046 - val_mae: 4.1596\n",
      "Epoch 241/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 166.0958 - mae: 8.3777 - val_loss: 27.2997 - val_mae: 3.8337\n",
      "Epoch 242/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 150.9599 - mae: 8.1631 - val_loss: 39.0291 - val_mae: 4.7161\n",
      "Epoch 243/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 156.0201 - mae: 8.1869 - val_loss: 26.2608 - val_mae: 3.7208\n",
      "Epoch 244/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 156.9668 - mae: 8.1291 - val_loss: 44.2055 - val_mae: 5.1006\n",
      "Epoch 245/550\n",
      "122/122 [==============================] - 0s 629us/step - loss: 158.2943 - mae: 8.1850 - val_loss: 29.7481 - val_mae: 3.9708\n",
      "Epoch 246/550\n",
      "122/122 [==============================] - 0s 693us/step - loss: 154.3678 - mae: 8.1953 - val_loss: 28.7615 - val_mae: 3.9251\n",
      "Epoch 247/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 152.2861 - mae: 8.1700 - val_loss: 24.4410 - val_mae: 3.5548\n",
      "Epoch 248/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 149.1263 - mae: 7.9883 - val_loss: 24.4537 - val_mae: 3.5429\n",
      "Epoch 249/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 145.8848 - mae: 8.0280 - val_loss: 24.0378 - val_mae: 3.5238\n",
      "Epoch 250/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 145.8262 - mae: 7.9417 - val_loss: 23.5344 - val_mae: 3.5099\n",
      "Epoch 251/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 148.1458 - mae: 7.9988 - val_loss: 39.1504 - val_mae: 4.7827\n",
      "Epoch 252/550\n",
      "122/122 [==============================] - 0s 692us/step - loss: 155.9815 - mae: 8.1930 - val_loss: 24.5260 - val_mae: 3.5511\n",
      "Epoch 253/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 151.1940 - mae: 7.9785 - val_loss: 34.6358 - val_mae: 4.4700\n",
      "Epoch 254/550\n",
      "122/122 [==============================] - 0s 593us/step - loss: 148.7766 - mae: 8.1112 - val_loss: 33.5812 - val_mae: 4.3989\n",
      "Epoch 255/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 142.0235 - mae: 7.8992 - val_loss: 24.7879 - val_mae: 3.5749\n",
      "Epoch 256/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 143.9231 - mae: 7.8893 - val_loss: 30.1622 - val_mae: 4.0287\n",
      "Epoch 257/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 147.9063 - mae: 8.0137 - val_loss: 38.7635 - val_mae: 4.7588\n",
      "Epoch 258/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 153.7759 - mae: 8.1011 - val_loss: 26.8242 - val_mae: 3.7269\n",
      "Epoch 259/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 150.7176 - mae: 8.1265 - val_loss: 26.4164 - val_mae: 3.6797\n",
      "Epoch 260/550\n",
      "122/122 [==============================] - 0s 705us/step - loss: 145.2829 - mae: 7.8947 - val_loss: 27.8837 - val_mae: 3.8147\n",
      "Epoch 261/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 134.7051 - mae: 7.6651 - val_loss: 26.9925 - val_mae: 3.7991\n",
      "Epoch 262/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 144.3361 - mae: 7.8723 - val_loss: 29.5383 - val_mae: 3.9212\n",
      "Epoch 263/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 146.5268 - mae: 7.8862 - val_loss: 33.1357 - val_mae: 4.2812\n",
      "Epoch 264/550\n",
      "122/122 [==============================] - 0s 603us/step - loss: 146.4226 - mae: 7.9509 - val_loss: 47.9511 - val_mae: 5.1443\n",
      "Epoch 265/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 135.0340 - mae: 7.8293 - val_loss: 32.4435 - val_mae: 4.3071\n",
      "Epoch 266/550\n",
      "122/122 [==============================] - 0s 617us/step - loss: 153.6896 - mae: 8.0754 - val_loss: 31.2986 - val_mae: 4.2502\n",
      "Epoch 267/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 143.9221 - mae: 7.8350 - val_loss: 26.6691 - val_mae: 3.6398\n",
      "Epoch 268/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 133.1427 - mae: 7.7699 - val_loss: 33.6447 - val_mae: 4.4126\n",
      "Epoch 269/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 135.9301 - mae: 7.7330 - val_loss: 31.0927 - val_mae: 4.1586\n",
      "Epoch 270/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 133.8508 - mae: 7.7270 - val_loss: 28.8032 - val_mae: 3.8809\n",
      "Epoch 271/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 135.6270 - mae: 7.8648 - val_loss: 31.0805 - val_mae: 4.1909\n",
      "Epoch 272/550\n",
      "122/122 [==============================] - 0s 611us/step - loss: 135.0370 - mae: 7.8120 - val_loss: 25.6060 - val_mae: 3.5718\n",
      "Epoch 273/550\n",
      "122/122 [==============================] - 0s 666us/step - loss: 132.0276 - mae: 7.7542 - val_loss: 26.1537 - val_mae: 3.7002\n",
      "Epoch 274/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 143.7139 - mae: 7.9242 - val_loss: 29.4910 - val_mae: 3.8804\n",
      "Epoch 275/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 145.9758 - mae: 7.9308 - val_loss: 29.8308 - val_mae: 4.0537\n",
      "Epoch 276/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 139.8728 - mae: 7.8540 - val_loss: 26.5256 - val_mae: 3.6814\n",
      "Epoch 277/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 136.4073 - mae: 7.8091 - val_loss: 38.8531 - val_mae: 4.7628\n",
      "Epoch 278/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 131.9943 - mae: 7.6243 - val_loss: 28.0621 - val_mae: 3.9408\n",
      "Epoch 279/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 134.5259 - mae: 7.7434 - val_loss: 27.3629 - val_mae: 3.7287\n",
      "Epoch 280/550\n",
      "122/122 [==============================] - 0s 659us/step - loss: 134.3739 - mae: 7.6915 - val_loss: 28.5877 - val_mae: 3.9147\n",
      "Epoch 281/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 133.7351 - mae: 7.7218 - val_loss: 29.3967 - val_mae: 3.7986\n",
      "Epoch 282/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 131.9791 - mae: 7.6939 - val_loss: 28.2497 - val_mae: 3.8630\n",
      "Epoch 283/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 134.1556 - mae: 7.7365 - val_loss: 33.9613 - val_mae: 4.3330\n",
      "Epoch 284/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 126.6498 - mae: 7.5116 - val_loss: 28.3912 - val_mae: 3.8243\n",
      "Epoch 285/550\n",
      "122/122 [==============================] - 0s 640us/step - loss: 132.4721 - mae: 7.6692 - val_loss: 26.0565 - val_mae: 3.6692\n",
      "Epoch 286/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 124.9158 - mae: 7.4710 - val_loss: 26.8696 - val_mae: 3.6856\n",
      "Epoch 287/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 131.2810 - mae: 7.6605 - val_loss: 26.8892 - val_mae: 3.7953\n",
      "Epoch 288/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 132.6217 - mae: 7.6594 - val_loss: 24.5872 - val_mae: 3.5252\n",
      "Epoch 289/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 121.6611 - mae: 7.5175 - val_loss: 25.2242 - val_mae: 3.5804\n",
      "Epoch 290/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 123.4605 - mae: 7.5184 - val_loss: 24.7202 - val_mae: 3.6028\n",
      "Epoch 291/550\n",
      "122/122 [==============================] - 0s 633us/step - loss: 130.7279 - mae: 7.7097 - val_loss: 28.4712 - val_mae: 3.9978\n",
      "Epoch 292/550\n",
      "122/122 [==============================] - 0s 630us/step - loss: 125.5975 - mae: 7.5988 - val_loss: 30.8719 - val_mae: 4.2096\n",
      "Epoch 293/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 128.2793 - mae: 7.6125 - val_loss: 26.4663 - val_mae: 3.6818\n",
      "Epoch 294/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 125.4286 - mae: 7.6215 - val_loss: 41.5320 - val_mae: 4.9662\n",
      "Epoch 295/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 135.4614 - mae: 7.6116 - val_loss: 27.2766 - val_mae: 3.7778\n",
      "Epoch 296/550\n",
      "122/122 [==============================] - 0s 601us/step - loss: 126.9682 - mae: 7.6275 - val_loss: 33.1072 - val_mae: 4.3118\n",
      "Epoch 297/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 134.8216 - mae: 7.7619 - val_loss: 27.0051 - val_mae: 3.7662\n",
      "Epoch 298/550\n",
      "122/122 [==============================] - 0s 652us/step - loss: 121.9736 - mae: 7.4915 - val_loss: 25.8527 - val_mae: 3.6337\n",
      "Epoch 299/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 122.5615 - mae: 7.5343 - val_loss: 26.3901 - val_mae: 3.7303\n",
      "Epoch 300/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 122.9530 - mae: 7.4761 - val_loss: 29.1992 - val_mae: 3.9576\n",
      "Epoch 301/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 124.9644 - mae: 7.4624 - val_loss: 26.4329 - val_mae: 3.6175\n",
      "Epoch 302/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 116.0598 - mae: 7.3764 - val_loss: 25.0018 - val_mae: 3.5651\n",
      "Epoch 303/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 117.1961 - mae: 7.3815 - val_loss: 23.5113 - val_mae: 3.4811\n",
      "Epoch 304/550\n",
      "122/122 [==============================] - 0s 692us/step - loss: 135.2923 - mae: 7.6977 - val_loss: 36.1216 - val_mae: 4.5564\n",
      "Epoch 305/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 115.5939 - mae: 7.3662 - val_loss: 38.5593 - val_mae: 4.8156\n",
      "Epoch 306/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 121.4839 - mae: 7.3657 - val_loss: 25.9299 - val_mae: 3.7426\n",
      "Epoch 307/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 126.2960 - mae: 7.5611 - val_loss: 27.5920 - val_mae: 3.8254\n",
      "Epoch 308/550\n",
      "122/122 [==============================] - 0s 632us/step - loss: 114.0892 - mae: 7.3108 - val_loss: 24.5374 - val_mae: 3.5245\n",
      "Epoch 309/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 115.7146 - mae: 7.3166 - val_loss: 26.8488 - val_mae: 3.7376\n",
      "Epoch 310/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 115.4039 - mae: 7.2878 - val_loss: 26.7661 - val_mae: 3.6515\n",
      "Epoch 311/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 120.2706 - mae: 7.4828 - val_loss: 26.4987 - val_mae: 3.6501\n",
      "Epoch 312/550\n",
      "122/122 [==============================] - 0s 640us/step - loss: 109.4888 - mae: 7.2343 - val_loss: 27.5193 - val_mae: 3.8221\n",
      "Epoch 313/550\n",
      "122/122 [==============================] - 0s 596us/step - loss: 110.1408 - mae: 7.2002 - val_loss: 32.0996 - val_mae: 4.2220\n",
      "Epoch 314/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 117.7811 - mae: 7.4265 - val_loss: 28.4110 - val_mae: 3.9652\n",
      "Epoch 315/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 112.2098 - mae: 7.3375 - val_loss: 43.6615 - val_mae: 5.1475\n",
      "Epoch 316/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 110.1630 - mae: 7.1910 - val_loss: 29.9323 - val_mae: 4.0732\n",
      "Epoch 317/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 117.6123 - mae: 7.3994 - val_loss: 29.7767 - val_mae: 3.9824\n",
      "Epoch 318/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 108.5296 - mae: 7.2076 - val_loss: 29.0031 - val_mae: 3.9922\n",
      "Epoch 319/550\n",
      "122/122 [==============================] - 0s 684us/step - loss: 115.0917 - mae: 7.2697 - val_loss: 24.6443 - val_mae: 3.5362\n",
      "Epoch 320/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 111.7812 - mae: 7.2483 - val_loss: 26.0898 - val_mae: 3.6089\n",
      "Epoch 321/550\n",
      "122/122 [==============================] - 0s 594us/step - loss: 111.8835 - mae: 7.2395 - val_loss: 25.8657 - val_mae: 3.6042\n",
      "Epoch 322/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 120.9473 - mae: 7.4575 - val_loss: 29.5682 - val_mae: 4.0486\n",
      "Epoch 323/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 104.3846 - mae: 7.1089 - val_loss: 25.2227 - val_mae: 3.5413\n",
      "Epoch 324/550\n",
      "122/122 [==============================] - 0s 563us/step - loss: 112.4018 - mae: 7.3350 - val_loss: 27.1234 - val_mae: 3.6683\n",
      "Epoch 325/550\n",
      "122/122 [==============================] - 0s 636us/step - loss: 117.4311 - mae: 7.4292 - val_loss: 28.8446 - val_mae: 3.9318\n",
      "Epoch 326/550\n",
      "122/122 [==============================] - 0s 623us/step - loss: 113.4069 - mae: 7.2463 - val_loss: 31.0204 - val_mae: 4.1222\n",
      "Epoch 327/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 106.8488 - mae: 7.0821 - val_loss: 25.7277 - val_mae: 3.6346\n",
      "Epoch 328/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 109.3409 - mae: 7.0990 - val_loss: 26.5558 - val_mae: 3.6437\n",
      "Epoch 329/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 108.4092 - mae: 7.2290 - val_loss: 26.6101 - val_mae: 3.7215\n",
      "Epoch 330/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 112.8238 - mae: 7.2343 - val_loss: 25.5760 - val_mae: 3.5930\n",
      "Epoch 331/550\n",
      "122/122 [==============================] - 0s 654us/step - loss: 108.7264 - mae: 7.1711 - val_loss: 32.1469 - val_mae: 4.1126\n",
      "Epoch 332/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 107.8577 - mae: 7.1332 - val_loss: 29.1383 - val_mae: 3.8348\n",
      "Epoch 333/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 109.3365 - mae: 7.1806 - val_loss: 30.1416 - val_mae: 4.0173\n",
      "Epoch 334/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 102.6071 - mae: 7.0236 - val_loss: 25.2993 - val_mae: 3.5746\n",
      "Epoch 335/550\n",
      "122/122 [==============================] - 0s 601us/step - loss: 104.8790 - mae: 7.0985 - val_loss: 34.3532 - val_mae: 4.3413\n",
      "Epoch 336/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 106.8156 - mae: 7.0833 - val_loss: 27.2559 - val_mae: 3.7065\n",
      "Epoch 337/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 106.4295 - mae: 7.0974 - val_loss: 26.7032 - val_mae: 3.6448\n",
      "Epoch 338/550\n",
      "122/122 [==============================] - 0s 653us/step - loss: 110.1730 - mae: 7.2383 - val_loss: 28.3911 - val_mae: 3.7709\n",
      "Epoch 339/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 105.4481 - mae: 7.0395 - val_loss: 26.6219 - val_mae: 3.6562\n",
      "Epoch 340/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 100.0778 - mae: 6.9719 - val_loss: 30.5624 - val_mae: 4.0500\n",
      "Epoch 341/550\n",
      "122/122 [==============================] - 0s 565us/step - loss: 107.2598 - mae: 7.1645 - val_loss: 27.8337 - val_mae: 3.6946\n",
      "Epoch 342/550\n",
      "122/122 [==============================] - 0s 562us/step - loss: 105.9459 - mae: 7.0748 - val_loss: 26.1345 - val_mae: 3.5661\n",
      "Epoch 343/550\n",
      "122/122 [==============================] - 0s 698us/step - loss: 100.9213 - mae: 7.0353 - val_loss: 38.8555 - val_mae: 4.5944\n",
      "Epoch 344/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 104.8760 - mae: 7.0031 - val_loss: 28.1045 - val_mae: 3.8100\n",
      "Epoch 345/550\n",
      "122/122 [==============================] - 0s 564us/step - loss: 109.6488 - mae: 7.2096 - val_loss: 45.2024 - val_mae: 5.0657\n",
      "Epoch 346/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 103.4073 - mae: 7.0246 - val_loss: 28.1509 - val_mae: 3.7721\n",
      "Epoch 347/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 103.6685 - mae: 6.9572 - val_loss: 25.6833 - val_mae: 3.5479\n",
      "Epoch 348/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 104.6652 - mae: 7.0597 - val_loss: 33.2265 - val_mae: 4.2230\n",
      "Epoch 349/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 103.7875 - mae: 7.0210 - val_loss: 31.0332 - val_mae: 4.0463\n",
      "Epoch 350/550\n",
      "122/122 [==============================] - 0s 635us/step - loss: 96.4308 - mae: 6.9566 - val_loss: 28.1660 - val_mae: 3.7894\n",
      "Epoch 351/550\n",
      "122/122 [==============================] - 0s 604us/step - loss: 97.4428 - mae: 6.9696 - val_loss: 33.9074 - val_mae: 4.2814\n",
      "Epoch 352/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 98.1296 - mae: 6.7852 - val_loss: 35.7922 - val_mae: 4.4637\n",
      "Epoch 353/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 99.2327 - mae: 6.8401 - val_loss: 31.2948 - val_mae: 3.9344\n",
      "Epoch 354/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 96.3778 - mae: 6.7946 - val_loss: 28.1930 - val_mae: 3.6613\n",
      "Epoch 355/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 98.4814 - mae: 6.9659 - val_loss: 29.9098 - val_mae: 3.9674\n",
      "Epoch 356/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 104.9471 - mae: 7.1245 - val_loss: 27.9746 - val_mae: 3.6984\n",
      "Epoch 357/550\n",
      "122/122 [==============================] - 0s 643us/step - loss: 104.6495 - mae: 7.0451 - val_loss: 36.6493 - val_mae: 4.5582\n",
      "Epoch 358/550\n",
      "122/122 [==============================] - 0s 615us/step - loss: 98.9980 - mae: 6.8488 - val_loss: 26.7130 - val_mae: 3.5512\n",
      "Epoch 359/550\n",
      "122/122 [==============================] - 0s 613us/step - loss: 99.2441 - mae: 6.8673 - val_loss: 38.3843 - val_mae: 4.5322\n",
      "Epoch 360/550\n",
      "122/122 [==============================] - 0s 588us/step - loss: 105.2250 - mae: 7.1018 - val_loss: 28.9331 - val_mae: 3.7226\n",
      "Epoch 361/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 103.5536 - mae: 6.9109 - val_loss: 35.1017 - val_mae: 4.3209\n",
      "Epoch 362/550\n",
      "122/122 [==============================] - 0s 688us/step - loss: 95.7896 - mae: 6.8273 - val_loss: 29.2897 - val_mae: 3.9313\n",
      "Epoch 363/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 99.8873 - mae: 7.0064 - val_loss: 30.0634 - val_mae: 3.8792\n",
      "Epoch 364/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 101.0805 - mae: 6.9304 - val_loss: 40.1364 - val_mae: 4.7415\n",
      "Epoch 365/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 96.9001 - mae: 6.8534 - val_loss: 32.6681 - val_mae: 4.1461\n",
      "Epoch 366/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 93.5721 - mae: 6.8373 - val_loss: 32.0778 - val_mae: 3.9578\n",
      "Epoch 367/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 94.2132 - mae: 6.8013 - val_loss: 40.8295 - val_mae: 4.7534\n",
      "Epoch 368/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 98.4492 - mae: 6.8651 - val_loss: 32.4303 - val_mae: 4.1404\n",
      "Epoch 369/550\n",
      "122/122 [==============================] - 0s 652us/step - loss: 91.8017 - mae: 6.7537 - val_loss: 30.7190 - val_mae: 4.0441\n",
      "Epoch 370/550\n",
      "122/122 [==============================] - 0s 612us/step - loss: 94.1337 - mae: 6.8019 - val_loss: 31.0839 - val_mae: 4.0478\n",
      "Epoch 371/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 100.3872 - mae: 6.9533 - val_loss: 25.6813 - val_mae: 3.5109\n",
      "Epoch 372/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 93.7847 - mae: 6.8460 - val_loss: 27.4764 - val_mae: 3.6348\n",
      "Epoch 373/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 96.0658 - mae: 6.8231 - val_loss: 32.3322 - val_mae: 4.1856\n",
      "Epoch 374/550\n",
      "122/122 [==============================] - 0s 655us/step - loss: 94.2675 - mae: 6.8141 - val_loss: 29.1994 - val_mae: 3.8547\n",
      "Epoch 375/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 93.8178 - mae: 6.7695 - val_loss: 29.2481 - val_mae: 3.8009\n",
      "Epoch 376/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 92.4482 - mae: 6.7191 - val_loss: 31.7107 - val_mae: 4.0954\n",
      "Epoch 377/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 89.0802 - mae: 6.6726 - val_loss: 29.1008 - val_mae: 3.7392\n",
      "Epoch 378/550\n",
      "122/122 [==============================] - 0s 596us/step - loss: 92.2051 - mae: 6.8034 - val_loss: 33.7733 - val_mae: 4.2030\n",
      "Epoch 379/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 91.6383 - mae: 6.7708 - val_loss: 27.9317 - val_mae: 3.6142\n",
      "Epoch 380/550\n",
      "122/122 [==============================] - 0s 653us/step - loss: 93.2619 - mae: 6.7756 - val_loss: 39.6194 - val_mae: 4.6471\n",
      "Epoch 381/550\n",
      "122/122 [==============================] - 0s 596us/step - loss: 89.9014 - mae: 6.6492 - val_loss: 28.9542 - val_mae: 3.7222\n",
      "Epoch 382/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 93.1973 - mae: 6.6472 - val_loss: 31.7739 - val_mae: 4.0661\n",
      "Epoch 383/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 88.1991 - mae: 6.6986 - val_loss: 31.2517 - val_mae: 3.9573\n",
      "Epoch 384/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 92.7385 - mae: 6.7369 - val_loss: 35.2772 - val_mae: 4.3352\n",
      "Epoch 385/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 97.9058 - mae: 6.9601 - val_loss: 28.8599 - val_mae: 3.7506\n",
      "Epoch 386/550\n",
      "122/122 [==============================] - 0s 594us/step - loss: 85.4170 - mae: 6.5366 - val_loss: 29.6201 - val_mae: 3.8382\n",
      "Epoch 387/550\n",
      "122/122 [==============================] - 0s 674us/step - loss: 91.4557 - mae: 6.6835 - val_loss: 36.7085 - val_mae: 4.2537\n",
      "Epoch 388/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 89.0988 - mae: 6.6945 - val_loss: 30.1624 - val_mae: 3.7707\n",
      "Epoch 389/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 93.9634 - mae: 6.7556 - val_loss: 39.9571 - val_mae: 4.7357\n",
      "Epoch 390/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 99.2474 - mae: 6.9041 - val_loss: 32.1181 - val_mae: 4.0563\n",
      "Epoch 391/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 89.0579 - mae: 6.6302 - val_loss: 29.1351 - val_mae: 3.7325\n",
      "Epoch 392/550\n",
      "122/122 [==============================] - 0s 628us/step - loss: 83.5073 - mae: 6.4772 - val_loss: 33.8471 - val_mae: 4.2855\n",
      "Epoch 393/550\n",
      "122/122 [==============================] - 0s 662us/step - loss: 91.7638 - mae: 6.7003 - val_loss: 41.1807 - val_mae: 4.8783\n",
      "Epoch 394/550\n",
      "122/122 [==============================] - 0s 605us/step - loss: 85.5215 - mae: 6.4965 - val_loss: 30.1733 - val_mae: 3.8011\n",
      "Epoch 395/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 92.8179 - mae: 6.6796 - val_loss: 29.8228 - val_mae: 3.7855\n",
      "Epoch 396/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 84.3087 - mae: 6.5243 - val_loss: 36.6260 - val_mae: 4.4376\n",
      "Epoch 397/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 88.7038 - mae: 6.5776 - val_loss: 39.7110 - val_mae: 4.5439\n",
      "Epoch 398/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 86.6625 - mae: 6.5810 - val_loss: 32.6845 - val_mae: 4.1372\n",
      "Epoch 399/550\n",
      "122/122 [==============================] - 0s 642us/step - loss: 90.1718 - mae: 6.6176 - val_loss: 51.9748 - val_mae: 5.4434\n",
      "Epoch 400/550\n",
      "122/122 [==============================] - 0s 610us/step - loss: 83.4159 - mae: 6.4526 - val_loss: 27.3411 - val_mae: 3.5839\n",
      "Epoch 401/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 89.8079 - mae: 6.6323 - val_loss: 36.2555 - val_mae: 4.2803\n",
      "Epoch 402/550\n",
      "122/122 [==============================] - 0s 676us/step - loss: 84.2106 - mae: 6.4539 - val_loss: 31.3452 - val_mae: 4.0289\n",
      "Epoch 403/550\n",
      "122/122 [==============================] - 0s 723us/step - loss: 88.3378 - mae: 6.5701 - val_loss: 31.1353 - val_mae: 3.8385\n",
      "Epoch 404/550\n",
      "122/122 [==============================] - 0s 951us/step - loss: 86.2314 - mae: 6.5185 - val_loss: 39.0061 - val_mae: 4.6702\n",
      "Epoch 405/550\n",
      "122/122 [==============================] - 0s 927us/step - loss: 87.0160 - mae: 6.5777 - val_loss: 46.7672 - val_mae: 5.1080\n",
      "Epoch 406/550\n",
      "122/122 [==============================] - 0s 856us/step - loss: 87.6575 - mae: 6.5304 - val_loss: 30.3550 - val_mae: 3.9063\n",
      "Epoch 407/550\n",
      "122/122 [==============================] - 0s 934us/step - loss: 86.7325 - mae: 6.5683 - val_loss: 30.6110 - val_mae: 3.9078\n",
      "Epoch 408/550\n",
      "122/122 [==============================] - 0s 683us/step - loss: 84.5667 - mae: 6.6185 - val_loss: 41.3283 - val_mae: 4.8076\n",
      "Epoch 409/550\n",
      "122/122 [==============================] - 0s 590us/step - loss: 82.1646 - mae: 6.5247 - val_loss: 37.2608 - val_mae: 4.4317\n",
      "Epoch 410/550\n",
      "122/122 [==============================] - 0s 656us/step - loss: 90.2639 - mae: 6.6625 - val_loss: 29.7978 - val_mae: 3.7604\n",
      "Epoch 411/550\n",
      "122/122 [==============================] - 0s 624us/step - loss: 78.3199 - mae: 6.3151 - val_loss: 32.2099 - val_mae: 4.0838\n",
      "Epoch 412/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 92.2900 - mae: 6.7781 - val_loss: 29.0117 - val_mae: 3.7509\n",
      "Epoch 413/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 84.0997 - mae: 6.5029 - val_loss: 32.9243 - val_mae: 4.1061\n",
      "Epoch 414/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 82.2732 - mae: 6.5415 - val_loss: 36.2631 - val_mae: 4.3381\n",
      "Epoch 415/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 78.8724 - mae: 6.4048 - val_loss: 31.5736 - val_mae: 4.0221\n",
      "Epoch 416/550\n",
      "122/122 [==============================] - 0s 672us/step - loss: 79.8909 - mae: 6.4345 - val_loss: 34.8214 - val_mae: 4.2444\n",
      "Epoch 417/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 78.3470 - mae: 6.3622 - val_loss: 30.3416 - val_mae: 3.7459\n",
      "Epoch 418/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 87.4426 - mae: 6.5969 - val_loss: 27.1192 - val_mae: 3.5763\n",
      "Epoch 419/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 83.6556 - mae: 6.4718 - val_loss: 30.8687 - val_mae: 3.8636\n",
      "Epoch 420/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 81.6640 - mae: 6.3939 - val_loss: 32.2793 - val_mae: 3.9898\n",
      "Epoch 421/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 80.3503 - mae: 6.4067 - val_loss: 40.1617 - val_mae: 4.7529\n",
      "Epoch 422/550\n",
      "122/122 [==============================] - 0s 707us/step - loss: 90.4125 - mae: 6.6210 - val_loss: 32.6299 - val_mae: 4.1030\n",
      "Epoch 423/550\n",
      "122/122 [==============================] - 0s 771us/step - loss: 79.9022 - mae: 6.3586 - val_loss: 27.8128 - val_mae: 3.7473\n",
      "Epoch 424/550\n",
      "122/122 [==============================] - 0s 618us/step - loss: 85.6495 - mae: 6.5601 - val_loss: 37.2966 - val_mae: 4.6156\n",
      "Epoch 425/550\n",
      "122/122 [==============================] - 0s 609us/step - loss: 78.2245 - mae: 6.2893 - val_loss: 28.3671 - val_mae: 3.7079\n",
      "Epoch 426/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 80.8939 - mae: 6.5017 - val_loss: 33.8108 - val_mae: 4.2228\n",
      "Epoch 427/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 86.6619 - mae: 6.6455 - val_loss: 29.1141 - val_mae: 3.7864\n",
      "Epoch 428/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 83.3908 - mae: 6.5614 - val_loss: 28.2133 - val_mae: 3.7336\n",
      "Epoch 429/550\n",
      "122/122 [==============================] - 0s 693us/step - loss: 77.8487 - mae: 6.2170 - val_loss: 29.3160 - val_mae: 3.8395\n",
      "Epoch 430/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 82.0288 - mae: 6.4797 - val_loss: 29.0942 - val_mae: 3.8000\n",
      "Epoch 431/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 82.3282 - mae: 6.3332 - val_loss: 54.9792 - val_mae: 5.6362\n",
      "Epoch 432/550\n",
      "122/122 [==============================] - 0s 604us/step - loss: 83.8542 - mae: 6.4802 - val_loss: 48.1848 - val_mae: 5.2863\n",
      "Epoch 433/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 80.9763 - mae: 6.4227 - val_loss: 38.0491 - val_mae: 4.5722\n",
      "Epoch 434/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 76.5850 - mae: 6.2219 - val_loss: 34.0769 - val_mae: 4.2860\n",
      "Epoch 435/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 74.2035 - mae: 6.1468 - val_loss: 33.5451 - val_mae: 4.2824\n",
      "Epoch 436/550\n",
      "122/122 [==============================] - 0s 702us/step - loss: 77.7617 - mae: 6.3031 - val_loss: 33.6137 - val_mae: 4.2054\n",
      "Epoch 437/550\n",
      "122/122 [==============================] - 0s 594us/step - loss: 82.2948 - mae: 6.4196 - val_loss: 34.7652 - val_mae: 4.1844\n",
      "Epoch 438/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 81.2749 - mae: 6.4633 - val_loss: 28.4007 - val_mae: 3.7510\n",
      "Epoch 439/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 81.1632 - mae: 6.4277 - val_loss: 34.1948 - val_mae: 4.2512\n",
      "Epoch 440/550\n",
      "122/122 [==============================] - 0s 607us/step - loss: 80.5113 - mae: 6.3373 - val_loss: 30.5530 - val_mae: 3.9381\n",
      "Epoch 441/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 75.1152 - mae: 6.2192 - val_loss: 28.6440 - val_mae: 3.6948\n",
      "Epoch 442/550\n",
      "122/122 [==============================] - 0s 663us/step - loss: 82.6268 - mae: 6.3390 - val_loss: 35.5921 - val_mae: 4.4123\n",
      "Epoch 443/550\n",
      "122/122 [==============================] - 0s 617us/step - loss: 75.4180 - mae: 6.2011 - val_loss: 28.0201 - val_mae: 3.7144\n",
      "Epoch 444/550\n",
      "122/122 [==============================] - 0s 588us/step - loss: 77.6656 - mae: 6.2594 - val_loss: 28.6584 - val_mae: 3.6986\n",
      "Epoch 445/550\n",
      "122/122 [==============================] - 0s 570us/step - loss: 79.1374 - mae: 6.3293 - val_loss: 28.9223 - val_mae: 3.7574\n",
      "Epoch 446/550\n",
      "122/122 [==============================] - 0s 593us/step - loss: 75.0637 - mae: 6.2967 - val_loss: 27.2483 - val_mae: 3.7270\n",
      "Epoch 447/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 77.0805 - mae: 6.3796 - val_loss: 31.1939 - val_mae: 4.0562\n",
      "Epoch 448/550\n",
      "122/122 [==============================] - 0s 567us/step - loss: 73.4357 - mae: 6.1928 - val_loss: 29.0089 - val_mae: 3.7410\n",
      "Epoch 449/550\n",
      "122/122 [==============================] - 0s 629us/step - loss: 76.5367 - mae: 6.3213 - val_loss: 32.0006 - val_mae: 3.9873\n",
      "Epoch 450/550\n",
      "122/122 [==============================] - 0s 633us/step - loss: 82.1119 - mae: 6.4395 - val_loss: 33.5574 - val_mae: 4.1323\n",
      "Epoch 451/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 81.0261 - mae: 6.3653 - val_loss: 35.3720 - val_mae: 4.3056\n",
      "Epoch 452/550\n",
      "122/122 [==============================] - 0s 602us/step - loss: 78.9654 - mae: 6.3370 - val_loss: 31.2421 - val_mae: 3.9948\n",
      "Epoch 453/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 74.7807 - mae: 6.1835 - val_loss: 29.6206 - val_mae: 3.8484\n",
      "Epoch 454/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 83.5610 - mae: 6.4109 - val_loss: 35.9665 - val_mae: 4.3958\n",
      "Epoch 455/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 77.2427 - mae: 6.2375 - val_loss: 29.8744 - val_mae: 3.8336\n",
      "Epoch 456/550\n",
      "122/122 [==============================] - 0s 594us/step - loss: 79.6652 - mae: 6.2794 - val_loss: 29.1550 - val_mae: 3.7235\n",
      "Epoch 457/550\n",
      "122/122 [==============================] - 0s 663us/step - loss: 77.3733 - mae: 6.3580 - val_loss: 36.2988 - val_mae: 4.3183\n",
      "Epoch 458/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 78.9249 - mae: 6.3737 - val_loss: 31.9742 - val_mae: 3.9546\n",
      "Epoch 459/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 75.7596 - mae: 6.2679 - val_loss: 31.6036 - val_mae: 3.9684\n",
      "Epoch 460/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 76.0548 - mae: 6.2371 - val_loss: 33.2813 - val_mae: 4.1432\n",
      "Epoch 461/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 76.7547 - mae: 6.2446 - val_loss: 30.5782 - val_mae: 3.8538\n",
      "Epoch 462/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 80.6769 - mae: 6.4185 - val_loss: 32.9983 - val_mae: 4.0509\n",
      "Epoch 463/550\n",
      "122/122 [==============================] - 0s 659us/step - loss: 72.1600 - mae: 6.1025 - val_loss: 30.9701 - val_mae: 3.9977\n",
      "Epoch 464/550\n",
      "122/122 [==============================] - 0s 603us/step - loss: 76.3779 - mae: 6.2354 - val_loss: 36.5495 - val_mae: 4.3539\n",
      "Epoch 465/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 74.9347 - mae: 6.2218 - val_loss: 32.8393 - val_mae: 3.9867\n",
      "Epoch 466/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 71.4212 - mae: 6.0478 - val_loss: 34.9723 - val_mae: 4.4455\n",
      "Epoch 467/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 73.4335 - mae: 6.1899 - val_loss: 30.0340 - val_mae: 3.8925\n",
      "Epoch 468/550\n",
      "122/122 [==============================] - 0s 564us/step - loss: 75.9465 - mae: 6.2888 - val_loss: 37.7819 - val_mae: 4.5566\n",
      "Epoch 469/550\n",
      "122/122 [==============================] - 0s 610us/step - loss: 77.7324 - mae: 6.3092 - val_loss: 27.0399 - val_mae: 3.6185\n",
      "Epoch 470/550\n",
      "122/122 [==============================] - 0s 670us/step - loss: 69.6706 - mae: 6.0874 - val_loss: 31.4169 - val_mae: 4.0523\n",
      "Epoch 471/550\n",
      "122/122 [==============================] - 0s 620us/step - loss: 77.6096 - mae: 6.2829 - val_loss: 39.0549 - val_mae: 4.5381\n",
      "Epoch 472/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 73.0576 - mae: 6.0747 - val_loss: 36.9977 - val_mae: 4.4921\n",
      "Epoch 473/550\n",
      "122/122 [==============================] - 0s 576us/step - loss: 75.4549 - mae: 6.1984 - val_loss: 36.0905 - val_mae: 4.3987\n",
      "Epoch 474/550\n",
      "122/122 [==============================] - 0s 593us/step - loss: 75.7530 - mae: 6.3557 - val_loss: 43.9662 - val_mae: 4.9339\n",
      "Epoch 475/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 79.5933 - mae: 6.3197 - val_loss: 35.6507 - val_mae: 4.2739\n",
      "Epoch 476/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 75.2578 - mae: 6.2074 - val_loss: 32.5282 - val_mae: 4.0542\n",
      "Epoch 477/550\n",
      "122/122 [==============================] - 0s 635us/step - loss: 75.2743 - mae: 6.2756 - val_loss: 33.2984 - val_mae: 4.1159\n",
      "Epoch 478/550\n",
      "122/122 [==============================] - 0s 617us/step - loss: 77.8409 - mae: 6.3948 - val_loss: 31.4353 - val_mae: 3.9342\n",
      "Epoch 479/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 73.1943 - mae: 6.0974 - val_loss: 33.5481 - val_mae: 3.9593\n",
      "Epoch 480/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 72.1068 - mae: 6.0824 - val_loss: 34.4923 - val_mae: 4.0530\n",
      "Epoch 481/550\n",
      "122/122 [==============================] - 0s 605us/step - loss: 71.8548 - mae: 6.0826 - val_loss: 40.4551 - val_mae: 4.6578\n",
      "Epoch 482/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 75.8429 - mae: 6.1608 - val_loss: 36.7798 - val_mae: 4.3375\n",
      "Epoch 483/550\n",
      "122/122 [==============================] - 0s 624us/step - loss: 72.6375 - mae: 6.1812 - val_loss: 31.1546 - val_mae: 3.8456\n",
      "Epoch 484/550\n",
      "122/122 [==============================] - 0s 598us/step - loss: 75.0965 - mae: 6.1979 - val_loss: 31.1473 - val_mae: 3.8570\n",
      "Epoch 485/550\n",
      "122/122 [==============================] - 0s 580us/step - loss: 72.6226 - mae: 6.1422 - val_loss: 27.9918 - val_mae: 3.6554\n",
      "Epoch 486/550\n",
      "122/122 [==============================] - 0s 574us/step - loss: 69.9247 - mae: 5.9940 - val_loss: 29.3975 - val_mae: 3.8452\n",
      "Epoch 487/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 67.3471 - mae: 5.9442 - val_loss: 31.0980 - val_mae: 3.9758\n",
      "Epoch 488/550\n",
      "122/122 [==============================] - 0s 604us/step - loss: 70.6789 - mae: 6.0022 - val_loss: 31.6167 - val_mae: 4.0157\n",
      "Epoch 489/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 76.4629 - mae: 6.1478 - val_loss: 41.7600 - val_mae: 4.7949\n",
      "Epoch 490/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 73.4446 - mae: 6.0919 - val_loss: 61.8448 - val_mae: 5.7812\n",
      "Epoch 491/550\n",
      "122/122 [==============================] - 0s 689us/step - loss: 72.3079 - mae: 6.1937 - val_loss: 33.8573 - val_mae: 4.1455\n",
      "Epoch 492/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 70.7139 - mae: 6.0470 - val_loss: 35.5801 - val_mae: 4.2880\n",
      "Epoch 493/550\n",
      "122/122 [==============================] - 0s 566us/step - loss: 71.9675 - mae: 6.0544 - val_loss: 37.3514 - val_mae: 4.2955\n",
      "Epoch 494/550\n",
      "122/122 [==============================] - 0s 578us/step - loss: 71.0057 - mae: 6.1452 - val_loss: 41.5129 - val_mae: 4.7950\n",
      "Epoch 495/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 73.0261 - mae: 6.1455 - val_loss: 32.4189 - val_mae: 3.9615\n",
      "Epoch 496/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 70.5603 - mae: 6.0951 - val_loss: 29.9588 - val_mae: 3.9366\n",
      "Epoch 497/550\n",
      "122/122 [==============================] - 0s 654us/step - loss: 75.9836 - mae: 6.2761 - val_loss: 31.9588 - val_mae: 3.9047\n",
      "Epoch 498/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 71.9926 - mae: 6.1183 - val_loss: 35.2243 - val_mae: 4.2858\n",
      "Epoch 499/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 71.0129 - mae: 6.0770 - val_loss: 41.0687 - val_mae: 4.7513\n",
      "Epoch 500/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 67.7838 - mae: 5.9859 - val_loss: 33.5381 - val_mae: 4.0796\n",
      "Epoch 501/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 72.0132 - mae: 6.0652 - val_loss: 30.6563 - val_mae: 3.6952\n",
      "Epoch 502/550\n",
      "122/122 [==============================] - 0s 588us/step - loss: 72.8264 - mae: 6.0860 - val_loss: 36.1043 - val_mae: 4.3042\n",
      "Epoch 503/550\n",
      "122/122 [==============================] - 0s 695us/step - loss: 69.5156 - mae: 6.0531 - val_loss: 33.9280 - val_mae: 4.0703\n",
      "Epoch 504/550\n",
      "122/122 [==============================] - 0s 594us/step - loss: 72.8058 - mae: 6.1367 - val_loss: 31.9351 - val_mae: 3.7928\n",
      "Epoch 505/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 72.4805 - mae: 6.1491 - val_loss: 36.8197 - val_mae: 4.2210\n",
      "Epoch 506/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 74.0392 - mae: 6.2030 - val_loss: 37.3748 - val_mae: 4.3108\n",
      "Epoch 507/550\n",
      "122/122 [==============================] - 0s 595us/step - loss: 66.3817 - mae: 5.8725 - val_loss: 35.8484 - val_mae: 4.1288\n",
      "Epoch 508/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 72.0529 - mae: 6.0111 - val_loss: 31.7386 - val_mae: 3.7734\n",
      "Epoch 509/550\n",
      "122/122 [==============================] - 0s 662us/step - loss: 72.8769 - mae: 6.0588 - val_loss: 34.9465 - val_mae: 4.1057\n",
      "Epoch 510/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 72.6444 - mae: 6.1486 - val_loss: 32.7149 - val_mae: 4.0431\n",
      "Epoch 511/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 65.7773 - mae: 5.9046 - val_loss: 37.6391 - val_mae: 4.4292\n",
      "Epoch 512/550\n",
      "122/122 [==============================] - 0s 582us/step - loss: 72.1936 - mae: 6.0381 - val_loss: 57.4283 - val_mae: 5.6589\n",
      "Epoch 513/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 73.1801 - mae: 6.1488 - val_loss: 31.5015 - val_mae: 3.8031\n",
      "Epoch 514/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 70.6538 - mae: 5.9782 - val_loss: 29.5124 - val_mae: 3.8502\n",
      "Epoch 515/550\n",
      "122/122 [==============================] - 0s 666us/step - loss: 72.6566 - mae: 6.0150 - val_loss: 43.1444 - val_mae: 4.9604\n",
      "Epoch 516/550\n",
      "122/122 [==============================] - 0s 592us/step - loss: 71.2392 - mae: 6.1043 - val_loss: 31.6302 - val_mae: 3.9208\n",
      "Epoch 517/550\n",
      "122/122 [==============================] - 0s 573us/step - loss: 75.3030 - mae: 6.1419 - val_loss: 33.8045 - val_mae: 4.1606\n",
      "Epoch 518/550\n",
      "122/122 [==============================] - 0s 586us/step - loss: 67.8658 - mae: 5.9993 - val_loss: 30.2904 - val_mae: 3.8117\n",
      "Epoch 519/550\n",
      "122/122 [==============================] - 0s 597us/step - loss: 70.3049 - mae: 6.0064 - val_loss: 34.5210 - val_mae: 4.1730\n",
      "Epoch 520/550\n",
      "122/122 [==============================] - 0s 641us/step - loss: 68.9898 - mae: 5.9735 - val_loss: 39.2346 - val_mae: 4.5973\n",
      "Epoch 521/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 66.0262 - mae: 5.9828 - val_loss: 37.8748 - val_mae: 4.6066\n",
      "Epoch 522/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 67.5347 - mae: 5.8917 - val_loss: 32.2535 - val_mae: 4.0039\n",
      "Epoch 523/550\n",
      "122/122 [==============================] - 0s 560us/step - loss: 66.3615 - mae: 5.8378 - val_loss: 32.4079 - val_mae: 4.0139\n",
      "Epoch 524/550\n",
      "122/122 [==============================] - 0s 581us/step - loss: 68.0119 - mae: 5.9943 - val_loss: 31.7278 - val_mae: 3.8664\n",
      "Epoch 525/550\n",
      "122/122 [==============================] - 0s 569us/step - loss: 68.2613 - mae: 5.8930 - val_loss: 40.2711 - val_mae: 4.6316\n",
      "Epoch 526/550\n",
      "122/122 [==============================] - 0s 651us/step - loss: 70.5084 - mae: 6.0019 - val_loss: 32.6204 - val_mae: 3.8670\n",
      "Epoch 527/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 69.5832 - mae: 6.0400 - val_loss: 35.4657 - val_mae: 4.2400\n",
      "Epoch 528/550\n",
      "122/122 [==============================] - 0s 585us/step - loss: 71.1536 - mae: 6.0436 - val_loss: 36.2910 - val_mae: 4.2251\n",
      "Epoch 529/550\n",
      "122/122 [==============================] - 0s 568us/step - loss: 71.9889 - mae: 6.0533 - val_loss: 32.3111 - val_mae: 4.0190\n",
      "Epoch 530/550\n",
      "122/122 [==============================] - 0s 583us/step - loss: 65.9835 - mae: 5.8484 - val_loss: 30.9416 - val_mae: 3.9371\n",
      "Epoch 531/550\n",
      "122/122 [==============================] - 0s 579us/step - loss: 65.2099 - mae: 5.8650 - val_loss: 30.9209 - val_mae: 3.9230\n",
      "Epoch 532/550\n",
      "122/122 [==============================] - 0s 661us/step - loss: 65.9673 - mae: 5.9480 - val_loss: 29.3143 - val_mae: 3.8167\n",
      "Epoch 533/550\n",
      "122/122 [==============================] - 0s 639us/step - loss: 65.2576 - mae: 5.8268 - val_loss: 30.6711 - val_mae: 3.8546\n",
      "Epoch 534/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 68.7685 - mae: 5.9699 - val_loss: 33.1634 - val_mae: 4.0798\n",
      "Epoch 535/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 65.5579 - mae: 5.8521 - val_loss: 31.0169 - val_mae: 3.8116\n",
      "Epoch 536/550\n",
      "122/122 [==============================] - 0s 587us/step - loss: 64.2268 - mae: 5.8697 - val_loss: 31.0522 - val_mae: 3.8982\n",
      "Epoch 537/550\n",
      "122/122 [==============================] - 0s 571us/step - loss: 68.3154 - mae: 5.8997 - val_loss: 32.6601 - val_mae: 3.9661\n",
      "Epoch 538/550\n",
      "122/122 [==============================] - 0s 687us/step - loss: 69.3943 - mae: 6.0083 - val_loss: 42.0220 - val_mae: 4.7367\n",
      "Epoch 539/550\n",
      "122/122 [==============================] - 0s 606us/step - loss: 67.5486 - mae: 5.9496 - val_loss: 36.5999 - val_mae: 4.2667\n",
      "Epoch 540/550\n",
      "122/122 [==============================] - 0s 584us/step - loss: 74.8711 - mae: 6.1602 - val_loss: 37.5305 - val_mae: 4.1598\n",
      "Epoch 541/550\n",
      "122/122 [==============================] - 0s 624us/step - loss: 69.3874 - mae: 6.0155 - val_loss: 31.1176 - val_mae: 3.8127\n",
      "Epoch 542/550\n",
      "122/122 [==============================] - 0s 591us/step - loss: 63.3786 - mae: 5.7899 - val_loss: 29.2685 - val_mae: 3.7546\n",
      "Epoch 543/550\n",
      "122/122 [==============================] - 0s 577us/step - loss: 63.1571 - mae: 5.7805 - val_loss: 34.9817 - val_mae: 4.1810\n",
      "Epoch 544/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 62.2341 - mae: 5.8090 - val_loss: 47.2227 - val_mae: 5.0930\n",
      "Epoch 545/550\n",
      "122/122 [==============================] - 0s 668us/step - loss: 66.8568 - mae: 5.8723 - val_loss: 33.8591 - val_mae: 4.0562\n",
      "Epoch 546/550\n",
      "122/122 [==============================] - 0s 596us/step - loss: 68.7555 - mae: 5.9884 - val_loss: 31.4842 - val_mae: 3.9635\n",
      "Epoch 547/550\n",
      "122/122 [==============================] - 0s 572us/step - loss: 73.2517 - mae: 6.1103 - val_loss: 32.3139 - val_mae: 3.8883\n",
      "Epoch 548/550\n",
      "122/122 [==============================] - 0s 589us/step - loss: 68.4065 - mae: 5.8900 - val_loss: 38.4945 - val_mae: 4.2826\n",
      "Epoch 549/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 66.0513 - mae: 5.9090 - val_loss: 38.2229 - val_mae: 4.2230\n",
      "Epoch 550/550\n",
      "122/122 [==============================] - 0s 575us/step - loss: 69.3076 - mae: 5.9431 - val_loss: 34.2632 - val_mae: 4.0090\n",
      "31/31 [==============================] - 0s 326us/step\n",
      "Epochs: 550 | MAE: 4.009001143391926\n",
      "Training model with 600 epochs\n",
      "Epoch 1/600\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68658.0625 - mae: 227.5601 - val_loss: 61243.3359 - val_mae: 214.7377\n",
      "Epoch 2/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 41466.1836 - mae: 172.0921 - val_loss: 15823.1562 - val_mae: 108.0130\n",
      "Epoch 3/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 7567.6475 - mae: 68.9907 - val_loss: 3810.4929 - val_mae: 46.5387\n",
      "Epoch 4/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 3641.6597 - mae: 45.7694 - val_loss: 2335.1370 - val_mae: 36.2565\n",
      "Epoch 5/600\n",
      "122/122 [==============================] - 0s 698us/step - loss: 2829.3149 - mae: 39.4450 - val_loss: 1776.1891 - val_mae: 31.7595\n",
      "Epoch 6/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 2287.8291 - mae: 36.3317 - val_loss: 1487.8088 - val_mae: 28.8602\n",
      "Epoch 7/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 2114.9563 - mae: 35.2615 - val_loss: 1316.7528 - val_mae: 27.2296\n",
      "Epoch 8/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 1999.8547 - mae: 34.2713 - val_loss: 1168.2882 - val_mae: 25.8246\n",
      "Epoch 9/600\n",
      "122/122 [==============================] - 0s 608us/step - loss: 1806.2788 - mae: 32.8451 - val_loss: 1051.7795 - val_mae: 24.5094\n",
      "Epoch 10/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 1674.6536 - mae: 31.5705 - val_loss: 927.4208 - val_mae: 23.0555\n",
      "Epoch 11/600\n",
      "122/122 [==============================] - 0s 677us/step - loss: 1528.3695 - mae: 30.2611 - val_loss: 863.0648 - val_mae: 22.2861\n",
      "Epoch 12/600\n",
      "122/122 [==============================] - 0s 612us/step - loss: 1460.4103 - mae: 29.7200 - val_loss: 759.9756 - val_mae: 20.7647\n",
      "Epoch 13/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 1348.4287 - mae: 28.3266 - val_loss: 695.8406 - val_mae: 19.8348\n",
      "Epoch 14/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 1288.4297 - mae: 27.9007 - val_loss: 649.3942 - val_mae: 19.0731\n",
      "Epoch 15/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 1185.7509 - mae: 27.0274 - val_loss: 550.5272 - val_mae: 17.6822\n",
      "Epoch 16/600\n",
      "122/122 [==============================] - 0s 638us/step - loss: 1207.5770 - mae: 26.6867 - val_loss: 491.5758 - val_mae: 16.7596\n",
      "Epoch 17/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 1081.9790 - mae: 25.6909 - val_loss: 446.1734 - val_mae: 15.8260\n",
      "Epoch 18/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 1034.8975 - mae: 24.9098 - val_loss: 399.5478 - val_mae: 14.9292\n",
      "Epoch 19/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 969.1675 - mae: 23.8758 - val_loss: 346.7050 - val_mae: 13.8954\n",
      "Epoch 20/600\n",
      "122/122 [==============================] - 0s 628us/step - loss: 945.8397 - mae: 23.8553 - val_loss: 309.9380 - val_mae: 13.2297\n",
      "Epoch 21/600\n",
      "122/122 [==============================] - 0s 654us/step - loss: 906.2081 - mae: 23.2828 - val_loss: 281.5298 - val_mae: 12.9253\n",
      "Epoch 22/600\n",
      "122/122 [==============================] - 0s 625us/step - loss: 881.2110 - mae: 22.7290 - val_loss: 243.3478 - val_mae: 11.5882\n",
      "Epoch 23/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 777.7487 - mae: 21.3494 - val_loss: 223.8325 - val_mae: 11.0201\n",
      "Epoch 24/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 795.4085 - mae: 21.5930 - val_loss: 191.0552 - val_mae: 10.1525\n",
      "Epoch 25/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 757.1918 - mae: 20.8929 - val_loss: 177.9061 - val_mae: 10.4772\n",
      "Epoch 26/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 761.5755 - mae: 20.7750 - val_loss: 156.4563 - val_mae: 9.1358\n",
      "Epoch 27/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 689.4485 - mae: 19.8529 - val_loss: 138.2948 - val_mae: 8.7348\n",
      "Epoch 28/600\n",
      "122/122 [==============================] - 0s 662us/step - loss: 690.3430 - mae: 19.6780 - val_loss: 119.4940 - val_mae: 8.3787\n",
      "Epoch 29/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 684.1213 - mae: 19.6576 - val_loss: 96.9455 - val_mae: 7.2446\n",
      "Epoch 30/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 649.5706 - mae: 19.0209 - val_loss: 106.7947 - val_mae: 7.9551\n",
      "Epoch 31/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 623.7086 - mae: 18.6350 - val_loss: 81.2637 - val_mae: 6.7113\n",
      "Epoch 32/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 608.3569 - mae: 18.4671 - val_loss: 77.0536 - val_mae: 6.4380\n",
      "Epoch 33/600\n",
      "122/122 [==============================] - 0s 643us/step - loss: 617.1390 - mae: 18.1929 - val_loss: 72.9851 - val_mae: 6.2854\n",
      "Epoch 34/600\n",
      "122/122 [==============================] - 0s 618us/step - loss: 610.7273 - mae: 18.1326 - val_loss: 61.1492 - val_mae: 5.8331\n",
      "Epoch 35/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 589.4485 - mae: 17.7193 - val_loss: 52.9856 - val_mae: 5.6335\n",
      "Epoch 36/600\n",
      "122/122 [==============================] - 0s 596us/step - loss: 588.8601 - mae: 17.5601 - val_loss: 47.7265 - val_mae: 5.0441\n",
      "Epoch 37/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 570.5228 - mae: 17.2876 - val_loss: 59.0745 - val_mae: 5.8833\n",
      "Epoch 38/600\n",
      "122/122 [==============================] - 0s 645us/step - loss: 594.1386 - mae: 17.4556 - val_loss: 52.4773 - val_mae: 5.3658\n",
      "Epoch 39/600\n",
      "122/122 [==============================] - 0s 618us/step - loss: 568.6949 - mae: 17.3347 - val_loss: 39.7398 - val_mae: 4.8639\n",
      "Epoch 40/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 555.9581 - mae: 17.1668 - val_loss: 41.8241 - val_mae: 4.6315\n",
      "Epoch 41/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 529.9677 - mae: 16.6652 - val_loss: 43.8794 - val_mae: 4.8280\n",
      "Epoch 42/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 533.5939 - mae: 16.6742 - val_loss: 34.1148 - val_mae: 4.2692\n",
      "Epoch 43/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 519.5146 - mae: 16.1746 - val_loss: 28.2128 - val_mae: 3.8986\n",
      "Epoch 44/600\n",
      "122/122 [==============================] - 0s 648us/step - loss: 546.2404 - mae: 16.5632 - val_loss: 32.8028 - val_mae: 4.2156\n",
      "Epoch 45/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 538.0086 - mae: 16.5927 - val_loss: 39.5989 - val_mae: 4.4655\n",
      "Epoch 46/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 489.0250 - mae: 15.9463 - val_loss: 29.1567 - val_mae: 4.0058\n",
      "Epoch 47/600\n",
      "122/122 [==============================] - 0s 617us/step - loss: 504.6448 - mae: 16.1792 - val_loss: 30.7333 - val_mae: 4.0844\n",
      "Epoch 48/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 485.4162 - mae: 15.7679 - val_loss: 24.3442 - val_mae: 3.5185\n",
      "Epoch 49/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 495.1603 - mae: 15.9913 - val_loss: 32.8849 - val_mae: 4.2200\n",
      "Epoch 50/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 492.8253 - mae: 15.6731 - val_loss: 37.1542 - val_mae: 4.6034\n",
      "Epoch 51/600\n",
      "122/122 [==============================] - 0s 643us/step - loss: 476.7532 - mae: 15.4203 - val_loss: 37.0929 - val_mae: 4.6408\n",
      "Epoch 52/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 479.4627 - mae: 15.4053 - val_loss: 31.5997 - val_mae: 4.1144\n",
      "Epoch 53/600\n",
      "122/122 [==============================] - 0s 608us/step - loss: 481.6472 - mae: 15.5069 - val_loss: 26.0201 - val_mae: 3.6585\n",
      "Epoch 54/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 457.8603 - mae: 15.4463 - val_loss: 31.7141 - val_mae: 4.4341\n",
      "Epoch 55/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 475.5030 - mae: 15.1708 - val_loss: 24.8546 - val_mae: 3.5957\n",
      "Epoch 56/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 451.3015 - mae: 15.1770 - val_loss: 50.9453 - val_mae: 5.6805\n",
      "Epoch 57/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 446.1216 - mae: 15.0437 - val_loss: 25.1720 - val_mae: 3.6705\n",
      "Epoch 58/600\n",
      "122/122 [==============================] - 0s 682us/step - loss: 460.9090 - mae: 15.1576 - val_loss: 22.6494 - val_mae: 3.4214\n",
      "Epoch 59/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 463.8194 - mae: 15.0450 - val_loss: 29.4913 - val_mae: 4.1878\n",
      "Epoch 60/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 438.3494 - mae: 14.8128 - val_loss: 39.1813 - val_mae: 4.9442\n",
      "Epoch 61/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 460.7165 - mae: 14.8928 - val_loss: 23.2751 - val_mae: 3.5136\n",
      "Epoch 62/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 431.5130 - mae: 14.5731 - val_loss: 29.7522 - val_mae: 4.1032\n",
      "Epoch 63/600\n",
      "122/122 [==============================] - 0s 612us/step - loss: 448.0464 - mae: 14.9802 - val_loss: 25.2023 - val_mae: 3.4760\n",
      "Epoch 64/600\n",
      "122/122 [==============================] - 0s 644us/step - loss: 453.9989 - mae: 14.7900 - val_loss: 21.9702 - val_mae: 3.3092\n",
      "Epoch 65/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 479.6577 - mae: 15.2751 - val_loss: 24.2174 - val_mae: 3.5410\n",
      "Epoch 66/600\n",
      "122/122 [==============================] - 0s 608us/step - loss: 425.8857 - mae: 14.5210 - val_loss: 28.4728 - val_mae: 4.0768\n",
      "Epoch 67/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 431.0191 - mae: 14.3518 - val_loss: 27.3875 - val_mae: 3.6876\n",
      "Epoch 68/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 433.8061 - mae: 14.4022 - val_loss: 26.1008 - val_mae: 3.8627\n",
      "Epoch 69/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 426.2399 - mae: 14.1947 - val_loss: 23.4030 - val_mae: 3.4808\n",
      "Epoch 70/600\n",
      "122/122 [==============================] - 0s 665us/step - loss: 435.6561 - mae: 14.4103 - val_loss: 21.5598 - val_mae: 3.3404\n",
      "Epoch 71/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 423.8390 - mae: 14.3233 - val_loss: 27.7306 - val_mae: 3.9232\n",
      "Epoch 72/600\n",
      "122/122 [==============================] - 0s 608us/step - loss: 401.6910 - mae: 13.8339 - val_loss: 24.9698 - val_mae: 3.5646\n",
      "Epoch 73/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 423.7814 - mae: 14.3075 - val_loss: 30.8963 - val_mae: 4.0747\n",
      "Epoch 74/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 413.7281 - mae: 14.0585 - val_loss: 31.5454 - val_mae: 4.3776\n",
      "Epoch 75/600\n",
      "122/122 [==============================] - 0s 675us/step - loss: 412.9755 - mae: 14.0493 - val_loss: 25.3192 - val_mae: 3.6942\n",
      "Epoch 76/600\n",
      "122/122 [==============================] - 0s 622us/step - loss: 405.7834 - mae: 13.8915 - val_loss: 73.7494 - val_mae: 6.7188\n",
      "Epoch 77/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 418.5002 - mae: 13.9464 - val_loss: 26.9991 - val_mae: 3.7421\n",
      "Epoch 78/600\n",
      "122/122 [==============================] - 0s 621us/step - loss: 403.3035 - mae: 13.8325 - val_loss: 24.8693 - val_mae: 3.6026\n",
      "Epoch 79/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 403.5959 - mae: 13.6916 - val_loss: 26.4749 - val_mae: 3.6246\n",
      "Epoch 80/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 397.7715 - mae: 13.7633 - val_loss: 22.5125 - val_mae: 3.3194\n",
      "Epoch 81/600\n",
      "122/122 [==============================] - 0s 649us/step - loss: 401.1947 - mae: 13.6667 - val_loss: 22.9854 - val_mae: 3.5411\n",
      "Epoch 82/600\n",
      "122/122 [==============================] - 0s 636us/step - loss: 392.0517 - mae: 13.4652 - val_loss: 25.9462 - val_mae: 3.8594\n",
      "Epoch 83/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 402.7357 - mae: 13.6898 - val_loss: 25.1064 - val_mae: 3.5707\n",
      "Epoch 84/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 374.4756 - mae: 13.2201 - val_loss: 23.5293 - val_mae: 3.4546\n",
      "Epoch 85/600\n",
      "122/122 [==============================] - 0s 609us/step - loss: 422.5158 - mae: 14.0007 - val_loss: 53.4559 - val_mae: 6.1124\n",
      "Epoch 86/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 392.6672 - mae: 13.6291 - val_loss: 31.8103 - val_mae: 4.2692\n",
      "Epoch 87/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 404.6375 - mae: 13.8333 - val_loss: 31.2509 - val_mae: 4.3061\n",
      "Epoch 88/600\n",
      "122/122 [==============================] - 0s 640us/step - loss: 377.9368 - mae: 13.3719 - val_loss: 25.4517 - val_mae: 3.6647\n",
      "Epoch 89/600\n",
      "122/122 [==============================] - 0s 716us/step - loss: 383.2085 - mae: 13.3818 - val_loss: 23.0846 - val_mae: 3.3492\n",
      "Epoch 90/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 362.5953 - mae: 13.0747 - val_loss: 29.8850 - val_mae: 4.0238\n",
      "Epoch 91/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 356.2464 - mae: 12.9783 - val_loss: 25.3059 - val_mae: 3.5618\n",
      "Epoch 92/600\n",
      "122/122 [==============================] - 0s 603us/step - loss: 366.2518 - mae: 12.9181 - val_loss: 24.2250 - val_mae: 3.5281\n",
      "Epoch 93/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 359.2299 - mae: 12.9441 - val_loss: 25.9649 - val_mae: 3.6586\n",
      "Epoch 94/600\n",
      "122/122 [==============================] - 0s 638us/step - loss: 353.7412 - mae: 12.7375 - val_loss: 25.6121 - val_mae: 3.6375\n",
      "Epoch 95/600\n",
      "122/122 [==============================] - 0s 612us/step - loss: 359.7882 - mae: 12.8073 - val_loss: 29.8106 - val_mae: 3.8385\n",
      "Epoch 96/600\n",
      "122/122 [==============================] - 0s 624us/step - loss: 386.4908 - mae: 13.1389 - val_loss: 30.2778 - val_mae: 4.1954\n",
      "Epoch 97/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 335.4539 - mae: 12.5652 - val_loss: 45.8829 - val_mae: 5.1992\n",
      "Epoch 98/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 365.5031 - mae: 12.9322 - val_loss: 37.1010 - val_mae: 4.5361\n",
      "Epoch 99/600\n",
      "122/122 [==============================] - 0s 696us/step - loss: 331.6049 - mae: 12.5884 - val_loss: 24.3012 - val_mae: 3.4870\n",
      "Epoch 100/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 362.0346 - mae: 12.7593 - val_loss: 22.8905 - val_mae: 3.4584\n",
      "Epoch 101/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 352.3760 - mae: 12.7109 - val_loss: 31.8969 - val_mae: 4.3695\n",
      "Epoch 102/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 358.7447 - mae: 12.7944 - val_loss: 29.8334 - val_mae: 4.2321\n",
      "Epoch 103/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 339.2893 - mae: 12.4388 - val_loss: 27.1311 - val_mae: 3.9270\n",
      "Epoch 104/600\n",
      "122/122 [==============================] - 0s 661us/step - loss: 349.6124 - mae: 12.7188 - val_loss: 23.5939 - val_mae: 3.4359\n",
      "Epoch 105/600\n",
      "122/122 [==============================] - 0s 627us/step - loss: 347.7423 - mae: 12.6155 - val_loss: 40.4985 - val_mae: 4.8789\n",
      "Epoch 106/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 323.4525 - mae: 12.1426 - val_loss: 23.9468 - val_mae: 3.4863\n",
      "Epoch 107/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 349.4923 - mae: 12.5927 - val_loss: 34.2966 - val_mae: 4.4697\n",
      "Epoch 108/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 341.5691 - mae: 12.3470 - val_loss: 26.2389 - val_mae: 3.7039\n",
      "Epoch 109/600\n",
      "122/122 [==============================] - 0s 653us/step - loss: 344.6098 - mae: 12.4751 - val_loss: 28.3096 - val_mae: 3.8841\n",
      "Epoch 110/600\n",
      "122/122 [==============================] - 0s 609us/step - loss: 338.4320 - mae: 12.3837 - val_loss: 22.1958 - val_mae: 3.3633\n",
      "Epoch 111/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 307.3591 - mae: 11.9805 - val_loss: 24.9230 - val_mae: 3.5554\n",
      "Epoch 112/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 331.5500 - mae: 12.2389 - val_loss: 24.6146 - val_mae: 3.5934\n",
      "Epoch 113/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 326.3852 - mae: 12.0826 - val_loss: 26.4067 - val_mae: 3.7912\n",
      "Epoch 114/600\n",
      "122/122 [==============================] - 0s 648us/step - loss: 324.3966 - mae: 12.0670 - val_loss: 25.1064 - val_mae: 3.6723\n",
      "Epoch 115/600\n",
      "122/122 [==============================] - 0s 652us/step - loss: 332.8146 - mae: 12.1193 - val_loss: 23.6968 - val_mae: 3.5145\n",
      "Epoch 116/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 316.2775 - mae: 11.9858 - val_loss: 27.3901 - val_mae: 3.9178\n",
      "Epoch 117/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 297.7303 - mae: 11.6345 - val_loss: 23.4019 - val_mae: 3.4784\n",
      "Epoch 118/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 311.5872 - mae: 11.7421 - val_loss: 25.6720 - val_mae: 3.6282\n",
      "Epoch 119/600\n",
      "122/122 [==============================] - 0s 640us/step - loss: 353.7008 - mae: 12.1941 - val_loss: 25.8766 - val_mae: 3.7646\n",
      "Epoch 120/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 334.4182 - mae: 11.9000 - val_loss: 26.3255 - val_mae: 3.6098\n",
      "Epoch 121/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 329.8479 - mae: 11.9887 - val_loss: 28.3432 - val_mae: 3.9762\n",
      "Epoch 122/600\n",
      "122/122 [==============================] - 0s 596us/step - loss: 316.6931 - mae: 11.7855 - val_loss: 25.3572 - val_mae: 3.6513\n",
      "Epoch 123/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 328.6438 - mae: 11.9468 - val_loss: 28.2588 - val_mae: 3.8550\n",
      "Epoch 124/600\n",
      "122/122 [==============================] - 0s 647us/step - loss: 273.6484 - mae: 11.1220 - val_loss: 23.5590 - val_mae: 3.4557\n",
      "Epoch 125/600\n",
      "122/122 [==============================] - 0s 650us/step - loss: 310.5190 - mae: 11.4758 - val_loss: 23.2683 - val_mae: 3.4432\n",
      "Epoch 126/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 284.1316 - mae: 11.3186 - val_loss: 25.3535 - val_mae: 3.5043\n",
      "Epoch 127/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 294.8689 - mae: 11.5209 - val_loss: 23.5021 - val_mae: 3.5012\n",
      "Epoch 128/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 282.8275 - mae: 11.3359 - val_loss: 28.2529 - val_mae: 3.9915\n",
      "Epoch 129/600\n",
      "122/122 [==============================] - 0s 635us/step - loss: 279.4312 - mae: 11.2453 - val_loss: 28.6063 - val_mae: 4.0079\n",
      "Epoch 130/600\n",
      "122/122 [==============================] - 0s 634us/step - loss: 284.2275 - mae: 11.2569 - val_loss: 22.6459 - val_mae: 3.4506\n",
      "Epoch 131/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 305.7573 - mae: 11.4596 - val_loss: 22.1976 - val_mae: 3.3110\n",
      "Epoch 132/600\n",
      "122/122 [==============================] - 0s 567us/step - loss: 273.2424 - mae: 10.9077 - val_loss: 24.2387 - val_mae: 3.5503\n",
      "Epoch 133/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 293.6575 - mae: 11.3983 - val_loss: 28.1926 - val_mae: 4.0123\n",
      "Epoch 134/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 303.4202 - mae: 11.5710 - val_loss: 26.2548 - val_mae: 3.6215\n",
      "Epoch 135/600\n",
      "122/122 [==============================] - 0s 664us/step - loss: 284.3255 - mae: 11.0179 - val_loss: 24.3986 - val_mae: 3.6411\n",
      "Epoch 136/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 276.6552 - mae: 10.9866 - val_loss: 23.2719 - val_mae: 3.4537\n",
      "Epoch 137/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 274.8080 - mae: 11.0807 - val_loss: 26.4617 - val_mae: 3.7662\n",
      "Epoch 138/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 283.3543 - mae: 11.0701 - val_loss: 23.0733 - val_mae: 3.4739\n",
      "Epoch 139/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 284.9555 - mae: 11.1090 - val_loss: 32.6886 - val_mae: 4.3530\n",
      "Epoch 140/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 264.6113 - mae: 10.9008 - val_loss: 23.4732 - val_mae: 3.4264\n",
      "Epoch 141/600\n",
      "122/122 [==============================] - 0s 701us/step - loss: 269.8134 - mae: 10.9037 - val_loss: 24.3631 - val_mae: 3.5138\n",
      "Epoch 142/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 276.8485 - mae: 10.9993 - val_loss: 25.5603 - val_mae: 3.6814\n",
      "Epoch 143/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 274.1756 - mae: 10.8662 - val_loss: 24.6564 - val_mae: 3.6474\n",
      "Epoch 144/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 265.3948 - mae: 10.8030 - val_loss: 29.1820 - val_mae: 3.8387\n",
      "Epoch 145/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 264.3727 - mae: 10.8271 - val_loss: 26.1239 - val_mae: 3.7978\n",
      "Epoch 146/600\n",
      "122/122 [==============================] - 0s 638us/step - loss: 260.3988 - mae: 10.5989 - val_loss: 24.0445 - val_mae: 3.5087\n",
      "Epoch 147/600\n",
      "122/122 [==============================] - 0s 639us/step - loss: 260.4207 - mae: 10.7161 - val_loss: 33.3726 - val_mae: 4.2911\n",
      "Epoch 148/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 260.9601 - mae: 10.5654 - val_loss: 31.0454 - val_mae: 4.2176\n",
      "Epoch 149/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 252.1899 - mae: 10.5036 - val_loss: 24.8220 - val_mae: 3.6238\n",
      "Epoch 150/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 240.9899 - mae: 10.3147 - val_loss: 23.1356 - val_mae: 3.4364\n",
      "Epoch 151/600\n",
      "122/122 [==============================] - 0s 572us/step - loss: 256.7843 - mae: 10.5007 - val_loss: 25.9208 - val_mae: 3.7212\n",
      "Epoch 152/600\n",
      "122/122 [==============================] - 0s 681us/step - loss: 261.1790 - mae: 10.5809 - val_loss: 32.6521 - val_mae: 4.3199\n",
      "Epoch 153/600\n",
      "122/122 [==============================] - 0s 609us/step - loss: 245.0455 - mae: 10.3299 - val_loss: 25.5149 - val_mae: 3.7007\n",
      "Epoch 154/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 261.6749 - mae: 10.5790 - val_loss: 31.7753 - val_mae: 4.2810\n",
      "Epoch 155/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 243.2838 - mae: 10.1684 - val_loss: 28.7985 - val_mae: 4.0011\n",
      "Epoch 156/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 235.8707 - mae: 10.1184 - val_loss: 24.1197 - val_mae: 3.5388\n",
      "Epoch 157/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 246.9145 - mae: 10.2651 - val_loss: 29.4800 - val_mae: 4.0253\n",
      "Epoch 158/600\n",
      "122/122 [==============================] - 0s 702us/step - loss: 226.3100 - mae: 9.9454 - val_loss: 43.7347 - val_mae: 5.0535\n",
      "Epoch 159/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 248.2432 - mae: 10.3324 - val_loss: 46.4420 - val_mae: 5.4347\n",
      "Epoch 160/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 242.8150 - mae: 10.2378 - val_loss: 22.3510 - val_mae: 3.3580\n",
      "Epoch 161/600\n",
      "122/122 [==============================] - 0s 597us/step - loss: 230.9202 - mae: 10.0981 - val_loss: 31.0055 - val_mae: 3.8711\n",
      "Epoch 162/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 252.2903 - mae: 10.2893 - val_loss: 24.1914 - val_mae: 3.4703\n",
      "Epoch 163/600\n",
      "122/122 [==============================] - 0s 650us/step - loss: 225.5340 - mae: 9.7860 - val_loss: 22.8608 - val_mae: 3.3714\n",
      "Epoch 164/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 234.5451 - mae: 10.0297 - val_loss: 26.0527 - val_mae: 3.7267\n",
      "Epoch 165/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 258.0410 - mae: 10.3992 - val_loss: 22.3089 - val_mae: 3.3807\n",
      "Epoch 166/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 236.7737 - mae: 9.9589 - val_loss: 25.4115 - val_mae: 3.5811\n",
      "Epoch 167/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 233.6112 - mae: 9.9344 - val_loss: 24.8237 - val_mae: 3.5344\n",
      "Epoch 168/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 231.1304 - mae: 9.8529 - val_loss: 27.8780 - val_mae: 3.8456\n",
      "Epoch 169/600\n",
      "122/122 [==============================] - 0s 611us/step - loss: 226.8556 - mae: 9.6931 - val_loss: 34.5967 - val_mae: 4.5253\n",
      "Epoch 170/600\n",
      "122/122 [==============================] - 0s 682us/step - loss: 221.1613 - mae: 9.7384 - val_loss: 22.2022 - val_mae: 3.3249\n",
      "Epoch 171/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 226.2753 - mae: 9.7836 - val_loss: 30.6357 - val_mae: 4.1186\n",
      "Epoch 172/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 214.8479 - mae: 9.5558 - val_loss: 22.0872 - val_mae: 3.3562\n",
      "Epoch 173/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 208.8146 - mae: 9.4497 - val_loss: 24.6806 - val_mae: 3.5615\n",
      "Epoch 174/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 211.9305 - mae: 9.4848 - val_loss: 27.2332 - val_mae: 3.9157\n",
      "Epoch 175/600\n",
      "122/122 [==============================] - 0s 687us/step - loss: 224.8789 - mae: 9.6912 - val_loss: 23.8654 - val_mae: 3.5262\n",
      "Epoch 176/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 231.0141 - mae: 9.7805 - val_loss: 22.6766 - val_mae: 3.3831\n",
      "Epoch 177/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 217.0757 - mae: 9.6853 - val_loss: 26.8046 - val_mae: 3.8171\n",
      "Epoch 178/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 220.7376 - mae: 9.7064 - val_loss: 25.0927 - val_mae: 3.5932\n",
      "Epoch 179/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 199.4526 - mae: 9.2696 - val_loss: 25.6870 - val_mae: 3.7334\n",
      "Epoch 180/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 222.3881 - mae: 9.6360 - val_loss: 23.6724 - val_mae: 3.4834\n",
      "Epoch 181/600\n",
      "122/122 [==============================] - 0s 690us/step - loss: 209.5895 - mae: 9.3853 - val_loss: 28.3612 - val_mae: 4.0121\n",
      "Epoch 182/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 223.9452 - mae: 9.5578 - val_loss: 32.0547 - val_mae: 4.2806\n",
      "Epoch 183/600\n",
      "122/122 [==============================] - 0s 574us/step - loss: 207.8001 - mae: 9.4512 - val_loss: 26.7162 - val_mae: 3.6700\n",
      "Epoch 184/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 227.6791 - mae: 9.6658 - val_loss: 24.4237 - val_mae: 3.5059\n",
      "Epoch 185/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 223.1918 - mae: 9.6277 - val_loss: 24.4934 - val_mae: 3.5556\n",
      "Epoch 186/600\n",
      "122/122 [==============================] - 0s 642us/step - loss: 200.8542 - mae: 9.3319 - val_loss: 29.2474 - val_mae: 4.0679\n",
      "Epoch 187/600\n",
      "122/122 [==============================] - 0s 625us/step - loss: 213.0727 - mae: 9.3370 - val_loss: 39.4211 - val_mae: 4.8038\n",
      "Epoch 188/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 197.3750 - mae: 9.0770 - val_loss: 31.2828 - val_mae: 4.2761\n",
      "Epoch 189/600\n",
      "122/122 [==============================] - 0s 574us/step - loss: 190.5237 - mae: 9.0638 - val_loss: 29.3188 - val_mae: 4.0810\n",
      "Epoch 190/600\n",
      "122/122 [==============================] - 0s 621us/step - loss: 201.0189 - mae: 9.1864 - val_loss: 23.8672 - val_mae: 3.4832\n",
      "Epoch 191/600\n",
      "122/122 [==============================] - 0s 637us/step - loss: 189.7393 - mae: 8.9524 - val_loss: 27.1367 - val_mae: 3.8780\n",
      "Epoch 192/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 192.7742 - mae: 9.0483 - val_loss: 30.6575 - val_mae: 4.1470\n",
      "Epoch 193/600\n",
      "122/122 [==============================] - 0s 612us/step - loss: 200.9454 - mae: 9.1698 - val_loss: 29.4398 - val_mae: 3.9731\n",
      "Epoch 194/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 204.4840 - mae: 9.2560 - val_loss: 28.1313 - val_mae: 3.7952\n",
      "Epoch 195/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 192.9850 - mae: 9.0793 - val_loss: 24.1321 - val_mae: 3.5608\n",
      "Epoch 196/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 179.9039 - mae: 8.7210 - val_loss: 25.2536 - val_mae: 3.5299\n",
      "Epoch 197/600\n",
      "122/122 [==============================] - 0s 631us/step - loss: 188.8468 - mae: 9.0028 - val_loss: 38.3990 - val_mae: 4.8033\n",
      "Epoch 198/600\n",
      "122/122 [==============================] - 0s 632us/step - loss: 192.2499 - mae: 8.9011 - val_loss: 23.9930 - val_mae: 3.5562\n",
      "Epoch 199/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 187.0925 - mae: 8.8679 - val_loss: 24.5258 - val_mae: 3.5750\n",
      "Epoch 200/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 173.2671 - mae: 8.6236 - val_loss: 23.5258 - val_mae: 3.4482\n",
      "Epoch 201/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 199.0381 - mae: 9.0066 - val_loss: 24.3294 - val_mae: 3.5686\n",
      "Epoch 202/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 203.4889 - mae: 9.0953 - val_loss: 31.9609 - val_mae: 4.1861\n",
      "Epoch 203/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 171.7606 - mae: 8.6533 - val_loss: 26.1393 - val_mae: 3.6550\n",
      "Epoch 204/600\n",
      "122/122 [==============================] - 0s 668us/step - loss: 184.2486 - mae: 8.8437 - val_loss: 25.7233 - val_mae: 3.7813\n",
      "Epoch 205/600\n",
      "122/122 [==============================] - 0s 642us/step - loss: 184.3039 - mae: 8.7777 - val_loss: 31.8706 - val_mae: 4.2725\n",
      "Epoch 206/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 182.2274 - mae: 8.6874 - val_loss: 27.0728 - val_mae: 3.7682\n",
      "Epoch 207/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 175.2146 - mae: 8.5811 - val_loss: 24.1395 - val_mae: 3.5414\n",
      "Epoch 208/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 173.0797 - mae: 8.5917 - val_loss: 29.9913 - val_mae: 4.0480\n",
      "Epoch 209/600\n",
      "122/122 [==============================] - 0s 659us/step - loss: 177.4379 - mae: 8.6803 - val_loss: 24.7418 - val_mae: 3.5673\n",
      "Epoch 210/600\n",
      "122/122 [==============================] - 0s 629us/step - loss: 179.1117 - mae: 8.7214 - val_loss: 23.9978 - val_mae: 3.5060\n",
      "Epoch 211/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 181.2295 - mae: 8.7938 - val_loss: 35.2748 - val_mae: 4.5221\n",
      "Epoch 212/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 170.9421 - mae: 8.5581 - val_loss: 29.4737 - val_mae: 3.8810\n",
      "Epoch 213/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 168.8792 - mae: 8.4690 - val_loss: 23.6163 - val_mae: 3.4727\n",
      "Epoch 214/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 179.2027 - mae: 8.5866 - val_loss: 23.5542 - val_mae: 3.4293\n",
      "Epoch 215/600\n",
      "122/122 [==============================] - 0s 643us/step - loss: 164.3797 - mae: 8.2321 - val_loss: 29.1748 - val_mae: 3.9268\n",
      "Epoch 216/600\n",
      "122/122 [==============================] - 0s 624us/step - loss: 169.5312 - mae: 8.4593 - val_loss: 27.6631 - val_mae: 3.9025\n",
      "Epoch 217/600\n",
      "122/122 [==============================] - 0s 620us/step - loss: 164.7826 - mae: 8.4379 - val_loss: 29.5225 - val_mae: 3.9731\n",
      "Epoch 218/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 170.1409 - mae: 8.5267 - val_loss: 36.7486 - val_mae: 4.7428\n",
      "Epoch 219/600\n",
      "122/122 [==============================] - 0s 596us/step - loss: 174.8947 - mae: 8.4583 - val_loss: 45.8282 - val_mae: 5.0663\n",
      "Epoch 220/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 178.2367 - mae: 8.6577 - val_loss: 22.6574 - val_mae: 3.4141\n",
      "Epoch 221/600\n",
      "122/122 [==============================] - 0s 634us/step - loss: 164.8844 - mae: 8.4017 - val_loss: 23.1777 - val_mae: 3.4680\n",
      "Epoch 222/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 164.8270 - mae: 8.1840 - val_loss: 23.5043 - val_mae: 3.4570\n",
      "Epoch 223/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 165.6194 - mae: 8.3320 - val_loss: 28.6794 - val_mae: 4.0316\n",
      "Epoch 224/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 161.5355 - mae: 8.2789 - val_loss: 24.2058 - val_mae: 3.5670\n",
      "Epoch 225/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 169.3840 - mae: 8.4549 - val_loss: 24.3678 - val_mae: 3.6002\n",
      "Epoch 226/600\n",
      "122/122 [==============================] - 0s 656us/step - loss: 162.0767 - mae: 8.3929 - val_loss: 23.9273 - val_mae: 3.4862\n",
      "Epoch 227/600\n",
      "122/122 [==============================] - 0s 641us/step - loss: 167.6649 - mae: 8.3942 - val_loss: 26.2265 - val_mae: 3.6401\n",
      "Epoch 228/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 172.0871 - mae: 8.4254 - val_loss: 36.5282 - val_mae: 4.6361\n",
      "Epoch 229/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 156.6040 - mae: 8.2255 - val_loss: 26.6642 - val_mae: 3.8042\n",
      "Epoch 230/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 173.3876 - mae: 8.4603 - val_loss: 28.3738 - val_mae: 3.9511\n",
      "Epoch 231/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 152.5726 - mae: 8.1941 - val_loss: 26.4585 - val_mae: 3.8487\n",
      "Epoch 232/600\n",
      "122/122 [==============================] - 0s 651us/step - loss: 160.2093 - mae: 8.2106 - val_loss: 24.9108 - val_mae: 3.6678\n",
      "Epoch 233/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 163.2110 - mae: 8.2937 - val_loss: 25.7662 - val_mae: 3.7410\n",
      "Epoch 234/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 144.5137 - mae: 7.9506 - val_loss: 23.7166 - val_mae: 3.4938\n",
      "Epoch 235/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 154.1550 - mae: 8.1110 - val_loss: 25.1145 - val_mae: 3.6387\n",
      "Epoch 236/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 158.7909 - mae: 8.3186 - val_loss: 25.4765 - val_mae: 3.6402\n",
      "Epoch 237/600\n",
      "122/122 [==============================] - 0s 679us/step - loss: 149.2130 - mae: 8.0042 - val_loss: 25.4745 - val_mae: 3.6406\n",
      "Epoch 238/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 147.2991 - mae: 7.9589 - val_loss: 24.7633 - val_mae: 3.5810\n",
      "Epoch 239/600\n",
      "122/122 [==============================] - 0s 632us/step - loss: 145.7750 - mae: 7.9359 - val_loss: 25.0588 - val_mae: 3.5748\n",
      "Epoch 240/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 146.5262 - mae: 7.8689 - val_loss: 24.0646 - val_mae: 3.4620\n",
      "Epoch 241/600\n",
      "122/122 [==============================] - 0s 596us/step - loss: 152.2607 - mae: 8.1042 - val_loss: 27.4035 - val_mae: 3.8169\n",
      "Epoch 242/600\n",
      "122/122 [==============================] - 0s 635us/step - loss: 153.2334 - mae: 8.0992 - val_loss: 24.5821 - val_mae: 3.5780\n",
      "Epoch 243/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 156.5047 - mae: 8.1998 - val_loss: 29.9858 - val_mae: 4.0649\n",
      "Epoch 244/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 151.8034 - mae: 7.9662 - val_loss: 30.6220 - val_mae: 4.1860\n",
      "Epoch 245/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 148.4360 - mae: 8.0133 - val_loss: 26.6230 - val_mae: 3.6893\n",
      "Epoch 246/600\n",
      "122/122 [==============================] - 0s 693us/step - loss: 161.6434 - mae: 8.1962 - val_loss: 23.7542 - val_mae: 3.5024\n",
      "Epoch 247/600\n",
      "122/122 [==============================] - 0s 569us/step - loss: 143.5874 - mae: 7.9340 - val_loss: 38.9901 - val_mae: 4.6698\n",
      "Epoch 248/600\n",
      "122/122 [==============================] - 0s 674us/step - loss: 137.8076 - mae: 7.7785 - val_loss: 28.9800 - val_mae: 4.0285\n",
      "Epoch 249/600\n",
      "122/122 [==============================] - 0s 597us/step - loss: 143.4228 - mae: 7.8966 - val_loss: 38.9557 - val_mae: 4.7480\n",
      "Epoch 250/600\n",
      "122/122 [==============================] - 0s 564us/step - loss: 138.9157 - mae: 7.8083 - val_loss: 27.3423 - val_mae: 3.8455\n",
      "Epoch 251/600\n",
      "122/122 [==============================] - 0s 574us/step - loss: 143.4943 - mae: 7.8350 - val_loss: 26.7914 - val_mae: 3.7865\n",
      "Epoch 252/600\n",
      "122/122 [==============================] - 0s 615us/step - loss: 139.9534 - mae: 7.8367 - val_loss: 23.9021 - val_mae: 3.5061\n",
      "Epoch 253/600\n",
      "122/122 [==============================] - 0s 565us/step - loss: 160.2048 - mae: 8.1612 - val_loss: 25.2542 - val_mae: 3.6158\n",
      "Epoch 254/600\n",
      "122/122 [==============================] - 0s 646us/step - loss: 151.9199 - mae: 7.9851 - val_loss: 34.2327 - val_mae: 4.3183\n",
      "Epoch 255/600\n",
      "122/122 [==============================] - 0s 712us/step - loss: 144.3374 - mae: 7.9927 - val_loss: 24.9035 - val_mae: 3.5932\n",
      "Epoch 256/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 137.2821 - mae: 7.8638 - val_loss: 27.4221 - val_mae: 3.8390\n",
      "Epoch 257/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 147.6788 - mae: 7.9284 - val_loss: 24.7767 - val_mae: 3.6141\n",
      "Epoch 258/600\n",
      "122/122 [==============================] - 0s 689us/step - loss: 140.3121 - mae: 7.8316 - val_loss: 26.0795 - val_mae: 3.6199\n",
      "Epoch 259/600\n",
      "122/122 [==============================] - 0s 621us/step - loss: 144.7198 - mae: 7.8929 - val_loss: 28.4231 - val_mae: 4.0061\n",
      "Epoch 260/600\n",
      "122/122 [==============================] - 0s 618us/step - loss: 135.2089 - mae: 7.7367 - val_loss: 26.2967 - val_mae: 3.7172\n",
      "Epoch 261/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 139.5849 - mae: 7.7886 - val_loss: 37.0258 - val_mae: 4.6521\n",
      "Epoch 262/600\n",
      "122/122 [==============================] - 0s 650us/step - loss: 136.6325 - mae: 7.7331 - val_loss: 34.6121 - val_mae: 4.5602\n",
      "Epoch 263/600\n",
      "122/122 [==============================] - 0s 633us/step - loss: 131.8752 - mae: 7.7282 - val_loss: 33.9936 - val_mae: 4.4257\n",
      "Epoch 264/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 141.3447 - mae: 7.7455 - val_loss: 24.6558 - val_mae: 3.5975\n",
      "Epoch 265/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 138.3545 - mae: 7.6486 - val_loss: 23.5441 - val_mae: 3.5108\n",
      "Epoch 266/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 128.7958 - mae: 7.6432 - val_loss: 27.0873 - val_mae: 3.9151\n",
      "Epoch 267/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 132.0562 - mae: 7.5962 - val_loss: 26.3021 - val_mae: 3.7788\n",
      "Epoch 268/600\n",
      "122/122 [==============================] - 0s 634us/step - loss: 138.4309 - mae: 7.6835 - val_loss: 29.3862 - val_mae: 4.0856\n",
      "Epoch 269/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 127.7391 - mae: 7.6071 - val_loss: 27.3856 - val_mae: 3.8099\n",
      "Epoch 270/600\n",
      "122/122 [==============================] - 0s 612us/step - loss: 130.5835 - mae: 7.5843 - val_loss: 32.3358 - val_mae: 4.1007\n",
      "Epoch 271/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 124.6568 - mae: 7.4708 - val_loss: 25.6999 - val_mae: 3.5708\n",
      "Epoch 272/600\n",
      "122/122 [==============================] - 0s 632us/step - loss: 133.2254 - mae: 7.6529 - val_loss: 34.7584 - val_mae: 4.4945\n",
      "Epoch 273/600\n",
      "122/122 [==============================] - 0s 639us/step - loss: 135.4955 - mae: 7.5287 - val_loss: 26.0018 - val_mae: 3.6681\n",
      "Epoch 274/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 133.7479 - mae: 7.6863 - val_loss: 25.9341 - val_mae: 3.7209\n",
      "Epoch 275/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 120.6501 - mae: 7.3846 - val_loss: 25.6020 - val_mae: 3.7124\n",
      "Epoch 276/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 138.8380 - mae: 7.7578 - val_loss: 25.3653 - val_mae: 3.6884\n",
      "Epoch 277/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 126.7971 - mae: 7.4681 - val_loss: 28.8239 - val_mae: 4.0696\n",
      "Epoch 278/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 130.4160 - mae: 7.6189 - val_loss: 23.0256 - val_mae: 3.4976\n",
      "Epoch 279/600\n",
      "122/122 [==============================] - 0s 687us/step - loss: 129.7087 - mae: 7.5730 - val_loss: 46.4970 - val_mae: 5.4783\n",
      "Epoch 280/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 127.6638 - mae: 7.5603 - val_loss: 26.4067 - val_mae: 3.6771\n",
      "Epoch 281/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 131.2289 - mae: 7.5631 - val_loss: 24.0505 - val_mae: 3.5209\n",
      "Epoch 282/600\n",
      "122/122 [==============================] - 0s 609us/step - loss: 129.1336 - mae: 7.5800 - val_loss: 34.2665 - val_mae: 4.4409\n",
      "Epoch 283/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 124.5848 - mae: 7.4516 - val_loss: 28.3588 - val_mae: 3.9369\n",
      "Epoch 284/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 130.1738 - mae: 7.6421 - val_loss: 26.0634 - val_mae: 3.6665\n",
      "Epoch 285/600\n",
      "122/122 [==============================] - 0s 689us/step - loss: 123.1017 - mae: 7.4296 - val_loss: 39.3366 - val_mae: 4.6938\n",
      "Epoch 286/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 123.4954 - mae: 7.3965 - val_loss: 25.0210 - val_mae: 3.4565\n",
      "Epoch 287/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 127.6683 - mae: 7.4667 - val_loss: 25.5726 - val_mae: 3.7400\n",
      "Epoch 288/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 118.7309 - mae: 7.4210 - val_loss: 25.1801 - val_mae: 3.5490\n",
      "Epoch 289/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 132.0427 - mae: 7.6205 - val_loss: 27.8072 - val_mae: 3.9391\n",
      "Epoch 290/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 123.8729 - mae: 7.5254 - val_loss: 31.1034 - val_mae: 4.1114\n",
      "Epoch 291/600\n",
      "122/122 [==============================] - 0s 643us/step - loss: 117.7091 - mae: 7.3118 - val_loss: 30.6415 - val_mae: 3.9982\n",
      "Epoch 292/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 124.5186 - mae: 7.5835 - val_loss: 24.4081 - val_mae: 3.4480\n",
      "Epoch 293/600\n",
      "122/122 [==============================] - 0s 622us/step - loss: 118.4501 - mae: 7.4386 - val_loss: 24.7303 - val_mae: 3.5745\n",
      "Epoch 294/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 123.4247 - mae: 7.5029 - val_loss: 24.8977 - val_mae: 3.6429\n",
      "Epoch 295/600\n",
      "122/122 [==============================] - 0s 687us/step - loss: 127.9781 - mae: 7.6099 - val_loss: 24.9120 - val_mae: 3.6369\n",
      "Epoch 296/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 130.9437 - mae: 7.6067 - val_loss: 27.9214 - val_mae: 3.9287\n",
      "Epoch 297/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 120.1089 - mae: 7.3962 - val_loss: 26.8310 - val_mae: 3.7950\n",
      "Epoch 298/600\n",
      "122/122 [==============================] - 0s 567us/step - loss: 115.5174 - mae: 7.2430 - val_loss: 25.3591 - val_mae: 3.5045\n",
      "Epoch 299/600\n",
      "122/122 [==============================] - 0s 566us/step - loss: 116.1201 - mae: 7.2818 - val_loss: 41.2239 - val_mae: 4.8439\n",
      "Epoch 300/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 117.8903 - mae: 7.3227 - val_loss: 28.8906 - val_mae: 4.0223\n",
      "Epoch 301/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 125.4888 - mae: 7.4100 - val_loss: 26.2254 - val_mae: 3.7708\n",
      "Epoch 302/600\n",
      "122/122 [==============================] - 0s 565us/step - loss: 122.7572 - mae: 7.4408 - val_loss: 24.4941 - val_mae: 3.6237\n",
      "Epoch 303/600\n",
      "122/122 [==============================] - 0s 664us/step - loss: 113.7217 - mae: 7.1581 - val_loss: 23.1370 - val_mae: 3.4085\n",
      "Epoch 304/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 110.1042 - mae: 7.1297 - val_loss: 31.4536 - val_mae: 4.3281\n",
      "Epoch 305/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 120.4978 - mae: 7.4175 - val_loss: 27.7978 - val_mae: 3.8389\n",
      "Epoch 306/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 115.1061 - mae: 7.2593 - val_loss: 28.4816 - val_mae: 3.9165\n",
      "Epoch 307/600\n",
      "122/122 [==============================] - 0s 689us/step - loss: 117.7204 - mae: 7.3138 - val_loss: 27.6708 - val_mae: 3.9188\n",
      "Epoch 308/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 112.8575 - mae: 7.1747 - val_loss: 25.4024 - val_mae: 3.5889\n",
      "Epoch 309/600\n",
      "122/122 [==============================] - 0s 572us/step - loss: 113.3144 - mae: 7.3364 - val_loss: 29.9003 - val_mae: 4.1001\n",
      "Epoch 310/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 122.4521 - mae: 7.3041 - val_loss: 33.0631 - val_mae: 4.4617\n",
      "Epoch 311/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 114.7998 - mae: 7.3229 - val_loss: 27.4079 - val_mae: 3.8538\n",
      "Epoch 312/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 111.0784 - mae: 7.1312 - val_loss: 24.7468 - val_mae: 3.5227\n",
      "Epoch 313/600\n",
      "122/122 [==============================] - 0s 638us/step - loss: 118.1350 - mae: 7.3076 - val_loss: 24.2721 - val_mae: 3.5611\n",
      "Epoch 314/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 117.5015 - mae: 7.2744 - val_loss: 28.0080 - val_mae: 3.8961\n",
      "Epoch 315/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 114.0617 - mae: 7.2974 - val_loss: 51.5087 - val_mae: 5.6299\n",
      "Epoch 316/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 114.3235 - mae: 7.3068 - val_loss: 30.5676 - val_mae: 4.1999\n",
      "Epoch 317/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 105.9151 - mae: 7.0868 - val_loss: 25.8098 - val_mae: 3.7666\n",
      "Epoch 318/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 107.1074 - mae: 7.0886 - val_loss: 28.1940 - val_mae: 3.8029\n",
      "Epoch 319/600\n",
      "122/122 [==============================] - 0s 661us/step - loss: 115.1023 - mae: 7.2595 - val_loss: 27.5154 - val_mae: 3.8145\n",
      "Epoch 320/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 103.4246 - mae: 6.9961 - val_loss: 25.8799 - val_mae: 3.7013\n",
      "Epoch 321/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 113.0612 - mae: 7.2575 - val_loss: 33.0743 - val_mae: 4.4342\n",
      "Epoch 322/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 109.7592 - mae: 7.2059 - val_loss: 24.2120 - val_mae: 3.5214\n",
      "Epoch 323/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 109.6450 - mae: 7.2042 - val_loss: 29.1831 - val_mae: 3.9261\n",
      "Epoch 324/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 118.6657 - mae: 7.3704 - val_loss: 44.1453 - val_mae: 5.1798\n",
      "Epoch 325/600\n",
      "122/122 [==============================] - 0s 693us/step - loss: 113.5929 - mae: 7.2177 - val_loss: 39.3606 - val_mae: 4.8090\n",
      "Epoch 326/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 114.7748 - mae: 7.3222 - val_loss: 35.4997 - val_mae: 4.6251\n",
      "Epoch 327/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 102.9849 - mae: 6.9861 - val_loss: 25.2751 - val_mae: 3.6900\n",
      "Epoch 328/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 101.5329 - mae: 6.8893 - val_loss: 37.0235 - val_mae: 4.7223\n",
      "Epoch 329/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 106.6492 - mae: 7.1004 - val_loss: 27.6830 - val_mae: 3.8378\n",
      "Epoch 330/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 111.5527 - mae: 7.1549 - val_loss: 25.1393 - val_mae: 3.6280\n",
      "Epoch 331/600\n",
      "122/122 [==============================] - 0s 656us/step - loss: 108.7901 - mae: 7.1893 - val_loss: 25.7294 - val_mae: 3.6784\n",
      "Epoch 332/600\n",
      "122/122 [==============================] - 0s 644us/step - loss: 110.5409 - mae: 7.1437 - val_loss: 42.8807 - val_mae: 5.2135\n",
      "Epoch 333/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 106.1811 - mae: 7.0665 - val_loss: 33.2015 - val_mae: 4.3891\n",
      "Epoch 334/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 108.4888 - mae: 7.1875 - val_loss: 29.2011 - val_mae: 4.0171\n",
      "Epoch 335/600\n",
      "122/122 [==============================] - 0s 571us/step - loss: 107.5028 - mae: 7.0717 - val_loss: 24.5259 - val_mae: 3.5371\n",
      "Epoch 336/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 107.6238 - mae: 7.1020 - val_loss: 32.9772 - val_mae: 4.3947\n",
      "Epoch 337/600\n",
      "122/122 [==============================] - 0s 645us/step - loss: 106.5479 - mae: 7.1627 - val_loss: 28.8474 - val_mae: 3.9729\n",
      "Epoch 338/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 108.7126 - mae: 7.0832 - val_loss: 25.4741 - val_mae: 3.5894\n",
      "Epoch 339/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 112.1548 - mae: 7.2417 - val_loss: 24.9917 - val_mae: 3.5169\n",
      "Epoch 340/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 105.9383 - mae: 6.9984 - val_loss: 26.8315 - val_mae: 3.7298\n",
      "Epoch 341/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 102.3765 - mae: 7.0610 - val_loss: 26.3635 - val_mae: 3.6781\n",
      "Epoch 342/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 101.6073 - mae: 7.0484 - val_loss: 29.7407 - val_mae: 4.0804\n",
      "Epoch 343/600\n",
      "122/122 [==============================] - 0s 688us/step - loss: 106.7616 - mae: 7.1743 - val_loss: 30.8959 - val_mae: 4.1632\n",
      "Epoch 344/600\n",
      "122/122 [==============================] - 0s 615us/step - loss: 111.2758 - mae: 7.2719 - val_loss: 24.7971 - val_mae: 3.5923\n",
      "Epoch 345/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 98.3298 - mae: 6.8796 - val_loss: 37.6096 - val_mae: 4.7246\n",
      "Epoch 346/600\n",
      "122/122 [==============================] - 0s 602us/step - loss: 102.5976 - mae: 7.0063 - val_loss: 25.4099 - val_mae: 3.6144\n",
      "Epoch 347/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 102.6276 - mae: 7.0428 - val_loss: 26.2338 - val_mae: 3.6473\n",
      "Epoch 348/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 102.8378 - mae: 6.9549 - val_loss: 35.8352 - val_mae: 4.5681\n",
      "Epoch 349/600\n",
      "122/122 [==============================] - 0s 639us/step - loss: 105.2092 - mae: 7.0676 - val_loss: 27.8033 - val_mae: 3.7647\n",
      "Epoch 350/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 107.9726 - mae: 7.1033 - val_loss: 42.7641 - val_mae: 5.0640\n",
      "Epoch 351/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 107.1612 - mae: 7.0543 - val_loss: 29.8490 - val_mae: 4.0595\n",
      "Epoch 352/600\n",
      "122/122 [==============================] - 0s 624us/step - loss: 102.5033 - mae: 7.0242 - val_loss: 28.2293 - val_mae: 3.8798\n",
      "Epoch 353/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 94.9674 - mae: 6.7822 - val_loss: 27.5652 - val_mae: 3.8609\n",
      "Epoch 354/600\n",
      "122/122 [==============================] - 0s 676us/step - loss: 101.0261 - mae: 6.9236 - val_loss: 31.7575 - val_mae: 4.2938\n",
      "Epoch 355/600\n",
      "122/122 [==============================] - 0s 608us/step - loss: 102.9347 - mae: 6.9689 - val_loss: 28.6587 - val_mae: 3.9524\n",
      "Epoch 356/600\n",
      "122/122 [==============================] - 0s 620us/step - loss: 96.9229 - mae: 6.8173 - val_loss: 32.9559 - val_mae: 4.3544\n",
      "Epoch 357/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 100.4873 - mae: 6.9562 - val_loss: 32.1726 - val_mae: 4.3078\n",
      "Epoch 358/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 97.3546 - mae: 6.8515 - val_loss: 30.3163 - val_mae: 4.1265\n",
      "Epoch 359/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 103.1767 - mae: 7.0390 - val_loss: 40.7441 - val_mae: 5.0197\n",
      "Epoch 360/600\n",
      "122/122 [==============================] - 0s 655us/step - loss: 96.5441 - mae: 6.8448 - val_loss: 25.2564 - val_mae: 3.5566\n",
      "Epoch 361/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 97.0351 - mae: 6.8376 - val_loss: 46.8306 - val_mae: 5.4798\n",
      "Epoch 362/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 102.3824 - mae: 7.0055 - val_loss: 26.8745 - val_mae: 3.8192\n",
      "Epoch 363/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 94.3123 - mae: 6.7967 - val_loss: 28.0429 - val_mae: 3.8406\n",
      "Epoch 364/600\n",
      "122/122 [==============================] - 0s 653us/step - loss: 102.7373 - mae: 7.1435 - val_loss: 29.4444 - val_mae: 3.8377\n",
      "Epoch 365/600\n",
      "122/122 [==============================] - 0s 644us/step - loss: 97.3593 - mae: 6.9147 - val_loss: 28.9857 - val_mae: 3.9469\n",
      "Epoch 366/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 97.2281 - mae: 6.7931 - val_loss: 32.4104 - val_mae: 4.3217\n",
      "Epoch 367/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 95.4231 - mae: 6.7304 - val_loss: 27.3150 - val_mae: 3.8631\n",
      "Epoch 368/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 90.9318 - mae: 6.7890 - val_loss: 33.1398 - val_mae: 4.3312\n",
      "Epoch 369/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 102.5647 - mae: 7.0396 - val_loss: 27.6856 - val_mae: 3.7883\n",
      "Epoch 370/600\n",
      "122/122 [==============================] - 0s 653us/step - loss: 95.0670 - mae: 6.7842 - val_loss: 27.1863 - val_mae: 3.7404\n",
      "Epoch 371/600\n",
      "122/122 [==============================] - 0s 641us/step - loss: 94.4214 - mae: 6.8144 - val_loss: 26.7127 - val_mae: 3.7439\n",
      "Epoch 372/600\n",
      "122/122 [==============================] - 0s 603us/step - loss: 105.5268 - mae: 7.0547 - val_loss: 27.7005 - val_mae: 3.8681\n",
      "Epoch 373/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 97.2245 - mae: 6.8901 - val_loss: 25.1497 - val_mae: 3.6465\n",
      "Epoch 374/600\n",
      "122/122 [==============================] - 0s 602us/step - loss: 101.8467 - mae: 6.9853 - val_loss: 32.7549 - val_mae: 4.2498\n",
      "Epoch 375/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 97.2350 - mae: 6.8277 - val_loss: 38.1343 - val_mae: 4.7349\n",
      "Epoch 376/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 101.1744 - mae: 6.9879 - val_loss: 25.3354 - val_mae: 3.6053\n",
      "Epoch 377/600\n",
      "122/122 [==============================] - 0s 636us/step - loss: 101.4887 - mae: 6.9651 - val_loss: 28.9524 - val_mae: 4.0041\n",
      "Epoch 378/600\n",
      "122/122 [==============================] - 0s 618us/step - loss: 90.8117 - mae: 6.7982 - val_loss: 31.3962 - val_mae: 4.2451\n",
      "Epoch 379/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 90.6159 - mae: 6.7352 - val_loss: 31.5777 - val_mae: 4.1733\n",
      "Epoch 380/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 98.8290 - mae: 6.8656 - val_loss: 26.6113 - val_mae: 3.7603\n",
      "Epoch 381/600\n",
      "122/122 [==============================] - 0s 597us/step - loss: 89.8963 - mae: 6.6199 - val_loss: 28.6799 - val_mae: 3.9238\n",
      "Epoch 382/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 91.1625 - mae: 6.7138 - val_loss: 25.9277 - val_mae: 3.6592\n",
      "Epoch 383/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 95.4591 - mae: 6.7864 - val_loss: 28.9064 - val_mae: 3.9634\n",
      "Epoch 384/600\n",
      "122/122 [==============================] - 0s 661us/step - loss: 94.4595 - mae: 6.7763 - val_loss: 32.1654 - val_mae: 4.3068\n",
      "Epoch 385/600\n",
      "122/122 [==============================] - 0s 623us/step - loss: 96.2183 - mae: 6.8880 - val_loss: 27.9991 - val_mae: 3.8643\n",
      "Epoch 386/600\n",
      "122/122 [==============================] - 0s 656us/step - loss: 90.5492 - mae: 6.7586 - val_loss: 27.0646 - val_mae: 3.7757\n",
      "Epoch 387/600\n",
      "122/122 [==============================] - 0s 684us/step - loss: 92.3945 - mae: 6.7658 - val_loss: 27.9127 - val_mae: 3.8917\n",
      "Epoch 388/600\n",
      "122/122 [==============================] - 0s 596us/step - loss: 94.3849 - mae: 6.8297 - val_loss: 31.7097 - val_mae: 4.1956\n",
      "Epoch 389/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 96.5249 - mae: 6.7865 - val_loss: 33.4242 - val_mae: 4.3875\n",
      "Epoch 390/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 97.2842 - mae: 6.9006 - val_loss: 28.4680 - val_mae: 3.9305\n",
      "Epoch 391/600\n",
      "122/122 [==============================] - 0s 593us/step - loss: 95.2797 - mae: 6.7849 - val_loss: 29.1693 - val_mae: 3.9536\n",
      "Epoch 392/600\n",
      "122/122 [==============================] - 0s 634us/step - loss: 89.4796 - mae: 6.6723 - val_loss: 29.9735 - val_mae: 4.0585\n",
      "Epoch 393/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 86.6421 - mae: 6.6409 - val_loss: 25.9417 - val_mae: 3.7036\n",
      "Epoch 394/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 87.2004 - mae: 6.6189 - val_loss: 34.1806 - val_mae: 4.5635\n",
      "Epoch 395/600\n",
      "122/122 [==============================] - 0s 617us/step - loss: 91.9142 - mae: 6.7367 - val_loss: 29.6049 - val_mae: 4.0546\n",
      "Epoch 396/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 93.9906 - mae: 6.7556 - val_loss: 29.8904 - val_mae: 4.1203\n",
      "Epoch 397/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 82.8508 - mae: 6.5060 - val_loss: 25.4612 - val_mae: 3.6109\n",
      "Epoch 398/600\n",
      "122/122 [==============================] - 0s 628us/step - loss: 100.2027 - mae: 6.9607 - val_loss: 38.5206 - val_mae: 4.8040\n",
      "Epoch 399/600\n",
      "122/122 [==============================] - 0s 615us/step - loss: 91.4963 - mae: 6.7675 - val_loss: 31.6166 - val_mae: 4.0922\n",
      "Epoch 400/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 99.4334 - mae: 6.9286 - val_loss: 27.7731 - val_mae: 3.7861\n",
      "Epoch 401/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 92.9376 - mae: 6.6904 - val_loss: 38.2865 - val_mae: 4.7907\n",
      "Epoch 402/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 88.4322 - mae: 6.6719 - val_loss: 25.9537 - val_mae: 3.6482\n",
      "Epoch 403/600\n",
      "122/122 [==============================] - 0s 670us/step - loss: 97.4788 - mae: 6.9069 - val_loss: 33.0103 - val_mae: 4.3962\n",
      "Epoch 404/600\n",
      "122/122 [==============================] - 0s 619us/step - loss: 89.4220 - mae: 6.6570 - val_loss: 24.7821 - val_mae: 3.5379\n",
      "Epoch 405/600\n",
      "122/122 [==============================] - 0s 615us/step - loss: 86.4823 - mae: 6.5986 - val_loss: 27.6741 - val_mae: 3.8436\n",
      "Epoch 406/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 91.2022 - mae: 6.7369 - val_loss: 34.7468 - val_mae: 4.4405\n",
      "Epoch 407/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 85.8428 - mae: 6.5514 - val_loss: 28.8129 - val_mae: 3.9347\n",
      "Epoch 408/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 90.3783 - mae: 6.7155 - val_loss: 46.5311 - val_mae: 5.3526\n",
      "Epoch 409/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 90.6377 - mae: 6.7134 - val_loss: 27.0746 - val_mae: 3.7839\n",
      "Epoch 410/600\n",
      "122/122 [==============================] - 0s 642us/step - loss: 91.1613 - mae: 6.7394 - val_loss: 30.1758 - val_mae: 3.9304\n",
      "Epoch 411/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 98.3737 - mae: 6.8867 - val_loss: 29.4718 - val_mae: 4.0063\n",
      "Epoch 412/600\n",
      "122/122 [==============================] - 0s 642us/step - loss: 91.1569 - mae: 6.6428 - val_loss: 44.3265 - val_mae: 5.2522\n",
      "Epoch 413/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 83.9040 - mae: 6.4840 - val_loss: 34.2944 - val_mae: 4.3116\n",
      "Epoch 414/600\n",
      "122/122 [==============================] - 0s 645us/step - loss: 91.7600 - mae: 6.7130 - val_loss: 28.1458 - val_mae: 3.7855\n",
      "Epoch 415/600\n",
      "122/122 [==============================] - 0s 625us/step - loss: 87.9221 - mae: 6.6000 - val_loss: 38.0740 - val_mae: 4.6425\n",
      "Epoch 416/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 89.8284 - mae: 6.6682 - val_loss: 27.8909 - val_mae: 3.7789\n",
      "Epoch 417/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 92.9819 - mae: 6.7319 - val_loss: 29.2213 - val_mae: 3.8890\n",
      "Epoch 418/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 85.5049 - mae: 6.5252 - val_loss: 27.9279 - val_mae: 3.7721\n",
      "Epoch 419/600\n",
      "122/122 [==============================] - 0s 640us/step - loss: 93.4911 - mae: 6.7919 - val_loss: 41.1554 - val_mae: 4.9826\n",
      "Epoch 420/600\n",
      "122/122 [==============================] - 0s 653us/step - loss: 94.2353 - mae: 6.8006 - val_loss: 35.2258 - val_mae: 4.3926\n",
      "Epoch 421/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 93.4430 - mae: 6.7603 - val_loss: 28.8724 - val_mae: 3.8046\n",
      "Epoch 422/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 93.2425 - mae: 6.7997 - val_loss: 35.1169 - val_mae: 4.4453\n",
      "Epoch 423/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 90.3263 - mae: 6.6757 - val_loss: 28.5539 - val_mae: 3.8336\n",
      "Epoch 424/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 87.3061 - mae: 6.6475 - val_loss: 36.8763 - val_mae: 4.5165\n",
      "Epoch 425/600\n",
      "122/122 [==============================] - 0s 618us/step - loss: 88.2035 - mae: 6.6759 - val_loss: 27.5337 - val_mae: 3.7915\n",
      "Epoch 426/600\n",
      "122/122 [==============================] - 0s 621us/step - loss: 88.5908 - mae: 6.7333 - val_loss: 26.2986 - val_mae: 3.6360\n",
      "Epoch 427/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 86.5204 - mae: 6.5773 - val_loss: 29.0385 - val_mae: 3.8408\n",
      "Epoch 428/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 81.2070 - mae: 6.4953 - val_loss: 30.2269 - val_mae: 4.0825\n",
      "Epoch 429/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 86.1928 - mae: 6.5984 - val_loss: 27.5734 - val_mae: 3.7927\n",
      "Epoch 430/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 79.5110 - mae: 6.3577 - val_loss: 31.1295 - val_mae: 4.0344\n",
      "Epoch 431/600\n",
      "122/122 [==============================] - 0s 651us/step - loss: 88.5514 - mae: 6.5970 - val_loss: 31.7382 - val_mae: 4.2754\n",
      "Epoch 432/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 84.4737 - mae: 6.4974 - val_loss: 28.2289 - val_mae: 3.7245\n",
      "Epoch 433/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 90.4261 - mae: 6.6884 - val_loss: 33.1339 - val_mae: 4.3789\n",
      "Epoch 434/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 87.0714 - mae: 6.5898 - val_loss: 29.3556 - val_mae: 3.9593\n",
      "Epoch 435/600\n",
      "122/122 [==============================] - 0s 571us/step - loss: 78.7052 - mae: 6.3495 - val_loss: 38.2475 - val_mae: 4.7227\n",
      "Epoch 436/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 81.4542 - mae: 6.4352 - val_loss: 37.6966 - val_mae: 4.5491\n",
      "Epoch 437/600\n",
      "122/122 [==============================] - 0s 699us/step - loss: 84.8200 - mae: 6.4726 - val_loss: 27.1959 - val_mae: 3.6778\n",
      "Epoch 438/600\n",
      "122/122 [==============================] - 0s 629us/step - loss: 85.4808 - mae: 6.5342 - val_loss: 27.1388 - val_mae: 3.7681\n",
      "Epoch 439/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 84.1730 - mae: 6.5155 - val_loss: 33.5671 - val_mae: 4.2922\n",
      "Epoch 440/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 90.0653 - mae: 6.7922 - val_loss: 28.5450 - val_mae: 3.9804\n",
      "Epoch 441/600\n",
      "122/122 [==============================] - 0s 602us/step - loss: 85.4967 - mae: 6.5354 - val_loss: 28.1446 - val_mae: 3.8733\n",
      "Epoch 442/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 82.6964 - mae: 6.5317 - val_loss: 26.5164 - val_mae: 3.6438\n",
      "Epoch 443/600\n",
      "122/122 [==============================] - 0s 661us/step - loss: 81.9632 - mae: 6.5010 - val_loss: 29.5584 - val_mae: 3.8921\n",
      "Epoch 444/600\n",
      "122/122 [==============================] - 0s 625us/step - loss: 89.1947 - mae: 6.6843 - val_loss: 27.2870 - val_mae: 3.7038\n",
      "Epoch 445/600\n",
      "122/122 [==============================] - 0s 636us/step - loss: 87.5447 - mae: 6.5254 - val_loss: 24.0782 - val_mae: 3.5698\n",
      "Epoch 446/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 80.3503 - mae: 6.4553 - val_loss: 32.6865 - val_mae: 4.2065\n",
      "Epoch 447/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 89.6204 - mae: 6.6505 - val_loss: 27.1972 - val_mae: 3.6334\n",
      "Epoch 448/600\n",
      "122/122 [==============================] - 0s 706us/step - loss: 86.6886 - mae: 6.5496 - val_loss: 32.9065 - val_mae: 4.2660\n",
      "Epoch 449/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 83.3274 - mae: 6.4831 - val_loss: 37.5523 - val_mae: 4.6246\n",
      "Epoch 450/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 81.0112 - mae: 6.4201 - val_loss: 31.9393 - val_mae: 4.0588\n",
      "Epoch 451/600\n",
      "122/122 [==============================] - 0s 609us/step - loss: 90.3970 - mae: 6.5701 - val_loss: 41.6868 - val_mae: 4.9207\n",
      "Epoch 452/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 81.3836 - mae: 6.5079 - val_loss: 34.9102 - val_mae: 4.4129\n",
      "Epoch 453/600\n",
      "122/122 [==============================] - 0s 571us/step - loss: 81.7424 - mae: 6.4792 - val_loss: 28.9586 - val_mae: 3.7652\n",
      "Epoch 454/600\n",
      "122/122 [==============================] - 0s 646us/step - loss: 80.6496 - mae: 6.3229 - val_loss: 28.8852 - val_mae: 3.7783\n",
      "Epoch 455/600\n",
      "122/122 [==============================] - 0s 612us/step - loss: 77.2283 - mae: 6.2874 - val_loss: 31.6343 - val_mae: 4.0101\n",
      "Epoch 456/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 76.9278 - mae: 6.3405 - val_loss: 37.2962 - val_mae: 4.6413\n",
      "Epoch 457/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 84.7577 - mae: 6.4890 - val_loss: 33.7754 - val_mae: 4.4264\n",
      "Epoch 458/600\n",
      "122/122 [==============================] - 0s 680us/step - loss: 77.9366 - mae: 6.3887 - val_loss: 27.3627 - val_mae: 3.7291\n",
      "Epoch 459/600\n",
      "122/122 [==============================] - 0s 621us/step - loss: 84.2162 - mae: 6.4686 - val_loss: 29.2844 - val_mae: 3.8766\n",
      "Epoch 460/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 80.0465 - mae: 6.3695 - val_loss: 26.8401 - val_mae: 3.7596\n",
      "Epoch 461/600\n",
      "122/122 [==============================] - 0s 589us/step - loss: 81.7903 - mae: 6.4827 - val_loss: 30.6842 - val_mae: 4.0658\n",
      "Epoch 462/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 78.6895 - mae: 6.3268 - val_loss: 30.5870 - val_mae: 4.0327\n",
      "Epoch 463/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 78.9564 - mae: 6.4514 - val_loss: 29.7035 - val_mae: 4.0291\n",
      "Epoch 464/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 77.2745 - mae: 6.2972 - val_loss: 32.3151 - val_mae: 4.2312\n",
      "Epoch 465/600\n",
      "122/122 [==============================] - 0s 638us/step - loss: 86.9719 - mae: 6.6143 - val_loss: 33.5426 - val_mae: 4.2041\n",
      "Epoch 466/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 76.9413 - mae: 6.2824 - val_loss: 28.3151 - val_mae: 3.8087\n",
      "Epoch 467/600\n",
      "122/122 [==============================] - 0s 574us/step - loss: 86.3879 - mae: 6.5761 - val_loss: 30.6810 - val_mae: 3.9849\n",
      "Epoch 468/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 86.1298 - mae: 6.4874 - val_loss: 29.2604 - val_mae: 3.9389\n",
      "Epoch 469/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 78.6632 - mae: 6.3210 - val_loss: 30.1017 - val_mae: 4.0590\n",
      "Epoch 470/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 82.5615 - mae: 6.4671 - val_loss: 35.5807 - val_mae: 4.4751\n",
      "Epoch 471/600\n",
      "122/122 [==============================] - 0s 639us/step - loss: 85.9407 - mae: 6.5629 - val_loss: 29.6635 - val_mae: 3.9240\n",
      "Epoch 472/600\n",
      "122/122 [==============================] - 0s 642us/step - loss: 77.5265 - mae: 6.4380 - val_loss: 55.8644 - val_mae: 5.9974\n",
      "Epoch 473/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 80.3637 - mae: 6.3725 - val_loss: 28.6696 - val_mae: 3.8502\n",
      "Epoch 474/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 80.6264 - mae: 6.3211 - val_loss: 35.6051 - val_mae: 4.4166\n",
      "Epoch 475/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 75.7989 - mae: 6.2403 - val_loss: 34.6699 - val_mae: 4.4437\n",
      "Epoch 476/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 75.4193 - mae: 6.2146 - val_loss: 29.4128 - val_mae: 3.9761\n",
      "Epoch 477/600\n",
      "122/122 [==============================] - 0s 636us/step - loss: 76.2628 - mae: 6.2882 - val_loss: 28.9618 - val_mae: 3.8222\n",
      "Epoch 478/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 76.0086 - mae: 6.2938 - val_loss: 26.9804 - val_mae: 3.6721\n",
      "Epoch 479/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 77.8421 - mae: 6.2699 - val_loss: 28.6563 - val_mae: 3.8615\n",
      "Epoch 480/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 79.9264 - mae: 6.3724 - val_loss: 28.3412 - val_mae: 3.8031\n",
      "Epoch 481/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 74.8908 - mae: 6.2570 - val_loss: 34.0785 - val_mae: 4.3277\n",
      "Epoch 482/600\n",
      "122/122 [==============================] - 0s 705us/step - loss: 76.7612 - mae: 6.2146 - val_loss: 29.1168 - val_mae: 3.9281\n",
      "Epoch 483/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 76.5288 - mae: 6.3009 - val_loss: 27.6690 - val_mae: 3.7527\n",
      "Epoch 484/600\n",
      "122/122 [==============================] - 0s 568us/step - loss: 80.0251 - mae: 6.4221 - val_loss: 43.2798 - val_mae: 5.2352\n",
      "Epoch 485/600\n",
      "122/122 [==============================] - 0s 582us/step - loss: 75.2391 - mae: 6.2769 - val_loss: 33.6874 - val_mae: 4.2778\n",
      "Epoch 486/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 77.5584 - mae: 6.2564 - val_loss: 35.8499 - val_mae: 4.4822\n",
      "Epoch 487/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 73.6213 - mae: 6.2002 - val_loss: 27.6309 - val_mae: 3.8097\n",
      "Epoch 488/600\n",
      "122/122 [==============================] - 0s 671us/step - loss: 77.7152 - mae: 6.3047 - val_loss: 35.8839 - val_mae: 4.4460\n",
      "Epoch 489/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 79.0167 - mae: 6.3962 - val_loss: 28.9556 - val_mae: 3.8331\n",
      "Epoch 490/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 79.4565 - mae: 6.3275 - val_loss: 32.2776 - val_mae: 4.0745\n",
      "Epoch 491/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 75.7902 - mae: 6.2448 - val_loss: 33.9332 - val_mae: 4.2252\n",
      "Epoch 492/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 77.9096 - mae: 6.2142 - val_loss: 29.5622 - val_mae: 3.9393\n",
      "Epoch 493/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 78.3822 - mae: 6.3074 - val_loss: 33.0795 - val_mae: 4.3563\n",
      "Epoch 494/600\n",
      "122/122 [==============================] - 0s 595us/step - loss: 80.2739 - mae: 6.3327 - val_loss: 29.5213 - val_mae: 4.0429\n",
      "Epoch 495/600\n",
      "122/122 [==============================] - 0s 638us/step - loss: 74.6985 - mae: 6.2051 - val_loss: 48.6998 - val_mae: 5.3785\n",
      "Epoch 496/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 78.0760 - mae: 6.3287 - val_loss: 28.7581 - val_mae: 3.8387\n",
      "Epoch 497/600\n",
      "122/122 [==============================] - 0s 618us/step - loss: 75.4374 - mae: 6.2284 - val_loss: 44.2998 - val_mae: 5.0641\n",
      "Epoch 498/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 71.3562 - mae: 6.1513 - val_loss: 31.4375 - val_mae: 4.1348\n",
      "Epoch 499/600\n",
      "122/122 [==============================] - 0s 603us/step - loss: 76.9496 - mae: 6.2204 - val_loss: 37.0546 - val_mae: 4.5241\n",
      "Epoch 500/600\n",
      "122/122 [==============================] - 0s 673us/step - loss: 80.5955 - mae: 6.3343 - val_loss: 29.2536 - val_mae: 3.8806\n",
      "Epoch 501/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 78.7379 - mae: 6.2558 - val_loss: 28.7297 - val_mae: 3.9191\n",
      "Epoch 502/600\n",
      "122/122 [==============================] - 0s 577us/step - loss: 80.9972 - mae: 6.4310 - val_loss: 37.7527 - val_mae: 4.6661\n",
      "Epoch 503/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 71.5717 - mae: 6.1658 - val_loss: 40.0549 - val_mae: 4.8075\n",
      "Epoch 504/600\n",
      "122/122 [==============================] - 0s 620us/step - loss: 76.0336 - mae: 6.2782 - val_loss: 28.9322 - val_mae: 3.8737\n",
      "Epoch 505/600\n",
      "122/122 [==============================] - 0s 628us/step - loss: 79.7340 - mae: 6.3414 - val_loss: 31.0525 - val_mae: 4.0376\n",
      "Epoch 506/600\n",
      "122/122 [==============================] - 0s 614us/step - loss: 75.7314 - mae: 6.2657 - val_loss: 35.9309 - val_mae: 4.5889\n",
      "Epoch 507/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 76.3902 - mae: 6.3548 - val_loss: 34.0795 - val_mae: 4.3347\n",
      "Epoch 508/600\n",
      "122/122 [==============================] - 0s 615us/step - loss: 77.1097 - mae: 6.3143 - val_loss: 34.8371 - val_mae: 4.4441\n",
      "Epoch 509/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 71.8306 - mae: 6.1450 - val_loss: 32.5346 - val_mae: 4.2014\n",
      "Epoch 510/600\n",
      "122/122 [==============================] - 0s 658us/step - loss: 74.4064 - mae: 6.2020 - val_loss: 36.7465 - val_mae: 4.6147\n",
      "Epoch 511/600\n",
      "122/122 [==============================] - 0s 600us/step - loss: 72.0896 - mae: 6.1162 - val_loss: 35.7376 - val_mae: 4.5106\n",
      "Epoch 512/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 74.7959 - mae: 6.2304 - val_loss: 27.9442 - val_mae: 3.8342\n",
      "Epoch 513/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 73.5404 - mae: 6.1894 - val_loss: 32.0589 - val_mae: 4.2419\n",
      "Epoch 514/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 72.4765 - mae: 6.1719 - val_loss: 36.0352 - val_mae: 4.5252\n",
      "Epoch 515/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 74.0686 - mae: 6.2335 - val_loss: 36.2167 - val_mae: 4.5855\n",
      "Epoch 516/600\n",
      "122/122 [==============================] - 0s 703us/step - loss: 71.4128 - mae: 6.1805 - val_loss: 28.3604 - val_mae: 3.9619\n",
      "Epoch 517/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 75.3988 - mae: 6.2134 - val_loss: 41.1556 - val_mae: 4.9196\n",
      "Epoch 518/600\n",
      "122/122 [==============================] - 0s 566us/step - loss: 77.2140 - mae: 6.3304 - val_loss: 44.1285 - val_mae: 5.2773\n",
      "Epoch 519/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 77.0126 - mae: 6.2890 - val_loss: 27.7137 - val_mae: 3.7950\n",
      "Epoch 520/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 73.3944 - mae: 6.1328 - val_loss: 29.9814 - val_mae: 4.0830\n",
      "Epoch 521/600\n",
      "122/122 [==============================] - 0s 625us/step - loss: 71.8629 - mae: 6.0254 - val_loss: 46.1056 - val_mae: 5.2612\n",
      "Epoch 522/600\n",
      "122/122 [==============================] - 0s 599us/step - loss: 73.5941 - mae: 6.1627 - val_loss: 31.5870 - val_mae: 4.1243\n",
      "Epoch 523/600\n",
      "122/122 [==============================] - 0s 583us/step - loss: 72.4856 - mae: 6.1461 - val_loss: 28.1060 - val_mae: 3.7799\n",
      "Epoch 524/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 74.0001 - mae: 6.2311 - val_loss: 59.0697 - val_mae: 5.8552\n",
      "Epoch 525/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 77.6735 - mae: 6.3448 - val_loss: 38.3805 - val_mae: 4.6621\n",
      "Epoch 526/600\n",
      "122/122 [==============================] - 0s 576us/step - loss: 73.5175 - mae: 6.2052 - val_loss: 31.0912 - val_mae: 4.0873\n",
      "Epoch 527/600\n",
      "122/122 [==============================] - 0s 652us/step - loss: 76.6503 - mae: 6.2398 - val_loss: 31.5261 - val_mae: 4.0674\n",
      "Epoch 528/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 73.3520 - mae: 6.1852 - val_loss: 30.9438 - val_mae: 4.0588\n",
      "Epoch 529/600\n",
      "122/122 [==============================] - 0s 569us/step - loss: 71.8031 - mae: 6.0901 - val_loss: 27.4957 - val_mae: 3.7344\n",
      "Epoch 530/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 73.0303 - mae: 6.1561 - val_loss: 38.5370 - val_mae: 4.7634\n",
      "Epoch 531/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 69.8836 - mae: 6.0808 - val_loss: 32.5302 - val_mae: 4.1963\n",
      "Epoch 532/600\n",
      "122/122 [==============================] - 0s 687us/step - loss: 75.4160 - mae: 6.2349 - val_loss: 30.7691 - val_mae: 3.9827\n",
      "Epoch 533/600\n",
      "122/122 [==============================] - 0s 573us/step - loss: 68.8644 - mae: 6.0479 - val_loss: 30.6350 - val_mae: 4.1601\n",
      "Epoch 534/600\n",
      "122/122 [==============================] - 0s 590us/step - loss: 72.1740 - mae: 6.1959 - val_loss: 28.2566 - val_mae: 3.8344\n",
      "Epoch 535/600\n",
      "122/122 [==============================] - 0s 604us/step - loss: 73.9063 - mae: 6.1475 - val_loss: 28.9277 - val_mae: 3.9426\n",
      "Epoch 536/600\n",
      "122/122 [==============================] - 0s 617us/step - loss: 70.6186 - mae: 6.0542 - val_loss: 31.9367 - val_mae: 4.1323\n",
      "Epoch 537/600\n",
      "122/122 [==============================] - 0s 649us/step - loss: 71.3237 - mae: 6.0523 - val_loss: 27.3367 - val_mae: 3.7507\n",
      "Epoch 538/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 68.1893 - mae: 5.9644 - val_loss: 40.5296 - val_mae: 4.9443\n",
      "Epoch 539/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 65.9896 - mae: 5.9381 - val_loss: 30.7360 - val_mae: 4.0313\n",
      "Epoch 540/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 71.0637 - mae: 6.1140 - val_loss: 31.2385 - val_mae: 4.0513\n",
      "Epoch 541/600\n",
      "122/122 [==============================] - 0s 606us/step - loss: 71.2309 - mae: 5.9574 - val_loss: 30.8240 - val_mae: 4.0607\n",
      "Epoch 542/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 68.5905 - mae: 5.9660 - val_loss: 30.2406 - val_mae: 4.0899\n",
      "Epoch 543/600\n",
      "122/122 [==============================] - 0s 652us/step - loss: 71.8261 - mae: 6.1018 - val_loss: 33.4189 - val_mae: 4.3651\n",
      "Epoch 544/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 70.7719 - mae: 6.1767 - val_loss: 29.5782 - val_mae: 4.0334\n",
      "Epoch 545/600\n",
      "122/122 [==============================] - 0s 603us/step - loss: 70.7116 - mae: 6.0813 - val_loss: 39.7197 - val_mae: 4.7475\n",
      "Epoch 546/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 70.9246 - mae: 6.0081 - val_loss: 28.6460 - val_mae: 3.9031\n",
      "Epoch 547/600\n",
      "122/122 [==============================] - 0s 575us/step - loss: 70.9843 - mae: 6.1630 - val_loss: 37.4225 - val_mae: 4.7286\n",
      "Epoch 548/600\n",
      "122/122 [==============================] - 0s 643us/step - loss: 65.0981 - mae: 5.9385 - val_loss: 28.8255 - val_mae: 3.9290\n",
      "Epoch 549/600\n",
      "122/122 [==============================] - 0s 617us/step - loss: 70.2522 - mae: 6.0874 - val_loss: 35.5151 - val_mae: 4.4173\n",
      "Epoch 550/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 69.3274 - mae: 6.0521 - val_loss: 29.3654 - val_mae: 3.9902\n",
      "Epoch 551/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 73.8763 - mae: 6.1139 - val_loss: 52.7131 - val_mae: 5.6625\n",
      "Epoch 552/600\n",
      "122/122 [==============================] - 0s 568us/step - loss: 72.4885 - mae: 6.0918 - val_loss: 36.2668 - val_mae: 4.4703\n",
      "Epoch 553/600\n",
      "122/122 [==============================] - 0s 629us/step - loss: 70.4608 - mae: 6.1109 - val_loss: 37.6963 - val_mae: 4.7708\n",
      "Epoch 554/600\n",
      "122/122 [==============================] - 0s 624us/step - loss: 69.3369 - mae: 6.0859 - val_loss: 39.0864 - val_mae: 4.8371\n",
      "Epoch 555/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 71.3047 - mae: 6.0909 - val_loss: 44.2718 - val_mae: 5.1507\n",
      "Epoch 556/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 70.6839 - mae: 6.0842 - val_loss: 41.0203 - val_mae: 4.8799\n",
      "Epoch 557/600\n",
      "122/122 [==============================] - 0s 597us/step - loss: 68.3567 - mae: 6.0446 - val_loss: 32.5236 - val_mae: 4.1439\n",
      "Epoch 558/600\n",
      "122/122 [==============================] - 0s 720us/step - loss: 66.3513 - mae: 5.8844 - val_loss: 27.3438 - val_mae: 3.7795\n",
      "Epoch 559/600\n",
      "122/122 [==============================] - 0s 620us/step - loss: 67.6026 - mae: 6.0197 - val_loss: 29.0599 - val_mae: 3.9452\n",
      "Epoch 560/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 68.1828 - mae: 5.9372 - val_loss: 35.7305 - val_mae: 4.2863\n",
      "Epoch 561/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 69.1269 - mae: 6.0405 - val_loss: 35.4121 - val_mae: 4.4297\n",
      "Epoch 562/600\n",
      "122/122 [==============================] - 0s 584us/step - loss: 70.6642 - mae: 6.0493 - val_loss: 40.7657 - val_mae: 4.8482\n",
      "Epoch 563/600\n",
      "122/122 [==============================] - 0s 658us/step - loss: 66.4808 - mae: 5.9429 - val_loss: 35.8596 - val_mae: 4.5924\n",
      "Epoch 564/600\n",
      "122/122 [==============================] - 0s 596us/step - loss: 73.9351 - mae: 6.1757 - val_loss: 35.1742 - val_mae: 4.4932\n",
      "Epoch 565/600\n",
      "122/122 [==============================] - 0s 581us/step - loss: 74.7109 - mae: 6.1892 - val_loss: 31.4780 - val_mae: 4.0767\n",
      "Epoch 566/600\n",
      "122/122 [==============================] - 0s 624us/step - loss: 67.9742 - mae: 5.9464 - val_loss: 29.8909 - val_mae: 3.9806\n",
      "Epoch 567/600\n",
      "122/122 [==============================] - 0s 598us/step - loss: 71.5939 - mae: 6.1382 - val_loss: 28.6305 - val_mae: 3.8349\n",
      "Epoch 568/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 66.8212 - mae: 5.9155 - val_loss: 27.1754 - val_mae: 3.7467\n",
      "Epoch 569/600\n",
      "122/122 [==============================] - 0s 672us/step - loss: 65.5166 - mae: 5.8426 - val_loss: 31.5134 - val_mae: 4.0512\n",
      "Epoch 570/600\n",
      "122/122 [==============================] - 0s 616us/step - loss: 71.7166 - mae: 6.0929 - val_loss: 33.0060 - val_mae: 4.1232\n",
      "Epoch 571/600\n",
      "122/122 [==============================] - 0s 601us/step - loss: 70.4536 - mae: 6.0525 - val_loss: 30.0102 - val_mae: 3.8750\n",
      "Epoch 572/600\n",
      "122/122 [==============================] - 0s 610us/step - loss: 71.0704 - mae: 5.9949 - val_loss: 32.5792 - val_mae: 4.2477\n",
      "Epoch 573/600\n",
      "122/122 [==============================] - 0s 588us/step - loss: 71.4588 - mae: 6.0823 - val_loss: 40.3294 - val_mae: 4.8754\n",
      "Epoch 574/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 67.3621 - mae: 5.9673 - val_loss: 32.3976 - val_mae: 4.2827\n",
      "Epoch 575/600\n",
      "122/122 [==============================] - 0s 653us/step - loss: 68.0901 - mae: 5.9297 - val_loss: 32.3283 - val_mae: 4.2666\n",
      "Epoch 576/600\n",
      "122/122 [==============================] - 0s 613us/step - loss: 67.6405 - mae: 6.0168 - val_loss: 31.5684 - val_mae: 4.1501\n",
      "Epoch 577/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 68.6024 - mae: 6.0152 - val_loss: 37.3618 - val_mae: 4.6809\n",
      "Epoch 578/600\n",
      "122/122 [==============================] - 0s 579us/step - loss: 67.2606 - mae: 5.8978 - val_loss: 30.4700 - val_mae: 4.1243\n",
      "Epoch 579/600\n",
      "122/122 [==============================] - 0s 644us/step - loss: 69.1184 - mae: 6.0081 - val_loss: 34.6101 - val_mae: 4.3902\n",
      "Epoch 580/600\n",
      "122/122 [==============================] - 0s 636us/step - loss: 66.4390 - mae: 5.9404 - val_loss: 29.6656 - val_mae: 3.9275\n",
      "Epoch 581/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 65.5127 - mae: 5.8933 - val_loss: 38.6726 - val_mae: 4.4806\n",
      "Epoch 582/600\n",
      "122/122 [==============================] - 0s 607us/step - loss: 74.7932 - mae: 6.1058 - val_loss: 32.6245 - val_mae: 4.3206\n",
      "Epoch 583/600\n",
      "122/122 [==============================] - 0s 578us/step - loss: 69.9339 - mae: 5.9885 - val_loss: 28.1350 - val_mae: 3.8752\n",
      "Epoch 584/600\n",
      "122/122 [==============================] - 0s 647us/step - loss: 71.7263 - mae: 6.0867 - val_loss: 32.5719 - val_mae: 4.3368\n",
      "Epoch 585/600\n",
      "122/122 [==============================] - 0s 615us/step - loss: 66.2115 - mae: 5.9242 - val_loss: 33.0323 - val_mae: 4.3138\n",
      "Epoch 586/600\n",
      "122/122 [==============================] - 0s 587us/step - loss: 64.3833 - mae: 5.9058 - val_loss: 29.6310 - val_mae: 3.9923\n",
      "Epoch 587/600\n",
      "122/122 [==============================] - 0s 592us/step - loss: 67.7162 - mae: 5.9686 - val_loss: 33.8078 - val_mae: 4.4278\n",
      "Epoch 588/600\n",
      "122/122 [==============================] - 0s 630us/step - loss: 64.1721 - mae: 5.8121 - val_loss: 43.7342 - val_mae: 5.0993\n",
      "Epoch 589/600\n",
      "122/122 [==============================] - 0s 752us/step - loss: 69.8146 - mae: 6.0284 - val_loss: 34.2493 - val_mae: 4.4253\n",
      "Epoch 590/600\n",
      "122/122 [==============================] - 0s 630us/step - loss: 70.9446 - mae: 6.0146 - val_loss: 29.9832 - val_mae: 3.9808\n",
      "Epoch 591/600\n",
      "122/122 [==============================] - 0s 619us/step - loss: 72.8149 - mae: 6.0379 - val_loss: 29.8431 - val_mae: 3.9221\n",
      "Epoch 592/600\n",
      "122/122 [==============================] - 0s 585us/step - loss: 68.8580 - mae: 5.9807 - val_loss: 33.7968 - val_mae: 4.2494\n",
      "Epoch 593/600\n",
      "122/122 [==============================] - 0s 591us/step - loss: 67.1748 - mae: 5.9356 - val_loss: 34.0138 - val_mae: 4.3819\n",
      "Epoch 594/600\n",
      "122/122 [==============================] - 0s 641us/step - loss: 68.9414 - mae: 5.9582 - val_loss: 31.4552 - val_mae: 4.0747\n",
      "Epoch 595/600\n",
      "122/122 [==============================] - 0s 605us/step - loss: 65.9613 - mae: 5.8940 - val_loss: 32.6992 - val_mae: 4.2896\n",
      "Epoch 596/600\n",
      "122/122 [==============================] - 0s 580us/step - loss: 68.3164 - mae: 5.9434 - val_loss: 33.8580 - val_mae: 4.3676\n",
      "Epoch 597/600\n",
      "122/122 [==============================] - 0s 594us/step - loss: 63.7801 - mae: 5.7578 - val_loss: 29.8042 - val_mae: 3.9276\n",
      "Epoch 598/600\n",
      "122/122 [==============================] - 0s 586us/step - loss: 67.1286 - mae: 5.9582 - val_loss: 38.6780 - val_mae: 4.6867\n",
      "Epoch 599/600\n",
      "122/122 [==============================] - 0s 670us/step - loss: 68.7081 - mae: 5.9917 - val_loss: 31.2317 - val_mae: 4.0457\n",
      "Epoch 600/600\n",
      "122/122 [==============================] - 0s 609us/step - loss: 62.3521 - mae: 5.8316 - val_loss: 28.6709 - val_mae: 3.8716\n",
      "31/31 [==============================] - 0s 320us/step\n",
      "Epochs: 600 | MAE: 3.8716499240973046\n",
      "Training model with 650 epochs\n",
      "Epoch 1/650\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 67161.9219 - mae: 224.0545 - val_loss: 56321.4180 - val_mae: 204.0246\n",
      "Epoch 2/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 32264.3262 - mae: 147.6980 - val_loss: 8495.6494 - val_mae: 76.5154\n",
      "Epoch 3/650\n",
      "122/122 [==============================] - 0s 658us/step - loss: 4769.7852 - mae: 52.9983 - val_loss: 2795.4036 - val_mae: 39.6876\n",
      "Epoch 4/650\n",
      "122/122 [==============================] - 0s 573us/step - loss: 2880.9531 - mae: 40.5965 - val_loss: 1892.5642 - val_mae: 32.4282\n",
      "Epoch 5/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 2268.8110 - mae: 36.6246 - val_loss: 1494.8713 - val_mae: 28.6965\n",
      "Epoch 6/650\n",
      "122/122 [==============================] - 0s 559us/step - loss: 1938.7467 - mae: 33.9968 - val_loss: 1300.8878 - val_mae: 26.6168\n",
      "Epoch 7/650\n",
      "122/122 [==============================] - 0s 581us/step - loss: 1871.8715 - mae: 33.3798 - val_loss: 1148.2351 - val_mae: 25.0753\n",
      "Epoch 8/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 1762.9863 - mae: 32.4153 - val_loss: 1063.5522 - val_mae: 24.0661\n",
      "Epoch 9/650\n",
      "122/122 [==============================] - 0s 683us/step - loss: 1612.9087 - mae: 31.1863 - val_loss: 945.5687 - val_mae: 22.6190\n",
      "Epoch 10/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 1504.0205 - mae: 29.9810 - val_loss: 819.9510 - val_mae: 21.3027\n",
      "Epoch 11/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 1413.9794 - mae: 28.9370 - val_loss: 730.0966 - val_mae: 20.0332\n",
      "Epoch 12/650\n",
      "122/122 [==============================] - 0s 578us/step - loss: 1368.7444 - mae: 28.6217 - val_loss: 677.9775 - val_mae: 19.4161\n",
      "Epoch 13/650\n",
      "122/122 [==============================] - 0s 576us/step - loss: 1282.1377 - mae: 27.6426 - val_loss: 621.0158 - val_mae: 18.7760\n",
      "Epoch 14/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 1188.5935 - mae: 26.8067 - val_loss: 542.8661 - val_mae: 17.2962\n",
      "Epoch 15/650\n",
      "122/122 [==============================] - 0s 638us/step - loss: 1116.5070 - mae: 25.9175 - val_loss: 504.2998 - val_mae: 16.7564\n",
      "Epoch 16/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 1066.6569 - mae: 25.2740 - val_loss: 448.9839 - val_mae: 15.8890\n",
      "Epoch 17/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 971.7535 - mae: 24.2954 - val_loss: 395.0793 - val_mae: 14.9339\n",
      "Epoch 18/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 956.7435 - mae: 24.1563 - val_loss: 354.3753 - val_mae: 13.9908\n",
      "Epoch 19/650\n",
      "122/122 [==============================] - 0s 564us/step - loss: 969.6792 - mae: 24.0792 - val_loss: 307.2287 - val_mae: 12.9463\n",
      "Epoch 20/650\n",
      "122/122 [==============================] - 0s 575us/step - loss: 907.1680 - mae: 23.3522 - val_loss: 288.4007 - val_mae: 12.6982\n",
      "Epoch 21/650\n",
      "122/122 [==============================] - 0s 630us/step - loss: 870.2340 - mae: 22.6911 - val_loss: 242.9870 - val_mae: 11.5389\n",
      "Epoch 22/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 808.6395 - mae: 21.8757 - val_loss: 221.1059 - val_mae: 11.1712\n",
      "Epoch 23/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 752.4376 - mae: 21.0939 - val_loss: 192.7587 - val_mae: 10.2784\n",
      "Epoch 24/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 764.7482 - mae: 21.0093 - val_loss: 182.7022 - val_mae: 9.9037\n",
      "Epoch 25/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 751.1429 - mae: 20.7963 - val_loss: 159.9809 - val_mae: 9.3280\n",
      "Epoch 26/650\n",
      "122/122 [==============================] - 0s 689us/step - loss: 685.9547 - mae: 20.1794 - val_loss: 145.1523 - val_mae: 9.0288\n",
      "Epoch 27/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 724.8909 - mae: 20.1337 - val_loss: 125.2126 - val_mae: 8.3832\n",
      "Epoch 28/650\n",
      "122/122 [==============================] - 0s 576us/step - loss: 653.3508 - mae: 19.2158 - val_loss: 108.1790 - val_mae: 7.7196\n",
      "Epoch 29/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 652.0674 - mae: 19.2740 - val_loss: 108.8215 - val_mae: 7.8523\n",
      "Epoch 30/650\n",
      "122/122 [==============================] - 0s 640us/step - loss: 632.4993 - mae: 18.7511 - val_loss: 87.8019 - val_mae: 7.0451\n",
      "Epoch 31/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 643.6143 - mae: 18.6645 - val_loss: 79.6126 - val_mae: 6.5989\n",
      "Epoch 32/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 603.3885 - mae: 18.4260 - val_loss: 71.7775 - val_mae: 6.3483\n",
      "Epoch 33/650\n",
      "122/122 [==============================] - 0s 581us/step - loss: 616.2508 - mae: 18.0849 - val_loss: 79.3437 - val_mae: 6.4989\n",
      "Epoch 34/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 591.7273 - mae: 17.6802 - val_loss: 59.2898 - val_mae: 5.8142\n",
      "Epoch 35/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 564.0099 - mae: 17.6665 - val_loss: 57.8053 - val_mae: 5.8294\n",
      "Epoch 36/650\n",
      "122/122 [==============================] - 0s 635us/step - loss: 576.5694 - mae: 17.6535 - val_loss: 49.9185 - val_mae: 5.4799\n",
      "Epoch 37/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 592.6299 - mae: 17.5528 - val_loss: 52.5105 - val_mae: 5.4260\n",
      "Epoch 38/650\n",
      "122/122 [==============================] - 0s 726us/step - loss: 545.5118 - mae: 16.9912 - val_loss: 43.6846 - val_mae: 5.0265\n",
      "Epoch 39/650\n",
      "122/122 [==============================] - 0s 869us/step - loss: 565.8785 - mae: 17.0227 - val_loss: 42.6632 - val_mae: 5.0588\n",
      "Epoch 40/650\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 558.3978 - mae: 16.9488 - val_loss: 65.0919 - val_mae: 6.5629\n",
      "Epoch 41/650\n",
      "122/122 [==============================] - 0s 787us/step - loss: 538.9478 - mae: 16.6204 - val_loss: 41.3842 - val_mae: 4.8963\n",
      "Epoch 42/650\n",
      "122/122 [==============================] - 0s 853us/step - loss: 544.9714 - mae: 16.8353 - val_loss: 47.3801 - val_mae: 5.2732\n",
      "Epoch 43/650\n",
      "122/122 [==============================] - 0s 679us/step - loss: 569.5481 - mae: 17.3095 - val_loss: 38.3487 - val_mae: 4.7145\n",
      "Epoch 44/650\n",
      "122/122 [==============================] - 0s 692us/step - loss: 528.4105 - mae: 16.3651 - val_loss: 37.9410 - val_mae: 4.5222\n",
      "Epoch 45/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 511.1476 - mae: 16.3785 - val_loss: 32.4328 - val_mae: 4.2513\n",
      "Epoch 46/650\n",
      "122/122 [==============================] - 0s 586us/step - loss: 491.7585 - mae: 15.8787 - val_loss: 53.8711 - val_mae: 5.6702\n",
      "Epoch 47/650\n",
      "122/122 [==============================] - 0s 577us/step - loss: 475.7086 - mae: 15.5554 - val_loss: 42.2884 - val_mae: 4.6442\n",
      "Epoch 48/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 507.5359 - mae: 16.0211 - val_loss: 29.9935 - val_mae: 4.1135\n",
      "Epoch 49/650\n",
      "122/122 [==============================] - 0s 577us/step - loss: 495.7851 - mae: 15.8021 - val_loss: 30.3274 - val_mae: 3.9587\n",
      "Epoch 50/650\n",
      "122/122 [==============================] - 0s 643us/step - loss: 499.9015 - mae: 15.7669 - val_loss: 39.1544 - val_mae: 4.9575\n",
      "Epoch 51/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 467.5572 - mae: 15.5679 - val_loss: 38.4342 - val_mae: 4.6065\n",
      "Epoch 52/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 485.0837 - mae: 15.5164 - val_loss: 28.5434 - val_mae: 4.0331\n",
      "Epoch 53/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 493.4427 - mae: 15.7021 - val_loss: 28.0232 - val_mae: 3.8879\n",
      "Epoch 54/650\n",
      "122/122 [==============================] - 0s 578us/step - loss: 501.3860 - mae: 15.7868 - val_loss: 45.5238 - val_mae: 5.3619\n",
      "Epoch 55/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 449.7056 - mae: 14.9952 - val_loss: 27.8067 - val_mae: 3.9462\n",
      "Epoch 56/650\n",
      "122/122 [==============================] - 0s 705us/step - loss: 495.7168 - mae: 15.6311 - val_loss: 29.0335 - val_mae: 3.9765\n",
      "Epoch 57/650\n",
      "122/122 [==============================] - 0s 698us/step - loss: 439.8495 - mae: 15.0431 - val_loss: 43.2765 - val_mae: 5.0652\n",
      "Epoch 58/650\n",
      "122/122 [==============================] - 0s 724us/step - loss: 486.5323 - mae: 15.3959 - val_loss: 26.7256 - val_mae: 3.8395\n",
      "Epoch 59/650\n",
      "122/122 [==============================] - 0s 699us/step - loss: 462.3929 - mae: 15.1188 - val_loss: 26.7205 - val_mae: 3.8448\n",
      "Epoch 60/650\n",
      "122/122 [==============================] - 0s 652us/step - loss: 461.3022 - mae: 15.0872 - val_loss: 28.5091 - val_mae: 3.7936\n",
      "Epoch 61/650\n",
      "122/122 [==============================] - 0s 584us/step - loss: 476.9799 - mae: 15.3121 - val_loss: 27.3377 - val_mae: 3.8468\n",
      "Epoch 62/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 445.4094 - mae: 14.9549 - val_loss: 28.1106 - val_mae: 3.9734\n",
      "Epoch 63/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 451.6412 - mae: 14.8574 - val_loss: 38.4445 - val_mae: 4.4960\n",
      "Epoch 64/650\n",
      "122/122 [==============================] - 0s 572us/step - loss: 450.9720 - mae: 14.9363 - val_loss: 26.5626 - val_mae: 3.6160\n",
      "Epoch 65/650\n",
      "122/122 [==============================] - 0s 578us/step - loss: 466.6905 - mae: 15.0231 - val_loss: 49.3685 - val_mae: 5.3513\n",
      "Epoch 66/650\n",
      "122/122 [==============================] - 0s 694us/step - loss: 466.5575 - mae: 14.9204 - val_loss: 27.0312 - val_mae: 3.6356\n",
      "Epoch 67/650\n",
      "122/122 [==============================] - 0s 579us/step - loss: 413.8894 - mae: 14.1999 - val_loss: 35.6643 - val_mae: 4.6075\n",
      "Epoch 68/650\n",
      "122/122 [==============================] - 0s 578us/step - loss: 469.1152 - mae: 14.9261 - val_loss: 23.1247 - val_mae: 3.4415\n",
      "Epoch 69/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 434.8775 - mae: 14.4649 - val_loss: 22.2407 - val_mae: 3.3378\n",
      "Epoch 70/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 433.6826 - mae: 14.3998 - val_loss: 37.3508 - val_mae: 4.5539\n",
      "Epoch 71/650\n",
      "122/122 [==============================] - 0s 584us/step - loss: 435.8514 - mae: 14.2744 - val_loss: 23.3654 - val_mae: 3.5027\n",
      "Epoch 72/650\n",
      "122/122 [==============================] - 0s 647us/step - loss: 437.0552 - mae: 14.4584 - val_loss: 26.9750 - val_mae: 3.7132\n",
      "Epoch 73/650\n",
      "122/122 [==============================] - 0s 581us/step - loss: 434.1958 - mae: 14.3400 - val_loss: 26.8951 - val_mae: 3.7635\n",
      "Epoch 74/650\n",
      "122/122 [==============================] - 0s 575us/step - loss: 396.1660 - mae: 14.0616 - val_loss: 27.8958 - val_mae: 3.9623\n",
      "Epoch 75/650\n",
      "122/122 [==============================] - 0s 604us/step - loss: 397.3762 - mae: 13.9056 - val_loss: 25.0550 - val_mae: 3.6109\n",
      "Epoch 76/650\n",
      "122/122 [==============================] - 0s 572us/step - loss: 418.3858 - mae: 14.0013 - val_loss: 48.6469 - val_mae: 5.2637\n",
      "Epoch 77/650\n",
      "122/122 [==============================] - 0s 567us/step - loss: 420.8077 - mae: 14.1617 - val_loss: 28.2921 - val_mae: 3.9748\n",
      "Epoch 78/650\n",
      "122/122 [==============================] - 0s 646us/step - loss: 408.9196 - mae: 14.0277 - val_loss: 34.1050 - val_mae: 4.2777\n",
      "Epoch 79/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 413.0932 - mae: 13.9218 - val_loss: 24.9088 - val_mae: 3.5543\n",
      "Epoch 80/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 438.0627 - mae: 14.3089 - val_loss: 27.5667 - val_mae: 3.8937\n",
      "Epoch 81/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 400.4899 - mae: 14.0257 - val_loss: 31.4192 - val_mae: 4.0125\n",
      "Epoch 82/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 428.8135 - mae: 14.1266 - val_loss: 28.0197 - val_mae: 3.6607\n",
      "Epoch 83/650\n",
      "122/122 [==============================] - 0s 684us/step - loss: 410.3396 - mae: 13.9081 - val_loss: 24.4774 - val_mae: 3.5735\n",
      "Epoch 84/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 393.8826 - mae: 13.6900 - val_loss: 29.5544 - val_mae: 3.9368\n",
      "Epoch 85/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 414.5833 - mae: 13.8870 - val_loss: 29.2731 - val_mae: 3.7461\n",
      "Epoch 86/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 381.2985 - mae: 13.3022 - val_loss: 29.1069 - val_mae: 3.9778\n",
      "Epoch 87/650\n",
      "122/122 [==============================] - 0s 568us/step - loss: 389.1538 - mae: 13.5478 - val_loss: 28.4828 - val_mae: 4.0118\n",
      "Epoch 88/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 389.9418 - mae: 13.4996 - val_loss: 25.1881 - val_mae: 3.6080\n",
      "Epoch 89/650\n",
      "122/122 [==============================] - 0s 661us/step - loss: 404.0500 - mae: 13.7926 - val_loss: 35.3614 - val_mae: 4.5998\n",
      "Epoch 90/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 359.6241 - mae: 13.2157 - val_loss: 27.8425 - val_mae: 3.8249\n",
      "Epoch 91/650\n",
      "122/122 [==============================] - 0s 609us/step - loss: 380.4015 - mae: 13.4089 - val_loss: 26.7143 - val_mae: 3.7392\n",
      "Epoch 92/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 364.1325 - mae: 13.0000 - val_loss: 29.2101 - val_mae: 4.0876\n",
      "Epoch 93/650\n",
      "122/122 [==============================] - 0s 630us/step - loss: 375.0842 - mae: 13.1399 - val_loss: 22.1996 - val_mae: 3.3086\n",
      "Epoch 94/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 390.1356 - mae: 13.4299 - val_loss: 26.2049 - val_mae: 3.7694\n",
      "Epoch 95/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 355.9940 - mae: 12.8042 - val_loss: 31.1479 - val_mae: 4.2899\n",
      "Epoch 96/650\n",
      "122/122 [==============================] - 0s 567us/step - loss: 361.0690 - mae: 12.9594 - val_loss: 25.8968 - val_mae: 3.6656\n",
      "Epoch 97/650\n",
      "122/122 [==============================] - 0s 569us/step - loss: 366.2682 - mae: 13.1318 - val_loss: 32.8335 - val_mae: 4.3393\n",
      "Epoch 98/650\n",
      "122/122 [==============================] - 0s 664us/step - loss: 391.3181 - mae: 13.4499 - val_loss: 27.6169 - val_mae: 3.6553\n",
      "Epoch 99/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 354.5020 - mae: 12.8384 - val_loss: 30.9130 - val_mae: 3.9862\n",
      "Epoch 100/650\n",
      "122/122 [==============================] - 0s 589us/step - loss: 364.0597 - mae: 13.0158 - val_loss: 33.3504 - val_mae: 4.3711\n",
      "Epoch 101/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 374.7460 - mae: 12.9758 - val_loss: 23.6464 - val_mae: 3.4269\n",
      "Epoch 102/650\n",
      "122/122 [==============================] - 0s 576us/step - loss: 351.6976 - mae: 12.7237 - val_loss: 29.6678 - val_mae: 3.7833\n",
      "Epoch 103/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 372.6073 - mae: 13.0913 - val_loss: 28.6829 - val_mae: 3.9336\n",
      "Epoch 104/650\n",
      "122/122 [==============================] - 0s 659us/step - loss: 367.2088 - mae: 12.8895 - val_loss: 22.0024 - val_mae: 3.3002\n",
      "Epoch 105/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 339.5482 - mae: 12.6007 - val_loss: 28.1307 - val_mae: 3.9680\n",
      "Epoch 106/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 351.0131 - mae: 12.5277 - val_loss: 37.8123 - val_mae: 4.8604\n",
      "Epoch 107/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 369.0126 - mae: 12.8894 - val_loss: 21.6223 - val_mae: 3.2888\n",
      "Epoch 108/650\n",
      "122/122 [==============================] - 0s 651us/step - loss: 355.5167 - mae: 12.7161 - val_loss: 26.6617 - val_mae: 3.8585\n",
      "Epoch 109/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 326.8770 - mae: 12.3762 - val_loss: 24.7378 - val_mae: 3.5446\n",
      "Epoch 110/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 338.1029 - mae: 12.4153 - val_loss: 25.5837 - val_mae: 3.6935\n",
      "Epoch 111/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 338.7980 - mae: 12.5608 - val_loss: 57.9545 - val_mae: 5.9719\n",
      "Epoch 112/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 323.8535 - mae: 12.1124 - val_loss: 21.7248 - val_mae: 3.2662\n",
      "Epoch 113/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 328.9952 - mae: 12.1677 - val_loss: 42.3068 - val_mae: 5.0003\n",
      "Epoch 114/650\n",
      "122/122 [==============================] - 0s 660us/step - loss: 319.1019 - mae: 12.0871 - val_loss: 28.1000 - val_mae: 3.8894\n",
      "Epoch 115/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 337.4624 - mae: 12.3122 - val_loss: 24.7316 - val_mae: 3.4834\n",
      "Epoch 116/650\n",
      "122/122 [==============================] - 0s 577us/step - loss: 325.1444 - mae: 12.2511 - val_loss: 23.9858 - val_mae: 3.5197\n",
      "Epoch 117/650\n",
      "122/122 [==============================] - 0s 575us/step - loss: 333.1341 - mae: 12.2297 - val_loss: 27.2557 - val_mae: 3.7484\n",
      "Epoch 118/650\n",
      "122/122 [==============================] - 0s 578us/step - loss: 327.6537 - mae: 12.0440 - val_loss: 30.3867 - val_mae: 3.7500\n",
      "Epoch 119/650\n",
      "122/122 [==============================] - 0s 729us/step - loss: 347.9420 - mae: 12.3515 - val_loss: 23.7299 - val_mae: 3.4884\n",
      "Epoch 120/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 338.9760 - mae: 12.2806 - val_loss: 24.4119 - val_mae: 3.5278\n",
      "Epoch 121/650\n",
      "122/122 [==============================] - 0s 573us/step - loss: 329.5464 - mae: 12.1172 - val_loss: 24.9102 - val_mae: 3.5513\n",
      "Epoch 122/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 305.6241 - mae: 11.6997 - val_loss: 22.3338 - val_mae: 3.3496\n",
      "Epoch 123/650\n",
      "122/122 [==============================] - 0s 665us/step - loss: 318.8921 - mae: 12.0160 - val_loss: 28.4565 - val_mae: 4.0052\n",
      "Epoch 124/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 333.4496 - mae: 12.0966 - val_loss: 25.9539 - val_mae: 3.7661\n",
      "Epoch 125/650\n",
      "122/122 [==============================] - 0s 578us/step - loss: 302.0747 - mae: 11.6238 - val_loss: 29.4920 - val_mae: 4.0505\n",
      "Epoch 126/650\n",
      "122/122 [==============================] - 0s 579us/step - loss: 311.3187 - mae: 11.7717 - val_loss: 31.8679 - val_mae: 4.2164\n",
      "Epoch 127/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 321.3376 - mae: 12.0231 - val_loss: 25.0127 - val_mae: 3.6579\n",
      "Epoch 128/650\n",
      "122/122 [==============================] - 0s 638us/step - loss: 294.0767 - mae: 11.4929 - val_loss: 24.3737 - val_mae: 3.5153\n",
      "Epoch 129/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 319.5025 - mae: 11.8225 - val_loss: 29.6423 - val_mae: 4.0527\n",
      "Epoch 130/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 314.4565 - mae: 11.5940 - val_loss: 26.2809 - val_mae: 3.7758\n",
      "Epoch 131/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 324.4011 - mae: 11.8844 - val_loss: 24.1391 - val_mae: 3.5226\n",
      "Epoch 132/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 321.5539 - mae: 11.8535 - val_loss: 28.7742 - val_mae: 3.9150\n",
      "Epoch 133/650\n",
      "122/122 [==============================] - 0s 647us/step - loss: 300.2891 - mae: 11.6795 - val_loss: 30.7869 - val_mae: 4.1980\n",
      "Epoch 134/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 316.7395 - mae: 11.7816 - val_loss: 26.7118 - val_mae: 3.8343\n",
      "Epoch 135/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 290.5969 - mae: 11.5596 - val_loss: 23.2175 - val_mae: 3.4501\n",
      "Epoch 136/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 302.3629 - mae: 11.5647 - val_loss: 22.3907 - val_mae: 3.3282\n",
      "Epoch 137/650\n",
      "122/122 [==============================] - 0s 575us/step - loss: 285.3667 - mae: 11.2581 - val_loss: 24.6748 - val_mae: 3.6127\n",
      "Epoch 138/650\n",
      "122/122 [==============================] - 0s 618us/step - loss: 289.9474 - mae: 11.3659 - val_loss: 28.6555 - val_mae: 4.0611\n",
      "Epoch 139/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 277.3428 - mae: 11.0936 - val_loss: 22.6291 - val_mae: 3.3673\n",
      "Epoch 140/650\n",
      "122/122 [==============================] - 0s 576us/step - loss: 278.8928 - mae: 11.1574 - val_loss: 23.5897 - val_mae: 3.4929\n",
      "Epoch 141/650\n",
      "122/122 [==============================] - 0s 589us/step - loss: 277.1452 - mae: 11.1204 - val_loss: 36.9875 - val_mae: 4.5753\n",
      "Epoch 142/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 282.6237 - mae: 11.2039 - val_loss: 22.6512 - val_mae: 3.3773\n",
      "Epoch 143/650\n",
      "122/122 [==============================] - 0s 665us/step - loss: 284.0199 - mae: 11.1822 - val_loss: 31.2936 - val_mae: 4.2635\n",
      "Epoch 144/650\n",
      "122/122 [==============================] - 0s 654us/step - loss: 292.6867 - mae: 11.0476 - val_loss: 44.6845 - val_mae: 5.0808\n",
      "Epoch 145/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 290.2942 - mae: 11.2687 - val_loss: 23.8103 - val_mae: 3.4615\n",
      "Epoch 146/650\n",
      "122/122 [==============================] - 0s 577us/step - loss: 299.8581 - mae: 11.3754 - val_loss: 31.5235 - val_mae: 4.2556\n",
      "Epoch 147/650\n",
      "122/122 [==============================] - 0s 655us/step - loss: 279.0602 - mae: 10.9001 - val_loss: 23.9357 - val_mae: 3.5077\n",
      "Epoch 148/650\n",
      "122/122 [==============================] - 0s 634us/step - loss: 271.2632 - mae: 10.9273 - val_loss: 23.6393 - val_mae: 3.4551\n",
      "Epoch 149/650\n",
      "122/122 [==============================] - 0s 636us/step - loss: 273.2759 - mae: 11.0074 - val_loss: 23.9308 - val_mae: 3.5583\n",
      "Epoch 150/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 297.8677 - mae: 11.2388 - val_loss: 26.9129 - val_mae: 3.6535\n",
      "Epoch 151/650\n",
      "122/122 [==============================] - 0s 690us/step - loss: 264.7235 - mae: 10.7548 - val_loss: 22.7815 - val_mae: 3.4190\n",
      "Epoch 152/650\n",
      "122/122 [==============================] - 0s 586us/step - loss: 267.0269 - mae: 10.6825 - val_loss: 29.5509 - val_mae: 4.1393\n",
      "Epoch 153/650\n",
      "122/122 [==============================] - 0s 586us/step - loss: 268.1983 - mae: 10.8259 - val_loss: 22.0783 - val_mae: 3.3566\n",
      "Epoch 154/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 284.5183 - mae: 10.9889 - val_loss: 31.8913 - val_mae: 4.2986\n",
      "Epoch 155/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 271.7087 - mae: 10.8400 - val_loss: 23.3281 - val_mae: 3.5034\n",
      "Epoch 156/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 265.6521 - mae: 10.6902 - val_loss: 22.6262 - val_mae: 3.4236\n",
      "Epoch 157/650\n",
      "122/122 [==============================] - 0s 639us/step - loss: 240.7575 - mae: 10.2410 - val_loss: 22.4338 - val_mae: 3.3857\n",
      "Epoch 158/650\n",
      "122/122 [==============================] - 0s 604us/step - loss: 261.1539 - mae: 10.7225 - val_loss: 28.1635 - val_mae: 3.9864\n",
      "Epoch 159/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 242.8432 - mae: 10.2099 - val_loss: 23.3002 - val_mae: 3.4761\n",
      "Epoch 160/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 271.6437 - mae: 10.7461 - val_loss: 23.9110 - val_mae: 3.5190\n",
      "Epoch 161/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 256.2649 - mae: 10.4568 - val_loss: 32.2759 - val_mae: 4.2752\n",
      "Epoch 162/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 258.9004 - mae: 10.3871 - val_loss: 34.8056 - val_mae: 4.5065\n",
      "Epoch 163/650\n",
      "122/122 [==============================] - 0s 648us/step - loss: 233.4370 - mae: 10.0782 - val_loss: 30.4587 - val_mae: 4.1385\n",
      "Epoch 164/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 242.9743 - mae: 10.3191 - val_loss: 23.5361 - val_mae: 3.4772\n",
      "Epoch 165/650\n",
      "122/122 [==============================] - 0s 579us/step - loss: 262.6351 - mae: 10.6161 - val_loss: 25.8761 - val_mae: 3.6319\n",
      "Epoch 166/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 237.1137 - mae: 10.3089 - val_loss: 22.1500 - val_mae: 3.3364\n",
      "Epoch 167/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 256.3898 - mae: 10.3111 - val_loss: 26.0032 - val_mae: 3.7487\n",
      "Epoch 168/650\n",
      "122/122 [==============================] - 0s 706us/step - loss: 221.7975 - mae: 9.8505 - val_loss: 33.4732 - val_mae: 4.4388\n",
      "Epoch 169/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 240.0882 - mae: 10.0916 - val_loss: 23.2746 - val_mae: 3.4867\n",
      "Epoch 170/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 244.8067 - mae: 10.1599 - val_loss: 27.9856 - val_mae: 3.9259\n",
      "Epoch 171/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 227.8373 - mae: 9.9410 - val_loss: 24.2045 - val_mae: 3.5985\n",
      "Epoch 172/650\n",
      "122/122 [==============================] - 0s 721us/step - loss: 240.3855 - mae: 10.0897 - val_loss: 25.5266 - val_mae: 3.7245\n",
      "Epoch 173/650\n",
      "122/122 [==============================] - 0s 782us/step - loss: 239.2174 - mae: 10.0658 - val_loss: 25.2160 - val_mae: 3.5624\n",
      "Epoch 174/650\n",
      "122/122 [==============================] - 0s 840us/step - loss: 225.3494 - mae: 9.9768 - val_loss: 27.0904 - val_mae: 3.8277\n",
      "Epoch 175/650\n",
      "122/122 [==============================] - 0s 736us/step - loss: 222.0936 - mae: 9.7996 - val_loss: 24.1409 - val_mae: 3.5555\n",
      "Epoch 176/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 241.6853 - mae: 9.8388 - val_loss: 31.7385 - val_mae: 4.3346\n",
      "Epoch 177/650\n",
      "122/122 [==============================] - 0s 639us/step - loss: 210.7788 - mae: 9.5945 - val_loss: 26.6021 - val_mae: 3.8237\n",
      "Epoch 178/650\n",
      "122/122 [==============================] - 0s 645us/step - loss: 231.0085 - mae: 9.7733 - val_loss: 25.0580 - val_mae: 3.5681\n",
      "Epoch 179/650\n",
      "122/122 [==============================] - 0s 647us/step - loss: 222.3482 - mae: 9.9057 - val_loss: 37.9614 - val_mae: 4.7552\n",
      "Epoch 180/650\n",
      "122/122 [==============================] - 0s 679us/step - loss: 223.4313 - mae: 9.7487 - val_loss: 27.5713 - val_mae: 3.9099\n",
      "Epoch 181/650\n",
      "122/122 [==============================] - 0s 663us/step - loss: 208.2800 - mae: 9.5954 - val_loss: 24.0398 - val_mae: 3.5000\n",
      "Epoch 182/650\n",
      "122/122 [==============================] - 0s 731us/step - loss: 222.9262 - mae: 9.7582 - val_loss: 25.0950 - val_mae: 3.5908\n",
      "Epoch 183/650\n",
      "122/122 [==============================] - 0s 611us/step - loss: 220.3527 - mae: 9.6050 - val_loss: 27.3836 - val_mae: 3.9538\n",
      "Epoch 184/650\n",
      "122/122 [==============================] - 0s 884us/step - loss: 222.4341 - mae: 9.6652 - val_loss: 28.0580 - val_mae: 3.9278\n",
      "Epoch 185/650\n",
      "122/122 [==============================] - 0s 766us/step - loss: 210.3473 - mae: 9.5863 - val_loss: 24.5947 - val_mae: 3.6429\n",
      "Epoch 186/650\n",
      "122/122 [==============================] - 0s 710us/step - loss: 217.8647 - mae: 9.6487 - val_loss: 25.0063 - val_mae: 3.6388\n",
      "Epoch 187/650\n",
      "122/122 [==============================] - 0s 666us/step - loss: 209.8831 - mae: 9.4482 - val_loss: 27.2594 - val_mae: 3.7879\n",
      "Epoch 188/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 213.8993 - mae: 9.6026 - val_loss: 29.4844 - val_mae: 4.0890\n",
      "Epoch 189/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 201.6930 - mae: 9.3411 - val_loss: 23.1200 - val_mae: 3.4431\n",
      "Epoch 190/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 204.8933 - mae: 9.3912 - val_loss: 33.6199 - val_mae: 4.3865\n",
      "Epoch 191/650\n",
      "122/122 [==============================] - 0s 656us/step - loss: 219.3924 - mae: 9.6527 - val_loss: 22.8423 - val_mae: 3.4297\n",
      "Epoch 192/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 197.4070 - mae: 9.0495 - val_loss: 28.3590 - val_mae: 4.0242\n",
      "Epoch 193/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 193.3320 - mae: 9.0905 - val_loss: 24.4833 - val_mae: 3.5418\n",
      "Epoch 194/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 209.2823 - mae: 9.4460 - val_loss: 31.1426 - val_mae: 4.2393\n",
      "Epoch 195/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 188.4502 - mae: 8.9563 - val_loss: 24.2473 - val_mae: 3.5008\n",
      "Epoch 196/650\n",
      "122/122 [==============================] - 0s 819us/step - loss: 200.5235 - mae: 9.1748 - val_loss: 28.5732 - val_mae: 4.0380\n",
      "Epoch 197/650\n",
      "122/122 [==============================] - 0s 641us/step - loss: 206.3389 - mae: 9.3850 - val_loss: 24.3953 - val_mae: 3.5656\n",
      "Epoch 198/650\n",
      "122/122 [==============================] - 0s 628us/step - loss: 195.7488 - mae: 9.0588 - val_loss: 33.2892 - val_mae: 4.4284\n",
      "Epoch 199/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 189.6129 - mae: 9.0819 - val_loss: 34.3392 - val_mae: 4.4333\n",
      "Epoch 200/650\n",
      "122/122 [==============================] - 0s 614us/step - loss: 192.8929 - mae: 9.0255 - val_loss: 26.8697 - val_mae: 3.7853\n",
      "Epoch 201/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 197.7603 - mae: 9.0712 - val_loss: 24.8132 - val_mae: 3.7476\n",
      "Epoch 202/650\n",
      "122/122 [==============================] - 0s 644us/step - loss: 210.7389 - mae: 9.2433 - val_loss: 26.8069 - val_mae: 3.8765\n",
      "Epoch 203/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 192.4495 - mae: 8.9207 - val_loss: 22.9805 - val_mae: 3.4504\n",
      "Epoch 204/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 199.5515 - mae: 9.2189 - val_loss: 25.6398 - val_mae: 3.7408\n",
      "Epoch 205/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 194.7862 - mae: 9.0575 - val_loss: 24.2593 - val_mae: 3.5808\n",
      "Epoch 206/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 196.7417 - mae: 9.1449 - val_loss: 25.2512 - val_mae: 3.7167\n",
      "Epoch 207/650\n",
      "122/122 [==============================] - 0s 584us/step - loss: 186.5456 - mae: 8.8299 - val_loss: 24.0226 - val_mae: 3.5315\n",
      "Epoch 208/650\n",
      "122/122 [==============================] - 0s 653us/step - loss: 191.6921 - mae: 8.9950 - val_loss: 24.7491 - val_mae: 3.5886\n",
      "Epoch 209/650\n",
      "122/122 [==============================] - 0s 622us/step - loss: 207.4082 - mae: 9.2544 - val_loss: 40.5263 - val_mae: 4.8271\n",
      "Epoch 210/650\n",
      "122/122 [==============================] - 0s 624us/step - loss: 189.3539 - mae: 8.8942 - val_loss: 42.0557 - val_mae: 5.0755\n",
      "Epoch 211/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 187.2173 - mae: 8.9300 - val_loss: 25.6318 - val_mae: 3.6311\n",
      "Epoch 212/650\n",
      "122/122 [==============================] - 0s 630us/step - loss: 181.8891 - mae: 8.8191 - val_loss: 25.4134 - val_mae: 3.6470\n",
      "Epoch 213/650\n",
      "122/122 [==============================] - 0s 686us/step - loss: 189.7836 - mae: 9.0941 - val_loss: 38.5527 - val_mae: 4.6433\n",
      "Epoch 214/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 184.4558 - mae: 8.8800 - val_loss: 23.3116 - val_mae: 3.4809\n",
      "Epoch 215/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 166.5944 - mae: 8.5280 - val_loss: 24.2598 - val_mae: 3.4707\n",
      "Epoch 216/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 166.5700 - mae: 8.5221 - val_loss: 24.8040 - val_mae: 3.5498\n",
      "Epoch 217/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 166.2076 - mae: 8.5228 - val_loss: 26.4833 - val_mae: 3.8145\n",
      "Epoch 218/650\n",
      "122/122 [==============================] - 0s 643us/step - loss: 185.7333 - mae: 8.9306 - val_loss: 33.1126 - val_mae: 4.3010\n",
      "Epoch 219/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 162.2074 - mae: 8.2855 - val_loss: 25.2121 - val_mae: 3.5516\n",
      "Epoch 220/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 183.9598 - mae: 8.6991 - val_loss: 25.1442 - val_mae: 3.6453\n",
      "Epoch 221/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 171.1072 - mae: 8.5438 - val_loss: 24.0994 - val_mae: 3.5638\n",
      "Epoch 222/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 176.1017 - mae: 8.6349 - val_loss: 24.5582 - val_mae: 3.5625\n",
      "Epoch 223/650\n",
      "122/122 [==============================] - 0s 708us/step - loss: 165.8775 - mae: 8.2907 - val_loss: 25.4555 - val_mae: 3.6391\n",
      "Epoch 224/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 170.1171 - mae: 8.5419 - val_loss: 38.3681 - val_mae: 4.6755\n",
      "Epoch 225/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 160.6010 - mae: 8.3407 - val_loss: 33.6852 - val_mae: 4.3951\n",
      "Epoch 226/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 156.2191 - mae: 8.3443 - val_loss: 25.6986 - val_mae: 3.5985\n",
      "Epoch 227/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 164.2079 - mae: 8.4459 - val_loss: 26.7807 - val_mae: 3.6981\n",
      "Epoch 228/650\n",
      "122/122 [==============================] - 0s 671us/step - loss: 160.5659 - mae: 8.4331 - val_loss: 24.1307 - val_mae: 3.5362\n",
      "Epoch 229/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 172.6400 - mae: 8.5244 - val_loss: 30.5273 - val_mae: 4.0975\n",
      "Epoch 230/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 172.3888 - mae: 8.5427 - val_loss: 27.2111 - val_mae: 3.9191\n",
      "Epoch 231/650\n",
      "122/122 [==============================] - 0s 618us/step - loss: 153.7435 - mae: 8.2068 - val_loss: 27.1656 - val_mae: 3.7887\n",
      "Epoch 232/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 165.5037 - mae: 8.3414 - val_loss: 27.5777 - val_mae: 3.6939\n",
      "Epoch 233/650\n",
      "122/122 [==============================] - 0s 670us/step - loss: 151.5182 - mae: 8.2078 - val_loss: 25.2238 - val_mae: 3.5644\n",
      "Epoch 234/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 155.8098 - mae: 8.1963 - val_loss: 29.2323 - val_mae: 4.1205\n",
      "Epoch 235/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 157.0179 - mae: 8.2976 - val_loss: 26.1206 - val_mae: 3.6942\n",
      "Epoch 236/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 157.8831 - mae: 8.3546 - val_loss: 27.6395 - val_mae: 3.8025\n",
      "Epoch 237/650\n",
      "122/122 [==============================] - 0s 631us/step - loss: 157.7126 - mae: 8.1322 - val_loss: 27.3040 - val_mae: 3.7341\n",
      "Epoch 238/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 148.5938 - mae: 8.1419 - val_loss: 41.7654 - val_mae: 4.9149\n",
      "Epoch 239/650\n",
      "122/122 [==============================] - 0s 631us/step - loss: 161.2326 - mae: 8.3054 - val_loss: 32.9982 - val_mae: 4.3711\n",
      "Epoch 240/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 154.6062 - mae: 8.1451 - val_loss: 35.7313 - val_mae: 4.5436\n",
      "Epoch 241/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 155.5576 - mae: 8.1917 - val_loss: 29.6641 - val_mae: 3.9964\n",
      "Epoch 242/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 150.9541 - mae: 8.1479 - val_loss: 34.4100 - val_mae: 4.4632\n",
      "Epoch 243/650\n",
      "122/122 [==============================] - 0s 650us/step - loss: 140.0387 - mae: 7.8154 - val_loss: 28.0303 - val_mae: 3.8764\n",
      "Epoch 244/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 140.9336 - mae: 7.9231 - val_loss: 26.9758 - val_mae: 3.7280\n",
      "Epoch 245/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 148.6160 - mae: 8.1504 - val_loss: 42.7158 - val_mae: 4.9540\n",
      "Epoch 246/650\n",
      "122/122 [==============================] - 0s 582us/step - loss: 135.3435 - mae: 7.7486 - val_loss: 27.4485 - val_mae: 3.8113\n",
      "Epoch 247/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 129.2647 - mae: 7.5730 - val_loss: 29.3446 - val_mae: 3.9086\n",
      "Epoch 248/650\n",
      "122/122 [==============================] - 0s 683us/step - loss: 153.9317 - mae: 8.1667 - val_loss: 44.4121 - val_mae: 5.0503\n",
      "Epoch 249/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 135.4277 - mae: 7.7036 - val_loss: 29.3840 - val_mae: 3.9895\n",
      "Epoch 250/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 141.1127 - mae: 7.9522 - val_loss: 29.2863 - val_mae: 4.0834\n",
      "Epoch 251/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 144.1395 - mae: 7.8771 - val_loss: 29.2356 - val_mae: 3.8502\n",
      "Epoch 252/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 135.4789 - mae: 7.7546 - val_loss: 28.6939 - val_mae: 3.8446\n",
      "Epoch 253/650\n",
      "122/122 [==============================] - 0s 643us/step - loss: 134.2237 - mae: 7.7188 - val_loss: 34.3351 - val_mae: 4.3584\n",
      "Epoch 254/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 138.7302 - mae: 7.7536 - val_loss: 30.6946 - val_mae: 4.0486\n",
      "Epoch 255/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 134.3185 - mae: 7.6748 - val_loss: 25.6530 - val_mae: 3.6654\n",
      "Epoch 256/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 149.5853 - mae: 7.9940 - val_loss: 33.5416 - val_mae: 4.3306\n",
      "Epoch 257/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 130.6435 - mae: 7.6754 - val_loss: 31.3811 - val_mae: 4.1572\n",
      "Epoch 258/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 141.0688 - mae: 7.8368 - val_loss: 47.2254 - val_mae: 5.1394\n",
      "Epoch 259/650\n",
      "122/122 [==============================] - 0s 673us/step - loss: 136.4262 - mae: 7.8803 - val_loss: 48.4374 - val_mae: 5.2227\n",
      "Epoch 260/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 140.0828 - mae: 7.7919 - val_loss: 26.8177 - val_mae: 3.7122\n",
      "Epoch 261/650\n",
      "122/122 [==============================] - 0s 580us/step - loss: 129.9848 - mae: 7.6054 - val_loss: 26.6537 - val_mae: 3.8330\n",
      "Epoch 262/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 132.9150 - mae: 7.7581 - val_loss: 30.6603 - val_mae: 4.1769\n",
      "Epoch 263/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 140.9000 - mae: 7.7793 - val_loss: 31.7475 - val_mae: 4.1260\n",
      "Epoch 264/650\n",
      "122/122 [==============================] - 0s 647us/step - loss: 141.9400 - mae: 7.8309 - val_loss: 29.6087 - val_mae: 4.0502\n",
      "Epoch 265/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 129.4082 - mae: 7.5192 - val_loss: 25.2079 - val_mae: 3.5379\n",
      "Epoch 266/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 130.1419 - mae: 7.6589 - val_loss: 28.6593 - val_mae: 3.8681\n",
      "Epoch 267/650\n",
      "122/122 [==============================] - 0s 681us/step - loss: 127.1285 - mae: 7.5495 - val_loss: 26.5800 - val_mae: 3.6794\n",
      "Epoch 268/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 127.4687 - mae: 7.4575 - val_loss: 26.3418 - val_mae: 3.6761\n",
      "Epoch 269/650\n",
      "122/122 [==============================] - 0s 727us/step - loss: 129.1041 - mae: 7.4348 - val_loss: 27.2424 - val_mae: 3.8096\n",
      "Epoch 270/650\n",
      "122/122 [==============================] - 0s 604us/step - loss: 129.7389 - mae: 7.6222 - val_loss: 28.1076 - val_mae: 3.8627\n",
      "Epoch 271/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 124.9354 - mae: 7.5089 - val_loss: 31.8663 - val_mae: 4.2289\n",
      "Epoch 272/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 129.4271 - mae: 7.5421 - val_loss: 29.5301 - val_mae: 3.9570\n",
      "Epoch 273/650\n",
      "122/122 [==============================] - 0s 622us/step - loss: 119.4603 - mae: 7.2482 - val_loss: 33.5426 - val_mae: 4.3171\n",
      "Epoch 274/650\n",
      "122/122 [==============================] - 0s 662us/step - loss: 122.0993 - mae: 7.4655 - val_loss: 27.7050 - val_mae: 3.8397\n",
      "Epoch 275/650\n",
      "122/122 [==============================] - 0s 609us/step - loss: 124.6562 - mae: 7.4640 - val_loss: 30.7617 - val_mae: 4.1133\n",
      "Epoch 276/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 114.0689 - mae: 7.3736 - val_loss: 30.9636 - val_mae: 3.9892\n",
      "Epoch 277/650\n",
      "122/122 [==============================] - 0s 638us/step - loss: 127.9330 - mae: 7.5148 - val_loss: 34.0503 - val_mae: 4.3963\n",
      "Epoch 278/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 125.3485 - mae: 7.5264 - val_loss: 32.6930 - val_mae: 4.3580\n",
      "Epoch 279/650\n",
      "122/122 [==============================] - 0s 694us/step - loss: 118.6727 - mae: 7.4136 - val_loss: 30.5508 - val_mae: 4.0077\n",
      "Epoch 280/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 122.8474 - mae: 7.4135 - val_loss: 31.0613 - val_mae: 4.2523\n",
      "Epoch 281/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 121.4706 - mae: 7.4405 - val_loss: 29.8458 - val_mae: 4.1083\n",
      "Epoch 282/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 124.6881 - mae: 7.4149 - val_loss: 27.7097 - val_mae: 3.7757\n",
      "Epoch 283/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 130.3789 - mae: 7.6183 - val_loss: 29.9542 - val_mae: 3.9202\n",
      "Epoch 284/650\n",
      "122/122 [==============================] - 0s 702us/step - loss: 117.7785 - mae: 7.3411 - val_loss: 30.1754 - val_mae: 4.1345\n",
      "Epoch 285/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 120.9708 - mae: 7.3164 - val_loss: 46.1339 - val_mae: 5.1100\n",
      "Epoch 286/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 117.6613 - mae: 7.3391 - val_loss: 36.0033 - val_mae: 4.5524\n",
      "Epoch 287/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 114.6651 - mae: 7.2167 - val_loss: 29.0679 - val_mae: 3.8390\n",
      "Epoch 288/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 120.7884 - mae: 7.2920 - val_loss: 29.2883 - val_mae: 3.9411\n",
      "Epoch 289/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 113.5807 - mae: 7.1929 - val_loss: 28.3394 - val_mae: 3.9160\n",
      "Epoch 290/650\n",
      "122/122 [==============================] - 0s 635us/step - loss: 112.2529 - mae: 7.2216 - val_loss: 27.1956 - val_mae: 3.7647\n",
      "Epoch 291/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 112.2684 - mae: 7.2068 - val_loss: 33.3266 - val_mae: 4.4736\n",
      "Epoch 292/650\n",
      "122/122 [==============================] - 0s 618us/step - loss: 111.7298 - mae: 7.1140 - val_loss: 26.4022 - val_mae: 3.7259\n",
      "Epoch 293/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 112.3914 - mae: 7.1098 - val_loss: 33.2501 - val_mae: 4.3042\n",
      "Epoch 294/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 122.2629 - mae: 7.4280 - val_loss: 30.1279 - val_mae: 4.0916\n",
      "Epoch 295/650\n",
      "122/122 [==============================] - 0s 634us/step - loss: 113.2491 - mae: 7.1982 - val_loss: 44.4477 - val_mae: 5.0668\n",
      "Epoch 296/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 109.8164 - mae: 7.0226 - val_loss: 39.8381 - val_mae: 4.7731\n",
      "Epoch 297/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 114.2485 - mae: 7.3232 - val_loss: 45.7680 - val_mae: 5.0666\n",
      "Epoch 298/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 113.8893 - mae: 7.2244 - val_loss: 35.1483 - val_mae: 4.4477\n",
      "Epoch 299/650\n",
      "122/122 [==============================] - 0s 604us/step - loss: 113.6759 - mae: 7.2612 - val_loss: 27.4375 - val_mae: 3.8408\n",
      "Epoch 300/650\n",
      "122/122 [==============================] - 0s 651us/step - loss: 112.2461 - mae: 7.0559 - val_loss: 30.8699 - val_mae: 4.0796\n",
      "Epoch 301/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 106.4179 - mae: 7.0044 - val_loss: 30.5884 - val_mae: 3.9688\n",
      "Epoch 302/650\n",
      "122/122 [==============================] - 0s 645us/step - loss: 108.1893 - mae: 7.0260 - val_loss: 28.8442 - val_mae: 4.0325\n",
      "Epoch 303/650\n",
      "122/122 [==============================] - 0s 612us/step - loss: 108.6643 - mae: 7.0108 - val_loss: 31.1593 - val_mae: 4.1591\n",
      "Epoch 304/650\n",
      "122/122 [==============================] - 0s 671us/step - loss: 101.7155 - mae: 6.9193 - val_loss: 29.4408 - val_mae: 4.0552\n",
      "Epoch 305/650\n",
      "122/122 [==============================] - 0s 631us/step - loss: 103.3232 - mae: 6.9996 - val_loss: 28.4777 - val_mae: 3.7406\n",
      "Epoch 306/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 106.7430 - mae: 7.0275 - val_loss: 28.3242 - val_mae: 3.7702\n",
      "Epoch 307/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 119.5137 - mae: 7.2400 - val_loss: 32.4939 - val_mae: 4.1471\n",
      "Epoch 308/650\n",
      "122/122 [==============================] - 0s 594us/step - loss: 112.6227 - mae: 7.1021 - val_loss: 28.9005 - val_mae: 4.0034\n",
      "Epoch 309/650\n",
      "122/122 [==============================] - 0s 665us/step - loss: 110.9932 - mae: 7.0757 - val_loss: 27.6883 - val_mae: 3.7835\n",
      "Epoch 310/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 103.6643 - mae: 6.9754 - val_loss: 30.7610 - val_mae: 4.0841\n",
      "Epoch 311/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 112.7276 - mae: 7.0981 - val_loss: 28.9492 - val_mae: 3.9587\n",
      "Epoch 312/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 103.1769 - mae: 6.8742 - val_loss: 29.6534 - val_mae: 3.9370\n",
      "Epoch 313/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 102.0589 - mae: 6.8891 - val_loss: 30.7840 - val_mae: 4.0575\n",
      "Epoch 314/650\n",
      "122/122 [==============================] - 0s 637us/step - loss: 105.9717 - mae: 7.0701 - val_loss: 28.6209 - val_mae: 3.8489\n",
      "Epoch 315/650\n",
      "122/122 [==============================] - 0s 612us/step - loss: 108.7931 - mae: 7.0717 - val_loss: 31.2831 - val_mae: 4.0037\n",
      "Epoch 316/650\n",
      "122/122 [==============================] - 0s 611us/step - loss: 107.1346 - mae: 6.9773 - val_loss: 27.3932 - val_mae: 3.6904\n",
      "Epoch 317/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 101.1537 - mae: 6.9498 - val_loss: 25.7648 - val_mae: 3.5384\n",
      "Epoch 318/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 98.3397 - mae: 6.8026 - val_loss: 30.4512 - val_mae: 4.0658\n",
      "Epoch 319/650\n",
      "122/122 [==============================] - 0s 670us/step - loss: 107.8532 - mae: 7.0389 - val_loss: 31.6965 - val_mae: 4.1554\n",
      "Epoch 320/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 101.5799 - mae: 6.8183 - val_loss: 35.7625 - val_mae: 4.4177\n",
      "Epoch 321/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 106.0615 - mae: 6.9600 - val_loss: 30.3029 - val_mae: 3.8967\n",
      "Epoch 322/650\n",
      "122/122 [==============================] - 0s 622us/step - loss: 98.8081 - mae: 6.8144 - val_loss: 31.2124 - val_mae: 3.9110\n",
      "Epoch 323/650\n",
      "122/122 [==============================] - 0s 646us/step - loss: 104.5730 - mae: 6.8750 - val_loss: 37.2444 - val_mae: 4.4743\n",
      "Epoch 324/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 95.4222 - mae: 6.6624 - val_loss: 29.4864 - val_mae: 3.9179\n",
      "Epoch 325/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 103.5238 - mae: 6.8560 - val_loss: 49.8406 - val_mae: 5.2933\n",
      "Epoch 326/650\n",
      "122/122 [==============================] - 0s 589us/step - loss: 97.9164 - mae: 6.7529 - val_loss: 26.6602 - val_mae: 3.7058\n",
      "Epoch 327/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 93.8034 - mae: 6.6304 - val_loss: 30.2281 - val_mae: 3.9628\n",
      "Epoch 328/650\n",
      "122/122 [==============================] - 0s 658us/step - loss: 112.9105 - mae: 7.0295 - val_loss: 29.5378 - val_mae: 3.8329\n",
      "Epoch 329/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 104.0712 - mae: 7.0464 - val_loss: 30.7007 - val_mae: 3.8990\n",
      "Epoch 330/650\n",
      "122/122 [==============================] - 0s 713us/step - loss: 99.1048 - mae: 6.8283 - val_loss: 34.1435 - val_mae: 4.3606\n",
      "Epoch 331/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 103.6565 - mae: 6.9555 - val_loss: 30.2496 - val_mae: 3.9537\n",
      "Epoch 332/650\n",
      "122/122 [==============================] - 0s 724us/step - loss: 98.6288 - mae: 6.7974 - val_loss: 32.5717 - val_mae: 4.1730\n",
      "Epoch 333/650\n",
      "122/122 [==============================] - 0s 673us/step - loss: 107.0006 - mae: 6.9620 - val_loss: 37.8981 - val_mae: 4.6743\n",
      "Epoch 334/650\n",
      "122/122 [==============================] - 0s 618us/step - loss: 99.6849 - mae: 6.8785 - val_loss: 27.5426 - val_mae: 3.7401\n",
      "Epoch 335/650\n",
      "122/122 [==============================] - 0s 594us/step - loss: 100.1593 - mae: 6.6940 - val_loss: 28.3377 - val_mae: 3.7750\n",
      "Epoch 336/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 99.7794 - mae: 6.8136 - val_loss: 28.1687 - val_mae: 3.7500\n",
      "Epoch 337/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 93.0610 - mae: 6.6636 - val_loss: 33.0871 - val_mae: 4.0450\n",
      "Epoch 338/650\n",
      "122/122 [==============================] - 0s 649us/step - loss: 98.9863 - mae: 6.7696 - val_loss: 37.4974 - val_mae: 4.5816\n",
      "Epoch 339/650\n",
      "122/122 [==============================] - 0s 636us/step - loss: 94.1647 - mae: 6.5404 - val_loss: 29.0333 - val_mae: 3.9684\n",
      "Epoch 340/650\n",
      "122/122 [==============================] - 0s 604us/step - loss: 93.9551 - mae: 6.6688 - val_loss: 26.5668 - val_mae: 3.6241\n",
      "Epoch 341/650\n",
      "122/122 [==============================] - 0s 625us/step - loss: 90.5237 - mae: 6.5648 - val_loss: 33.1413 - val_mae: 4.2611\n",
      "Epoch 342/650\n",
      "122/122 [==============================] - 0s 640us/step - loss: 92.5573 - mae: 6.6360 - val_loss: 26.7099 - val_mae: 3.6718\n",
      "Epoch 343/650\n",
      "122/122 [==============================] - 0s 614us/step - loss: 92.0642 - mae: 6.5729 - val_loss: 58.1412 - val_mae: 5.7941\n",
      "Epoch 344/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 92.0164 - mae: 6.5560 - val_loss: 32.3115 - val_mae: 4.2420\n",
      "Epoch 345/650\n",
      "122/122 [==============================] - 0s 609us/step - loss: 102.8417 - mae: 6.8656 - val_loss: 35.8223 - val_mae: 4.4879\n",
      "Epoch 346/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 91.8438 - mae: 6.6425 - val_loss: 27.3537 - val_mae: 3.7803\n",
      "Epoch 347/650\n",
      "122/122 [==============================] - 0s 659us/step - loss: 84.4515 - mae: 6.3655 - val_loss: 29.9345 - val_mae: 4.0206\n",
      "Epoch 348/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 86.1627 - mae: 6.5048 - val_loss: 27.8444 - val_mae: 3.8040\n",
      "Epoch 349/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 95.6375 - mae: 6.7288 - val_loss: 34.5579 - val_mae: 4.2722\n",
      "Epoch 350/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 93.8837 - mae: 6.5698 - val_loss: 28.5277 - val_mae: 3.8571\n",
      "Epoch 351/650\n",
      "122/122 [==============================] - 0s 655us/step - loss: 95.7217 - mae: 6.6509 - val_loss: 27.5237 - val_mae: 3.7376\n",
      "Epoch 352/650\n",
      "122/122 [==============================] - 0s 658us/step - loss: 95.7134 - mae: 6.5443 - val_loss: 44.6565 - val_mae: 4.8674\n",
      "Epoch 353/650\n",
      "122/122 [==============================] - 0s 612us/step - loss: 87.6045 - mae: 6.4379 - val_loss: 36.6974 - val_mae: 4.5154\n",
      "Epoch 354/650\n",
      "122/122 [==============================] - 0s 587us/step - loss: 92.7815 - mae: 6.4812 - val_loss: 30.0658 - val_mae: 4.0199\n",
      "Epoch 355/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 94.3632 - mae: 6.5380 - val_loss: 36.0259 - val_mae: 4.4427\n",
      "Epoch 356/650\n",
      "122/122 [==============================] - 0s 667us/step - loss: 84.6142 - mae: 6.4062 - val_loss: 30.8506 - val_mae: 3.8148\n",
      "Epoch 357/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 89.8468 - mae: 6.4308 - val_loss: 32.1723 - val_mae: 4.1284\n",
      "Epoch 358/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 92.6547 - mae: 6.5874 - val_loss: 34.2975 - val_mae: 4.3194\n",
      "Epoch 359/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 92.7898 - mae: 6.6690 - val_loss: 27.3804 - val_mae: 3.6577\n",
      "Epoch 360/650\n",
      "122/122 [==============================] - 0s 654us/step - loss: 90.0047 - mae: 6.4379 - val_loss: 29.4594 - val_mae: 3.8191\n",
      "Epoch 361/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 85.3437 - mae: 6.4379 - val_loss: 35.1156 - val_mae: 4.3172\n",
      "Epoch 362/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 99.4194 - mae: 6.7051 - val_loss: 33.4272 - val_mae: 4.2536\n",
      "Epoch 363/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 84.2996 - mae: 6.3393 - val_loss: 32.5723 - val_mae: 4.1880\n",
      "Epoch 364/650\n",
      "122/122 [==============================] - 0s 594us/step - loss: 92.5757 - mae: 6.5102 - val_loss: 33.7900 - val_mae: 4.3532\n",
      "Epoch 365/650\n",
      "122/122 [==============================] - 0s 663us/step - loss: 87.0487 - mae: 6.4428 - val_loss: 29.7005 - val_mae: 3.8657\n",
      "Epoch 366/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 87.4745 - mae: 6.4532 - val_loss: 32.7529 - val_mae: 4.1059\n",
      "Epoch 367/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 93.1908 - mae: 6.5216 - val_loss: 30.4944 - val_mae: 3.8562\n",
      "Epoch 368/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 85.2483 - mae: 6.3069 - val_loss: 29.9093 - val_mae: 3.9640\n",
      "Epoch 369/650\n",
      "122/122 [==============================] - 0s 685us/step - loss: 86.0374 - mae: 6.4187 - val_loss: 30.2175 - val_mae: 3.9557\n",
      "Epoch 370/650\n",
      "122/122 [==============================] - 0s 611us/step - loss: 84.9294 - mae: 6.4483 - val_loss: 40.3080 - val_mae: 4.7025\n",
      "Epoch 371/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 88.9554 - mae: 6.4668 - val_loss: 28.9630 - val_mae: 3.7341\n",
      "Epoch 372/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 85.4460 - mae: 6.3819 - val_loss: 31.5211 - val_mae: 4.0529\n",
      "Epoch 373/650\n",
      "122/122 [==============================] - 0s 624us/step - loss: 88.3593 - mae: 6.3386 - val_loss: 38.6380 - val_mae: 4.6110\n",
      "Epoch 374/650\n",
      "122/122 [==============================] - 0s 614us/step - loss: 81.3745 - mae: 6.2738 - val_loss: 38.5981 - val_mae: 4.6138\n",
      "Epoch 375/650\n",
      "122/122 [==============================] - 0s 653us/step - loss: 82.0504 - mae: 6.3250 - val_loss: 29.4839 - val_mae: 3.8246\n",
      "Epoch 376/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 90.3198 - mae: 6.4695 - val_loss: 28.2633 - val_mae: 3.8275\n",
      "Epoch 377/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 85.5917 - mae: 6.2218 - val_loss: 30.8979 - val_mae: 3.9517\n",
      "Epoch 378/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 84.9501 - mae: 6.3928 - val_loss: 29.1661 - val_mae: 3.8116\n",
      "Epoch 379/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 80.0333 - mae: 6.3032 - val_loss: 44.7364 - val_mae: 5.0368\n",
      "Epoch 380/650\n",
      "122/122 [==============================] - 0s 641us/step - loss: 89.6563 - mae: 6.4447 - val_loss: 38.6675 - val_mae: 4.7161\n",
      "Epoch 381/650\n",
      "122/122 [==============================] - 0s 644us/step - loss: 81.2921 - mae: 6.2609 - val_loss: 30.0255 - val_mae: 3.9524\n",
      "Epoch 382/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 84.6550 - mae: 6.3265 - val_loss: 28.6245 - val_mae: 3.8231\n",
      "Epoch 383/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 78.0001 - mae: 6.2732 - val_loss: 33.5684 - val_mae: 4.2895\n",
      "Epoch 384/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 84.4869 - mae: 6.2297 - val_loss: 27.9206 - val_mae: 3.7643\n",
      "Epoch 385/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 86.1817 - mae: 6.3598 - val_loss: 36.5548 - val_mae: 4.5360\n",
      "Epoch 386/650\n",
      "122/122 [==============================] - 0s 644us/step - loss: 89.6061 - mae: 6.4562 - val_loss: 33.0709 - val_mae: 4.2153\n",
      "Epoch 387/650\n",
      "122/122 [==============================] - 0s 648us/step - loss: 77.2295 - mae: 6.1605 - val_loss: 34.0629 - val_mae: 4.3052\n",
      "Epoch 388/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 84.3788 - mae: 6.3608 - val_loss: 28.3784 - val_mae: 3.6851\n",
      "Epoch 389/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 80.2808 - mae: 6.2194 - val_loss: 40.6589 - val_mae: 4.7764\n",
      "Epoch 390/650\n",
      "122/122 [==============================] - 0s 686us/step - loss: 82.9190 - mae: 6.3784 - val_loss: 31.2614 - val_mae: 4.0494\n",
      "Epoch 391/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 83.8459 - mae: 6.2649 - val_loss: 32.2909 - val_mae: 3.8395\n",
      "Epoch 392/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 84.2717 - mae: 6.2599 - val_loss: 30.8103 - val_mae: 3.7714\n",
      "Epoch 393/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 87.0117 - mae: 6.4069 - val_loss: 31.7353 - val_mae: 3.9984\n",
      "Epoch 394/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 89.1749 - mae: 6.4057 - val_loss: 32.2425 - val_mae: 3.9915\n",
      "Epoch 395/650\n",
      "122/122 [==============================] - 0s 666us/step - loss: 79.8082 - mae: 6.1030 - val_loss: 31.1578 - val_mae: 3.8151\n",
      "Epoch 396/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 77.2406 - mae: 6.1442 - val_loss: 34.7408 - val_mae: 4.2450\n",
      "Epoch 397/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 84.5616 - mae: 6.2941 - val_loss: 30.2647 - val_mae: 3.8107\n",
      "Epoch 398/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 74.1441 - mae: 5.9489 - val_loss: 35.0811 - val_mae: 4.2529\n",
      "Epoch 399/650\n",
      "122/122 [==============================] - 0s 671us/step - loss: 75.8368 - mae: 6.1462 - val_loss: 39.0369 - val_mae: 4.6131\n",
      "Epoch 400/650\n",
      "122/122 [==============================] - 0s 624us/step - loss: 80.2295 - mae: 6.2020 - val_loss: 37.2365 - val_mae: 4.3618\n",
      "Epoch 401/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 80.8022 - mae: 6.2653 - val_loss: 42.2793 - val_mae: 4.7632\n",
      "Epoch 402/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 83.5670 - mae: 6.2974 - val_loss: 33.4773 - val_mae: 3.9550\n",
      "Epoch 403/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 87.8366 - mae: 6.3721 - val_loss: 29.5698 - val_mae: 3.7586\n",
      "Epoch 404/650\n",
      "122/122 [==============================] - 0s 723us/step - loss: 81.7443 - mae: 6.2587 - val_loss: 31.0652 - val_mae: 3.8158\n",
      "Epoch 405/650\n",
      "122/122 [==============================] - 0s 637us/step - loss: 78.7633 - mae: 6.2014 - val_loss: 35.7243 - val_mae: 4.3599\n",
      "Epoch 406/650\n",
      "122/122 [==============================] - 0s 643us/step - loss: 81.8920 - mae: 6.2188 - val_loss: 31.9086 - val_mae: 4.1032\n",
      "Epoch 407/650\n",
      "122/122 [==============================] - 0s 632us/step - loss: 81.3924 - mae: 6.1524 - val_loss: 33.9577 - val_mae: 4.2455\n",
      "Epoch 408/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 81.9806 - mae: 6.2203 - val_loss: 36.9152 - val_mae: 4.5595\n",
      "Epoch 409/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 80.3130 - mae: 6.1714 - val_loss: 34.2078 - val_mae: 4.3463\n",
      "Epoch 410/650\n",
      "122/122 [==============================] - 0s 627us/step - loss: 83.9105 - mae: 6.2893 - val_loss: 29.4401 - val_mae: 3.8713\n",
      "Epoch 411/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 79.9679 - mae: 6.1931 - val_loss: 32.4761 - val_mae: 3.9606\n",
      "Epoch 412/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 82.2529 - mae: 6.2727 - val_loss: 33.1290 - val_mae: 4.0225\n",
      "Epoch 413/650\n",
      "122/122 [==============================] - 0s 657us/step - loss: 79.9105 - mae: 6.2544 - val_loss: 32.3266 - val_mae: 3.9088\n",
      "Epoch 414/650\n",
      "122/122 [==============================] - 0s 627us/step - loss: 80.9710 - mae: 6.1811 - val_loss: 40.0822 - val_mae: 4.7186\n",
      "Epoch 415/650\n",
      "122/122 [==============================] - 0s 594us/step - loss: 78.1793 - mae: 6.0908 - val_loss: 30.8962 - val_mae: 3.8907\n",
      "Epoch 416/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 78.4679 - mae: 6.1742 - val_loss: 30.6944 - val_mae: 3.7845\n",
      "Epoch 417/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 77.5732 - mae: 6.0536 - val_loss: 32.7830 - val_mae: 3.9857\n",
      "Epoch 418/650\n",
      "122/122 [==============================] - 0s 685us/step - loss: 81.1265 - mae: 6.2216 - val_loss: 38.9106 - val_mae: 4.5709\n",
      "Epoch 419/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 80.6018 - mae: 6.2142 - val_loss: 31.3242 - val_mae: 3.8184\n",
      "Epoch 420/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 79.1585 - mae: 6.0339 - val_loss: 36.9203 - val_mae: 4.4705\n",
      "Epoch 421/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 74.4467 - mae: 6.0251 - val_loss: 44.4278 - val_mae: 4.9647\n",
      "Epoch 422/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 77.8559 - mae: 6.1340 - val_loss: 34.1556 - val_mae: 4.2794\n",
      "Epoch 423/650\n",
      "122/122 [==============================] - 0s 643us/step - loss: 74.4379 - mae: 6.0684 - val_loss: 32.0657 - val_mae: 4.0156\n",
      "Epoch 424/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 76.9358 - mae: 6.0753 - val_loss: 52.8330 - val_mae: 5.3925\n",
      "Epoch 425/650\n",
      "122/122 [==============================] - 0s 630us/step - loss: 79.1884 - mae: 6.1162 - val_loss: 27.9582 - val_mae: 3.6910\n",
      "Epoch 426/650\n",
      "122/122 [==============================] - 0s 655us/step - loss: 76.5857 - mae: 6.1134 - val_loss: 32.8741 - val_mae: 3.9924\n",
      "Epoch 427/650\n",
      "122/122 [==============================] - 0s 663us/step - loss: 75.2568 - mae: 6.0598 - val_loss: 33.2973 - val_mae: 4.1044\n",
      "Epoch 428/650\n",
      "122/122 [==============================] - 0s 653us/step - loss: 79.0184 - mae: 6.1216 - val_loss: 42.8233 - val_mae: 4.8820\n",
      "Epoch 429/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 73.5133 - mae: 5.9206 - val_loss: 38.8099 - val_mae: 4.5567\n",
      "Epoch 430/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 77.1752 - mae: 6.0738 - val_loss: 42.1174 - val_mae: 4.8177\n",
      "Epoch 431/650\n",
      "122/122 [==============================] - 0s 635us/step - loss: 72.6130 - mae: 5.9959 - val_loss: 36.9831 - val_mae: 4.5144\n",
      "Epoch 432/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 77.4103 - mae: 6.1203 - val_loss: 47.8124 - val_mae: 5.1434\n",
      "Epoch 433/650\n",
      "122/122 [==============================] - 0s 612us/step - loss: 85.0530 - mae: 6.3603 - val_loss: 33.4528 - val_mae: 4.1982\n",
      "Epoch 434/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 84.5850 - mae: 6.2680 - val_loss: 41.1893 - val_mae: 4.6220\n",
      "Epoch 435/650\n",
      "122/122 [==============================] - 0s 592us/step - loss: 74.9997 - mae: 6.0484 - val_loss: 32.9792 - val_mae: 4.1388\n",
      "Epoch 436/650\n",
      "122/122 [==============================] - 0s 614us/step - loss: 72.1685 - mae: 5.9586 - val_loss: 31.1806 - val_mae: 4.0622\n",
      "Epoch 437/650\n",
      "122/122 [==============================] - 0s 683us/step - loss: 71.9315 - mae: 5.9243 - val_loss: 38.7295 - val_mae: 4.6405\n",
      "Epoch 438/650\n",
      "122/122 [==============================] - 0s 628us/step - loss: 75.4528 - mae: 6.0078 - val_loss: 33.0106 - val_mae: 4.1039\n",
      "Epoch 439/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 75.8507 - mae: 6.0614 - val_loss: 37.2215 - val_mae: 4.4846\n",
      "Epoch 440/650\n",
      "122/122 [==============================] - 0s 633us/step - loss: 73.1647 - mae: 6.0475 - val_loss: 30.5222 - val_mae: 3.8649\n",
      "Epoch 441/650\n",
      "122/122 [==============================] - 0s 703us/step - loss: 76.3761 - mae: 6.0597 - val_loss: 40.8967 - val_mae: 4.7509\n",
      "Epoch 442/650\n",
      "122/122 [==============================] - 0s 650us/step - loss: 71.9595 - mae: 5.9849 - val_loss: 34.9469 - val_mae: 4.2031\n",
      "Epoch 443/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 70.4507 - mae: 5.8558 - val_loss: 49.3730 - val_mae: 5.2051\n",
      "Epoch 444/650\n",
      "122/122 [==============================] - 0s 586us/step - loss: 78.4659 - mae: 6.0898 - val_loss: 38.1868 - val_mae: 4.6299\n",
      "Epoch 445/650\n",
      "122/122 [==============================] - 0s 714us/step - loss: 72.4996 - mae: 5.9836 - val_loss: 41.9281 - val_mae: 4.8462\n",
      "Epoch 446/650\n",
      "122/122 [==============================] - 0s 609us/step - loss: 73.0593 - mae: 6.0426 - val_loss: 36.3317 - val_mae: 4.2355\n",
      "Epoch 447/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 76.3227 - mae: 6.0961 - val_loss: 34.1012 - val_mae: 4.1595\n",
      "Epoch 448/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 74.8617 - mae: 6.0418 - val_loss: 29.3616 - val_mae: 3.9341\n",
      "Epoch 449/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 78.5313 - mae: 6.0259 - val_loss: 34.6611 - val_mae: 4.3829\n",
      "Epoch 450/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 78.3442 - mae: 6.1558 - val_loss: 29.2891 - val_mae: 3.7799\n",
      "Epoch 451/650\n",
      "122/122 [==============================] - 0s 614us/step - loss: 69.9219 - mae: 5.8836 - val_loss: 33.4777 - val_mae: 4.1376\n",
      "Epoch 452/650\n",
      "122/122 [==============================] - 0s 614us/step - loss: 77.9161 - mae: 6.0728 - val_loss: 33.6549 - val_mae: 4.1616\n",
      "Epoch 453/650\n",
      "122/122 [==============================] - 0s 584us/step - loss: 72.9410 - mae: 5.9712 - val_loss: 33.3723 - val_mae: 4.0076\n",
      "Epoch 454/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 69.0698 - mae: 5.8447 - val_loss: 33.4686 - val_mae: 4.1004\n",
      "Epoch 455/650\n",
      "122/122 [==============================] - 0s 710us/step - loss: 75.7851 - mae: 6.0623 - val_loss: 33.4678 - val_mae: 4.2129\n",
      "Epoch 456/650\n",
      "122/122 [==============================] - 0s 612us/step - loss: 69.4633 - mae: 5.7755 - val_loss: 36.7020 - val_mae: 4.3841\n",
      "Epoch 457/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 75.7591 - mae: 6.0080 - val_loss: 32.4156 - val_mae: 4.0135\n",
      "Epoch 458/650\n",
      "122/122 [==============================] - 0s 622us/step - loss: 74.5083 - mae: 5.9905 - val_loss: 43.7054 - val_mae: 4.8947\n",
      "Epoch 459/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 70.1172 - mae: 5.8377 - val_loss: 43.5063 - val_mae: 4.8490\n",
      "Epoch 460/650\n",
      "122/122 [==============================] - 0s 594us/step - loss: 79.4713 - mae: 6.0765 - val_loss: 29.8485 - val_mae: 3.7847\n",
      "Epoch 461/650\n",
      "122/122 [==============================] - 0s 702us/step - loss: 73.3940 - mae: 6.0123 - val_loss: 31.7630 - val_mae: 3.9262\n",
      "Epoch 462/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 73.0007 - mae: 5.9568 - val_loss: 35.3039 - val_mae: 4.3195\n",
      "Epoch 463/650\n",
      "122/122 [==============================] - 0s 648us/step - loss: 69.7196 - mae: 5.8516 - val_loss: 32.4013 - val_mae: 3.9192\n",
      "Epoch 464/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 69.9329 - mae: 5.8568 - val_loss: 34.6860 - val_mae: 4.2151\n",
      "Epoch 465/650\n",
      "122/122 [==============================] - 0s 647us/step - loss: 72.9279 - mae: 5.9625 - val_loss: 32.2032 - val_mae: 4.1158\n",
      "Epoch 466/650\n",
      "122/122 [==============================] - 0s 630us/step - loss: 70.5007 - mae: 5.8409 - val_loss: 33.9771 - val_mae: 4.1215\n",
      "Epoch 467/650\n",
      "122/122 [==============================] - 0s 688us/step - loss: 69.1075 - mae: 5.8445 - val_loss: 29.6013 - val_mae: 3.8146\n",
      "Epoch 468/650\n",
      "122/122 [==============================] - 0s 611us/step - loss: 73.1628 - mae: 6.0090 - val_loss: 31.0767 - val_mae: 4.0185\n",
      "Epoch 469/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 69.4114 - mae: 5.8707 - val_loss: 51.9012 - val_mae: 5.3609\n",
      "Epoch 470/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 72.6459 - mae: 6.0440 - val_loss: 31.9713 - val_mae: 4.1210\n",
      "Epoch 471/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 79.8340 - mae: 6.0489 - val_loss: 37.8427 - val_mae: 4.5895\n",
      "Epoch 472/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 65.4033 - mae: 5.7097 - val_loss: 39.9513 - val_mae: 4.7976\n",
      "Epoch 473/650\n",
      "122/122 [==============================] - 0s 703us/step - loss: 74.5163 - mae: 5.9429 - val_loss: 32.8752 - val_mae: 4.1612\n",
      "Epoch 474/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 68.6085 - mae: 5.8553 - val_loss: 32.6169 - val_mae: 4.0427\n",
      "Epoch 475/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 71.2765 - mae: 5.9505 - val_loss: 33.2926 - val_mae: 4.2490\n",
      "Epoch 476/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 74.0637 - mae: 5.9441 - val_loss: 44.9314 - val_mae: 5.0239\n",
      "Epoch 477/650\n",
      "122/122 [==============================] - 0s 671us/step - loss: 75.8569 - mae: 6.1280 - val_loss: 33.9947 - val_mae: 4.1488\n",
      "Epoch 478/650\n",
      "122/122 [==============================] - 0s 612us/step - loss: 66.9117 - mae: 5.7737 - val_loss: 32.5802 - val_mae: 4.0483\n",
      "Epoch 479/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 74.7957 - mae: 5.9950 - val_loss: 31.9820 - val_mae: 4.0266\n",
      "Epoch 480/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 69.8427 - mae: 5.9015 - val_loss: 35.1877 - val_mae: 4.4116\n",
      "Epoch 481/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 71.6779 - mae: 5.9412 - val_loss: 30.3953 - val_mae: 3.9830\n",
      "Epoch 482/650\n",
      "122/122 [==============================] - 0s 655us/step - loss: 68.3847 - mae: 5.8282 - val_loss: 33.8448 - val_mae: 4.0452\n",
      "Epoch 483/650\n",
      "122/122 [==============================] - 0s 625us/step - loss: 74.4421 - mae: 5.9914 - val_loss: 43.3956 - val_mae: 5.0132\n",
      "Epoch 484/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 63.1447 - mae: 5.6637 - val_loss: 29.4814 - val_mae: 3.8017\n",
      "Epoch 485/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 65.6876 - mae: 5.7036 - val_loss: 34.4365 - val_mae: 4.3016\n",
      "Epoch 486/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 71.9833 - mae: 5.8857 - val_loss: 34.1554 - val_mae: 4.0454\n",
      "Epoch 487/650\n",
      "122/122 [==============================] - 0s 662us/step - loss: 68.1602 - mae: 5.7704 - val_loss: 31.7250 - val_mae: 3.8462\n",
      "Epoch 488/650\n",
      "122/122 [==============================] - 0s 672us/step - loss: 72.5810 - mae: 5.9347 - val_loss: 31.6849 - val_mae: 3.9179\n",
      "Epoch 489/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 68.3988 - mae: 5.7751 - val_loss: 35.3126 - val_mae: 4.4769\n",
      "Epoch 490/650\n",
      "122/122 [==============================] - 0s 649us/step - loss: 67.2517 - mae: 5.7901 - val_loss: 34.7654 - val_mae: 4.0463\n",
      "Epoch 491/650\n",
      "122/122 [==============================] - 0s 638us/step - loss: 69.1699 - mae: 5.9286 - val_loss: 35.9438 - val_mae: 4.2441\n",
      "Epoch 492/650\n",
      "122/122 [==============================] - 0s 627us/step - loss: 68.6372 - mae: 5.9268 - val_loss: 33.3343 - val_mae: 4.2276\n",
      "Epoch 493/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 69.6645 - mae: 5.8439 - val_loss: 33.8586 - val_mae: 4.2864\n",
      "Epoch 494/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 70.0564 - mae: 5.9141 - val_loss: 31.6192 - val_mae: 4.0365\n",
      "Epoch 495/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 69.5186 - mae: 5.9324 - val_loss: 33.3825 - val_mae: 4.1182\n",
      "Epoch 496/650\n",
      "122/122 [==============================] - 0s 683us/step - loss: 68.6264 - mae: 5.8605 - val_loss: 35.2663 - val_mae: 4.4121\n",
      "Epoch 497/650\n",
      "122/122 [==============================] - 0s 634us/step - loss: 71.4770 - mae: 5.9295 - val_loss: 32.7255 - val_mae: 4.1437\n",
      "Epoch 498/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 67.0432 - mae: 5.8015 - val_loss: 37.3916 - val_mae: 4.3407\n",
      "Epoch 499/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 71.1380 - mae: 5.9393 - val_loss: 34.6783 - val_mae: 4.3621\n",
      "Epoch 500/650\n",
      "122/122 [==============================] - 0s 602us/step - loss: 74.9107 - mae: 6.0162 - val_loss: 29.6152 - val_mae: 3.7869\n",
      "Epoch 501/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 66.9571 - mae: 5.8222 - val_loss: 31.8940 - val_mae: 3.9750\n",
      "Epoch 502/650\n",
      "122/122 [==============================] - 0s 676us/step - loss: 67.7925 - mae: 5.8081 - val_loss: 35.8789 - val_mae: 4.4173\n",
      "Epoch 503/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 67.1261 - mae: 5.8154 - val_loss: 34.4917 - val_mae: 4.0863\n",
      "Epoch 504/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 68.9762 - mae: 5.9275 - val_loss: 29.8204 - val_mae: 3.8257\n",
      "Epoch 505/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 67.0636 - mae: 5.7911 - val_loss: 33.4873 - val_mae: 4.1914\n",
      "Epoch 506/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 63.2548 - mae: 5.6997 - val_loss: 37.4075 - val_mae: 4.6079\n",
      "Epoch 507/650\n",
      "122/122 [==============================] - 0s 640us/step - loss: 61.9764 - mae: 5.6451 - val_loss: 29.0818 - val_mae: 3.9026\n",
      "Epoch 508/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 70.3421 - mae: 5.8482 - val_loss: 35.0680 - val_mae: 4.3792\n",
      "Epoch 509/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 67.6470 - mae: 5.9586 - val_loss: 39.9773 - val_mae: 4.7508\n",
      "Epoch 510/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 67.8232 - mae: 5.8270 - val_loss: 44.6404 - val_mae: 4.9090\n",
      "Epoch 511/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 70.1299 - mae: 5.8268 - val_loss: 37.4148 - val_mae: 4.4019\n",
      "Epoch 512/650\n",
      "122/122 [==============================] - 0s 636us/step - loss: 70.6028 - mae: 5.9233 - val_loss: 36.6023 - val_mae: 4.3213\n",
      "Epoch 513/650\n",
      "122/122 [==============================] - 0s 632us/step - loss: 65.6559 - mae: 5.7149 - val_loss: 34.3959 - val_mae: 4.1370\n",
      "Epoch 514/650\n",
      "122/122 [==============================] - 0s 630us/step - loss: 63.8355 - mae: 5.6653 - val_loss: 35.4123 - val_mae: 4.3671\n",
      "Epoch 515/650\n",
      "122/122 [==============================] - 0s 628us/step - loss: 65.6551 - mae: 5.7093 - val_loss: 32.9609 - val_mae: 3.9503\n",
      "Epoch 516/650\n",
      "122/122 [==============================] - 0s 652us/step - loss: 67.2032 - mae: 5.7282 - val_loss: 35.9360 - val_mae: 4.3347\n",
      "Epoch 517/650\n",
      "122/122 [==============================] - 0s 624us/step - loss: 70.4706 - mae: 5.8635 - val_loss: 33.5290 - val_mae: 4.2289\n",
      "Epoch 518/650\n",
      "122/122 [==============================] - 0s 629us/step - loss: 69.7002 - mae: 5.8639 - val_loss: 32.8138 - val_mae: 4.2009\n",
      "Epoch 519/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 64.2744 - mae: 5.7525 - val_loss: 35.6541 - val_mae: 4.5228\n",
      "Epoch 520/650\n",
      "122/122 [==============================] - 0s 693us/step - loss: 71.6944 - mae: 5.9285 - val_loss: 32.9926 - val_mae: 4.2088\n",
      "Epoch 521/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 70.6331 - mae: 5.9104 - val_loss: 33.7744 - val_mae: 4.0690\n",
      "Epoch 522/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 69.1560 - mae: 5.8654 - val_loss: 36.3408 - val_mae: 4.4314\n",
      "Epoch 523/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 62.3780 - mae: 5.6614 - val_loss: 37.5575 - val_mae: 4.5442\n",
      "Epoch 524/650\n",
      "122/122 [==============================] - 0s 724us/step - loss: 65.4684 - mae: 5.7173 - val_loss: 40.6365 - val_mae: 4.7184\n",
      "Epoch 525/650\n",
      "122/122 [==============================] - 0s 667us/step - loss: 64.4624 - mae: 5.6723 - val_loss: 30.4539 - val_mae: 4.0224\n",
      "Epoch 526/650\n",
      "122/122 [==============================] - 0s 633us/step - loss: 68.6752 - mae: 5.8389 - val_loss: 33.0615 - val_mae: 4.2132\n",
      "Epoch 527/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 67.0272 - mae: 5.8094 - val_loss: 32.4438 - val_mae: 3.9910\n",
      "Epoch 528/650\n",
      "122/122 [==============================] - 0s 647us/step - loss: 63.4689 - mae: 5.6983 - val_loss: 35.0176 - val_mae: 4.1605\n",
      "Epoch 529/650\n",
      "122/122 [==============================] - 0s 655us/step - loss: 64.2199 - mae: 5.7049 - val_loss: 36.7895 - val_mae: 4.4856\n",
      "Epoch 530/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 65.5514 - mae: 5.7543 - val_loss: 31.0756 - val_mae: 3.9435\n",
      "Epoch 531/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 64.4033 - mae: 5.7477 - val_loss: 30.7356 - val_mae: 3.9901\n",
      "Epoch 532/650\n",
      "122/122 [==============================] - 0s 687us/step - loss: 61.1177 - mae: 5.5532 - val_loss: 41.7482 - val_mae: 4.8664\n",
      "Epoch 533/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 67.1592 - mae: 5.7306 - val_loss: 32.1614 - val_mae: 4.0556\n",
      "Epoch 534/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 65.3039 - mae: 5.6534 - val_loss: 31.4706 - val_mae: 4.0467\n",
      "Epoch 535/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 68.0969 - mae: 5.8009 - val_loss: 34.0889 - val_mae: 4.1746\n",
      "Epoch 536/650\n",
      "122/122 [==============================] - 0s 607us/step - loss: 74.1465 - mae: 6.0203 - val_loss: 35.4548 - val_mae: 4.1908\n",
      "Epoch 537/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 64.2360 - mae: 5.7278 - val_loss: 33.2139 - val_mae: 3.9980\n",
      "Epoch 538/650\n",
      "122/122 [==============================] - 0s 707us/step - loss: 71.0930 - mae: 5.9216 - val_loss: 36.4024 - val_mae: 4.3623\n",
      "Epoch 539/650\n",
      "122/122 [==============================] - 0s 595us/step - loss: 67.0776 - mae: 5.8343 - val_loss: 30.8448 - val_mae: 3.8878\n",
      "Epoch 540/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 65.3106 - mae: 5.7574 - val_loss: 31.4692 - val_mae: 3.9378\n",
      "Epoch 541/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 66.1596 - mae: 5.7249 - val_loss: 33.8052 - val_mae: 4.0919\n",
      "Epoch 542/650\n",
      "122/122 [==============================] - 0s 638us/step - loss: 66.8629 - mae: 5.7958 - val_loss: 35.6012 - val_mae: 4.3249\n",
      "Epoch 543/650\n",
      "122/122 [==============================] - 0s 628us/step - loss: 63.7145 - mae: 5.7611 - val_loss: 32.8740 - val_mae: 4.0607\n",
      "Epoch 544/650\n",
      "122/122 [==============================] - 0s 633us/step - loss: 59.8979 - mae: 5.5394 - val_loss: 34.7411 - val_mae: 4.0904\n",
      "Epoch 545/650\n",
      "122/122 [==============================] - 0s 631us/step - loss: 60.1791 - mae: 5.6189 - val_loss: 38.3285 - val_mae: 4.4677\n",
      "Epoch 546/650\n",
      "122/122 [==============================] - 0s 655us/step - loss: 65.3059 - mae: 5.7814 - val_loss: 34.8769 - val_mae: 4.1250\n",
      "Epoch 547/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 66.4243 - mae: 5.7280 - val_loss: 33.7330 - val_mae: 4.1416\n",
      "Epoch 548/650\n",
      "122/122 [==============================] - 0s 626us/step - loss: 68.7860 - mae: 5.8180 - val_loss: 33.9964 - val_mae: 3.9940\n",
      "Epoch 549/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 64.0097 - mae: 5.6918 - val_loss: 41.9874 - val_mae: 4.7647\n",
      "Epoch 550/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 64.9466 - mae: 5.7573 - val_loss: 38.0508 - val_mae: 4.5309\n",
      "Epoch 551/650\n",
      "122/122 [==============================] - 0s 699us/step - loss: 64.7938 - mae: 5.6776 - val_loss: 33.1278 - val_mae: 4.0234\n",
      "Epoch 552/650\n",
      "122/122 [==============================] - 0s 610us/step - loss: 64.4948 - mae: 5.7470 - val_loss: 32.6399 - val_mae: 3.9618\n",
      "Epoch 553/650\n",
      "122/122 [==============================] - 0s 588us/step - loss: 66.0813 - mae: 5.7968 - val_loss: 32.3632 - val_mae: 4.0518\n",
      "Epoch 554/650\n",
      "122/122 [==============================] - 0s 598us/step - loss: 63.1137 - mae: 5.6166 - val_loss: 47.2075 - val_mae: 4.9877\n",
      "Epoch 555/650\n",
      "122/122 [==============================] - 0s 611us/step - loss: 64.0675 - mae: 5.6721 - val_loss: 37.3049 - val_mae: 4.4257\n",
      "Epoch 556/650\n",
      "122/122 [==============================] - 0s 636us/step - loss: 67.9049 - mae: 5.7702 - val_loss: 34.7603 - val_mae: 4.0515\n",
      "Epoch 557/650\n",
      "122/122 [==============================] - 0s 628us/step - loss: 66.7957 - mae: 5.8036 - val_loss: 32.4955 - val_mae: 4.1338\n",
      "Epoch 558/650\n",
      "122/122 [==============================] - 0s 625us/step - loss: 64.3644 - mae: 5.7215 - val_loss: 37.4381 - val_mae: 4.5280\n",
      "Epoch 559/650\n",
      "122/122 [==============================] - 0s 621us/step - loss: 67.8457 - mae: 5.7476 - val_loss: 30.7315 - val_mae: 3.8368\n",
      "Epoch 560/650\n",
      "122/122 [==============================] - 0s 651us/step - loss: 69.8507 - mae: 5.8272 - val_loss: 32.8118 - val_mae: 4.1298\n",
      "Epoch 561/650\n",
      "122/122 [==============================] - 0s 623us/step - loss: 64.2741 - mae: 5.7689 - val_loss: 43.2816 - val_mae: 4.8221\n",
      "Epoch 562/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 65.5068 - mae: 5.7674 - val_loss: 38.8482 - val_mae: 4.5438\n",
      "Epoch 563/650\n",
      "122/122 [==============================] - 0s 586us/step - loss: 61.1620 - mae: 5.5610 - val_loss: 35.4568 - val_mae: 4.3448\n",
      "Epoch 564/650\n",
      "122/122 [==============================] - 0s 591us/step - loss: 64.7998 - mae: 5.7228 - val_loss: 33.5056 - val_mae: 4.0897\n",
      "Epoch 565/650\n",
      "122/122 [==============================] - 0s 694us/step - loss: 64.3890 - mae: 5.6595 - val_loss: 40.4105 - val_mae: 4.6626\n",
      "Epoch 566/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 67.3475 - mae: 5.8191 - val_loss: 32.5740 - val_mae: 3.9863\n",
      "Epoch 567/650\n",
      "122/122 [==============================] - 0s 609us/step - loss: 57.2582 - mae: 5.5039 - val_loss: 34.4759 - val_mae: 4.1779\n",
      "Epoch 568/650\n",
      "122/122 [==============================] - 0s 589us/step - loss: 65.3506 - mae: 5.6700 - val_loss: 33.6447 - val_mae: 3.9963\n",
      "Epoch 569/650\n",
      "122/122 [==============================] - 0s 608us/step - loss: 63.4865 - mae: 5.6421 - val_loss: 33.6673 - val_mae: 4.1074\n",
      "Epoch 570/650\n",
      "122/122 [==============================] - 0s 672us/step - loss: 59.7715 - mae: 5.5572 - val_loss: 35.4265 - val_mae: 4.3854\n",
      "Epoch 571/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 63.7570 - mae: 5.7465 - val_loss: 32.4668 - val_mae: 4.0705\n",
      "Epoch 572/650\n",
      "122/122 [==============================] - 0s 594us/step - loss: 60.3203 - mae: 5.6370 - val_loss: 32.2672 - val_mae: 4.1625\n",
      "Epoch 573/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 62.9451 - mae: 5.6223 - val_loss: 42.5937 - val_mae: 4.9204\n",
      "Epoch 574/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 62.9017 - mae: 5.5225 - val_loss: 40.0989 - val_mae: 4.6616\n",
      "Epoch 575/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 63.8054 - mae: 5.7410 - val_loss: 32.3171 - val_mae: 3.9516\n",
      "Epoch 576/650\n",
      "122/122 [==============================] - 0s 658us/step - loss: 63.8184 - mae: 5.6526 - val_loss: 35.4211 - val_mae: 4.3286\n",
      "Epoch 577/650\n",
      "122/122 [==============================] - 0s 640us/step - loss: 62.8088 - mae: 5.7282 - val_loss: 32.8457 - val_mae: 4.0784\n",
      "Epoch 578/650\n",
      "122/122 [==============================] - 0s 657us/step - loss: 64.3578 - mae: 5.6606 - val_loss: 35.0762 - val_mae: 4.2605\n",
      "Epoch 579/650\n",
      "122/122 [==============================] - 0s 662us/step - loss: 64.5763 - mae: 5.7374 - val_loss: 33.9907 - val_mae: 4.1203\n",
      "Epoch 580/650\n",
      "122/122 [==============================] - 0s 639us/step - loss: 58.4674 - mae: 5.5639 - val_loss: 33.8867 - val_mae: 4.2440\n",
      "Epoch 581/650\n",
      "122/122 [==============================] - 0s 599us/step - loss: 62.3832 - mae: 5.7039 - val_loss: 32.3506 - val_mae: 4.0841\n",
      "Epoch 582/650\n",
      "122/122 [==============================] - 0s 604us/step - loss: 63.5323 - mae: 5.6371 - val_loss: 32.3120 - val_mae: 4.1368\n",
      "Epoch 583/650\n",
      "122/122 [==============================] - 0s 613us/step - loss: 60.0177 - mae: 5.6203 - val_loss: 36.0174 - val_mae: 4.4162\n",
      "Epoch 584/650\n",
      "122/122 [==============================] - 0s 652us/step - loss: 60.7820 - mae: 5.5982 - val_loss: 32.5414 - val_mae: 4.0568\n",
      "Epoch 585/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 61.7676 - mae: 5.5559 - val_loss: 33.5915 - val_mae: 4.2656\n",
      "Epoch 586/650\n",
      "122/122 [==============================] - 0s 637us/step - loss: 62.8379 - mae: 5.6117 - val_loss: 32.6485 - val_mae: 4.0702\n",
      "Epoch 587/650\n",
      "122/122 [==============================] - 0s 631us/step - loss: 66.0511 - mae: 5.6898 - val_loss: 37.3155 - val_mae: 4.4563\n",
      "Epoch 588/650\n",
      "122/122 [==============================] - 0s 661us/step - loss: 59.8365 - mae: 5.6162 - val_loss: 31.9243 - val_mae: 3.7889\n",
      "Epoch 589/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 65.6961 - mae: 5.7853 - val_loss: 32.9641 - val_mae: 4.0903\n",
      "Epoch 590/650\n",
      "122/122 [==============================] - 0s 583us/step - loss: 65.5870 - mae: 5.7981 - val_loss: 36.4030 - val_mae: 4.4001\n",
      "Epoch 591/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 61.3848 - mae: 5.6734 - val_loss: 31.5703 - val_mae: 3.7877\n",
      "Epoch 592/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 66.1363 - mae: 5.6319 - val_loss: 35.4021 - val_mae: 4.2765\n",
      "Epoch 593/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 60.2429 - mae: 5.5787 - val_loss: 38.0603 - val_mae: 4.3704\n",
      "Epoch 594/650\n",
      "122/122 [==============================] - 0s 634us/step - loss: 61.7513 - mae: 5.6151 - val_loss: 39.3482 - val_mae: 4.2304\n",
      "Epoch 595/650\n",
      "122/122 [==============================] - 0s 590us/step - loss: 70.9965 - mae: 5.9558 - val_loss: 49.5289 - val_mae: 4.9500\n",
      "Epoch 596/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 65.9670 - mae: 5.6800 - val_loss: 45.9064 - val_mae: 5.0920\n",
      "Epoch 597/650\n",
      "122/122 [==============================] - 0s 627us/step - loss: 59.2598 - mae: 5.5734 - val_loss: 37.1344 - val_mae: 4.0637\n",
      "Epoch 598/650\n",
      "122/122 [==============================] - 0s 642us/step - loss: 60.0077 - mae: 5.5382 - val_loss: 42.9272 - val_mae: 4.4101\n",
      "Epoch 599/650\n",
      "122/122 [==============================] - 0s 628us/step - loss: 65.3353 - mae: 5.6712 - val_loss: 34.2758 - val_mae: 3.9173\n",
      "Epoch 600/650\n",
      "122/122 [==============================] - 0s 633us/step - loss: 62.5514 - mae: 5.5582 - val_loss: 36.2388 - val_mae: 4.1628\n",
      "Epoch 601/650\n",
      "122/122 [==============================] - 0s 658us/step - loss: 65.8094 - mae: 5.7102 - val_loss: 41.8875 - val_mae: 4.6682\n",
      "Epoch 602/650\n",
      "122/122 [==============================] - 0s 627us/step - loss: 61.5246 - mae: 5.6982 - val_loss: 34.0468 - val_mae: 3.9392\n",
      "Epoch 603/650\n",
      "122/122 [==============================] - 0s 636us/step - loss: 63.8513 - mae: 5.6246 - val_loss: 39.1396 - val_mae: 4.3711\n",
      "Epoch 604/650\n",
      "122/122 [==============================] - 0s 657us/step - loss: 63.7314 - mae: 5.6444 - val_loss: 37.5368 - val_mae: 4.0389\n",
      "Epoch 605/650\n",
      "122/122 [==============================] - 0s 633us/step - loss: 59.2883 - mae: 5.4901 - val_loss: 37.4651 - val_mae: 4.1555\n",
      "Epoch 606/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 62.1905 - mae: 5.6343 - val_loss: 38.7233 - val_mae: 4.3465\n",
      "Epoch 607/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 59.8511 - mae: 5.6140 - val_loss: 38.0866 - val_mae: 4.2621\n",
      "Epoch 608/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 63.2522 - mae: 5.6980 - val_loss: 36.0228 - val_mae: 4.2570\n",
      "Epoch 609/650\n",
      "122/122 [==============================] - 0s 679us/step - loss: 63.5545 - mae: 5.6517 - val_loss: 34.4706 - val_mae: 4.0706\n",
      "Epoch 610/650\n",
      "122/122 [==============================] - 0s 622us/step - loss: 60.0049 - mae: 5.6689 - val_loss: 34.8778 - val_mae: 4.1409\n",
      "Epoch 611/650\n",
      "122/122 [==============================] - 0s 597us/step - loss: 58.4981 - mae: 5.4541 - val_loss: 39.2106 - val_mae: 4.5011\n",
      "Epoch 612/650\n",
      "122/122 [==============================] - 0s 586us/step - loss: 61.2257 - mae: 5.5898 - val_loss: 34.1110 - val_mae: 3.9286\n",
      "Epoch 613/650\n",
      "122/122 [==============================] - 0s 619us/step - loss: 61.5861 - mae: 5.5992 - val_loss: 34.9673 - val_mae: 3.9484\n",
      "Epoch 614/650\n",
      "122/122 [==============================] - 0s 584us/step - loss: 62.4275 - mae: 5.6347 - val_loss: 35.6093 - val_mae: 4.0398\n",
      "Epoch 615/650\n",
      "122/122 [==============================] - 0s 659us/step - loss: 60.0234 - mae: 5.5756 - val_loss: 34.4594 - val_mae: 3.9890\n",
      "Epoch 616/650\n",
      "122/122 [==============================] - 0s 631us/step - loss: 59.9858 - mae: 5.5133 - val_loss: 37.3887 - val_mae: 4.4058\n",
      "Epoch 617/650\n",
      "122/122 [==============================] - 0s 589us/step - loss: 56.6747 - mae: 5.5009 - val_loss: 36.1838 - val_mae: 4.2894\n",
      "Epoch 618/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 60.1535 - mae: 5.6555 - val_loss: 34.6049 - val_mae: 4.0385\n",
      "Epoch 619/650\n",
      "122/122 [==============================] - 0s 686us/step - loss: 62.3777 - mae: 5.6085 - val_loss: 36.6554 - val_mae: 4.2536\n",
      "Epoch 620/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 60.0814 - mae: 5.6527 - val_loss: 37.9807 - val_mae: 4.0656\n",
      "Epoch 621/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 60.0318 - mae: 5.6109 - val_loss: 48.7524 - val_mae: 5.1340\n",
      "Epoch 622/650\n",
      "122/122 [==============================] - 0s 601us/step - loss: 60.8663 - mae: 5.5287 - val_loss: 41.9769 - val_mae: 4.4849\n",
      "Epoch 623/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 60.4720 - mae: 5.5984 - val_loss: 33.8432 - val_mae: 3.9749\n",
      "Epoch 624/650\n",
      "122/122 [==============================] - 0s 661us/step - loss: 61.0872 - mae: 5.6126 - val_loss: 34.7654 - val_mae: 4.0592\n",
      "Epoch 625/650\n",
      "122/122 [==============================] - 0s 616us/step - loss: 67.7202 - mae: 5.7521 - val_loss: 38.8839 - val_mae: 4.4335\n",
      "Epoch 626/650\n",
      "122/122 [==============================] - 0s 620us/step - loss: 58.0147 - mae: 5.4714 - val_loss: 39.8465 - val_mae: 4.6165\n",
      "Epoch 627/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 62.1630 - mae: 5.6802 - val_loss: 34.2156 - val_mae: 4.1485\n",
      "Epoch 628/650\n",
      "122/122 [==============================] - 0s 636us/step - loss: 59.0844 - mae: 5.5398 - val_loss: 37.3444 - val_mae: 4.4070\n",
      "Epoch 629/650\n",
      "122/122 [==============================] - 0s 633us/step - loss: 60.2895 - mae: 5.5987 - val_loss: 34.8985 - val_mae: 3.9882\n",
      "Epoch 630/650\n",
      "122/122 [==============================] - 0s 637us/step - loss: 66.2084 - mae: 5.7362 - val_loss: 36.9193 - val_mae: 4.1782\n",
      "Epoch 631/650\n",
      "122/122 [==============================] - 0s 606us/step - loss: 57.3047 - mae: 5.4670 - val_loss: 34.9273 - val_mae: 4.0086\n",
      "Epoch 632/650\n",
      "122/122 [==============================] - 0s 684us/step - loss: 60.7025 - mae: 5.5433 - val_loss: 36.3293 - val_mae: 4.0757\n",
      "Epoch 633/650\n",
      "122/122 [==============================] - 0s 615us/step - loss: 61.4119 - mae: 5.6363 - val_loss: 38.2205 - val_mae: 4.1661\n",
      "Epoch 634/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 56.6650 - mae: 5.4603 - val_loss: 41.2224 - val_mae: 4.6853\n",
      "Epoch 635/650\n",
      "122/122 [==============================] - 0s 605us/step - loss: 68.7481 - mae: 5.6288 - val_loss: 33.9016 - val_mae: 3.8677\n",
      "Epoch 636/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 61.6522 - mae: 5.5682 - val_loss: 42.8504 - val_mae: 4.8456\n",
      "Epoch 637/650\n",
      "122/122 [==============================] - 0s 641us/step - loss: 60.8157 - mae: 5.6229 - val_loss: 35.6535 - val_mae: 3.9694\n",
      "Epoch 638/650\n",
      "122/122 [==============================] - 0s 638us/step - loss: 60.8422 - mae: 5.5952 - val_loss: 34.9885 - val_mae: 4.2763\n",
      "Epoch 639/650\n",
      "122/122 [==============================] - 0s 603us/step - loss: 59.8835 - mae: 5.5131 - val_loss: 35.9653 - val_mae: 4.2516\n",
      "Epoch 640/650\n",
      "122/122 [==============================] - 0s 596us/step - loss: 61.8812 - mae: 5.5716 - val_loss: 40.3317 - val_mae: 4.6528\n",
      "Epoch 641/650\n",
      "122/122 [==============================] - 0s 678us/step - loss: 61.0913 - mae: 5.5469 - val_loss: 53.2537 - val_mae: 5.3637\n",
      "Epoch 642/650\n",
      "122/122 [==============================] - 0s 611us/step - loss: 66.0243 - mae: 5.8302 - val_loss: 42.9225 - val_mae: 4.8122\n",
      "Epoch 643/650\n",
      "122/122 [==============================] - 0s 600us/step - loss: 63.6443 - mae: 5.7162 - val_loss: 32.2601 - val_mae: 4.0240\n",
      "Epoch 644/650\n",
      "122/122 [==============================] - 0s 585us/step - loss: 59.3029 - mae: 5.5006 - val_loss: 39.9071 - val_mae: 4.6005\n",
      "Epoch 645/650\n",
      "122/122 [==============================] - 0s 609us/step - loss: 60.6940 - mae: 5.6225 - val_loss: 36.7044 - val_mae: 4.3136\n",
      "Epoch 646/650\n",
      "122/122 [==============================] - 0s 664us/step - loss: 64.6281 - mae: 5.5981 - val_loss: 36.6321 - val_mae: 4.4464\n",
      "Epoch 647/650\n",
      "122/122 [==============================] - 0s 617us/step - loss: 57.8743 - mae: 5.5351 - val_loss: 35.1378 - val_mae: 4.1397\n",
      "Epoch 648/650\n",
      "122/122 [==============================] - 0s 593us/step - loss: 58.4562 - mae: 5.5167 - val_loss: 34.2662 - val_mae: 4.0696\n",
      "Epoch 649/650\n",
      "122/122 [==============================] - 0s 622us/step - loss: 60.3284 - mae: 5.5944 - val_loss: 34.4341 - val_mae: 3.9647\n",
      "Epoch 650/650\n",
      "122/122 [==============================] - 0s 625us/step - loss: 66.9985 - mae: 5.6455 - val_loss: 35.0673 - val_mae: 4.0390\n",
      "31/31 [==============================] - 0s 347us/step\n",
      "Epochs: 650 | MAE: 4.038987292010968\n",
      "Training model with 700 epochs\n",
      "Epoch 1/700\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 69273.8281 - mae: 228.5453 - val_loss: 63755.0195 - val_mae: 219.0555\n",
      "Epoch 2/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 47542.1250 - mae: 185.0278 - val_loss: 23330.2930 - val_mae: 130.1381\n",
      "Epoch 3/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 9597.5723 - mae: 77.1186 - val_loss: 3521.0532 - val_mae: 43.9854\n",
      "Epoch 4/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 3377.5271 - mae: 43.9952 - val_loss: 2305.6960 - val_mae: 35.2194\n",
      "Epoch 5/700\n",
      "122/122 [==============================] - 0s 665us/step - loss: 2568.0220 - mae: 38.5164 - val_loss: 1786.6461 - val_mae: 31.0092\n",
      "Epoch 6/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 2250.7461 - mae: 36.5921 - val_loss: 1515.3201 - val_mae: 28.5524\n",
      "Epoch 7/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 1967.4226 - mae: 34.5556 - val_loss: 1338.6909 - val_mae: 27.0654\n",
      "Epoch 8/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 1851.9910 - mae: 33.5411 - val_loss: 1175.8212 - val_mae: 25.2651\n",
      "Epoch 9/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 1773.2557 - mae: 32.4384 - val_loss: 1055.1018 - val_mae: 24.1434\n",
      "Epoch 10/700\n",
      "122/122 [==============================] - 0s 661us/step - loss: 1602.8221 - mae: 31.0263 - val_loss: 934.1267 - val_mae: 22.7576\n",
      "Epoch 11/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 1533.3309 - mae: 30.5827 - val_loss: 849.1403 - val_mae: 21.7079\n",
      "Epoch 12/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 1359.7380 - mae: 28.6506 - val_loss: 769.2082 - val_mae: 20.6552\n",
      "Epoch 13/700\n",
      "122/122 [==============================] - 0s 592us/step - loss: 1344.4869 - mae: 28.5654 - val_loss: 723.8730 - val_mae: 19.9589\n",
      "Epoch 14/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 1286.0540 - mae: 27.7565 - val_loss: 636.5195 - val_mae: 18.7629\n",
      "Epoch 15/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 1187.1328 - mae: 26.6897 - val_loss: 565.0799 - val_mae: 17.7189\n",
      "Epoch 16/700\n",
      "122/122 [==============================] - 0s 625us/step - loss: 1151.8099 - mae: 26.4002 - val_loss: 510.8808 - val_mae: 16.8136\n",
      "Epoch 17/700\n",
      "122/122 [==============================] - 0s 640us/step - loss: 1148.2511 - mae: 26.1373 - val_loss: 454.3962 - val_mae: 15.7499\n",
      "Epoch 18/700\n",
      "122/122 [==============================] - 0s 621us/step - loss: 1019.6130 - mae: 24.7369 - val_loss: 385.3724 - val_mae: 14.6030\n",
      "Epoch 19/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 995.1591 - mae: 24.1935 - val_loss: 367.6478 - val_mae: 14.1975\n",
      "Epoch 20/700\n",
      "122/122 [==============================] - 0s 613us/step - loss: 949.6893 - mae: 23.6219 - val_loss: 320.6676 - val_mae: 13.3106\n",
      "Epoch 21/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 908.6651 - mae: 22.9953 - val_loss: 282.0811 - val_mae: 12.2930\n",
      "Epoch 22/700\n",
      "122/122 [==============================] - 0s 603us/step - loss: 879.1050 - mae: 22.7203 - val_loss: 246.3706 - val_mae: 11.6712\n",
      "Epoch 23/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 861.2232 - mae: 22.1812 - val_loss: 225.5523 - val_mae: 10.9745\n",
      "Epoch 24/700\n",
      "122/122 [==============================] - 0s 613us/step - loss: 770.1641 - mae: 21.1112 - val_loss: 213.6115 - val_mae: 10.5553\n",
      "Epoch 25/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 789.9836 - mae: 21.1992 - val_loss: 167.4387 - val_mae: 9.6775\n",
      "Epoch 26/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 744.9612 - mae: 20.6577 - val_loss: 145.6036 - val_mae: 8.9669\n",
      "Epoch 27/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 697.3861 - mae: 19.7363 - val_loss: 126.6471 - val_mae: 8.6223\n",
      "Epoch 28/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 718.6891 - mae: 20.0080 - val_loss: 106.8302 - val_mae: 7.5111\n",
      "Epoch 29/700\n",
      "122/122 [==============================] - 0s 656us/step - loss: 684.8484 - mae: 19.6053 - val_loss: 98.1623 - val_mae: 7.2740\n",
      "Epoch 30/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 659.0073 - mae: 18.9867 - val_loss: 83.5644 - val_mae: 6.9766\n",
      "Epoch 31/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 645.9690 - mae: 18.7394 - val_loss: 80.1500 - val_mae: 6.6192\n",
      "Epoch 32/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 655.9206 - mae: 18.8373 - val_loss: 72.1334 - val_mae: 6.2009\n",
      "Epoch 33/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 625.9402 - mae: 18.4700 - val_loss: 63.9031 - val_mae: 5.9908\n",
      "Epoch 34/700\n",
      "122/122 [==============================] - 0s 621us/step - loss: 576.3107 - mae: 17.6670 - val_loss: 54.7369 - val_mae: 5.6813\n",
      "Epoch 35/700\n",
      "122/122 [==============================] - 0s 608us/step - loss: 595.9695 - mae: 17.8845 - val_loss: 56.9496 - val_mae: 5.5048\n",
      "Epoch 36/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 553.6573 - mae: 17.3254 - val_loss: 49.5983 - val_mae: 5.1740\n",
      "Epoch 37/700\n",
      "122/122 [==============================] - 0s 582us/step - loss: 580.4871 - mae: 17.5548 - val_loss: 42.5660 - val_mae: 4.7784\n",
      "Epoch 38/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 571.2655 - mae: 17.3043 - val_loss: 39.6978 - val_mae: 4.6931\n",
      "Epoch 39/700\n",
      "122/122 [==============================] - 0s 600us/step - loss: 549.6613 - mae: 16.9416 - val_loss: 39.6699 - val_mae: 4.8255\n",
      "Epoch 40/700\n",
      "122/122 [==============================] - 0s 639us/step - loss: 571.8515 - mae: 17.4294 - val_loss: 38.0811 - val_mae: 4.5838\n",
      "Epoch 41/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 522.0481 - mae: 16.5168 - val_loss: 42.0197 - val_mae: 4.7744\n",
      "Epoch 42/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 553.3654 - mae: 17.0376 - val_loss: 36.6409 - val_mae: 4.5766\n",
      "Epoch 43/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 512.2834 - mae: 16.2300 - val_loss: 40.4900 - val_mae: 4.5792\n",
      "Epoch 44/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 513.8282 - mae: 16.2389 - val_loss: 30.3375 - val_mae: 3.9376\n",
      "Epoch 45/700\n",
      "122/122 [==============================] - 0s 632us/step - loss: 509.7394 - mae: 16.2169 - val_loss: 30.2989 - val_mae: 3.9326\n",
      "Epoch 46/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 512.9160 - mae: 16.1161 - val_loss: 41.0258 - val_mae: 4.8735\n",
      "Epoch 47/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 555.2278 - mae: 16.6849 - val_loss: 37.9295 - val_mae: 4.6469\n",
      "Epoch 48/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 539.8259 - mae: 16.5120 - val_loss: 25.4033 - val_mae: 3.6651\n",
      "Epoch 49/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 483.0021 - mae: 15.5247 - val_loss: 34.8661 - val_mae: 4.4999\n",
      "Epoch 50/700\n",
      "122/122 [==============================] - 0s 621us/step - loss: 502.5144 - mae: 15.7878 - val_loss: 36.6935 - val_mae: 4.6802\n",
      "Epoch 51/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 476.1412 - mae: 15.6469 - val_loss: 27.3463 - val_mae: 3.7871\n",
      "Epoch 52/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 496.5267 - mae: 15.6419 - val_loss: 23.6496 - val_mae: 3.4545\n",
      "Epoch 53/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 468.9279 - mae: 15.3151 - val_loss: 28.1354 - val_mae: 4.0407\n",
      "Epoch 54/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 475.8392 - mae: 15.4838 - val_loss: 26.3892 - val_mae: 3.8362\n",
      "Epoch 55/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 488.6109 - mae: 15.6282 - val_loss: 32.8673 - val_mae: 4.3282\n",
      "Epoch 56/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 481.9396 - mae: 15.4301 - val_loss: 35.0909 - val_mae: 4.2331\n",
      "Epoch 57/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 460.6758 - mae: 14.9515 - val_loss: 39.2966 - val_mae: 4.8263\n",
      "Epoch 58/700\n",
      "122/122 [==============================] - 0s 677us/step - loss: 460.0647 - mae: 15.1102 - val_loss: 44.3214 - val_mae: 5.3415\n",
      "Epoch 59/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 442.0742 - mae: 14.8349 - val_loss: 33.7668 - val_mae: 4.2246\n",
      "Epoch 60/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 489.5390 - mae: 15.4621 - val_loss: 23.5170 - val_mae: 3.5036\n",
      "Epoch 61/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 458.0453 - mae: 14.9990 - val_loss: 33.3445 - val_mae: 4.3528\n",
      "Epoch 62/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 446.2752 - mae: 14.7515 - val_loss: 30.6703 - val_mae: 4.1050\n",
      "Epoch 63/700\n",
      "122/122 [==============================] - 0s 655us/step - loss: 443.5417 - mae: 14.7647 - val_loss: 25.5679 - val_mae: 3.6386\n",
      "Epoch 64/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 459.4714 - mae: 14.9035 - val_loss: 41.1480 - val_mae: 5.0095\n",
      "Epoch 65/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 480.4763 - mae: 15.0341 - val_loss: 28.9700 - val_mae: 4.0672\n",
      "Epoch 66/700\n",
      "122/122 [==============================] - 0s 684us/step - loss: 440.6838 - mae: 14.6210 - val_loss: 22.3050 - val_mae: 3.3906\n",
      "Epoch 67/700\n",
      "122/122 [==============================] - 0s 659us/step - loss: 461.7737 - mae: 15.0209 - val_loss: 29.7215 - val_mae: 4.0580\n",
      "Epoch 68/700\n",
      "122/122 [==============================] - 0s 592us/step - loss: 462.0474 - mae: 14.9032 - val_loss: 26.4579 - val_mae: 3.8536\n",
      "Epoch 69/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 434.7832 - mae: 14.6843 - val_loss: 31.5517 - val_mae: 4.2992\n",
      "Epoch 70/700\n",
      "122/122 [==============================] - 0s 622us/step - loss: 441.4832 - mae: 14.7171 - val_loss: 31.4111 - val_mae: 4.3803\n",
      "Epoch 71/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 404.0988 - mae: 14.1618 - val_loss: 36.1688 - val_mae: 4.5781\n",
      "Epoch 72/700\n",
      "122/122 [==============================] - 0s 603us/step - loss: 453.8272 - mae: 14.6280 - val_loss: 30.1813 - val_mae: 4.1541\n",
      "Epoch 73/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 464.0448 - mae: 14.8368 - val_loss: 23.7102 - val_mae: 3.4074\n",
      "Epoch 74/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 437.2763 - mae: 14.4881 - val_loss: 37.0607 - val_mae: 4.6792\n",
      "Epoch 75/700\n",
      "122/122 [==============================] - 0s 669us/step - loss: 438.5762 - mae: 14.4959 - val_loss: 23.9764 - val_mae: 3.4585\n",
      "Epoch 76/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 427.7991 - mae: 14.4044 - val_loss: 25.2264 - val_mae: 3.7986\n",
      "Epoch 77/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 434.6835 - mae: 14.3844 - val_loss: 23.6447 - val_mae: 3.4741\n",
      "Epoch 78/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 426.3351 - mae: 14.2261 - val_loss: 32.0268 - val_mae: 4.2276\n",
      "Epoch 79/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 433.7066 - mae: 14.2465 - val_loss: 28.3736 - val_mae: 3.9844\n",
      "Epoch 80/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 424.6240 - mae: 14.1379 - val_loss: 36.3023 - val_mae: 4.8181\n",
      "Epoch 81/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 436.6196 - mae: 14.1227 - val_loss: 27.9435 - val_mae: 3.9170\n",
      "Epoch 82/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 404.9779 - mae: 13.9343 - val_loss: 33.6442 - val_mae: 4.6436\n",
      "Epoch 83/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 442.2526 - mae: 14.2665 - val_loss: 27.9113 - val_mae: 3.8916\n",
      "Epoch 84/700\n",
      "122/122 [==============================] - 0s 566us/step - loss: 415.4223 - mae: 14.0543 - val_loss: 23.4386 - val_mae: 3.4668\n",
      "Epoch 85/700\n",
      "122/122 [==============================] - 0s 665us/step - loss: 422.9167 - mae: 13.9552 - val_loss: 31.9805 - val_mae: 4.3274\n",
      "Epoch 86/700\n",
      "122/122 [==============================] - 0s 600us/step - loss: 388.4447 - mae: 13.6474 - val_loss: 27.8702 - val_mae: 4.0684\n",
      "Epoch 87/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 381.3297 - mae: 13.4717 - val_loss: 23.6262 - val_mae: 3.4767\n",
      "Epoch 88/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 413.7865 - mae: 14.0265 - val_loss: 27.3782 - val_mae: 3.8155\n",
      "Epoch 89/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 387.2125 - mae: 13.6501 - val_loss: 28.6041 - val_mae: 4.0633\n",
      "Epoch 90/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 394.5580 - mae: 13.7275 - val_loss: 23.1292 - val_mae: 3.4784\n",
      "Epoch 91/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 429.1110 - mae: 14.0200 - val_loss: 25.9353 - val_mae: 3.7483\n",
      "Epoch 92/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 405.2260 - mae: 13.6297 - val_loss: 26.6116 - val_mae: 3.7946\n",
      "Epoch 93/700\n",
      "122/122 [==============================] - 0s 629us/step - loss: 370.5743 - mae: 13.2171 - val_loss: 24.0867 - val_mae: 3.4812\n",
      "Epoch 94/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 382.3378 - mae: 13.3966 - val_loss: 34.7352 - val_mae: 4.5299\n",
      "Epoch 95/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 409.3850 - mae: 13.8778 - val_loss: 40.8497 - val_mae: 4.8841\n",
      "Epoch 96/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 403.1768 - mae: 13.6549 - val_loss: 24.1789 - val_mae: 3.5476\n",
      "Epoch 97/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 396.2299 - mae: 13.5799 - val_loss: 27.2280 - val_mae: 3.8504\n",
      "Epoch 98/700\n",
      "122/122 [==============================] - 0s 643us/step - loss: 388.0131 - mae: 13.5877 - val_loss: 27.0004 - val_mae: 3.8361\n",
      "Epoch 99/700\n",
      "122/122 [==============================] - 0s 600us/step - loss: 406.8206 - mae: 13.5837 - val_loss: 30.8793 - val_mae: 4.0576\n",
      "Epoch 100/700\n",
      "122/122 [==============================] - 0s 563us/step - loss: 383.8061 - mae: 13.3001 - val_loss: 23.5765 - val_mae: 3.5287\n",
      "Epoch 101/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 388.1124 - mae: 13.2792 - val_loss: 37.8277 - val_mae: 4.5256\n",
      "Epoch 102/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 355.3934 - mae: 12.8907 - val_loss: 24.2590 - val_mae: 3.4654\n",
      "Epoch 103/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 391.0836 - mae: 13.3508 - val_loss: 39.1961 - val_mae: 4.7529\n",
      "Epoch 104/700\n",
      "122/122 [==============================] - 0s 627us/step - loss: 389.4821 - mae: 13.3418 - val_loss: 25.3854 - val_mae: 3.6587\n",
      "Epoch 105/700\n",
      "122/122 [==============================] - 0s 599us/step - loss: 370.4696 - mae: 13.1033 - val_loss: 26.7277 - val_mae: 3.6827\n",
      "Epoch 106/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 361.7527 - mae: 13.0676 - val_loss: 31.2278 - val_mae: 4.2105\n",
      "Epoch 107/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 377.3321 - mae: 13.1581 - val_loss: 31.1450 - val_mae: 4.1597\n",
      "Epoch 108/700\n",
      "122/122 [==============================] - 0s 699us/step - loss: 375.8438 - mae: 13.1081 - val_loss: 24.6667 - val_mae: 3.6315\n",
      "Epoch 109/700\n",
      "122/122 [==============================] - 0s 625us/step - loss: 362.5816 - mae: 13.0885 - val_loss: 28.8979 - val_mae: 4.0646\n",
      "Epoch 110/700\n",
      "122/122 [==============================] - 0s 599us/step - loss: 367.4222 - mae: 12.8615 - val_loss: 26.6069 - val_mae: 3.8596\n",
      "Epoch 111/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 364.3676 - mae: 12.9485 - val_loss: 28.1433 - val_mae: 3.9437\n",
      "Epoch 112/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 349.0183 - mae: 12.5243 - val_loss: 28.2451 - val_mae: 3.9773\n",
      "Epoch 113/700\n",
      "122/122 [==============================] - 0s 663us/step - loss: 358.4021 - mae: 12.8138 - val_loss: 27.5471 - val_mae: 3.8096\n",
      "Epoch 114/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 375.3654 - mae: 12.9337 - val_loss: 31.4612 - val_mae: 4.3684\n",
      "Epoch 115/700\n",
      "122/122 [==============================] - 0s 603us/step - loss: 333.4972 - mae: 12.4924 - val_loss: 26.9941 - val_mae: 3.8040\n",
      "Epoch 116/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 345.2328 - mae: 12.6033 - val_loss: 29.2948 - val_mae: 4.2095\n",
      "Epoch 117/700\n",
      "122/122 [==============================] - 0s 629us/step - loss: 329.5360 - mae: 12.2321 - val_loss: 24.2006 - val_mae: 3.4506\n",
      "Epoch 118/700\n",
      "122/122 [==============================] - 0s 596us/step - loss: 333.3061 - mae: 12.3222 - val_loss: 23.0238 - val_mae: 3.4250\n",
      "Epoch 119/700\n",
      "122/122 [==============================] - 0s 600us/step - loss: 360.4907 - mae: 12.6401 - val_loss: 29.3488 - val_mae: 4.1433\n",
      "Epoch 120/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 337.0312 - mae: 12.1987 - val_loss: 25.9876 - val_mae: 3.6697\n",
      "Epoch 121/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 346.7804 - mae: 12.5601 - val_loss: 28.1701 - val_mae: 3.8988\n",
      "Epoch 122/700\n",
      "122/122 [==============================] - 0s 651us/step - loss: 353.3597 - mae: 12.5745 - val_loss: 36.1132 - val_mae: 4.6710\n",
      "Epoch 123/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 361.9298 - mae: 12.5575 - val_loss: 26.7832 - val_mae: 3.7454\n",
      "Epoch 124/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 327.1017 - mae: 12.0689 - val_loss: 23.1436 - val_mae: 3.4876\n",
      "Epoch 125/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 322.1955 - mae: 12.0696 - val_loss: 23.9288 - val_mae: 3.4449\n",
      "Epoch 126/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 333.1770 - mae: 12.0263 - val_loss: 41.4306 - val_mae: 4.9128\n",
      "Epoch 127/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 332.6096 - mae: 12.2903 - val_loss: 25.1863 - val_mae: 3.5952\n",
      "Epoch 128/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 331.1336 - mae: 12.1182 - val_loss: 28.0793 - val_mae: 3.8111\n",
      "Epoch 129/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 315.4506 - mae: 11.8479 - val_loss: 25.1682 - val_mae: 3.4598\n",
      "Epoch 130/700\n",
      "122/122 [==============================] - 0s 646us/step - loss: 305.7570 - mae: 11.7469 - val_loss: 25.4018 - val_mae: 3.6766\n",
      "Epoch 131/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 314.3979 - mae: 11.9555 - val_loss: 25.2788 - val_mae: 3.6843\n",
      "Epoch 132/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 331.1350 - mae: 12.1140 - val_loss: 26.6571 - val_mae: 3.7574\n",
      "Epoch 133/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 330.7377 - mae: 12.0061 - val_loss: 36.3544 - val_mae: 4.6202\n",
      "Epoch 134/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 311.8758 - mae: 11.9591 - val_loss: 27.9414 - val_mae: 3.9346\n",
      "Epoch 135/700\n",
      "122/122 [==============================] - 0s 625us/step - loss: 335.8974 - mae: 12.1466 - val_loss: 24.7158 - val_mae: 3.5942\n",
      "Epoch 136/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 323.4735 - mae: 11.8780 - val_loss: 25.0068 - val_mae: 3.5382\n",
      "Epoch 137/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 293.1495 - mae: 11.6099 - val_loss: 22.7211 - val_mae: 3.4040\n",
      "Epoch 138/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 306.4267 - mae: 11.6516 - val_loss: 24.4620 - val_mae: 3.4869\n",
      "Epoch 139/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 305.4426 - mae: 11.6049 - val_loss: 22.9232 - val_mae: 3.4022\n",
      "Epoch 140/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 317.7272 - mae: 11.8223 - val_loss: 28.7585 - val_mae: 4.0595\n",
      "Epoch 141/700\n",
      "122/122 [==============================] - 0s 622us/step - loss: 316.3926 - mae: 11.6781 - val_loss: 27.9362 - val_mae: 3.8658\n",
      "Epoch 142/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 294.8907 - mae: 11.4506 - val_loss: 26.3538 - val_mae: 3.8471\n",
      "Epoch 143/700\n",
      "122/122 [==============================] - 0s 583us/step - loss: 310.6652 - mae: 11.6145 - val_loss: 22.8948 - val_mae: 3.4237\n",
      "Epoch 144/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 300.4799 - mae: 11.4430 - val_loss: 29.5207 - val_mae: 3.7456\n",
      "Epoch 145/700\n",
      "122/122 [==============================] - 0s 692us/step - loss: 307.3795 - mae: 11.4217 - val_loss: 25.3599 - val_mae: 3.6174\n",
      "Epoch 146/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 286.0706 - mae: 11.2519 - val_loss: 32.3882 - val_mae: 4.1669\n",
      "Epoch 147/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 313.6201 - mae: 11.6434 - val_loss: 22.3265 - val_mae: 3.3408\n",
      "Epoch 148/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 302.0997 - mae: 11.5036 - val_loss: 28.3166 - val_mae: 3.8741\n",
      "Epoch 149/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 279.9825 - mae: 10.9610 - val_loss: 35.8422 - val_mae: 4.5161\n",
      "Epoch 150/700\n",
      "122/122 [==============================] - 0s 596us/step - loss: 308.5602 - mae: 11.4760 - val_loss: 28.8626 - val_mae: 3.8665\n",
      "Epoch 151/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 297.8641 - mae: 11.2975 - val_loss: 26.2415 - val_mae: 3.6965\n",
      "Epoch 152/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 280.7942 - mae: 11.2170 - val_loss: 27.2487 - val_mae: 3.9323\n",
      "Epoch 153/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 290.2646 - mae: 11.1768 - val_loss: 40.6247 - val_mae: 4.8688\n",
      "Epoch 154/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 298.4298 - mae: 11.3959 - val_loss: 29.4747 - val_mae: 3.8319\n",
      "Epoch 155/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 276.5629 - mae: 10.7291 - val_loss: 32.1390 - val_mae: 4.1351\n",
      "Epoch 156/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 271.2573 - mae: 11.0553 - val_loss: 27.9855 - val_mae: 3.8806\n",
      "Epoch 157/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 262.2459 - mae: 10.8683 - val_loss: 26.7446 - val_mae: 3.6996\n",
      "Epoch 158/700\n",
      "122/122 [==============================] - 0s 630us/step - loss: 278.4071 - mae: 11.1026 - val_loss: 26.6334 - val_mae: 3.7745\n",
      "Epoch 159/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 284.6806 - mae: 11.0492 - val_loss: 30.8920 - val_mae: 4.2045\n",
      "Epoch 160/700\n",
      "122/122 [==============================] - 0s 626us/step - loss: 289.6069 - mae: 11.1774 - val_loss: 29.1489 - val_mae: 3.9756\n",
      "Epoch 161/700\n",
      "122/122 [==============================] - 0s 629us/step - loss: 265.8770 - mae: 10.7220 - val_loss: 25.1291 - val_mae: 3.5745\n",
      "Epoch 162/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 271.1802 - mae: 10.8283 - val_loss: 27.5497 - val_mae: 3.8227\n",
      "Epoch 163/700\n",
      "122/122 [==============================] - 0s 643us/step - loss: 248.8328 - mae: 10.6042 - val_loss: 27.1640 - val_mae: 3.7517\n",
      "Epoch 164/700\n",
      "122/122 [==============================] - 0s 627us/step - loss: 278.8141 - mae: 10.9200 - val_loss: 40.3098 - val_mae: 4.7269\n",
      "Epoch 165/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 270.6912 - mae: 10.7668 - val_loss: 26.6502 - val_mae: 3.7841\n",
      "Epoch 166/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 264.3023 - mae: 10.5773 - val_loss: 26.8233 - val_mae: 3.6109\n",
      "Epoch 167/700\n",
      "122/122 [==============================] - 0s 596us/step - loss: 278.8385 - mae: 10.6521 - val_loss: 25.8660 - val_mae: 3.6773\n",
      "Epoch 168/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 257.5866 - mae: 10.4997 - val_loss: 33.1291 - val_mae: 4.1778\n",
      "Epoch 169/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 257.7904 - mae: 10.6694 - val_loss: 40.6587 - val_mae: 5.0571\n",
      "Epoch 170/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 250.2490 - mae: 10.4488 - val_loss: 35.5892 - val_mae: 4.5661\n",
      "Epoch 171/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 247.0428 - mae: 10.3734 - val_loss: 25.2322 - val_mae: 3.6677\n",
      "Epoch 172/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 255.4577 - mae: 10.3127 - val_loss: 28.4824 - val_mae: 4.0423\n",
      "Epoch 173/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 257.2369 - mae: 10.6062 - val_loss: 31.3911 - val_mae: 4.1587\n",
      "Epoch 174/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 264.8211 - mae: 10.5286 - val_loss: 33.6496 - val_mae: 4.3481\n",
      "Epoch 175/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 263.9099 - mae: 10.4721 - val_loss: 28.3367 - val_mae: 3.8812\n",
      "Epoch 176/700\n",
      "122/122 [==============================] - 0s 625us/step - loss: 256.7467 - mae: 10.4908 - val_loss: 24.2093 - val_mae: 3.4954\n",
      "Epoch 177/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 227.3336 - mae: 9.8654 - val_loss: 39.1994 - val_mae: 4.7735\n",
      "Epoch 178/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 251.0429 - mae: 10.2160 - val_loss: 23.8528 - val_mae: 3.4996\n",
      "Epoch 179/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 266.1792 - mae: 10.5137 - val_loss: 23.9930 - val_mae: 3.4459\n",
      "Epoch 180/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 254.5711 - mae: 10.3982 - val_loss: 26.6556 - val_mae: 3.7886\n",
      "Epoch 181/700\n",
      "122/122 [==============================] - 0s 682us/step - loss: 232.2302 - mae: 9.9251 - val_loss: 27.3668 - val_mae: 3.8161\n",
      "Epoch 182/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 229.8677 - mae: 9.8353 - val_loss: 37.0793 - val_mae: 4.7043\n",
      "Epoch 183/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 234.1283 - mae: 10.0443 - val_loss: 24.6593 - val_mae: 3.5609\n",
      "Epoch 184/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 237.6062 - mae: 10.0875 - val_loss: 25.8010 - val_mae: 3.6705\n",
      "Epoch 185/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 266.3892 - mae: 10.4493 - val_loss: 41.6421 - val_mae: 4.9141\n",
      "Epoch 186/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 244.4773 - mae: 10.1650 - val_loss: 30.7863 - val_mae: 4.1072\n",
      "Epoch 187/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 233.1454 - mae: 10.0642 - val_loss: 25.5752 - val_mae: 3.7314\n",
      "Epoch 188/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 233.9140 - mae: 10.0271 - val_loss: 24.8988 - val_mae: 3.6688\n",
      "Epoch 189/700\n",
      "122/122 [==============================] - 0s 794us/step - loss: 254.8161 - mae: 10.3274 - val_loss: 27.7841 - val_mae: 3.9135\n",
      "Epoch 190/700\n",
      "122/122 [==============================] - 0s 998us/step - loss: 243.5388 - mae: 10.1123 - val_loss: 24.9421 - val_mae: 3.5583\n",
      "Epoch 191/700\n",
      "122/122 [==============================] - 0s 901us/step - loss: 246.0061 - mae: 10.1011 - val_loss: 31.7941 - val_mae: 4.2672\n",
      "Epoch 192/700\n",
      "122/122 [==============================] - 0s 925us/step - loss: 223.8561 - mae: 9.8828 - val_loss: 26.9760 - val_mae: 3.7720\n",
      "Epoch 193/700\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 222.2764 - mae: 9.6342 - val_loss: 25.5516 - val_mae: 3.6494\n",
      "Epoch 194/700\n",
      "122/122 [==============================] - 0s 706us/step - loss: 221.5730 - mae: 9.6806 - val_loss: 27.9703 - val_mae: 3.7635\n",
      "Epoch 195/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 228.3369 - mae: 9.9196 - val_loss: 32.0220 - val_mae: 4.2588\n",
      "Epoch 196/700\n",
      "122/122 [==============================] - 0s 718us/step - loss: 224.9017 - mae: 9.6046 - val_loss: 27.2397 - val_mae: 3.8009\n",
      "Epoch 197/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 236.3140 - mae: 9.8363 - val_loss: 24.7707 - val_mae: 3.6149\n",
      "Epoch 198/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 247.6997 - mae: 10.1122 - val_loss: 31.2349 - val_mae: 4.2123\n",
      "Epoch 199/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 222.7418 - mae: 9.7080 - val_loss: 27.9733 - val_mae: 3.9164\n",
      "Epoch 200/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 220.5063 - mae: 9.7661 - val_loss: 24.2866 - val_mae: 3.5057\n",
      "Epoch 201/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 218.9667 - mae: 9.6764 - val_loss: 27.1120 - val_mae: 3.7310\n",
      "Epoch 202/700\n",
      "122/122 [==============================] - 0s 630us/step - loss: 223.1114 - mae: 9.6489 - val_loss: 32.5011 - val_mae: 4.1445\n",
      "Epoch 203/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 203.3927 - mae: 9.3523 - val_loss: 24.6799 - val_mae: 3.5059\n",
      "Epoch 204/700\n",
      "122/122 [==============================] - 0s 560us/step - loss: 219.5759 - mae: 9.5211 - val_loss: 34.9440 - val_mae: 4.5912\n",
      "Epoch 205/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 201.5273 - mae: 9.2690 - val_loss: 24.9114 - val_mae: 3.5611\n",
      "Epoch 206/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 193.6892 - mae: 9.2250 - val_loss: 24.1355 - val_mae: 3.4996\n",
      "Epoch 207/700\n",
      "122/122 [==============================] - 0s 784us/step - loss: 204.8620 - mae: 9.3403 - val_loss: 27.9467 - val_mae: 3.9418\n",
      "Epoch 208/700\n",
      "122/122 [==============================] - 0s 664us/step - loss: 206.7113 - mae: 9.4565 - val_loss: 28.2769 - val_mae: 3.9004\n",
      "Epoch 209/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 215.2766 - mae: 9.6033 - val_loss: 30.2980 - val_mae: 3.9998\n",
      "Epoch 210/700\n",
      "122/122 [==============================] - 0s 621us/step - loss: 221.3827 - mae: 9.5240 - val_loss: 28.7422 - val_mae: 4.0469\n",
      "Epoch 211/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 222.7849 - mae: 9.5708 - val_loss: 26.9663 - val_mae: 3.7098\n",
      "Epoch 212/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 202.3415 - mae: 9.3552 - val_loss: 29.6897 - val_mae: 4.0303\n",
      "Epoch 213/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 217.5092 - mae: 9.6192 - val_loss: 27.5638 - val_mae: 3.6634\n",
      "Epoch 214/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 213.0785 - mae: 9.3866 - val_loss: 24.1312 - val_mae: 3.4654\n",
      "Epoch 215/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 211.4452 - mae: 9.3276 - val_loss: 32.6163 - val_mae: 4.2974\n",
      "Epoch 216/700\n",
      "122/122 [==============================] - 0s 599us/step - loss: 198.9760 - mae: 9.1762 - val_loss: 28.0756 - val_mae: 3.7698\n",
      "Epoch 217/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 210.6794 - mae: 9.5515 - val_loss: 27.8629 - val_mae: 3.7136\n",
      "Epoch 218/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 203.3946 - mae: 9.1575 - val_loss: 26.2879 - val_mae: 3.6896\n",
      "Epoch 219/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 200.0739 - mae: 9.0961 - val_loss: 26.9706 - val_mae: 3.6855\n",
      "Epoch 220/700\n",
      "122/122 [==============================] - 0s 563us/step - loss: 191.8773 - mae: 9.0142 - val_loss: 26.4385 - val_mae: 3.5968\n",
      "Epoch 221/700\n",
      "122/122 [==============================] - 0s 683us/step - loss: 217.5601 - mae: 9.3236 - val_loss: 36.5325 - val_mae: 4.6545\n",
      "Epoch 222/700\n",
      "122/122 [==============================] - 0s 599us/step - loss: 194.9658 - mae: 9.0066 - val_loss: 26.7367 - val_mae: 3.6418\n",
      "Epoch 223/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 201.5680 - mae: 9.1863 - val_loss: 28.3668 - val_mae: 3.8041\n",
      "Epoch 224/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 192.0850 - mae: 9.0855 - val_loss: 25.0935 - val_mae: 3.5251\n",
      "Epoch 225/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 175.2417 - mae: 8.7376 - val_loss: 23.0588 - val_mae: 3.4169\n",
      "Epoch 226/700\n",
      "122/122 [==============================] - 0s 654us/step - loss: 202.0384 - mae: 9.2529 - val_loss: 31.1946 - val_mae: 4.2277\n",
      "Epoch 227/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 196.1198 - mae: 9.0123 - val_loss: 31.8779 - val_mae: 4.3020\n",
      "Epoch 228/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 203.3826 - mae: 9.2255 - val_loss: 23.1503 - val_mae: 3.5172\n",
      "Epoch 229/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 193.5934 - mae: 9.0165 - val_loss: 31.3399 - val_mae: 4.2269\n",
      "Epoch 230/700\n",
      "122/122 [==============================] - 0s 656us/step - loss: 188.7383 - mae: 9.0494 - val_loss: 27.8559 - val_mae: 3.7747\n",
      "Epoch 231/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 181.3641 - mae: 8.8606 - val_loss: 25.4320 - val_mae: 3.5868\n",
      "Epoch 232/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 171.6278 - mae: 8.5832 - val_loss: 31.6411 - val_mae: 4.2643\n",
      "Epoch 233/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 174.6830 - mae: 8.7291 - val_loss: 26.5837 - val_mae: 3.6699\n",
      "Epoch 234/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 171.1208 - mae: 8.5833 - val_loss: 30.1372 - val_mae: 4.1720\n",
      "Epoch 235/700\n",
      "122/122 [==============================] - 0s 648us/step - loss: 182.3596 - mae: 8.9074 - val_loss: 39.8837 - val_mae: 4.8615\n",
      "Epoch 236/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 164.1718 - mae: 8.5090 - val_loss: 29.8133 - val_mae: 4.0180\n",
      "Epoch 237/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 165.1301 - mae: 8.5496 - val_loss: 28.8247 - val_mae: 3.8290\n",
      "Epoch 238/700\n",
      "122/122 [==============================] - 0s 583us/step - loss: 174.6222 - mae: 8.7749 - val_loss: 34.4127 - val_mae: 4.4227\n",
      "Epoch 239/700\n",
      "122/122 [==============================] - 0s 669us/step - loss: 170.4507 - mae: 8.5036 - val_loss: 34.7709 - val_mae: 4.4410\n",
      "Epoch 240/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 167.6261 - mae: 8.6043 - val_loss: 28.1061 - val_mae: 4.0181\n",
      "Epoch 241/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 171.4619 - mae: 8.5967 - val_loss: 26.2409 - val_mae: 3.6863\n",
      "Epoch 242/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 162.5057 - mae: 8.3936 - val_loss: 35.2895 - val_mae: 4.6378\n",
      "Epoch 243/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 175.8448 - mae: 8.6643 - val_loss: 28.6146 - val_mae: 3.9520\n",
      "Epoch 244/700\n",
      "122/122 [==============================] - 0s 634us/step - loss: 157.5574 - mae: 8.2337 - val_loss: 25.5242 - val_mae: 3.6164\n",
      "Epoch 245/700\n",
      "122/122 [==============================] - 0s 614us/step - loss: 166.5597 - mae: 8.4746 - val_loss: 34.4461 - val_mae: 4.4932\n",
      "Epoch 246/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 167.9771 - mae: 8.4181 - val_loss: 37.9531 - val_mae: 4.5930\n",
      "Epoch 247/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 158.0995 - mae: 8.2674 - val_loss: 51.6826 - val_mae: 5.5119\n",
      "Epoch 248/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 161.0121 - mae: 8.2826 - val_loss: 66.5238 - val_mae: 6.1148\n",
      "Epoch 249/700\n",
      "122/122 [==============================] - 0s 665us/step - loss: 163.4048 - mae: 8.5061 - val_loss: 68.1831 - val_mae: 6.3019\n",
      "Epoch 250/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 158.5958 - mae: 8.3320 - val_loss: 57.2333 - val_mae: 5.7005\n",
      "Epoch 251/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 155.1610 - mae: 8.2455 - val_loss: 55.4561 - val_mae: 5.6299\n",
      "Epoch 252/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 160.9926 - mae: 8.3434 - val_loss: 62.3549 - val_mae: 5.9444\n",
      "Epoch 253/700\n",
      "122/122 [==============================] - 0s 599us/step - loss: 154.5505 - mae: 8.1555 - val_loss: 41.1573 - val_mae: 4.8693\n",
      "Epoch 254/700\n",
      "122/122 [==============================] - 0s 693us/step - loss: 139.7209 - mae: 7.9877 - val_loss: 45.3559 - val_mae: 5.1074\n",
      "Epoch 255/700\n",
      "122/122 [==============================] - 0s 619us/step - loss: 144.0992 - mae: 7.9860 - val_loss: 60.3878 - val_mae: 5.8478\n",
      "Epoch 256/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 139.3930 - mae: 7.8726 - val_loss: 39.6888 - val_mae: 4.8395\n",
      "Epoch 257/700\n",
      "122/122 [==============================] - 0s 638us/step - loss: 134.8139 - mae: 7.8627 - val_loss: 56.0421 - val_mae: 5.6310\n",
      "Epoch 258/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 136.3756 - mae: 7.7537 - val_loss: 41.7363 - val_mae: 4.9232\n",
      "Epoch 259/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 139.2691 - mae: 7.8080 - val_loss: 64.0770 - val_mae: 6.1622\n",
      "Epoch 260/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 144.2076 - mae: 7.9808 - val_loss: 58.7813 - val_mae: 5.7662\n",
      "Epoch 261/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 137.4818 - mae: 7.8979 - val_loss: 76.7962 - val_mae: 6.3938\n",
      "Epoch 262/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 133.7218 - mae: 7.7338 - val_loss: 46.0912 - val_mae: 5.2169\n",
      "Epoch 263/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 135.6884 - mae: 7.7297 - val_loss: 80.6343 - val_mae: 6.4746\n",
      "Epoch 264/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 124.6169 - mae: 7.5777 - val_loss: 82.8772 - val_mae: 6.6918\n",
      "Epoch 265/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 129.5251 - mae: 7.7693 - val_loss: 129.3947 - val_mae: 7.9732\n",
      "Epoch 266/700\n",
      "122/122 [==============================] - 0s 651us/step - loss: 139.1019 - mae: 7.8224 - val_loss: 44.8295 - val_mae: 5.0201\n",
      "Epoch 267/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 130.8138 - mae: 7.6632 - val_loss: 66.6200 - val_mae: 6.1350\n",
      "Epoch 268/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 131.5452 - mae: 7.6566 - val_loss: 81.0647 - val_mae: 6.5292\n",
      "Epoch 269/700\n",
      "122/122 [==============================] - 0s 592us/step - loss: 125.1827 - mae: 7.4941 - val_loss: 125.0375 - val_mae: 7.7963\n",
      "Epoch 270/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 132.0994 - mae: 7.6906 - val_loss: 65.7018 - val_mae: 6.1413\n",
      "Epoch 271/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 129.5889 - mae: 7.6264 - val_loss: 66.1524 - val_mae: 6.0926\n",
      "Epoch 272/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 123.6331 - mae: 7.5144 - val_loss: 59.6760 - val_mae: 5.7817\n",
      "Epoch 273/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 124.2194 - mae: 7.5032 - val_loss: 107.9074 - val_mae: 7.2360\n",
      "Epoch 274/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 118.8792 - mae: 7.4181 - val_loss: 60.4727 - val_mae: 5.7359\n",
      "Epoch 275/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 117.1284 - mae: 7.2835 - val_loss: 72.5384 - val_mae: 6.2812\n",
      "Epoch 276/700\n",
      "122/122 [==============================] - 0s 627us/step - loss: 122.1639 - mae: 7.3485 - val_loss: 63.5010 - val_mae: 5.9104\n",
      "Epoch 277/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 118.7326 - mae: 7.4191 - val_loss: 79.2604 - val_mae: 6.5085\n",
      "Epoch 278/700\n",
      "122/122 [==============================] - 0s 565us/step - loss: 122.8949 - mae: 7.5116 - val_loss: 112.1333 - val_mae: 7.4203\n",
      "Epoch 279/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 122.4997 - mae: 7.5161 - val_loss: 61.1935 - val_mae: 5.9180\n",
      "Epoch 280/700\n",
      "122/122 [==============================] - 0s 625us/step - loss: 110.2039 - mae: 7.1885 - val_loss: 92.3073 - val_mae: 6.8987\n",
      "Epoch 281/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 115.9369 - mae: 7.3997 - val_loss: 90.2257 - val_mae: 6.9304\n",
      "Epoch 282/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 110.2058 - mae: 7.0656 - val_loss: 83.6680 - val_mae: 6.7046\n",
      "Epoch 283/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 115.2460 - mae: 7.2252 - val_loss: 116.0351 - val_mae: 7.5729\n",
      "Epoch 284/700\n",
      "122/122 [==============================] - 0s 619us/step - loss: 109.8444 - mae: 7.2220 - val_loss: 143.0747 - val_mae: 8.1392\n",
      "Epoch 285/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 107.0286 - mae: 7.1133 - val_loss: 117.4797 - val_mae: 7.6953\n",
      "Epoch 286/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 118.3510 - mae: 7.3912 - val_loss: 137.3409 - val_mae: 8.1223\n",
      "Epoch 287/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 112.3834 - mae: 7.2569 - val_loss: 149.0463 - val_mae: 8.3662\n",
      "Epoch 288/700\n",
      "122/122 [==============================] - 0s 637us/step - loss: 113.8230 - mae: 7.2870 - val_loss: 117.2126 - val_mae: 7.5612\n",
      "Epoch 289/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 113.3879 - mae: 7.1765 - val_loss: 82.0433 - val_mae: 6.8410\n",
      "Epoch 290/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 116.5294 - mae: 7.1653 - val_loss: 147.6451 - val_mae: 8.1929\n",
      "Epoch 291/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 100.7257 - mae: 6.8993 - val_loss: 123.3929 - val_mae: 7.7147\n",
      "Epoch 292/700\n",
      "122/122 [==============================] - 0s 622us/step - loss: 100.4321 - mae: 6.9385 - val_loss: 94.7033 - val_mae: 7.0176\n",
      "Epoch 293/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 103.8935 - mae: 7.1327 - val_loss: 146.1610 - val_mae: 8.3711\n",
      "Epoch 294/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 105.9693 - mae: 6.9820 - val_loss: 109.4232 - val_mae: 7.3898\n",
      "Epoch 295/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 98.2962 - mae: 6.8563 - val_loss: 140.5858 - val_mae: 8.3833\n",
      "Epoch 296/700\n",
      "122/122 [==============================] - 0s 614us/step - loss: 105.8674 - mae: 7.0967 - val_loss: 132.0247 - val_mae: 7.8759\n",
      "Epoch 297/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 103.6399 - mae: 6.9738 - val_loss: 135.6237 - val_mae: 8.0925\n",
      "Epoch 298/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 106.5007 - mae: 6.9445 - val_loss: 94.3750 - val_mae: 6.9936\n",
      "Epoch 299/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 107.7445 - mae: 7.0112 - val_loss: 140.9134 - val_mae: 8.1270\n",
      "Epoch 300/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 102.0889 - mae: 6.8910 - val_loss: 111.6760 - val_mae: 7.3545\n",
      "Epoch 301/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 96.9626 - mae: 6.7291 - val_loss: 168.1037 - val_mae: 8.6548\n",
      "Epoch 302/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 106.7079 - mae: 7.0027 - val_loss: 183.8044 - val_mae: 8.9230\n",
      "Epoch 303/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 99.6453 - mae: 6.7969 - val_loss: 122.3730 - val_mae: 7.7484\n",
      "Epoch 304/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 92.1094 - mae: 6.6627 - val_loss: 109.9220 - val_mae: 7.3452\n",
      "Epoch 305/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 100.8583 - mae: 6.7484 - val_loss: 155.4755 - val_mae: 8.4250\n",
      "Epoch 306/700\n",
      "122/122 [==============================] - 0s 646us/step - loss: 95.6645 - mae: 6.7225 - val_loss: 149.2529 - val_mae: 8.4393\n",
      "Epoch 307/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 105.9700 - mae: 6.8396 - val_loss: 133.9451 - val_mae: 7.9114\n",
      "Epoch 308/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 96.3173 - mae: 6.6648 - val_loss: 190.7939 - val_mae: 9.0595\n",
      "Epoch 309/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 90.6805 - mae: 6.6094 - val_loss: 133.3079 - val_mae: 8.1745\n",
      "Epoch 310/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 92.6159 - mae: 6.5649 - val_loss: 152.8983 - val_mae: 8.4198\n",
      "Epoch 311/700\n",
      "122/122 [==============================] - 0s 691us/step - loss: 94.8195 - mae: 6.6295 - val_loss: 82.7861 - val_mae: 6.6243\n",
      "Epoch 312/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 100.0958 - mae: 6.7510 - val_loss: 152.3881 - val_mae: 8.2783\n",
      "Epoch 313/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 99.2178 - mae: 6.6642 - val_loss: 150.4720 - val_mae: 8.2567\n",
      "Epoch 314/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 89.2868 - mae: 6.5091 - val_loss: 226.9872 - val_mae: 9.7661\n",
      "Epoch 315/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 97.6576 - mae: 6.6880 - val_loss: 143.9262 - val_mae: 9.0253\n",
      "Epoch 316/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 85.7346 - mae: 6.4258 - val_loss: 107.2531 - val_mae: 7.2598\n",
      "Epoch 317/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 96.1957 - mae: 6.6828 - val_loss: 196.7465 - val_mae: 9.2700\n",
      "Epoch 318/700\n",
      "122/122 [==============================] - 0s 599us/step - loss: 87.4213 - mae: 6.4431 - val_loss: 163.2500 - val_mae: 8.6159\n",
      "Epoch 319/700\n",
      "122/122 [==============================] - 0s 596us/step - loss: 92.2976 - mae: 6.6139 - val_loss: 121.8074 - val_mae: 7.7847\n",
      "Epoch 320/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 91.3388 - mae: 6.4739 - val_loss: 150.8808 - val_mae: 8.4138\n",
      "Epoch 321/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 92.0847 - mae: 6.6030 - val_loss: 161.3551 - val_mae: 8.5241\n",
      "Epoch 322/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 89.9425 - mae: 6.4734 - val_loss: 184.2447 - val_mae: 9.0456\n",
      "Epoch 323/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 86.9664 - mae: 6.4009 - val_loss: 187.4180 - val_mae: 9.0537\n",
      "Epoch 324/700\n",
      "122/122 [==============================] - 0s 626us/step - loss: 85.3579 - mae: 6.3461 - val_loss: 172.0689 - val_mae: 8.7853\n",
      "Epoch 325/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 88.9010 - mae: 6.4893 - val_loss: 159.8672 - val_mae: 8.3544\n",
      "Epoch 326/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 89.5141 - mae: 6.5319 - val_loss: 128.8872 - val_mae: 7.9310\n",
      "Epoch 327/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 85.2999 - mae: 6.3546 - val_loss: 171.3625 - val_mae: 8.8637\n",
      "Epoch 328/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 87.0370 - mae: 6.4113 - val_loss: 150.3259 - val_mae: 8.3141\n",
      "Epoch 329/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 93.0975 - mae: 6.5883 - val_loss: 149.2671 - val_mae: 8.3355\n",
      "Epoch 330/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 85.7668 - mae: 6.4157 - val_loss: 151.9628 - val_mae: 8.4980\n",
      "Epoch 331/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 88.3370 - mae: 6.3425 - val_loss: 189.8675 - val_mae: 9.3494\n",
      "Epoch 332/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 80.8762 - mae: 6.2503 - val_loss: 163.6101 - val_mae: 8.6608\n",
      "Epoch 333/700\n",
      "122/122 [==============================] - 0s 564us/step - loss: 91.0770 - mae: 6.5368 - val_loss: 103.6321 - val_mae: 7.3848\n",
      "Epoch 334/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 85.1324 - mae: 6.3542 - val_loss: 152.0735 - val_mae: 8.3793\n",
      "Epoch 335/700\n",
      "122/122 [==============================] - 0s 626us/step - loss: 81.1284 - mae: 6.2274 - val_loss: 159.2019 - val_mae: 8.6519\n",
      "Epoch 336/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 83.3833 - mae: 6.2446 - val_loss: 133.6447 - val_mae: 7.9911\n",
      "Epoch 337/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 88.4057 - mae: 6.4625 - val_loss: 191.2265 - val_mae: 9.0218\n",
      "Epoch 338/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 87.4815 - mae: 6.3521 - val_loss: 195.8058 - val_mae: 9.2977\n",
      "Epoch 339/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 84.2674 - mae: 6.2598 - val_loss: 144.0455 - val_mae: 8.2305\n",
      "Epoch 340/700\n",
      "122/122 [==============================] - 0s 661us/step - loss: 85.7931 - mae: 6.2501 - val_loss: 165.9773 - val_mae: 8.5172\n",
      "Epoch 341/700\n",
      "122/122 [==============================] - 0s 592us/step - loss: 86.5079 - mae: 6.4080 - val_loss: 165.0705 - val_mae: 8.8561\n",
      "Epoch 342/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 87.8920 - mae: 6.4955 - val_loss: 185.7407 - val_mae: 8.9689\n",
      "Epoch 343/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 81.2522 - mae: 6.2742 - val_loss: 170.0637 - val_mae: 8.7678\n",
      "Epoch 344/700\n",
      "122/122 [==============================] - 0s 564us/step - loss: 88.1803 - mae: 6.3855 - val_loss: 165.5169 - val_mae: 8.5484\n",
      "Epoch 345/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 82.1639 - mae: 6.2613 - val_loss: 166.4708 - val_mae: 8.5053\n",
      "Epoch 346/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 86.1687 - mae: 6.3291 - val_loss: 230.3730 - val_mae: 9.9084\n",
      "Epoch 347/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 80.4505 - mae: 6.1519 - val_loss: 216.3995 - val_mae: 9.5448\n",
      "Epoch 348/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 86.0278 - mae: 6.4434 - val_loss: 142.3121 - val_mae: 8.3190\n",
      "Epoch 349/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 77.6790 - mae: 6.1941 - val_loss: 165.7518 - val_mae: 8.6290\n",
      "Epoch 350/700\n",
      "122/122 [==============================] - 0s 687us/step - loss: 91.0438 - mae: 6.3946 - val_loss: 159.1987 - val_mae: 8.3965\n",
      "Epoch 351/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 81.3049 - mae: 6.1333 - val_loss: 153.9608 - val_mae: 8.3181\n",
      "Epoch 352/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 85.0126 - mae: 6.2685 - val_loss: 163.8329 - val_mae: 8.5175\n",
      "Epoch 353/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 81.6911 - mae: 6.2532 - val_loss: 154.9517 - val_mae: 8.4236\n",
      "Epoch 354/700\n",
      "122/122 [==============================] - 0s 582us/step - loss: 81.9807 - mae: 6.2092 - val_loss: 183.2049 - val_mae: 8.8571\n",
      "Epoch 355/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 86.2849 - mae: 6.3339 - val_loss: 127.3802 - val_mae: 7.7756\n",
      "Epoch 356/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 76.5577 - mae: 6.0321 - val_loss: 234.5964 - val_mae: 9.8386\n",
      "Epoch 357/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 80.6363 - mae: 6.1203 - val_loss: 185.5566 - val_mae: 8.8954\n",
      "Epoch 358/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 73.5055 - mae: 5.9344 - val_loss: 149.0518 - val_mae: 8.3028\n",
      "Epoch 359/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 79.7222 - mae: 6.1495 - val_loss: 106.7803 - val_mae: 7.4692\n",
      "Epoch 360/700\n",
      "122/122 [==============================] - 0s 636us/step - loss: 77.7278 - mae: 6.0454 - val_loss: 216.9265 - val_mae: 9.7937\n",
      "Epoch 361/700\n",
      "122/122 [==============================] - 0s 600us/step - loss: 71.9573 - mae: 5.8927 - val_loss: 197.3295 - val_mae: 9.1744\n",
      "Epoch 362/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 74.2466 - mae: 6.0055 - val_loss: 146.1844 - val_mae: 8.2404\n",
      "Epoch 363/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 82.3904 - mae: 6.1450 - val_loss: 220.9341 - val_mae: 9.5541\n",
      "Epoch 364/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 75.4903 - mae: 5.9686 - val_loss: 169.9470 - val_mae: 8.7249\n",
      "Epoch 365/700\n",
      "122/122 [==============================] - 0s 679us/step - loss: 77.7284 - mae: 6.0687 - val_loss: 184.8473 - val_mae: 9.0083\n",
      "Epoch 366/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 78.5340 - mae: 6.0674 - val_loss: 127.3290 - val_mae: 7.8465\n",
      "Epoch 367/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 78.6182 - mae: 6.0138 - val_loss: 192.3900 - val_mae: 9.0903\n",
      "Epoch 368/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 78.2614 - mae: 6.0786 - val_loss: 215.9359 - val_mae: 9.6483\n",
      "Epoch 369/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 79.4484 - mae: 6.1258 - val_loss: 150.0517 - val_mae: 8.2878\n",
      "Epoch 370/700\n",
      "122/122 [==============================] - 0s 619us/step - loss: 76.8161 - mae: 6.0373 - val_loss: 140.2020 - val_mae: 8.0187\n",
      "Epoch 371/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 81.2297 - mae: 6.1282 - val_loss: 188.0293 - val_mae: 9.0186\n",
      "Epoch 372/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 74.0561 - mae: 5.8711 - val_loss: 191.5409 - val_mae: 9.0941\n",
      "Epoch 373/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 77.2123 - mae: 5.9708 - val_loss: 162.6262 - val_mae: 8.6133\n",
      "Epoch 374/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 74.5315 - mae: 5.9889 - val_loss: 154.8958 - val_mae: 8.2580\n",
      "Epoch 375/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 72.1215 - mae: 5.9031 - val_loss: 192.2070 - val_mae: 9.1296\n",
      "Epoch 376/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 70.8191 - mae: 5.7976 - val_loss: 227.7287 - val_mae: 9.7779\n",
      "Epoch 377/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 75.9299 - mae: 5.9715 - val_loss: 158.1086 - val_mae: 8.5757\n",
      "Epoch 378/700\n",
      "122/122 [==============================] - 0s 583us/step - loss: 72.9102 - mae: 5.9065 - val_loss: 213.5714 - val_mae: 9.4671\n",
      "Epoch 379/700\n",
      "122/122 [==============================] - 0s 673us/step - loss: 73.4441 - mae: 5.9788 - val_loss: 132.6739 - val_mae: 7.7888\n",
      "Epoch 380/700\n",
      "122/122 [==============================] - 0s 633us/step - loss: 71.8028 - mae: 5.9413 - val_loss: 130.7793 - val_mae: 7.9293\n",
      "Epoch 381/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 74.1034 - mae: 5.8919 - val_loss: 156.1296 - val_mae: 8.3362\n",
      "Epoch 382/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 72.9918 - mae: 5.9170 - val_loss: 192.8443 - val_mae: 9.1183\n",
      "Epoch 383/700\n",
      "122/122 [==============================] - 0s 613us/step - loss: 76.7613 - mae: 5.8819 - val_loss: 156.4339 - val_mae: 8.3792\n",
      "Epoch 384/700\n",
      "122/122 [==============================] - 0s 647us/step - loss: 77.8089 - mae: 5.9017 - val_loss: 137.2275 - val_mae: 8.0164\n",
      "Epoch 385/700\n",
      "122/122 [==============================] - 0s 574us/step - loss: 71.1713 - mae: 5.8192 - val_loss: 171.5409 - val_mae: 8.6694\n",
      "Epoch 386/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 70.3232 - mae: 5.7627 - val_loss: 205.7057 - val_mae: 9.3334\n",
      "Epoch 387/700\n",
      "122/122 [==============================] - 0s 640us/step - loss: 68.6940 - mae: 5.8281 - val_loss: 136.2122 - val_mae: 8.4115\n",
      "Epoch 388/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 69.4158 - mae: 5.8379 - val_loss: 169.2458 - val_mae: 8.7553\n",
      "Epoch 389/700\n",
      "122/122 [==============================] - 0s 619us/step - loss: 72.9651 - mae: 5.8629 - val_loss: 137.1119 - val_mae: 8.0929\n",
      "Epoch 390/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 70.9320 - mae: 5.8308 - val_loss: 182.4147 - val_mae: 8.8861\n",
      "Epoch 391/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 73.1809 - mae: 5.8285 - val_loss: 182.6432 - val_mae: 8.9233\n",
      "Epoch 392/700\n",
      "122/122 [==============================] - 0s 638us/step - loss: 73.2585 - mae: 5.8012 - val_loss: 153.4977 - val_mae: 8.3563\n",
      "Epoch 393/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 68.5233 - mae: 5.6665 - val_loss: 212.4222 - val_mae: 9.4438\n",
      "Epoch 394/700\n",
      "122/122 [==============================] - 0s 664us/step - loss: 74.8183 - mae: 5.8743 - val_loss: 192.0658 - val_mae: 9.1397\n",
      "Epoch 395/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 72.6718 - mae: 5.7941 - val_loss: 179.8613 - val_mae: 8.7724\n",
      "Epoch 396/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 66.5778 - mae: 5.7054 - val_loss: 159.0094 - val_mae: 8.2850\n",
      "Epoch 397/700\n",
      "122/122 [==============================] - 0s 587us/step - loss: 68.2030 - mae: 5.6743 - val_loss: 136.7202 - val_mae: 8.2070\n",
      "Epoch 398/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 65.5295 - mae: 5.6700 - val_loss: 221.2210 - val_mae: 9.5978\n",
      "Epoch 399/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 68.1165 - mae: 5.6909 - val_loss: 167.9799 - val_mae: 8.7282\n",
      "Epoch 400/700\n",
      "122/122 [==============================] - 0s 626us/step - loss: 66.1339 - mae: 5.6217 - val_loss: 88.9638 - val_mae: 6.6599\n",
      "Epoch 401/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 68.6291 - mae: 5.7347 - val_loss: 179.2471 - val_mae: 8.7397\n",
      "Epoch 402/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 62.5741 - mae: 5.5463 - val_loss: 184.9526 - val_mae: 8.7803\n",
      "Epoch 403/700\n",
      "122/122 [==============================] - 0s 605us/step - loss: 73.3856 - mae: 5.8135 - val_loss: 155.1541 - val_mae: 8.4919\n",
      "Epoch 404/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 75.7394 - mae: 5.7580 - val_loss: 190.9276 - val_mae: 9.2164\n",
      "Epoch 405/700\n",
      "122/122 [==============================] - 0s 564us/step - loss: 69.3782 - mae: 5.7062 - val_loss: 181.1532 - val_mae: 8.9035\n",
      "Epoch 406/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 70.2579 - mae: 5.7220 - val_loss: 192.8165 - val_mae: 9.0323\n",
      "Epoch 407/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 70.2780 - mae: 5.6809 - val_loss: 167.1806 - val_mae: 8.5723\n",
      "Epoch 408/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 65.3975 - mae: 5.5928 - val_loss: 152.5366 - val_mae: 8.3614\n",
      "Epoch 409/700\n",
      "122/122 [==============================] - 0s 659us/step - loss: 62.5070 - mae: 5.5209 - val_loss: 213.1013 - val_mae: 9.4467\n",
      "Epoch 410/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 70.8290 - mae: 5.7084 - val_loss: 138.2731 - val_mae: 7.8567\n",
      "Epoch 411/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 68.3838 - mae: 5.6333 - val_loss: 119.0431 - val_mae: 7.8935\n",
      "Epoch 412/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 67.2588 - mae: 5.6484 - val_loss: 145.3953 - val_mae: 8.1957\n",
      "Epoch 413/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 62.9016 - mae: 5.5191 - val_loss: 206.7358 - val_mae: 9.3280\n",
      "Epoch 414/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 66.7550 - mae: 5.5810 - val_loss: 126.9138 - val_mae: 7.6606\n",
      "Epoch 415/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 66.6187 - mae: 5.6122 - val_loss: 152.3073 - val_mae: 8.2716\n",
      "Epoch 416/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 65.8011 - mae: 5.5239 - val_loss: 206.4515 - val_mae: 9.4100\n",
      "Epoch 417/700\n",
      "122/122 [==============================] - 0s 631us/step - loss: 75.8861 - mae: 5.7773 - val_loss: 156.3380 - val_mae: 8.4724\n",
      "Epoch 418/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 64.1092 - mae: 5.5874 - val_loss: 231.4523 - val_mae: 9.8126\n",
      "Epoch 419/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 68.6679 - mae: 5.6417 - val_loss: 161.5821 - val_mae: 8.3992\n",
      "Epoch 420/700\n",
      "122/122 [==============================] - 0s 634us/step - loss: 70.7992 - mae: 5.6979 - val_loss: 262.2431 - val_mae: 10.2044\n",
      "Epoch 421/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 62.2191 - mae: 5.3678 - val_loss: 268.1205 - val_mae: 10.2750\n",
      "Epoch 422/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 66.2724 - mae: 5.5705 - val_loss: 126.5152 - val_mae: 7.9211\n",
      "Epoch 423/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 69.3919 - mae: 5.6813 - val_loss: 206.4464 - val_mae: 9.2494\n",
      "Epoch 424/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 62.6942 - mae: 5.6126 - val_loss: 153.6870 - val_mae: 8.3808\n",
      "Epoch 425/700\n",
      "122/122 [==============================] - 0s 677us/step - loss: 65.7391 - mae: 5.5245 - val_loss: 185.0141 - val_mae: 9.0414\n",
      "Epoch 426/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 67.4686 - mae: 5.6044 - val_loss: 199.6651 - val_mae: 9.1478\n",
      "Epoch 427/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 64.1750 - mae: 5.6173 - val_loss: 145.1461 - val_mae: 8.0376\n",
      "Epoch 428/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 62.7648 - mae: 5.4260 - val_loss: 152.4474 - val_mae: 8.2251\n",
      "Epoch 429/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 62.6526 - mae: 5.4972 - val_loss: 187.9480 - val_mae: 8.9234\n",
      "Epoch 430/700\n",
      "122/122 [==============================] - 0s 608us/step - loss: 69.0162 - mae: 5.6193 - val_loss: 159.5780 - val_mae: 8.3073\n",
      "Epoch 431/700\n",
      "122/122 [==============================] - 0s 639us/step - loss: 68.7760 - mae: 5.5748 - val_loss: 163.9783 - val_mae: 8.4254\n",
      "Epoch 432/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 68.5513 - mae: 5.6547 - val_loss: 154.6935 - val_mae: 8.4869\n",
      "Epoch 433/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 64.0011 - mae: 5.5384 - val_loss: 181.0722 - val_mae: 8.7671\n",
      "Epoch 434/700\n",
      "122/122 [==============================] - 0s 665us/step - loss: 66.2994 - mae: 5.5523 - val_loss: 142.7316 - val_mae: 7.9652\n",
      "Epoch 435/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 70.5495 - mae: 5.6363 - val_loss: 116.9239 - val_mae: 7.6246\n",
      "Epoch 436/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 67.3406 - mae: 5.5340 - val_loss: 221.7292 - val_mae: 9.5433\n",
      "Epoch 437/700\n",
      "122/122 [==============================] - 0s 621us/step - loss: 63.5987 - mae: 5.4224 - val_loss: 159.0463 - val_mae: 8.4238\n",
      "Epoch 438/700\n",
      "122/122 [==============================] - 0s 652us/step - loss: 63.6632 - mae: 5.4505 - val_loss: 140.5345 - val_mae: 7.9815\n",
      "Epoch 439/700\n",
      "122/122 [==============================] - 0s 718us/step - loss: 65.1559 - mae: 5.5121 - val_loss: 242.2256 - val_mae: 9.8065\n",
      "Epoch 440/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 65.1295 - mae: 5.5382 - val_loss: 208.6055 - val_mae: 9.2602\n",
      "Epoch 441/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 65.5034 - mae: 5.4786 - val_loss: 205.8983 - val_mae: 9.2935\n",
      "Epoch 442/700\n",
      "122/122 [==============================] - 0s 594us/step - loss: 64.6294 - mae: 5.4509 - val_loss: 140.7959 - val_mae: 7.9000\n",
      "Epoch 443/700\n",
      "122/122 [==============================] - 0s 716us/step - loss: 60.6977 - mae: 5.2718 - val_loss: 196.3926 - val_mae: 9.0870\n",
      "Epoch 444/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 61.7840 - mae: 5.4171 - val_loss: 172.4111 - val_mae: 8.7455\n",
      "Epoch 445/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 64.6822 - mae: 5.4620 - val_loss: 202.6072 - val_mae: 9.2124\n",
      "Epoch 446/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 67.5493 - mae: 5.5378 - val_loss: 166.3776 - val_mae: 8.7108\n",
      "Epoch 447/700\n",
      "122/122 [==============================] - 0s 650us/step - loss: 64.1411 - mae: 5.3982 - val_loss: 150.6372 - val_mae: 8.1303\n",
      "Epoch 448/700\n",
      "122/122 [==============================] - 0s 661us/step - loss: 64.0902 - mae: 5.4972 - val_loss: 199.9140 - val_mae: 9.2086\n",
      "Epoch 449/700\n",
      "122/122 [==============================] - 0s 636us/step - loss: 64.2216 - mae: 5.4282 - val_loss: 186.7080 - val_mae: 8.9961\n",
      "Epoch 450/700\n",
      "122/122 [==============================] - 0s 637us/step - loss: 59.3721 - mae: 5.3348 - val_loss: 187.5242 - val_mae: 8.9372\n",
      "Epoch 451/700\n",
      "122/122 [==============================] - 0s 711us/step - loss: 55.7687 - mae: 5.1781 - val_loss: 212.7057 - val_mae: 9.4373\n",
      "Epoch 452/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 60.5399 - mae: 5.3643 - val_loss: 234.5560 - val_mae: 9.7288\n",
      "Epoch 453/700\n",
      "122/122 [==============================] - 0s 645us/step - loss: 64.2711 - mae: 5.4083 - val_loss: 171.1241 - val_mae: 8.6356\n",
      "Epoch 454/700\n",
      "122/122 [==============================] - 0s 748us/step - loss: 68.4971 - mae: 5.4189 - val_loss: 170.7212 - val_mae: 8.6602\n",
      "Epoch 455/700\n",
      "122/122 [==============================] - 0s 661us/step - loss: 60.8404 - mae: 5.3561 - val_loss: 145.6463 - val_mae: 8.0983\n",
      "Epoch 456/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 60.3904 - mae: 5.3743 - val_loss: 150.9430 - val_mae: 8.3588\n",
      "Epoch 457/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 65.8573 - mae: 5.5514 - val_loss: 169.1041 - val_mae: 8.6784\n",
      "Epoch 458/700\n",
      "122/122 [==============================] - 0s 637us/step - loss: 65.2241 - mae: 5.4791 - val_loss: 130.5918 - val_mae: 7.8121\n",
      "Epoch 459/700\n",
      "122/122 [==============================] - 0s 664us/step - loss: 58.7930 - mae: 5.3108 - val_loss: 106.1283 - val_mae: 7.2759\n",
      "Epoch 460/700\n",
      "122/122 [==============================] - 0s 682us/step - loss: 66.9579 - mae: 5.5084 - val_loss: 193.5029 - val_mae: 9.0219\n",
      "Epoch 461/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 58.3336 - mae: 5.2816 - val_loss: 208.6116 - val_mae: 9.4447\n",
      "Epoch 462/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 61.8106 - mae: 5.3171 - val_loss: 177.1517 - val_mae: 8.7005\n",
      "Epoch 463/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 59.8423 - mae: 5.2947 - val_loss: 280.6527 - val_mae: 10.3706\n",
      "Epoch 464/700\n",
      "122/122 [==============================] - 0s 692us/step - loss: 62.1080 - mae: 5.4165 - val_loss: 147.3826 - val_mae: 8.1236\n",
      "Epoch 465/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 67.1523 - mae: 5.4696 - val_loss: 179.6267 - val_mae: 8.7834\n",
      "Epoch 466/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 61.5253 - mae: 5.3900 - val_loss: 229.2247 - val_mae: 9.6345\n",
      "Epoch 467/700\n",
      "122/122 [==============================] - 0s 632us/step - loss: 60.5256 - mae: 5.3152 - val_loss: 206.0283 - val_mae: 9.2277\n",
      "Epoch 468/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 61.9870 - mae: 5.3706 - val_loss: 124.9905 - val_mae: 7.8641\n",
      "Epoch 469/700\n",
      "122/122 [==============================] - 0s 600us/step - loss: 60.3913 - mae: 5.3541 - val_loss: 160.6125 - val_mae: 8.3582\n",
      "Epoch 470/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 56.8618 - mae: 5.2068 - val_loss: 183.1952 - val_mae: 9.0114\n",
      "Epoch 471/700\n",
      "122/122 [==============================] - 0s 676us/step - loss: 61.7178 - mae: 5.3909 - val_loss: 166.7284 - val_mae: 8.6889\n",
      "Epoch 472/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 59.4534 - mae: 5.2890 - val_loss: 172.5850 - val_mae: 8.5228\n",
      "Epoch 473/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 62.3172 - mae: 5.3978 - val_loss: 147.5094 - val_mae: 8.0620\n",
      "Epoch 474/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 65.4781 - mae: 5.4813 - val_loss: 169.0410 - val_mae: 8.5586\n",
      "Epoch 475/700\n",
      "122/122 [==============================] - 0s 732us/step - loss: 57.3838 - mae: 5.1755 - val_loss: 168.5043 - val_mae: 8.6708\n",
      "Epoch 476/700\n",
      "122/122 [==============================] - 0s 752us/step - loss: 64.6774 - mae: 5.5207 - val_loss: 212.7140 - val_mae: 9.3904\n",
      "Epoch 477/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 64.0644 - mae: 5.3764 - val_loss: 213.7818 - val_mae: 9.4328\n",
      "Epoch 478/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 56.3956 - mae: 5.2676 - val_loss: 156.3092 - val_mae: 8.4428\n",
      "Epoch 479/700\n",
      "122/122 [==============================] - 0s 666us/step - loss: 55.2834 - mae: 5.1497 - val_loss: 150.3913 - val_mae: 8.3075\n",
      "Epoch 480/700\n",
      "122/122 [==============================] - 0s 693us/step - loss: 61.8512 - mae: 5.3259 - val_loss: 154.0472 - val_mae: 8.3343\n",
      "Epoch 481/700\n",
      "122/122 [==============================] - 0s 663us/step - loss: 61.3309 - mae: 5.3580 - val_loss: 246.7184 - val_mae: 10.0017\n",
      "Epoch 482/700\n",
      "122/122 [==============================] - 0s 645us/step - loss: 60.1314 - mae: 5.2487 - val_loss: 173.2172 - val_mae: 8.8929\n",
      "Epoch 483/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 56.2479 - mae: 5.1800 - val_loss: 122.5375 - val_mae: 7.7306\n",
      "Epoch 484/700\n",
      "122/122 [==============================] - 0s 665us/step - loss: 61.4363 - mae: 5.2808 - val_loss: 146.8478 - val_mae: 8.1518\n",
      "Epoch 485/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 59.4947 - mae: 5.1969 - val_loss: 166.1374 - val_mae: 8.5753\n",
      "Epoch 486/700\n",
      "122/122 [==============================] - 0s 641us/step - loss: 65.8959 - mae: 5.3253 - val_loss: 139.1876 - val_mae: 8.0420\n",
      "Epoch 487/700\n",
      "122/122 [==============================] - 0s 647us/step - loss: 62.6396 - mae: 5.3645 - val_loss: 159.6317 - val_mae: 8.5908\n",
      "Epoch 488/700\n",
      "122/122 [==============================] - 0s 634us/step - loss: 61.6703 - mae: 5.3310 - val_loss: 182.5548 - val_mae: 8.8605\n",
      "Epoch 489/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 60.2074 - mae: 5.2824 - val_loss: 263.4648 - val_mae: 10.1485\n",
      "Epoch 490/700\n",
      "122/122 [==============================] - 0s 700us/step - loss: 58.6769 - mae: 5.1982 - val_loss: 173.0782 - val_mae: 8.8022\n",
      "Epoch 491/700\n",
      "122/122 [==============================] - 0s 680us/step - loss: 64.5887 - mae: 5.3513 - val_loss: 226.7999 - val_mae: 9.6001\n",
      "Epoch 492/700\n",
      "122/122 [==============================] - 0s 649us/step - loss: 59.7753 - mae: 5.2101 - val_loss: 210.1616 - val_mae: 9.2552\n",
      "Epoch 493/700\n",
      "122/122 [==============================] - 0s 679us/step - loss: 55.3484 - mae: 5.1165 - val_loss: 198.4699 - val_mae: 9.0932\n",
      "Epoch 494/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 58.0745 - mae: 5.2138 - val_loss: 160.7648 - val_mae: 8.3300\n",
      "Epoch 495/700\n",
      "122/122 [==============================] - 0s 682us/step - loss: 54.6220 - mae: 5.0358 - val_loss: 215.5832 - val_mae: 9.4706\n",
      "Epoch 496/700\n",
      "122/122 [==============================] - 0s 651us/step - loss: 59.0228 - mae: 5.2085 - val_loss: 254.9771 - val_mae: 9.9945\n",
      "Epoch 497/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 58.9936 - mae: 5.2729 - val_loss: 200.0123 - val_mae: 9.1193\n",
      "Epoch 498/700\n",
      "122/122 [==============================] - 0s 627us/step - loss: 55.1671 - mae: 5.0577 - val_loss: 144.8578 - val_mae: 8.0332\n",
      "Epoch 499/700\n",
      "122/122 [==============================] - 0s 693us/step - loss: 60.4848 - mae: 5.1927 - val_loss: 128.4921 - val_mae: 7.8108\n",
      "Epoch 500/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 59.4974 - mae: 5.2123 - val_loss: 207.5112 - val_mae: 9.2145\n",
      "Epoch 501/700\n",
      "122/122 [==============================] - 0s 622us/step - loss: 57.5272 - mae: 5.2236 - val_loss: 168.7061 - val_mae: 8.6898\n",
      "Epoch 502/700\n",
      "122/122 [==============================] - 0s 633us/step - loss: 61.1877 - mae: 5.2726 - val_loss: 189.6927 - val_mae: 8.9740\n",
      "Epoch 503/700\n",
      "122/122 [==============================] - 0s 634us/step - loss: 57.7332 - mae: 5.1183 - val_loss: 210.9100 - val_mae: 9.3594\n",
      "Epoch 504/700\n",
      "122/122 [==============================] - 0s 630us/step - loss: 64.9525 - mae: 5.4470 - val_loss: 231.0933 - val_mae: 9.7113\n",
      "Epoch 505/700\n",
      "122/122 [==============================] - 0s 592us/step - loss: 65.8793 - mae: 5.3059 - val_loss: 188.7040 - val_mae: 8.9850\n",
      "Epoch 506/700\n",
      "122/122 [==============================] - 0s 670us/step - loss: 56.9237 - mae: 5.2653 - val_loss: 193.7667 - val_mae: 9.1269\n",
      "Epoch 507/700\n",
      "122/122 [==============================] - 0s 672us/step - loss: 63.2732 - mae: 5.3402 - val_loss: 139.3973 - val_mae: 8.1686\n",
      "Epoch 508/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 61.9873 - mae: 5.2542 - val_loss: 138.1117 - val_mae: 7.8670\n",
      "Epoch 509/700\n",
      "122/122 [==============================] - 0s 664us/step - loss: 57.2921 - mae: 5.1144 - val_loss: 276.6861 - val_mae: 10.4331\n",
      "Epoch 510/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 58.7602 - mae: 5.2550 - val_loss: 185.7211 - val_mae: 8.9206\n",
      "Epoch 511/700\n",
      "122/122 [==============================] - 0s 569us/step - loss: 59.1162 - mae: 5.2395 - val_loss: 218.1084 - val_mae: 9.5525\n",
      "Epoch 512/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 54.3637 - mae: 5.0540 - val_loss: 203.1334 - val_mae: 9.3172\n",
      "Epoch 513/700\n",
      "122/122 [==============================] - 0s 621us/step - loss: 59.0941 - mae: 5.2161 - val_loss: 192.6530 - val_mae: 9.0579\n",
      "Epoch 514/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 58.4333 - mae: 5.1363 - val_loss: 212.1481 - val_mae: 9.4324\n",
      "Epoch 515/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 58.2661 - mae: 5.1849 - val_loss: 180.0445 - val_mae: 8.8627\n",
      "Epoch 516/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 59.2694 - mae: 5.2090 - val_loss: 147.5629 - val_mae: 8.3841\n",
      "Epoch 517/700\n",
      "122/122 [==============================] - 0s 629us/step - loss: 68.1799 - mae: 5.4033 - val_loss: 170.2705 - val_mae: 8.5508\n",
      "Epoch 518/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 58.9985 - mae: 5.2341 - val_loss: 168.2024 - val_mae: 8.7757\n",
      "Epoch 519/700\n",
      "122/122 [==============================] - 0s 619us/step - loss: 59.2574 - mae: 5.2098 - val_loss: 133.5715 - val_mae: 7.8771\n",
      "Epoch 520/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 58.9329 - mae: 5.1472 - val_loss: 179.1905 - val_mae: 8.7845\n",
      "Epoch 521/700\n",
      "122/122 [==============================] - 0s 719us/step - loss: 58.6494 - mae: 5.2951 - val_loss: 153.1886 - val_mae: 8.2348\n",
      "Epoch 522/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 62.4331 - mae: 5.2255 - val_loss: 190.1071 - val_mae: 8.8670\n",
      "Epoch 523/700\n",
      "122/122 [==============================] - 0s 625us/step - loss: 52.2099 - mae: 5.1197 - val_loss: 130.7102 - val_mae: 7.7573\n",
      "Epoch 524/700\n",
      "122/122 [==============================] - 0s 618us/step - loss: 55.7096 - mae: 5.1191 - val_loss: 202.7309 - val_mae: 9.2235\n",
      "Epoch 525/700\n",
      "122/122 [==============================] - 0s 620us/step - loss: 56.9175 - mae: 5.1469 - val_loss: 181.6287 - val_mae: 8.9729\n",
      "Epoch 526/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 52.4425 - mae: 4.9950 - val_loss: 159.7105 - val_mae: 8.3076\n",
      "Epoch 527/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 55.4624 - mae: 5.0915 - val_loss: 152.7731 - val_mae: 8.3201\n",
      "Epoch 528/700\n",
      "122/122 [==============================] - 0s 627us/step - loss: 56.7913 - mae: 5.1579 - val_loss: 147.5000 - val_mae: 8.2156\n",
      "Epoch 529/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 57.1994 - mae: 5.1210 - val_loss: 218.4689 - val_mae: 9.4621\n",
      "Epoch 530/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 60.0947 - mae: 5.1801 - val_loss: 170.6696 - val_mae: 8.7352\n",
      "Epoch 531/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 56.0512 - mae: 5.1140 - val_loss: 212.3619 - val_mae: 9.3387\n",
      "Epoch 532/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 53.0824 - mae: 5.0053 - val_loss: 162.7887 - val_mae: 8.4954\n",
      "Epoch 533/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 58.6213 - mae: 5.1370 - val_loss: 231.9149 - val_mae: 9.6410\n",
      "Epoch 534/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 59.0803 - mae: 5.2464 - val_loss: 152.4061 - val_mae: 8.2258\n",
      "Epoch 535/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 54.2169 - mae: 5.1524 - val_loss: 189.3292 - val_mae: 9.0866\n",
      "Epoch 536/700\n",
      "122/122 [==============================] - 0s 598us/step - loss: 54.9946 - mae: 5.0517 - val_loss: 162.2661 - val_mae: 8.5278\n",
      "Epoch 537/700\n",
      "122/122 [==============================] - 0s 618us/step - loss: 57.1869 - mae: 5.0989 - val_loss: 147.2624 - val_mae: 8.5259\n",
      "Epoch 538/700\n",
      "122/122 [==============================] - 0s 683us/step - loss: 56.9432 - mae: 5.2018 - val_loss: 255.1584 - val_mae: 10.2572\n",
      "Epoch 539/700\n",
      "122/122 [==============================] - 0s 743us/step - loss: 54.6310 - mae: 5.0524 - val_loss: 176.5766 - val_mae: 8.8273\n",
      "Epoch 540/700\n",
      "122/122 [==============================] - 0s 665us/step - loss: 55.2601 - mae: 5.0952 - val_loss: 212.3134 - val_mae: 9.4828\n",
      "Epoch 541/700\n",
      "122/122 [==============================] - 0s 683us/step - loss: 53.8847 - mae: 5.0146 - val_loss: 184.1788 - val_mae: 8.9482\n",
      "Epoch 542/700\n",
      "122/122 [==============================] - 0s 668us/step - loss: 52.7723 - mae: 4.9896 - val_loss: 193.0385 - val_mae: 9.2029\n",
      "Epoch 543/700\n",
      "122/122 [==============================] - 0s 707us/step - loss: 54.5309 - mae: 5.0435 - val_loss: 244.9960 - val_mae: 9.8959\n",
      "Epoch 544/700\n",
      "122/122 [==============================] - 0s 650us/step - loss: 56.2111 - mae: 5.0598 - val_loss: 179.9750 - val_mae: 8.7647\n",
      "Epoch 545/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 53.5048 - mae: 5.0903 - val_loss: 207.0584 - val_mae: 9.3858\n",
      "Epoch 546/700\n",
      "122/122 [==============================] - 0s 629us/step - loss: 57.4475 - mae: 5.0834 - val_loss: 224.2358 - val_mae: 9.6972\n",
      "Epoch 547/700\n",
      "122/122 [==============================] - 0s 651us/step - loss: 64.1217 - mae: 5.2426 - val_loss: 134.2759 - val_mae: 8.0590\n",
      "Epoch 548/700\n",
      "122/122 [==============================] - 0s 644us/step - loss: 57.8157 - mae: 5.1879 - val_loss: 196.8612 - val_mae: 9.1903\n",
      "Epoch 549/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 64.7229 - mae: 5.2796 - val_loss: 171.2144 - val_mae: 8.7389\n",
      "Epoch 550/700\n",
      "122/122 [==============================] - 0s 655us/step - loss: 58.5593 - mae: 5.1244 - val_loss: 220.9666 - val_mae: 9.5953\n",
      "Epoch 551/700\n",
      "122/122 [==============================] - 0s 673us/step - loss: 55.8719 - mae: 5.0396 - val_loss: 207.4656 - val_mae: 9.3904\n",
      "Epoch 552/700\n",
      "122/122 [==============================] - 0s 636us/step - loss: 51.7921 - mae: 5.0459 - val_loss: 162.2533 - val_mae: 8.4633\n",
      "Epoch 553/700\n",
      "122/122 [==============================] - 0s 608us/step - loss: 54.4034 - mae: 5.0412 - val_loss: 269.9724 - val_mae: 10.2324\n",
      "Epoch 554/700\n",
      "122/122 [==============================] - 0s 651us/step - loss: 59.7557 - mae: 5.2733 - val_loss: 204.1766 - val_mae: 9.1562\n",
      "Epoch 555/700\n",
      "122/122 [==============================] - 0s 694us/step - loss: 54.4697 - mae: 5.0037 - val_loss: 221.2593 - val_mae: 9.6067\n",
      "Epoch 556/700\n",
      "122/122 [==============================] - 0s 654us/step - loss: 56.0725 - mae: 5.0929 - val_loss: 178.9232 - val_mae: 8.7570\n",
      "Epoch 557/700\n",
      "122/122 [==============================] - 0s 624us/step - loss: 56.1465 - mae: 5.0743 - val_loss: 203.2382 - val_mae: 9.1912\n",
      "Epoch 558/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 54.4969 - mae: 5.1093 - val_loss: 182.2766 - val_mae: 8.8186\n",
      "Epoch 559/700\n",
      "122/122 [==============================] - 0s 626us/step - loss: 51.5653 - mae: 4.8773 - val_loss: 216.0310 - val_mae: 9.4250\n",
      "Epoch 560/700\n",
      "122/122 [==============================] - 0s 608us/step - loss: 58.1195 - mae: 5.1299 - val_loss: 182.6620 - val_mae: 8.9492\n",
      "Epoch 561/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 56.9743 - mae: 5.1267 - val_loss: 204.7633 - val_mae: 9.2776\n",
      "Epoch 562/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 55.7229 - mae: 5.0212 - val_loss: 167.2343 - val_mae: 8.5032\n",
      "Epoch 563/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 56.2865 - mae: 5.0709 - val_loss: 206.2869 - val_mae: 9.5199\n",
      "Epoch 564/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 57.9695 - mae: 5.2225 - val_loss: 189.6742 - val_mae: 8.9347\n",
      "Epoch 565/700\n",
      "122/122 [==============================] - 0s 707us/step - loss: 60.2867 - mae: 5.2236 - val_loss: 131.2120 - val_mae: 7.9288\n",
      "Epoch 566/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 58.8817 - mae: 5.0836 - val_loss: 200.8181 - val_mae: 9.2027\n",
      "Epoch 567/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 56.2351 - mae: 5.0622 - val_loss: 139.1373 - val_mae: 7.9715\n",
      "Epoch 568/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 55.9928 - mae: 5.0838 - val_loss: 207.3850 - val_mae: 9.2913\n",
      "Epoch 569/700\n",
      "122/122 [==============================] - 0s 577us/step - loss: 51.4447 - mae: 4.9313 - val_loss: 181.0965 - val_mae: 8.6659\n",
      "Epoch 570/700\n",
      "122/122 [==============================] - 0s 564us/step - loss: 55.1018 - mae: 5.0405 - val_loss: 175.1404 - val_mae: 8.7352\n",
      "Epoch 571/700\n",
      "122/122 [==============================] - 0s 655us/step - loss: 57.3107 - mae: 4.9890 - val_loss: 237.2280 - val_mae: 9.7635\n",
      "Epoch 572/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 52.4692 - mae: 4.9089 - val_loss: 237.1273 - val_mae: 9.8270\n",
      "Epoch 573/700\n",
      "122/122 [==============================] - 0s 566us/step - loss: 59.6605 - mae: 5.1533 - val_loss: 220.0535 - val_mae: 9.5976\n",
      "Epoch 574/700\n",
      "122/122 [==============================] - 0s 613us/step - loss: 55.3050 - mae: 5.0705 - val_loss: 172.8801 - val_mae: 8.6891\n",
      "Epoch 575/700\n",
      "122/122 [==============================] - 0s 586us/step - loss: 57.0810 - mae: 5.0565 - val_loss: 174.9672 - val_mae: 8.6868\n",
      "Epoch 576/700\n",
      "122/122 [==============================] - 0s 678us/step - loss: 51.9052 - mae: 5.0546 - val_loss: 155.6850 - val_mae: 8.3495\n",
      "Epoch 577/700\n",
      "122/122 [==============================] - 0s 596us/step - loss: 58.6678 - mae: 5.2276 - val_loss: 168.7757 - val_mae: 8.5756\n",
      "Epoch 578/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 50.1243 - mae: 4.8948 - val_loss: 195.9531 - val_mae: 9.3526\n",
      "Epoch 579/700\n",
      "122/122 [==============================] - 0s 566us/step - loss: 58.4749 - mae: 5.1464 - val_loss: 218.8524 - val_mae: 9.4821\n",
      "Epoch 580/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 56.1444 - mae: 5.0312 - val_loss: 190.8400 - val_mae: 9.3738\n",
      "Epoch 581/700\n",
      "122/122 [==============================] - 0s 649us/step - loss: 47.9299 - mae: 4.8482 - val_loss: 153.1638 - val_mae: 8.5780\n",
      "Epoch 582/700\n",
      "122/122 [==============================] - 0s 646us/step - loss: 54.0439 - mae: 5.0480 - val_loss: 182.8654 - val_mae: 8.9534\n",
      "Epoch 583/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 50.1966 - mae: 4.9004 - val_loss: 167.2649 - val_mae: 8.5610\n",
      "Epoch 584/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 54.9361 - mae: 5.0198 - val_loss: 150.1634 - val_mae: 8.1734\n",
      "Epoch 585/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 53.1639 - mae: 4.9226 - val_loss: 190.0629 - val_mae: 9.0086\n",
      "Epoch 586/700\n",
      "122/122 [==============================] - 0s 661us/step - loss: 50.6524 - mae: 4.9567 - val_loss: 159.6184 - val_mae: 8.4462\n",
      "Epoch 587/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 55.1287 - mae: 5.0048 - val_loss: 123.3200 - val_mae: 7.6282\n",
      "Epoch 588/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 63.3460 - mae: 5.1833 - val_loss: 197.0889 - val_mae: 9.0871\n",
      "Epoch 589/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 56.5222 - mae: 5.0943 - val_loss: 177.8416 - val_mae: 8.8668\n",
      "Epoch 590/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 58.4484 - mae: 5.2054 - val_loss: 195.9718 - val_mae: 9.2060\n",
      "Epoch 591/700\n",
      "122/122 [==============================] - 0s 656us/step - loss: 51.8035 - mae: 5.0258 - val_loss: 222.0631 - val_mae: 9.5611\n",
      "Epoch 592/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 52.7657 - mae: 5.0152 - val_loss: 195.1927 - val_mae: 9.0368\n",
      "Epoch 593/700\n",
      "122/122 [==============================] - 0s 567us/step - loss: 55.7179 - mae: 5.0370 - val_loss: 135.1326 - val_mae: 7.8804\n",
      "Epoch 594/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 57.2563 - mae: 5.1353 - val_loss: 220.7732 - val_mae: 9.5392\n",
      "Epoch 595/700\n",
      "122/122 [==============================] - 0s 632us/step - loss: 61.7264 - mae: 5.2717 - val_loss: 209.9112 - val_mae: 9.3703\n",
      "Epoch 596/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 57.9120 - mae: 5.1463 - val_loss: 184.4479 - val_mae: 8.9512\n",
      "Epoch 597/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 58.8148 - mae: 5.2479 - val_loss: 207.8067 - val_mae: 9.2651\n",
      "Epoch 598/700\n",
      "122/122 [==============================] - 0s 597us/step - loss: 50.2956 - mae: 4.9363 - val_loss: 190.0456 - val_mae: 9.0088\n",
      "Epoch 599/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 55.3630 - mae: 5.0738 - val_loss: 148.4622 - val_mae: 8.2234\n",
      "Epoch 600/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 51.9069 - mae: 4.9691 - val_loss: 177.0092 - val_mae: 8.8218\n",
      "Epoch 601/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 47.9011 - mae: 4.8265 - val_loss: 161.1614 - val_mae: 8.4210\n",
      "Epoch 602/700\n",
      "122/122 [==============================] - 0s 615us/step - loss: 53.8316 - mae: 4.9504 - val_loss: 191.8630 - val_mae: 8.9879\n",
      "Epoch 603/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 59.1939 - mae: 5.1094 - val_loss: 157.2746 - val_mae: 8.4121\n",
      "Epoch 604/700\n",
      "122/122 [==============================] - 0s 582us/step - loss: 56.5235 - mae: 5.0945 - val_loss: 165.7318 - val_mae: 8.5418\n",
      "Epoch 605/700\n",
      "122/122 [==============================] - 0s 652us/step - loss: 57.3834 - mae: 5.1118 - val_loss: 144.6989 - val_mae: 8.0935\n",
      "Epoch 606/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 52.1603 - mae: 4.9859 - val_loss: 146.4101 - val_mae: 8.0933\n",
      "Epoch 607/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 54.0010 - mae: 5.0388 - val_loss: 200.9495 - val_mae: 9.2228\n",
      "Epoch 608/700\n",
      "122/122 [==============================] - 0s 589us/step - loss: 51.2102 - mae: 4.9369 - val_loss: 216.2020 - val_mae: 9.3520\n",
      "Epoch 609/700\n",
      "122/122 [==============================] - 0s 614us/step - loss: 49.2375 - mae: 4.8366 - val_loss: 169.2541 - val_mae: 8.7219\n",
      "Epoch 610/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 55.2624 - mae: 5.1279 - val_loss: 142.2075 - val_mae: 8.0848\n",
      "Epoch 611/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 53.0678 - mae: 4.9682 - val_loss: 172.3575 - val_mae: 8.7027\n",
      "Epoch 612/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 51.1243 - mae: 4.9208 - val_loss: 168.7540 - val_mae: 8.8253\n",
      "Epoch 613/700\n",
      "122/122 [==============================] - 0s 566us/step - loss: 55.5620 - mae: 5.0168 - val_loss: 186.7944 - val_mae: 9.0133\n",
      "Epoch 614/700\n",
      "122/122 [==============================] - 0s 559us/step - loss: 53.9868 - mae: 5.0568 - val_loss: 175.2094 - val_mae: 8.8551\n",
      "Epoch 615/700\n",
      "122/122 [==============================] - 0s 801us/step - loss: 52.3976 - mae: 4.9963 - val_loss: 190.9589 - val_mae: 9.0054\n",
      "Epoch 616/700\n",
      "122/122 [==============================] - 0s 690us/step - loss: 56.4736 - mae: 5.1128 - val_loss: 164.9582 - val_mae: 8.6201\n",
      "Epoch 617/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 52.2189 - mae: 5.0280 - val_loss: 148.3196 - val_mae: 8.2215\n",
      "Epoch 618/700\n",
      "122/122 [==============================] - 0s 562us/step - loss: 57.8488 - mae: 5.0702 - val_loss: 227.8335 - val_mae: 9.6066\n",
      "Epoch 619/700\n",
      "122/122 [==============================] - 0s 667us/step - loss: 50.7287 - mae: 4.8824 - val_loss: 223.9113 - val_mae: 9.5200\n",
      "Epoch 620/700\n",
      "122/122 [==============================] - 0s 601us/step - loss: 54.4589 - mae: 5.0204 - val_loss: 252.5156 - val_mae: 10.0026\n",
      "Epoch 621/700\n",
      "122/122 [==============================] - 0s 566us/step - loss: 53.7076 - mae: 5.0049 - val_loss: 153.2096 - val_mae: 8.2800\n",
      "Epoch 622/700\n",
      "122/122 [==============================] - 0s 561us/step - loss: 50.4478 - mae: 4.9187 - val_loss: 222.4863 - val_mae: 9.5024\n",
      "Epoch 623/700\n",
      "122/122 [==============================] - 0s 656us/step - loss: 49.9187 - mae: 4.7973 - val_loss: 191.2643 - val_mae: 8.9193\n",
      "Epoch 624/700\n",
      "122/122 [==============================] - 0s 596us/step - loss: 57.9999 - mae: 5.1185 - val_loss: 140.1299 - val_mae: 8.0504\n",
      "Epoch 625/700\n",
      "122/122 [==============================] - 0s 647us/step - loss: 49.6762 - mae: 4.9167 - val_loss: 216.0122 - val_mae: 9.3746\n",
      "Epoch 626/700\n",
      "122/122 [==============================] - 0s 643us/step - loss: 51.5220 - mae: 4.9375 - val_loss: 188.6205 - val_mae: 8.8189\n",
      "Epoch 627/700\n",
      "122/122 [==============================] - 0s 602us/step - loss: 55.2072 - mae: 4.9616 - val_loss: 121.4161 - val_mae: 7.6022\n",
      "Epoch 628/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 52.9988 - mae: 4.9774 - val_loss: 182.0141 - val_mae: 9.0308\n",
      "Epoch 629/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 55.0433 - mae: 5.0683 - val_loss: 222.9543 - val_mae: 9.4924\n",
      "Epoch 630/700\n",
      "122/122 [==============================] - 0s 616us/step - loss: 52.7196 - mae: 4.9357 - val_loss: 209.8790 - val_mae: 9.4211\n",
      "Epoch 631/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 51.4715 - mae: 4.9580 - val_loss: 250.1539 - val_mae: 10.0807\n",
      "Epoch 632/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 53.9938 - mae: 5.0066 - val_loss: 272.6556 - val_mae: 10.3424\n",
      "Epoch 633/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 52.6401 - mae: 4.8703 - val_loss: 181.1800 - val_mae: 8.6435\n",
      "Epoch 634/700\n",
      "122/122 [==============================] - 0s 606us/step - loss: 55.9860 - mae: 5.0593 - val_loss: 199.1452 - val_mae: 9.1302\n",
      "Epoch 635/700\n",
      "122/122 [==============================] - 0s 635us/step - loss: 54.3627 - mae: 5.0074 - val_loss: 196.0362 - val_mae: 8.9515\n",
      "Epoch 636/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 54.9747 - mae: 5.0721 - val_loss: 168.7406 - val_mae: 8.4909\n",
      "Epoch 637/700\n",
      "122/122 [==============================] - 0s 627us/step - loss: 51.3196 - mae: 4.8957 - val_loss: 226.9337 - val_mae: 9.5898\n",
      "Epoch 638/700\n",
      "122/122 [==============================] - 0s 634us/step - loss: 51.7105 - mae: 4.8870 - val_loss: 183.5593 - val_mae: 8.8859\n",
      "Epoch 639/700\n",
      "122/122 [==============================] - 0s 583us/step - loss: 50.7768 - mae: 4.8919 - val_loss: 131.1340 - val_mae: 7.7394\n",
      "Epoch 640/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 50.1520 - mae: 4.8768 - val_loss: 175.3599 - val_mae: 8.7516\n",
      "Epoch 641/700\n",
      "122/122 [==============================] - 0s 583us/step - loss: 53.5053 - mae: 4.9469 - val_loss: 158.8501 - val_mae: 8.6777\n",
      "Epoch 642/700\n",
      "122/122 [==============================] - 0s 662us/step - loss: 52.1301 - mae: 5.0041 - val_loss: 178.1007 - val_mae: 8.8845\n",
      "Epoch 643/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 50.7583 - mae: 4.8695 - val_loss: 143.8191 - val_mae: 8.0346\n",
      "Epoch 644/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 52.0247 - mae: 4.9208 - val_loss: 191.8422 - val_mae: 8.9499\n",
      "Epoch 645/700\n",
      "122/122 [==============================] - 0s 623us/step - loss: 55.7926 - mae: 5.0973 - val_loss: 163.3524 - val_mae: 8.5609\n",
      "Epoch 646/700\n",
      "122/122 [==============================] - 0s 603us/step - loss: 49.8083 - mae: 4.8395 - val_loss: 141.8761 - val_mae: 8.0497\n",
      "Epoch 647/700\n",
      "122/122 [==============================] - 0s 579us/step - loss: 50.7790 - mae: 4.8640 - val_loss: 192.6115 - val_mae: 8.9429\n",
      "Epoch 648/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 56.5076 - mae: 5.0781 - val_loss: 176.1165 - val_mae: 8.6748\n",
      "Epoch 649/700\n",
      "122/122 [==============================] - 0s 593us/step - loss: 52.7050 - mae: 4.8705 - val_loss: 176.0371 - val_mae: 8.7407\n",
      "Epoch 650/700\n",
      "122/122 [==============================] - 0s 570us/step - loss: 50.8053 - mae: 4.8537 - val_loss: 206.4592 - val_mae: 9.3068\n",
      "Epoch 651/700\n",
      "122/122 [==============================] - 0s 568us/step - loss: 48.4090 - mae: 4.7955 - val_loss: 224.6620 - val_mae: 9.6928\n",
      "Epoch 652/700\n",
      "122/122 [==============================] - 0s 651us/step - loss: 54.4189 - mae: 5.0075 - val_loss: 237.6085 - val_mae: 9.7341\n",
      "Epoch 653/700\n",
      "122/122 [==============================] - 0s 610us/step - loss: 57.7503 - mae: 5.1164 - val_loss: 177.0026 - val_mae: 8.9612\n",
      "Epoch 654/700\n",
      "122/122 [==============================] - 0s 581us/step - loss: 50.1858 - mae: 4.9685 - val_loss: 299.0966 - val_mae: 10.7404\n",
      "Epoch 655/700\n",
      "122/122 [==============================] - 0s 562us/step - loss: 51.8520 - mae: 4.9046 - val_loss: 246.3740 - val_mae: 9.8924\n",
      "Epoch 656/700\n",
      "122/122 [==============================] - 0s 611us/step - loss: 50.2865 - mae: 4.9725 - val_loss: 228.4568 - val_mae: 9.6902\n",
      "Epoch 657/700\n",
      "122/122 [==============================] - 0s 671us/step - loss: 52.8806 - mae: 4.8829 - val_loss: 179.1608 - val_mae: 8.7477\n",
      "Epoch 658/700\n",
      "122/122 [==============================] - 0s 603us/step - loss: 50.2358 - mae: 4.9286 - val_loss: 228.1616 - val_mae: 9.5345\n",
      "Epoch 659/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 50.4787 - mae: 4.8992 - val_loss: 141.9614 - val_mae: 8.1750\n",
      "Epoch 660/700\n",
      "122/122 [==============================] - 0s 595us/step - loss: 56.6365 - mae: 5.1723 - val_loss: 232.5345 - val_mae: 9.7250\n",
      "Epoch 661/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 50.5363 - mae: 4.8651 - val_loss: 207.8953 - val_mae: 9.3730\n",
      "Epoch 662/700\n",
      "122/122 [==============================] - 0s 571us/step - loss: 51.3845 - mae: 4.9394 - val_loss: 181.8519 - val_mae: 9.0085\n",
      "Epoch 663/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 53.4947 - mae: 4.9835 - val_loss: 155.8899 - val_mae: 8.4568\n",
      "Epoch 664/700\n",
      "122/122 [==============================] - 0s 628us/step - loss: 49.1927 - mae: 4.8922 - val_loss: 247.8845 - val_mae: 9.8909\n",
      "Epoch 665/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 54.2836 - mae: 5.0110 - val_loss: 192.1366 - val_mae: 9.0774\n",
      "Epoch 666/700\n",
      "122/122 [==============================] - 0s 585us/step - loss: 54.1823 - mae: 5.0552 - val_loss: 166.0297 - val_mae: 8.5159\n",
      "Epoch 667/700\n",
      "122/122 [==============================] - 0s 562us/step - loss: 51.4089 - mae: 4.9246 - val_loss: 144.8729 - val_mae: 8.1393\n",
      "Epoch 668/700\n",
      "122/122 [==============================] - 0s 649us/step - loss: 55.6368 - mae: 5.0438 - val_loss: 272.0957 - val_mae: 10.1987\n",
      "Epoch 669/700\n",
      "122/122 [==============================] - 0s 608us/step - loss: 54.6309 - mae: 4.9832 - val_loss: 147.6295 - val_mae: 8.1866\n",
      "Epoch 670/700\n",
      "122/122 [==============================] - 0s 590us/step - loss: 53.9952 - mae: 4.9103 - val_loss: 212.7044 - val_mae: 9.4974\n",
      "Epoch 671/700\n",
      "122/122 [==============================] - 0s 578us/step - loss: 56.2496 - mae: 5.1728 - val_loss: 265.9215 - val_mae: 10.1820\n",
      "Epoch 672/700\n",
      "122/122 [==============================] - 0s 588us/step - loss: 47.4771 - mae: 4.7576 - val_loss: 244.3893 - val_mae: 9.9091\n",
      "Epoch 673/700\n",
      "122/122 [==============================] - 0s 619us/step - loss: 52.1941 - mae: 4.9287 - val_loss: 189.4919 - val_mae: 8.9840\n",
      "Epoch 674/700\n",
      "122/122 [==============================] - 0s 650us/step - loss: 53.9982 - mae: 4.9815 - val_loss: 145.9167 - val_mae: 8.4170\n",
      "Epoch 675/700\n",
      "122/122 [==============================] - 0s 562us/step - loss: 48.4322 - mae: 4.8326 - val_loss: 240.4678 - val_mae: 9.9191\n",
      "Epoch 676/700\n",
      "122/122 [==============================] - 0s 566us/step - loss: 51.9051 - mae: 4.8775 - val_loss: 140.0728 - val_mae: 8.0392\n",
      "Epoch 677/700\n",
      "122/122 [==============================] - 0s 560us/step - loss: 53.1896 - mae: 4.9459 - val_loss: 218.7359 - val_mae: 9.4678\n",
      "Epoch 678/700\n",
      "122/122 [==============================] - 0s 670us/step - loss: 51.4205 - mae: 4.8898 - val_loss: 195.9184 - val_mae: 9.0463\n",
      "Epoch 679/700\n",
      "122/122 [==============================] - 0s 604us/step - loss: 57.4552 - mae: 5.0659 - val_loss: 243.6147 - val_mae: 9.9217\n",
      "Epoch 680/700\n",
      "122/122 [==============================] - 0s 674us/step - loss: 49.9185 - mae: 4.8825 - val_loss: 222.8497 - val_mae: 9.6006\n",
      "Epoch 681/700\n",
      "122/122 [==============================] - 0s 583us/step - loss: 51.4775 - mae: 4.9703 - val_loss: 175.8008 - val_mae: 8.7734\n",
      "Epoch 682/700\n",
      "122/122 [==============================] - 0s 575us/step - loss: 47.5971 - mae: 4.7893 - val_loss: 176.1851 - val_mae: 8.7535\n",
      "Epoch 683/700\n",
      "122/122 [==============================] - 0s 584us/step - loss: 51.2515 - mae: 4.8946 - val_loss: 169.6835 - val_mae: 8.7589\n",
      "Epoch 684/700\n",
      "122/122 [==============================] - 0s 563us/step - loss: 47.9088 - mae: 4.8386 - val_loss: 201.7228 - val_mae: 9.1961\n",
      "Epoch 685/700\n",
      "122/122 [==============================] - 0s 609us/step - loss: 52.0422 - mae: 4.9876 - val_loss: 203.7504 - val_mae: 9.2120\n",
      "Epoch 686/700\n",
      "122/122 [==============================] - 0s 617us/step - loss: 49.0145 - mae: 4.8650 - val_loss: 173.8924 - val_mae: 8.6153\n",
      "Epoch 687/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 54.1343 - mae: 4.8984 - val_loss: 269.4128 - val_mae: 10.2380\n",
      "Epoch 688/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 52.5227 - mae: 4.9340 - val_loss: 180.1104 - val_mae: 8.7902\n",
      "Epoch 689/700\n",
      "122/122 [==============================] - 0s 572us/step - loss: 50.2637 - mae: 4.8499 - val_loss: 212.8572 - val_mae: 9.2765\n",
      "Epoch 690/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 51.2829 - mae: 4.8968 - val_loss: 171.0556 - val_mae: 8.6234\n",
      "Epoch 691/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 53.8972 - mae: 5.1493 - val_loss: 169.1407 - val_mae: 8.5116\n",
      "Epoch 692/700\n",
      "122/122 [==============================] - 0s 655us/step - loss: 52.2073 - mae: 4.9087 - val_loss: 173.2066 - val_mae: 8.6530\n",
      "Epoch 693/700\n",
      "122/122 [==============================] - 0s 591us/step - loss: 60.0250 - mae: 5.1112 - val_loss: 191.9978 - val_mae: 9.0733\n",
      "Epoch 694/700\n",
      "122/122 [==============================] - 0s 576us/step - loss: 51.4151 - mae: 4.9086 - val_loss: 195.0779 - val_mae: 9.2343\n",
      "Epoch 695/700\n",
      "122/122 [==============================] - 0s 580us/step - loss: 51.2335 - mae: 4.9160 - val_loss: 191.2170 - val_mae: 9.0634\n",
      "Epoch 696/700\n",
      "122/122 [==============================] - 0s 607us/step - loss: 47.7953 - mae: 4.8140 - val_loss: 127.5185 - val_mae: 7.8489\n",
      "Epoch 697/700\n",
      "122/122 [==============================] - 0s 612us/step - loss: 54.6787 - mae: 5.0336 - val_loss: 189.2764 - val_mae: 8.9842\n",
      "Epoch 698/700\n",
      "122/122 [==============================] - 0s 592us/step - loss: 50.8607 - mae: 4.8184 - val_loss: 175.0017 - val_mae: 8.6625\n",
      "Epoch 699/700\n",
      "122/122 [==============================] - 0s 573us/step - loss: 48.7056 - mae: 4.8396 - val_loss: 177.3735 - val_mae: 8.8688\n",
      "Epoch 700/700\n",
      "122/122 [==============================] - 0s 618us/step - loss: 52.7977 - mae: 4.9097 - val_loss: 129.7015 - val_mae: 8.0519\n",
      "31/31 [==============================] - 0s 419us/step\n",
      "Epochs: 700 | MAE: 8.05191028129382\n",
      "Training model with 750 epochs\n",
      "Epoch 1/750\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 65187.5039 - mae: 220.4984 - val_loss: 51309.8242 - val_mae: 194.9362\n",
      "Epoch 2/750\n",
      "122/122 [==============================] - 0s 641us/step - loss: 27294.2207 - mae: 136.2344 - val_loss: 7185.7207 - val_mae: 69.5368\n",
      "Epoch 3/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 4887.8188 - mae: 54.1340 - val_loss: 2874.9031 - val_mae: 40.3330\n",
      "Epoch 4/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 2942.4875 - mae: 40.9085 - val_loss: 1950.8086 - val_mae: 33.3177\n",
      "Epoch 5/750\n",
      "122/122 [==============================] - 0s 569us/step - loss: 2487.9941 - mae: 37.9255 - val_loss: 1502.7761 - val_mae: 29.1550\n",
      "Epoch 6/750\n",
      "122/122 [==============================] - 0s 572us/step - loss: 2129.8491 - mae: 34.9648 - val_loss: 1279.1160 - val_mae: 26.9322\n",
      "Epoch 7/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 1868.0558 - mae: 33.5051 - val_loss: 1156.9916 - val_mae: 25.4975\n",
      "Epoch 8/750\n",
      "122/122 [==============================] - 0s 640us/step - loss: 1818.8962 - mae: 33.2425 - val_loss: 1038.5994 - val_mae: 24.1155\n",
      "Epoch 9/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 1699.3938 - mae: 31.5287 - val_loss: 914.0200 - val_mae: 22.6331\n",
      "Epoch 10/750\n",
      "122/122 [==============================] - 0s 575us/step - loss: 1483.0950 - mae: 30.1185 - val_loss: 829.3292 - val_mae: 21.2773\n",
      "Epoch 11/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 1412.5913 - mae: 29.0360 - val_loss: 739.6962 - val_mae: 20.1411\n",
      "Epoch 12/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 1301.1948 - mae: 27.8645 - val_loss: 652.1663 - val_mae: 18.9796\n",
      "Epoch 13/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 1226.3090 - mae: 27.0204 - val_loss: 595.5972 - val_mae: 17.9722\n",
      "Epoch 14/750\n",
      "122/122 [==============================] - 0s 570us/step - loss: 1102.3135 - mae: 25.9350 - val_loss: 533.3156 - val_mae: 17.1560\n",
      "Epoch 15/750\n",
      "122/122 [==============================] - 0s 719us/step - loss: 1096.1112 - mae: 25.7433 - val_loss: 472.1886 - val_mae: 16.2660\n",
      "Epoch 16/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 1016.0357 - mae: 24.7902 - val_loss: 440.2409 - val_mae: 15.8440\n",
      "Epoch 17/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 991.6978 - mae: 24.1516 - val_loss: 372.4402 - val_mae: 14.2136\n",
      "Epoch 18/750\n",
      "122/122 [==============================] - 0s 587us/step - loss: 927.4822 - mae: 23.5771 - val_loss: 319.4629 - val_mae: 13.4830\n",
      "Epoch 19/750\n",
      "122/122 [==============================] - 0s 670us/step - loss: 861.4907 - mae: 22.7506 - val_loss: 282.2415 - val_mae: 12.4301\n",
      "Epoch 20/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 856.8712 - mae: 22.3765 - val_loss: 270.0228 - val_mae: 12.3542\n",
      "Epoch 21/750\n",
      "122/122 [==============================] - 0s 565us/step - loss: 831.2477 - mae: 22.1650 - val_loss: 224.6322 - val_mae: 11.2224\n",
      "Epoch 22/750\n",
      "122/122 [==============================] - 0s 571us/step - loss: 784.8680 - mae: 21.3931 - val_loss: 212.0873 - val_mae: 11.0165\n",
      "Epoch 23/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 769.9036 - mae: 21.0370 - val_loss: 180.4872 - val_mae: 9.9933\n",
      "Epoch 24/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 748.6667 - mae: 20.8239 - val_loss: 175.4577 - val_mae: 10.0449\n",
      "Epoch 25/750\n",
      "122/122 [==============================] - 0s 578us/step - loss: 740.2646 - mae: 20.7009 - val_loss: 153.9186 - val_mae: 9.5208\n",
      "Epoch 26/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 694.8328 - mae: 19.9643 - val_loss: 126.2937 - val_mae: 8.1156\n",
      "Epoch 27/750\n",
      "122/122 [==============================] - 0s 641us/step - loss: 699.9186 - mae: 19.8061 - val_loss: 120.4430 - val_mae: 8.4462\n",
      "Epoch 28/750\n",
      "122/122 [==============================] - 0s 711us/step - loss: 656.8355 - mae: 19.2463 - val_loss: 108.4187 - val_mae: 7.9618\n",
      "Epoch 29/750\n",
      "122/122 [==============================] - 0s 597us/step - loss: 670.0728 - mae: 19.3182 - val_loss: 95.5330 - val_mae: 7.1247\n",
      "Epoch 30/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 639.1293 - mae: 18.8551 - val_loss: 95.0886 - val_mae: 7.1461\n",
      "Epoch 31/750\n",
      "122/122 [==============================] - 0s 689us/step - loss: 609.6102 - mae: 18.4240 - val_loss: 81.6363 - val_mae: 6.6015\n",
      "Epoch 32/750\n",
      "122/122 [==============================] - 0s 596us/step - loss: 604.7542 - mae: 18.2857 - val_loss: 70.0277 - val_mae: 6.0866\n",
      "Epoch 33/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 637.9249 - mae: 18.6750 - val_loss: 72.9175 - val_mae: 6.2738\n",
      "Epoch 34/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 634.3276 - mae: 18.4111 - val_loss: 72.0463 - val_mae: 6.8423\n",
      "Epoch 35/750\n",
      "122/122 [==============================] - 0s 674us/step - loss: 596.0488 - mae: 17.9468 - val_loss: 57.8864 - val_mae: 5.6304\n",
      "Epoch 36/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 574.8701 - mae: 17.4882 - val_loss: 54.5006 - val_mae: 5.4861\n",
      "Epoch 37/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 589.1151 - mae: 17.7314 - val_loss: 51.0440 - val_mae: 5.3672\n",
      "Epoch 38/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 565.7391 - mae: 17.2872 - val_loss: 52.1387 - val_mae: 5.3114\n",
      "Epoch 39/750\n",
      "122/122 [==============================] - 0s 727us/step - loss: 564.1058 - mae: 17.1910 - val_loss: 49.6943 - val_mae: 5.1017\n",
      "Epoch 40/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 573.5571 - mae: 17.3647 - val_loss: 46.1403 - val_mae: 4.8908\n",
      "Epoch 41/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 553.5903 - mae: 17.0539 - val_loss: 40.1740 - val_mae: 4.8562\n",
      "Epoch 42/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 544.4586 - mae: 16.6821 - val_loss: 33.8050 - val_mae: 4.2735\n",
      "Epoch 43/750\n",
      "122/122 [==============================] - 0s 771us/step - loss: 567.5739 - mae: 17.0912 - val_loss: 35.3165 - val_mae: 4.5526\n",
      "Epoch 44/750\n",
      "122/122 [==============================] - 0s 635us/step - loss: 514.0103 - mae: 16.4790 - val_loss: 35.3669 - val_mae: 4.5708\n",
      "Epoch 45/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 515.7717 - mae: 16.1965 - val_loss: 44.6452 - val_mae: 5.0061\n",
      "Epoch 46/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 530.8550 - mae: 16.3306 - val_loss: 29.2821 - val_mae: 3.9368\n",
      "Epoch 47/750\n",
      "122/122 [==============================] - 0s 742us/step - loss: 529.2586 - mae: 16.3832 - val_loss: 27.5127 - val_mae: 3.7541\n",
      "Epoch 48/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 513.4128 - mae: 16.0754 - val_loss: 40.8036 - val_mae: 4.7687\n",
      "Epoch 49/750\n",
      "122/122 [==============================] - 0s 624us/step - loss: 500.5126 - mae: 15.8769 - val_loss: 28.0605 - val_mae: 3.8329\n",
      "Epoch 50/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 486.8110 - mae: 15.8055 - val_loss: 29.2236 - val_mae: 3.8075\n",
      "Epoch 51/750\n",
      "122/122 [==============================] - 0s 644us/step - loss: 514.4073 - mae: 16.0397 - val_loss: 31.1405 - val_mae: 4.2309\n",
      "Epoch 52/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 480.3236 - mae: 15.7963 - val_loss: 36.3308 - val_mae: 4.5198\n",
      "Epoch 53/750\n",
      "122/122 [==============================] - 0s 652us/step - loss: 497.5890 - mae: 15.6159 - val_loss: 27.1728 - val_mae: 3.7397\n",
      "Epoch 54/750\n",
      "122/122 [==============================] - 0s 661us/step - loss: 491.3490 - mae: 15.8530 - val_loss: 32.7786 - val_mae: 4.1301\n",
      "Epoch 55/750\n",
      "122/122 [==============================] - 0s 627us/step - loss: 468.4221 - mae: 15.2974 - val_loss: 28.4922 - val_mae: 3.9302\n",
      "Epoch 56/750\n",
      "122/122 [==============================] - 0s 620us/step - loss: 480.8015 - mae: 15.3228 - val_loss: 33.4377 - val_mae: 4.4146\n",
      "Epoch 57/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 487.0378 - mae: 15.5315 - val_loss: 28.9469 - val_mae: 3.8596\n",
      "Epoch 58/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 490.9618 - mae: 15.7149 - val_loss: 40.2462 - val_mae: 4.8506\n",
      "Epoch 59/750\n",
      "122/122 [==============================] - 0s 672us/step - loss: 469.9994 - mae: 15.1638 - val_loss: 29.8899 - val_mae: 3.9234\n",
      "Epoch 60/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 491.2078 - mae: 15.2472 - val_loss: 25.2813 - val_mae: 3.5662\n",
      "Epoch 61/750\n",
      "122/122 [==============================] - 0s 606us/step - loss: 467.5245 - mae: 15.1716 - val_loss: 28.2194 - val_mae: 3.6309\n",
      "Epoch 62/750\n",
      "122/122 [==============================] - 0s 621us/step - loss: 472.8360 - mae: 15.2967 - val_loss: 27.2424 - val_mae: 3.8897\n",
      "Epoch 63/750\n",
      "122/122 [==============================] - 0s 661us/step - loss: 447.2660 - mae: 15.0066 - val_loss: 31.0576 - val_mae: 3.9730\n",
      "Epoch 64/750\n",
      "122/122 [==============================] - 0s 631us/step - loss: 470.0720 - mae: 15.1593 - val_loss: 30.6113 - val_mae: 4.1045\n",
      "Epoch 65/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 471.4536 - mae: 14.9177 - val_loss: 25.8668 - val_mae: 3.5570\n",
      "Epoch 66/750\n",
      "122/122 [==============================] - 0s 657us/step - loss: 463.6994 - mae: 15.1013 - val_loss: 36.0089 - val_mae: 4.4858\n",
      "Epoch 67/750\n",
      "122/122 [==============================] - 0s 674us/step - loss: 416.7884 - mae: 14.2728 - val_loss: 28.4063 - val_mae: 3.8423\n",
      "Epoch 68/750\n",
      "122/122 [==============================] - 0s 606us/step - loss: 451.3329 - mae: 14.8170 - val_loss: 25.2182 - val_mae: 3.6501\n",
      "Epoch 69/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 440.0400 - mae: 14.5030 - val_loss: 28.1339 - val_mae: 3.8862\n",
      "Epoch 70/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 435.7679 - mae: 14.6170 - val_loss: 26.3699 - val_mae: 3.6720\n",
      "Epoch 71/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 441.9698 - mae: 14.5090 - val_loss: 34.4268 - val_mae: 4.3768\n",
      "Epoch 72/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 467.8562 - mae: 14.9092 - val_loss: 30.3930 - val_mae: 3.8510\n",
      "Epoch 73/750\n",
      "122/122 [==============================] - 0s 641us/step - loss: 435.2520 - mae: 14.4771 - val_loss: 29.4505 - val_mae: 3.9698\n",
      "Epoch 74/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 440.0188 - mae: 14.4315 - val_loss: 33.6909 - val_mae: 3.9734\n",
      "Epoch 75/750\n",
      "122/122 [==============================] - 0s 629us/step - loss: 469.5499 - mae: 14.8362 - val_loss: 31.4771 - val_mae: 4.0449\n",
      "Epoch 76/750\n",
      "122/122 [==============================] - 0s 691us/step - loss: 413.0074 - mae: 14.1017 - val_loss: 26.8940 - val_mae: 3.8039\n",
      "Epoch 77/750\n",
      "122/122 [==============================] - 0s 676us/step - loss: 441.8335 - mae: 14.6213 - val_loss: 31.4120 - val_mae: 4.1100\n",
      "Epoch 78/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 414.8770 - mae: 13.9873 - val_loss: 27.2018 - val_mae: 3.7110\n",
      "Epoch 79/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 432.7466 - mae: 14.4477 - val_loss: 51.2471 - val_mae: 5.6344\n",
      "Epoch 80/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 365.4446 - mae: 13.3803 - val_loss: 31.6195 - val_mae: 4.1498\n",
      "Epoch 81/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 409.6750 - mae: 13.9151 - val_loss: 30.7983 - val_mae: 4.1957\n",
      "Epoch 82/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 402.6292 - mae: 13.7792 - val_loss: 33.6407 - val_mae: 4.5298\n",
      "Epoch 83/750\n",
      "122/122 [==============================] - 0s 657us/step - loss: 395.7181 - mae: 13.7553 - val_loss: 30.6332 - val_mae: 4.1030\n",
      "Epoch 84/750\n",
      "122/122 [==============================] - 0s 635us/step - loss: 388.6727 - mae: 13.6776 - val_loss: 31.0467 - val_mae: 4.2214\n",
      "Epoch 85/750\n",
      "122/122 [==============================] - 0s 649us/step - loss: 422.7096 - mae: 14.1250 - val_loss: 27.9283 - val_mae: 3.9619\n",
      "Epoch 86/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 392.5157 - mae: 13.7462 - val_loss: 25.3875 - val_mae: 3.5863\n",
      "Epoch 87/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 384.0136 - mae: 13.4218 - val_loss: 27.1262 - val_mae: 3.7175\n",
      "Epoch 88/750\n",
      "122/122 [==============================] - 0s 615us/step - loss: 395.9363 - mae: 13.5353 - val_loss: 30.7347 - val_mae: 4.1780\n",
      "Epoch 89/750\n",
      "122/122 [==============================] - 0s 653us/step - loss: 387.0459 - mae: 13.4158 - val_loss: 38.0909 - val_mae: 4.1936\n",
      "Epoch 90/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 399.4882 - mae: 13.7378 - val_loss: 33.0685 - val_mae: 4.3634\n",
      "Epoch 91/750\n",
      "122/122 [==============================] - 0s 872us/step - loss: 389.9016 - mae: 13.6385 - val_loss: 25.2307 - val_mae: 3.5858\n",
      "Epoch 92/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 350.8708 - mae: 12.9912 - val_loss: 35.1893 - val_mae: 4.6195\n",
      "Epoch 93/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 366.0624 - mae: 13.0722 - val_loss: 26.1609 - val_mae: 3.6082\n",
      "Epoch 94/750\n",
      "122/122 [==============================] - 0s 603us/step - loss: 387.6786 - mae: 13.3560 - val_loss: 27.8310 - val_mae: 3.8904\n",
      "Epoch 95/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 360.0023 - mae: 13.0038 - val_loss: 26.6863 - val_mae: 3.6122\n",
      "Epoch 96/750\n",
      "122/122 [==============================] - 0s 673us/step - loss: 386.7154 - mae: 13.3347 - val_loss: 28.7397 - val_mae: 3.9825\n",
      "Epoch 97/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 347.4391 - mae: 12.8053 - val_loss: 29.9874 - val_mae: 3.7912\n",
      "Epoch 98/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 381.5159 - mae: 13.2007 - val_loss: 23.5557 - val_mae: 3.3966\n",
      "Epoch 99/750\n",
      "122/122 [==============================] - 0s 596us/step - loss: 364.7613 - mae: 12.9247 - val_loss: 32.3878 - val_mae: 4.2461\n",
      "Epoch 100/750\n",
      "122/122 [==============================] - 0s 597us/step - loss: 370.0484 - mae: 13.1204 - val_loss: 32.8677 - val_mae: 4.3996\n",
      "Epoch 101/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 370.4737 - mae: 13.0424 - val_loss: 23.1950 - val_mae: 3.3395\n",
      "Epoch 102/750\n",
      "122/122 [==============================] - 0s 728us/step - loss: 329.2831 - mae: 12.5871 - val_loss: 26.0019 - val_mae: 3.7154\n",
      "Epoch 103/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 379.8509 - mae: 13.1829 - val_loss: 28.8672 - val_mae: 3.8160\n",
      "Epoch 104/750\n",
      "122/122 [==============================] - 0s 580us/step - loss: 331.7564 - mae: 12.3289 - val_loss: 29.7114 - val_mae: 4.1465\n",
      "Epoch 105/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 365.8698 - mae: 12.9392 - val_loss: 25.1126 - val_mae: 3.6331\n",
      "Epoch 106/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 349.2363 - mae: 12.6402 - val_loss: 24.7621 - val_mae: 3.4609\n",
      "Epoch 107/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 361.9210 - mae: 12.9309 - val_loss: 29.4076 - val_mae: 3.9366\n",
      "Epoch 108/750\n",
      "122/122 [==============================] - 0s 716us/step - loss: 344.4705 - mae: 12.6147 - val_loss: 23.1562 - val_mae: 3.4542\n",
      "Epoch 109/750\n",
      "122/122 [==============================] - 0s 603us/step - loss: 340.0632 - mae: 12.5375 - val_loss: 27.2099 - val_mae: 3.8493\n",
      "Epoch 110/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 334.1603 - mae: 12.3852 - val_loss: 25.6068 - val_mae: 3.6127\n",
      "Epoch 111/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 330.6932 - mae: 12.4095 - val_loss: 33.2645 - val_mae: 4.2884\n",
      "Epoch 112/750\n",
      "122/122 [==============================] - 0s 564us/step - loss: 360.3134 - mae: 12.6543 - val_loss: 25.9407 - val_mae: 3.5789\n",
      "Epoch 113/750\n",
      "122/122 [==============================] - 0s 685us/step - loss: 337.0152 - mae: 12.4943 - val_loss: 32.1921 - val_mae: 4.3143\n",
      "Epoch 114/750\n",
      "122/122 [==============================] - 0s 596us/step - loss: 343.0208 - mae: 12.7070 - val_loss: 28.6142 - val_mae: 3.8605\n",
      "Epoch 115/750\n",
      "122/122 [==============================] - 0s 573us/step - loss: 308.3645 - mae: 11.9302 - val_loss: 29.0348 - val_mae: 3.8076\n",
      "Epoch 116/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 325.0506 - mae: 12.1388 - val_loss: 25.3131 - val_mae: 3.6277\n",
      "Epoch 117/750\n",
      "122/122 [==============================] - 0s 570us/step - loss: 342.4169 - mae: 12.5185 - val_loss: 27.9526 - val_mae: 3.8223\n",
      "Epoch 118/750\n",
      "122/122 [==============================] - 0s 652us/step - loss: 321.9487 - mae: 12.1820 - val_loss: 30.0617 - val_mae: 4.1881\n",
      "Epoch 119/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 319.3962 - mae: 12.1183 - val_loss: 32.6252 - val_mae: 4.2461\n",
      "Epoch 120/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 331.1466 - mae: 12.2057 - val_loss: 33.4789 - val_mae: 4.3939\n",
      "Epoch 121/750\n",
      "122/122 [==============================] - 0s 571us/step - loss: 337.4375 - mae: 12.4541 - val_loss: 24.7939 - val_mae: 3.4772\n",
      "Epoch 122/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 324.9917 - mae: 12.0203 - val_loss: 31.4766 - val_mae: 4.0136\n",
      "Epoch 123/750\n",
      "122/122 [==============================] - 0s 580us/step - loss: 309.4365 - mae: 11.8082 - val_loss: 24.1454 - val_mae: 3.5362\n",
      "Epoch 124/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 332.0219 - mae: 12.2279 - val_loss: 25.1862 - val_mae: 3.7033\n",
      "Epoch 125/750\n",
      "122/122 [==============================] - 0s 575us/step - loss: 320.1754 - mae: 11.8719 - val_loss: 22.5001 - val_mae: 3.3496\n",
      "Epoch 126/750\n",
      "122/122 [==============================] - 0s 721us/step - loss: 324.0073 - mae: 12.0673 - val_loss: 22.6305 - val_mae: 3.3849\n",
      "Epoch 127/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 316.1088 - mae: 11.9787 - val_loss: 24.6581 - val_mae: 3.4935\n",
      "Epoch 128/750\n",
      "122/122 [==============================] - 0s 573us/step - loss: 312.4164 - mae: 11.8243 - val_loss: 31.4956 - val_mae: 4.2661\n",
      "Epoch 129/750\n",
      "122/122 [==============================] - 0s 561us/step - loss: 325.1272 - mae: 12.0294 - val_loss: 34.5556 - val_mae: 4.5583\n",
      "Epoch 130/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 305.7349 - mae: 11.7519 - val_loss: 42.5636 - val_mae: 5.1860\n",
      "Epoch 131/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 295.3187 - mae: 11.4415 - val_loss: 30.8983 - val_mae: 4.3001\n",
      "Epoch 132/750\n",
      "122/122 [==============================] - 0s 567us/step - loss: 284.1753 - mae: 11.3154 - val_loss: 23.6393 - val_mae: 3.4778\n",
      "Epoch 133/750\n",
      "122/122 [==============================] - 0s 573us/step - loss: 297.9273 - mae: 11.5590 - val_loss: 26.1128 - val_mae: 3.8063\n",
      "Epoch 134/750\n",
      "122/122 [==============================] - 0s 558us/step - loss: 313.0076 - mae: 11.7944 - val_loss: 30.3726 - val_mae: 4.1456\n",
      "Epoch 135/750\n",
      "122/122 [==============================] - 0s 597us/step - loss: 290.7456 - mae: 11.4169 - val_loss: 26.0645 - val_mae: 3.7364\n",
      "Epoch 136/750\n",
      "122/122 [==============================] - 0s 569us/step - loss: 292.2459 - mae: 11.5368 - val_loss: 36.2944 - val_mae: 4.5370\n",
      "Epoch 137/750\n",
      "122/122 [==============================] - 0s 645us/step - loss: 304.0342 - mae: 11.6253 - val_loss: 27.3816 - val_mae: 3.7963\n",
      "Epoch 138/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 291.7186 - mae: 11.4484 - val_loss: 26.5463 - val_mae: 3.8328\n",
      "Epoch 139/750\n",
      "122/122 [==============================] - 0s 561us/step - loss: 266.0721 - mae: 10.9232 - val_loss: 26.0099 - val_mae: 3.6936\n",
      "Epoch 140/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 277.9411 - mae: 11.1475 - val_loss: 23.1775 - val_mae: 3.4538\n",
      "Epoch 141/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 285.9817 - mae: 11.3195 - val_loss: 26.9315 - val_mae: 3.8977\n",
      "Epoch 142/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 275.6297 - mae: 11.1925 - val_loss: 22.7735 - val_mae: 3.4222\n",
      "Epoch 143/750\n",
      "122/122 [==============================] - 0s 570us/step - loss: 287.6211 - mae: 11.2209 - val_loss: 25.9877 - val_mae: 3.7114\n",
      "Epoch 144/750\n",
      "122/122 [==============================] - 0s 698us/step - loss: 276.1223 - mae: 11.1963 - val_loss: 27.4152 - val_mae: 3.8148\n",
      "Epoch 145/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 266.7807 - mae: 10.9254 - val_loss: 28.0912 - val_mae: 3.7603\n",
      "Epoch 146/750\n",
      "122/122 [==============================] - 0s 568us/step - loss: 286.7945 - mae: 11.2722 - val_loss: 27.1028 - val_mae: 3.9056\n",
      "Epoch 147/750\n",
      "122/122 [==============================] - 0s 575us/step - loss: 271.1748 - mae: 11.0571 - val_loss: 30.2478 - val_mae: 4.1307\n",
      "Epoch 148/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 255.3489 - mae: 10.6917 - val_loss: 52.7705 - val_mae: 5.5130\n",
      "Epoch 149/750\n",
      "122/122 [==============================] - 0s 688us/step - loss: 278.2041 - mae: 11.0428 - val_loss: 23.4536 - val_mae: 3.4375\n",
      "Epoch 150/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 266.0347 - mae: 10.8709 - val_loss: 27.7841 - val_mae: 3.9562\n",
      "Epoch 151/750\n",
      "122/122 [==============================] - 0s 568us/step - loss: 266.1334 - mae: 10.7853 - val_loss: 24.0730 - val_mae: 3.4693\n",
      "Epoch 152/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 266.9293 - mae: 10.7656 - val_loss: 23.3304 - val_mae: 3.4580\n",
      "Epoch 153/750\n",
      "122/122 [==============================] - 0s 586us/step - loss: 264.0152 - mae: 10.6911 - val_loss: 27.0909 - val_mae: 3.6745\n",
      "Epoch 154/750\n",
      "122/122 [==============================] - 0s 617us/step - loss: 255.7213 - mae: 10.6001 - val_loss: 30.0861 - val_mae: 4.0056\n",
      "Epoch 155/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 250.2846 - mae: 10.5570 - val_loss: 25.7677 - val_mae: 3.6388\n",
      "Epoch 156/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 245.9422 - mae: 10.4676 - val_loss: 24.9514 - val_mae: 3.5688\n",
      "Epoch 157/750\n",
      "122/122 [==============================] - 0s 620us/step - loss: 263.3073 - mae: 10.6415 - val_loss: 30.8695 - val_mae: 4.0203\n",
      "Epoch 158/750\n",
      "122/122 [==============================] - 0s 635us/step - loss: 257.5084 - mae: 10.5590 - val_loss: 31.1880 - val_mae: 4.1365\n",
      "Epoch 159/750\n",
      "122/122 [==============================] - 0s 610us/step - loss: 254.8547 - mae: 10.5671 - val_loss: 39.2908 - val_mae: 4.7282\n",
      "Epoch 160/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 230.5261 - mae: 10.1211 - val_loss: 25.2417 - val_mae: 3.5613\n",
      "Epoch 161/750\n",
      "122/122 [==============================] - 0s 565us/step - loss: 233.9333 - mae: 10.2543 - val_loss: 39.8067 - val_mae: 4.7038\n",
      "Epoch 162/750\n",
      "122/122 [==============================] - 0s 620us/step - loss: 254.6285 - mae: 10.4345 - val_loss: 26.0505 - val_mae: 3.5875\n",
      "Epoch 163/750\n",
      "122/122 [==============================] - 0s 575us/step - loss: 250.1729 - mae: 10.3778 - val_loss: 40.3872 - val_mae: 4.8498\n",
      "Epoch 164/750\n",
      "122/122 [==============================] - 0s 718us/step - loss: 250.1035 - mae: 10.4173 - val_loss: 29.0320 - val_mae: 4.0718\n",
      "Epoch 165/750\n",
      "122/122 [==============================] - 0s 617us/step - loss: 246.6373 - mae: 10.3190 - val_loss: 29.5325 - val_mae: 3.9260\n",
      "Epoch 166/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 245.4621 - mae: 10.4449 - val_loss: 42.3405 - val_mae: 5.0266\n",
      "Epoch 167/750\n",
      "122/122 [==============================] - 0s 606us/step - loss: 228.1213 - mae: 10.0675 - val_loss: 31.9321 - val_mae: 4.3536\n",
      "Epoch 168/750\n",
      "122/122 [==============================] - 0s 633us/step - loss: 219.7402 - mae: 9.9536 - val_loss: 29.7571 - val_mae: 4.1265\n",
      "Epoch 169/750\n",
      "122/122 [==============================] - 0s 655us/step - loss: 224.6419 - mae: 9.9410 - val_loss: 26.7296 - val_mae: 3.8084\n",
      "Epoch 170/750\n",
      "122/122 [==============================] - 0s 686us/step - loss: 223.4574 - mae: 9.9696 - val_loss: 33.9780 - val_mae: 4.3748\n",
      "Epoch 171/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 235.5465 - mae: 10.1035 - val_loss: 24.7279 - val_mae: 3.6258\n",
      "Epoch 172/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 225.1040 - mae: 9.9361 - val_loss: 36.7190 - val_mae: 4.7308\n",
      "Epoch 173/750\n",
      "122/122 [==============================] - 0s 617us/step - loss: 226.6889 - mae: 10.0141 - val_loss: 28.6795 - val_mae: 3.9057\n",
      "Epoch 174/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 234.7748 - mae: 10.0397 - val_loss: 24.4291 - val_mae: 3.4803\n",
      "Epoch 175/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 225.1133 - mae: 9.9049 - val_loss: 24.1683 - val_mae: 3.4733\n",
      "Epoch 176/750\n",
      "122/122 [==============================] - 0s 666us/step - loss: 222.2979 - mae: 9.8440 - val_loss: 27.9172 - val_mae: 3.9288\n",
      "Epoch 177/750\n",
      "122/122 [==============================] - 0s 620us/step - loss: 220.9161 - mae: 9.7645 - val_loss: 36.4503 - val_mae: 4.6949\n",
      "Epoch 178/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 224.6433 - mae: 9.8468 - val_loss: 24.3126 - val_mae: 3.5822\n",
      "Epoch 179/750\n",
      "122/122 [==============================] - 0s 654us/step - loss: 215.3280 - mae: 9.7683 - val_loss: 25.0606 - val_mae: 3.5490\n",
      "Epoch 180/750\n",
      "122/122 [==============================] - 0s 679us/step - loss: 224.3337 - mae: 9.7196 - val_loss: 23.4832 - val_mae: 3.4401\n",
      "Epoch 181/750\n",
      "122/122 [==============================] - 0s 811us/step - loss: 208.7022 - mae: 9.5026 - val_loss: 22.7314 - val_mae: 3.3977\n",
      "Epoch 182/750\n",
      "122/122 [==============================] - 0s 666us/step - loss: 214.4337 - mae: 9.5477 - val_loss: 33.6967 - val_mae: 4.4507\n",
      "Epoch 183/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 216.7426 - mae: 9.7175 - val_loss: 22.4529 - val_mae: 3.3701\n",
      "Epoch 184/750\n",
      "122/122 [==============================] - 0s 665us/step - loss: 208.6375 - mae: 9.4229 - val_loss: 28.0540 - val_mae: 3.9492\n",
      "Epoch 185/750\n",
      "122/122 [==============================] - 0s 674us/step - loss: 221.2925 - mae: 9.6290 - val_loss: 23.9061 - val_mae: 3.4524\n",
      "Epoch 186/750\n",
      "122/122 [==============================] - 0s 693us/step - loss: 208.2491 - mae: 9.5503 - val_loss: 32.6115 - val_mae: 4.3319\n",
      "Epoch 187/750\n",
      "122/122 [==============================] - 0s 672us/step - loss: 224.0802 - mae: 9.7098 - val_loss: 26.1869 - val_mae: 3.5672\n",
      "Epoch 188/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 212.7798 - mae: 9.5774 - val_loss: 27.1384 - val_mae: 3.8036\n",
      "Epoch 189/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 213.4725 - mae: 9.4976 - val_loss: 24.6110 - val_mae: 3.5414\n",
      "Epoch 190/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 199.0333 - mae: 9.1678 - val_loss: 29.6783 - val_mae: 3.8375\n",
      "Epoch 191/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 194.0209 - mae: 9.2636 - val_loss: 24.8799 - val_mae: 3.5668\n",
      "Epoch 192/750\n",
      "122/122 [==============================] - 0s 650us/step - loss: 203.5362 - mae: 9.2447 - val_loss: 27.1033 - val_mae: 3.8523\n",
      "Epoch 193/750\n",
      "122/122 [==============================] - 0s 620us/step - loss: 194.9914 - mae: 9.1183 - val_loss: 46.5785 - val_mae: 5.3673\n",
      "Epoch 194/750\n",
      "122/122 [==============================] - 0s 617us/step - loss: 206.2215 - mae: 9.3593 - val_loss: 33.3116 - val_mae: 4.3842\n",
      "Epoch 195/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 187.6320 - mae: 9.0339 - val_loss: 29.2095 - val_mae: 4.0410\n",
      "Epoch 196/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 191.6010 - mae: 9.0130 - val_loss: 33.2748 - val_mae: 4.3070\n",
      "Epoch 197/750\n",
      "122/122 [==============================] - 0s 693us/step - loss: 180.8447 - mae: 8.8373 - val_loss: 25.2673 - val_mae: 3.6257\n",
      "Epoch 198/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 181.4945 - mae: 8.8795 - val_loss: 28.7236 - val_mae: 3.8149\n",
      "Epoch 199/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 189.6848 - mae: 9.0906 - val_loss: 26.1950 - val_mae: 3.7442\n",
      "Epoch 200/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 194.6046 - mae: 9.1894 - val_loss: 36.3397 - val_mae: 4.6111\n",
      "Epoch 201/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 181.2126 - mae: 8.9224 - val_loss: 25.7414 - val_mae: 3.6698\n",
      "Epoch 202/750\n",
      "122/122 [==============================] - 0s 843us/step - loss: 188.4160 - mae: 8.8889 - val_loss: 29.5235 - val_mae: 4.0326\n",
      "Epoch 203/750\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 201.5921 - mae: 9.2192 - val_loss: 29.4059 - val_mae: 3.9797\n",
      "Epoch 204/750\n",
      "122/122 [==============================] - 0s 945us/step - loss: 176.7243 - mae: 8.7663 - val_loss: 26.2305 - val_mae: 3.6537\n",
      "Epoch 205/750\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 194.9384 - mae: 9.0037 - val_loss: 24.5576 - val_mae: 3.5584\n",
      "Epoch 206/750\n",
      "122/122 [==============================] - 0s 746us/step - loss: 180.0963 - mae: 8.8004 - val_loss: 23.4776 - val_mae: 3.4982\n",
      "Epoch 207/750\n",
      "122/122 [==============================] - 0s 674us/step - loss: 184.8245 - mae: 8.8577 - val_loss: 24.2032 - val_mae: 3.5089\n",
      "Epoch 208/750\n",
      "122/122 [==============================] - 0s 650us/step - loss: 180.6458 - mae: 8.6817 - val_loss: 26.8986 - val_mae: 3.7847\n",
      "Epoch 209/750\n",
      "122/122 [==============================] - 0s 684us/step - loss: 187.1424 - mae: 9.0574 - val_loss: 24.6627 - val_mae: 3.5022\n",
      "Epoch 210/750\n",
      "122/122 [==============================] - 0s 623us/step - loss: 173.8333 - mae: 8.4796 - val_loss: 32.1738 - val_mae: 4.2978\n",
      "Epoch 211/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 169.1996 - mae: 8.5783 - val_loss: 25.1030 - val_mae: 3.5918\n",
      "Epoch 212/750\n",
      "122/122 [==============================] - 0s 658us/step - loss: 177.3615 - mae: 8.6431 - val_loss: 37.9680 - val_mae: 4.6664\n",
      "Epoch 213/750\n",
      "122/122 [==============================] - 0s 808us/step - loss: 170.4673 - mae: 8.5399 - val_loss: 25.2401 - val_mae: 3.6694\n",
      "Epoch 214/750\n",
      "122/122 [==============================] - 0s 663us/step - loss: 173.0323 - mae: 8.6955 - val_loss: 33.0859 - val_mae: 4.3126\n",
      "Epoch 215/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 173.1959 - mae: 8.6277 - val_loss: 25.8396 - val_mae: 3.7929\n",
      "Epoch 216/750\n",
      "122/122 [==============================] - 0s 620us/step - loss: 189.0924 - mae: 8.8799 - val_loss: 26.9164 - val_mae: 3.6690\n",
      "Epoch 217/750\n",
      "122/122 [==============================] - 0s 668us/step - loss: 171.5847 - mae: 8.5531 - val_loss: 25.9304 - val_mae: 3.6487\n",
      "Epoch 218/750\n",
      "122/122 [==============================] - 0s 578us/step - loss: 168.4601 - mae: 8.5604 - val_loss: 27.4851 - val_mae: 3.8514\n",
      "Epoch 219/750\n",
      "122/122 [==============================] - 0s 730us/step - loss: 170.1388 - mae: 8.5419 - val_loss: 25.2627 - val_mae: 3.6088\n",
      "Epoch 220/750\n",
      "122/122 [==============================] - 0s 738us/step - loss: 173.2455 - mae: 8.6891 - val_loss: 40.1293 - val_mae: 4.8531\n",
      "Epoch 221/750\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 158.8293 - mae: 8.3337 - val_loss: 27.4763 - val_mae: 3.8855\n",
      "Epoch 222/750\n",
      "122/122 [==============================] - 0s 667us/step - loss: 162.6050 - mae: 8.4119 - val_loss: 26.2757 - val_mae: 3.7023\n",
      "Epoch 223/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 163.4621 - mae: 8.5285 - val_loss: 25.8915 - val_mae: 3.6560\n",
      "Epoch 224/750\n",
      "122/122 [==============================] - 0s 642us/step - loss: 162.7763 - mae: 8.3765 - val_loss: 27.8291 - val_mae: 3.9836\n",
      "Epoch 225/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 150.2213 - mae: 8.1868 - val_loss: 25.3726 - val_mae: 3.6172\n",
      "Epoch 226/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 165.7952 - mae: 8.4959 - val_loss: 28.8766 - val_mae: 3.9836\n",
      "Epoch 227/750\n",
      "122/122 [==============================] - 0s 714us/step - loss: 164.4607 - mae: 8.3989 - val_loss: 24.6309 - val_mae: 3.5788\n",
      "Epoch 228/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 170.6875 - mae: 8.5836 - val_loss: 26.2768 - val_mae: 3.7648\n",
      "Epoch 229/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 159.7454 - mae: 8.3284 - val_loss: 23.9058 - val_mae: 3.5742\n",
      "Epoch 230/750\n",
      "122/122 [==============================] - 0s 629us/step - loss: 164.0359 - mae: 8.4224 - val_loss: 26.2849 - val_mae: 3.7473\n",
      "Epoch 231/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 167.6510 - mae: 8.4369 - val_loss: 24.9619 - val_mae: 3.6522\n",
      "Epoch 232/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 168.9485 - mae: 8.5197 - val_loss: 30.3271 - val_mae: 4.1785\n",
      "Epoch 233/750\n",
      "122/122 [==============================] - 0s 657us/step - loss: 151.2028 - mae: 8.1108 - val_loss: 27.3835 - val_mae: 3.8575\n",
      "Epoch 234/750\n",
      "122/122 [==============================] - 0s 565us/step - loss: 159.8386 - mae: 8.3825 - val_loss: 48.9254 - val_mae: 5.4160\n",
      "Epoch 235/750\n",
      "122/122 [==============================] - 0s 725us/step - loss: 153.8031 - mae: 8.1297 - val_loss: 43.1200 - val_mae: 5.1249\n",
      "Epoch 236/750\n",
      "122/122 [==============================] - 0s 633us/step - loss: 153.3404 - mae: 8.2589 - val_loss: 32.9091 - val_mae: 4.2520\n",
      "Epoch 237/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 156.7041 - mae: 8.3207 - val_loss: 27.3694 - val_mae: 3.8419\n",
      "Epoch 238/750\n",
      "122/122 [==============================] - 0s 597us/step - loss: 153.6630 - mae: 8.1643 - val_loss: 24.5481 - val_mae: 3.5516\n",
      "Epoch 239/750\n",
      "122/122 [==============================] - 0s 641us/step - loss: 159.4694 - mae: 8.3497 - val_loss: 40.6846 - val_mae: 4.8234\n",
      "Epoch 240/750\n",
      "122/122 [==============================] - 0s 644us/step - loss: 155.9097 - mae: 8.1955 - val_loss: 24.2985 - val_mae: 3.5080\n",
      "Epoch 241/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 153.5645 - mae: 8.1101 - val_loss: 24.6988 - val_mae: 3.6436\n",
      "Epoch 242/750\n",
      "122/122 [==============================] - 0s 596us/step - loss: 151.6404 - mae: 8.1332 - val_loss: 34.0825 - val_mae: 4.4781\n",
      "Epoch 243/750\n",
      "122/122 [==============================] - 0s 754us/step - loss: 150.9323 - mae: 8.1279 - val_loss: 24.5434 - val_mae: 3.5235\n",
      "Epoch 244/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 143.9759 - mae: 8.0625 - val_loss: 25.3213 - val_mae: 3.6318\n",
      "Epoch 245/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 153.8504 - mae: 8.0869 - val_loss: 24.2725 - val_mae: 3.5541\n",
      "Epoch 246/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 158.6300 - mae: 8.2308 - val_loss: 25.5083 - val_mae: 3.6317\n",
      "Epoch 247/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 143.4987 - mae: 7.9304 - val_loss: 24.6184 - val_mae: 3.5953\n",
      "Epoch 248/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 148.7005 - mae: 8.0065 - val_loss: 25.1517 - val_mae: 3.5690\n",
      "Epoch 249/750\n",
      "122/122 [==============================] - 0s 684us/step - loss: 141.4863 - mae: 7.9990 - val_loss: 43.7946 - val_mae: 5.0125\n",
      "Epoch 250/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 136.2466 - mae: 7.9228 - val_loss: 23.8089 - val_mae: 3.5108\n",
      "Epoch 251/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 147.2231 - mae: 8.0622 - val_loss: 31.9999 - val_mae: 4.2730\n",
      "Epoch 252/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 142.7580 - mae: 7.9964 - val_loss: 26.2881 - val_mae: 3.7436\n",
      "Epoch 253/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 145.0742 - mae: 7.9501 - val_loss: 28.4678 - val_mae: 3.9827\n",
      "Epoch 254/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 155.3788 - mae: 8.2343 - val_loss: 27.6572 - val_mae: 3.9003\n",
      "Epoch 255/750\n",
      "122/122 [==============================] - 0s 709us/step - loss: 146.7452 - mae: 8.1127 - val_loss: 27.9862 - val_mae: 3.8697\n",
      "Epoch 256/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 145.9330 - mae: 7.9956 - val_loss: 25.7878 - val_mae: 3.7341\n",
      "Epoch 257/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 141.9545 - mae: 8.0453 - val_loss: 41.9018 - val_mae: 4.8832\n",
      "Epoch 258/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 138.4489 - mae: 7.8297 - val_loss: 24.5783 - val_mae: 3.6150\n",
      "Epoch 259/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 131.8489 - mae: 7.7716 - val_loss: 34.9456 - val_mae: 4.5029\n",
      "Epoch 260/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 145.5209 - mae: 7.9210 - val_loss: 25.2230 - val_mae: 3.7281\n",
      "Epoch 261/750\n",
      "122/122 [==============================] - 0s 640us/step - loss: 138.1904 - mae: 7.7756 - val_loss: 24.7989 - val_mae: 3.5735\n",
      "Epoch 262/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 146.3213 - mae: 8.0274 - val_loss: 30.5766 - val_mae: 4.2689\n",
      "Epoch 263/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 142.3339 - mae: 7.9189 - val_loss: 27.0573 - val_mae: 3.7603\n",
      "Epoch 264/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 140.3327 - mae: 7.8048 - val_loss: 54.3287 - val_mae: 5.6847\n",
      "Epoch 265/750\n",
      "122/122 [==============================] - 0s 652us/step - loss: 135.1745 - mae: 7.8250 - val_loss: 26.3515 - val_mae: 3.7959\n",
      "Epoch 266/750\n",
      "122/122 [==============================] - 0s 645us/step - loss: 137.6505 - mae: 7.7786 - val_loss: 31.7203 - val_mae: 4.2880\n",
      "Epoch 267/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 129.3853 - mae: 7.6838 - val_loss: 26.8675 - val_mae: 3.8701\n",
      "Epoch 268/750\n",
      "122/122 [==============================] - 0s 576us/step - loss: 137.3426 - mae: 7.8646 - val_loss: 33.5980 - val_mae: 4.4192\n",
      "Epoch 269/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 129.5995 - mae: 7.6869 - val_loss: 24.7430 - val_mae: 3.6608\n",
      "Epoch 270/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 139.6905 - mae: 7.8280 - val_loss: 33.9665 - val_mae: 4.4786\n",
      "Epoch 271/750\n",
      "122/122 [==============================] - 0s 623us/step - loss: 138.7739 - mae: 7.8826 - val_loss: 29.8691 - val_mae: 4.1367\n",
      "Epoch 272/750\n",
      "122/122 [==============================] - 0s 679us/step - loss: 126.8301 - mae: 7.6580 - val_loss: 25.0082 - val_mae: 3.6041\n",
      "Epoch 273/750\n",
      "122/122 [==============================] - 0s 720us/step - loss: 136.0957 - mae: 7.7472 - val_loss: 27.1758 - val_mae: 3.8700\n",
      "Epoch 274/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 126.6297 - mae: 7.6449 - val_loss: 24.9814 - val_mae: 3.6091\n",
      "Epoch 275/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 119.7055 - mae: 7.5886 - val_loss: 35.2345 - val_mae: 4.6086\n",
      "Epoch 276/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 134.0171 - mae: 7.7226 - val_loss: 30.3221 - val_mae: 4.1793\n",
      "Epoch 277/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 126.2233 - mae: 7.5437 - val_loss: 25.9597 - val_mae: 3.7366\n",
      "Epoch 278/750\n",
      "122/122 [==============================] - 0s 639us/step - loss: 123.1878 - mae: 7.4831 - val_loss: 26.4892 - val_mae: 3.7538\n",
      "Epoch 279/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 126.7602 - mae: 7.5696 - val_loss: 23.9371 - val_mae: 3.5263\n",
      "Epoch 280/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 129.9408 - mae: 7.5905 - val_loss: 28.2005 - val_mae: 3.9455\n",
      "Epoch 281/750\n",
      "122/122 [==============================] - 0s 657us/step - loss: 123.7002 - mae: 7.5610 - val_loss: 29.1945 - val_mae: 4.0289\n",
      "Epoch 282/750\n",
      "122/122 [==============================] - 0s 606us/step - loss: 120.8216 - mae: 7.4901 - val_loss: 30.9155 - val_mae: 4.2289\n",
      "Epoch 283/750\n",
      "122/122 [==============================] - 0s 639us/step - loss: 128.9107 - mae: 7.7038 - val_loss: 24.7702 - val_mae: 3.6027\n",
      "Epoch 284/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 120.8624 - mae: 7.4220 - val_loss: 25.4583 - val_mae: 3.7203\n",
      "Epoch 285/750\n",
      "122/122 [==============================] - 0s 596us/step - loss: 121.9177 - mae: 7.4744 - val_loss: 24.5351 - val_mae: 3.6326\n",
      "Epoch 286/750\n",
      "122/122 [==============================] - 0s 641us/step - loss: 129.4557 - mae: 7.6141 - val_loss: 24.2732 - val_mae: 3.5536\n",
      "Epoch 287/750\n",
      "122/122 [==============================] - 0s 696us/step - loss: 131.5279 - mae: 7.7242 - val_loss: 25.6289 - val_mae: 3.6518\n",
      "Epoch 288/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 122.4109 - mae: 7.4349 - val_loss: 35.7183 - val_mae: 4.6560\n",
      "Epoch 289/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 123.4171 - mae: 7.5403 - val_loss: 26.9228 - val_mae: 3.6827\n",
      "Epoch 290/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 121.3649 - mae: 7.4502 - val_loss: 27.5592 - val_mae: 3.8730\n",
      "Epoch 291/750\n",
      "122/122 [==============================] - 0s 680us/step - loss: 131.6907 - mae: 7.6528 - val_loss: 25.6065 - val_mae: 3.6740\n",
      "Epoch 292/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 119.1077 - mae: 7.4451 - val_loss: 34.6137 - val_mae: 4.5465\n",
      "Epoch 293/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 121.1286 - mae: 7.4962 - val_loss: 24.8537 - val_mae: 3.6018\n",
      "Epoch 294/750\n",
      "122/122 [==============================] - 0s 644us/step - loss: 128.0825 - mae: 7.6026 - val_loss: 35.2414 - val_mae: 4.4472\n",
      "Epoch 295/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 133.5657 - mae: 7.7468 - val_loss: 26.2009 - val_mae: 3.7564\n",
      "Epoch 296/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 124.3110 - mae: 7.5374 - val_loss: 28.3556 - val_mae: 4.0696\n",
      "Epoch 297/750\n",
      "122/122 [==============================] - 0s 586us/step - loss: 116.8080 - mae: 7.4234 - val_loss: 41.1638 - val_mae: 5.0768\n",
      "Epoch 298/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 114.3548 - mae: 7.3383 - val_loss: 40.1984 - val_mae: 5.0265\n",
      "Epoch 299/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 120.9083 - mae: 7.5310 - val_loss: 25.7903 - val_mae: 3.6940\n",
      "Epoch 300/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 115.6435 - mae: 7.4077 - val_loss: 26.3756 - val_mae: 3.8532\n",
      "Epoch 301/750\n",
      "122/122 [==============================] - 0s 642us/step - loss: 118.2595 - mae: 7.4443 - val_loss: 23.5754 - val_mae: 3.5563\n",
      "Epoch 302/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 122.0665 - mae: 7.4488 - val_loss: 24.3599 - val_mae: 3.6254\n",
      "Epoch 303/750\n",
      "122/122 [==============================] - 0s 663us/step - loss: 107.3744 - mae: 7.1807 - val_loss: 24.5040 - val_mae: 3.6225\n",
      "Epoch 304/750\n",
      "122/122 [==============================] - 0s 679us/step - loss: 115.0899 - mae: 7.4496 - val_loss: 24.6528 - val_mae: 3.6233\n",
      "Epoch 305/750\n",
      "122/122 [==============================] - 0s 639us/step - loss: 124.1062 - mae: 7.4404 - val_loss: 29.5728 - val_mae: 4.0555\n",
      "Epoch 306/750\n",
      "122/122 [==============================] - 0s 624us/step - loss: 120.7586 - mae: 7.4817 - val_loss: 26.5619 - val_mae: 3.8367\n",
      "Epoch 307/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 111.9508 - mae: 7.2995 - val_loss: 23.0395 - val_mae: 3.5336\n",
      "Epoch 308/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 118.1028 - mae: 7.3339 - val_loss: 35.3041 - val_mae: 4.5775\n",
      "Epoch 309/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 105.5948 - mae: 7.1148 - val_loss: 24.8387 - val_mae: 3.6422\n",
      "Epoch 310/750\n",
      "122/122 [==============================] - 0s 741us/step - loss: 112.0579 - mae: 7.2455 - val_loss: 43.9007 - val_mae: 5.0540\n",
      "Epoch 311/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 109.2403 - mae: 7.2815 - val_loss: 29.3395 - val_mae: 3.9982\n",
      "Epoch 312/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 109.0279 - mae: 7.1838 - val_loss: 25.7235 - val_mae: 3.6586\n",
      "Epoch 313/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 109.5041 - mae: 7.3393 - val_loss: 24.6605 - val_mae: 3.6888\n",
      "Epoch 314/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 114.1483 - mae: 7.3314 - val_loss: 26.3416 - val_mae: 3.8461\n",
      "Epoch 315/750\n",
      "122/122 [==============================] - 0s 654us/step - loss: 116.9414 - mae: 7.5087 - val_loss: 31.7755 - val_mae: 4.1576\n",
      "Epoch 316/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 111.7198 - mae: 7.2542 - val_loss: 47.1032 - val_mae: 5.3598\n",
      "Epoch 317/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 115.7822 - mae: 7.3147 - val_loss: 25.5684 - val_mae: 3.6774\n",
      "Epoch 318/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 115.5648 - mae: 7.3809 - val_loss: 26.8493 - val_mae: 3.8301\n",
      "Epoch 319/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 112.3057 - mae: 7.3638 - val_loss: 26.0033 - val_mae: 3.7504\n",
      "Epoch 320/750\n",
      "122/122 [==============================] - 0s 699us/step - loss: 116.8962 - mae: 7.3839 - val_loss: 25.7177 - val_mae: 3.7183\n",
      "Epoch 321/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 112.0661 - mae: 7.2623 - val_loss: 38.5342 - val_mae: 4.8407\n",
      "Epoch 322/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 114.4707 - mae: 7.3532 - val_loss: 26.6433 - val_mae: 3.8492\n",
      "Epoch 323/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 110.0007 - mae: 7.1894 - val_loss: 28.3481 - val_mae: 3.8983\n",
      "Epoch 324/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 109.0248 - mae: 7.1888 - val_loss: 25.6271 - val_mae: 3.7310\n",
      "Epoch 325/750\n",
      "122/122 [==============================] - 0s 713us/step - loss: 113.7922 - mae: 7.2697 - val_loss: 28.5055 - val_mae: 3.8474\n",
      "Epoch 326/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 109.3283 - mae: 7.3094 - val_loss: 28.1848 - val_mae: 3.8802\n",
      "Epoch 327/750\n",
      "122/122 [==============================] - 0s 644us/step - loss: 117.8365 - mae: 7.3994 - val_loss: 27.0412 - val_mae: 3.7698\n",
      "Epoch 328/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 108.1578 - mae: 7.1700 - val_loss: 28.4757 - val_mae: 4.0124\n",
      "Epoch 329/750\n",
      "122/122 [==============================] - 0s 642us/step - loss: 104.0110 - mae: 6.9848 - val_loss: 25.1131 - val_mae: 3.6362\n",
      "Epoch 330/750\n",
      "122/122 [==============================] - 0s 707us/step - loss: 106.4193 - mae: 7.2554 - val_loss: 25.7510 - val_mae: 3.8027\n",
      "Epoch 331/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 108.3093 - mae: 7.3449 - val_loss: 43.9373 - val_mae: 5.2044\n",
      "Epoch 332/750\n",
      "122/122 [==============================] - 0s 633us/step - loss: 106.6975 - mae: 7.1992 - val_loss: 27.6440 - val_mae: 3.8687\n",
      "Epoch 333/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 105.1943 - mae: 7.0567 - val_loss: 30.2692 - val_mae: 4.0527\n",
      "Epoch 334/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 108.2369 - mae: 7.1484 - val_loss: 38.7711 - val_mae: 4.9628\n",
      "Epoch 335/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 106.8884 - mae: 7.1896 - val_loss: 27.6144 - val_mae: 3.9239\n",
      "Epoch 336/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 101.9959 - mae: 7.0113 - val_loss: 36.2629 - val_mae: 4.6655\n",
      "Epoch 337/750\n",
      "122/122 [==============================] - 0s 652us/step - loss: 98.1817 - mae: 6.9309 - val_loss: 30.7219 - val_mae: 4.1169\n",
      "Epoch 338/750\n",
      "122/122 [==============================] - 0s 650us/step - loss: 112.9512 - mae: 7.3012 - val_loss: 28.2336 - val_mae: 3.9794\n",
      "Epoch 339/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 102.6524 - mae: 7.0428 - val_loss: 30.8228 - val_mae: 4.1979\n",
      "Epoch 340/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 105.6957 - mae: 7.1621 - val_loss: 26.3332 - val_mae: 3.7031\n",
      "Epoch 341/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 101.3569 - mae: 6.9896 - val_loss: 31.8449 - val_mae: 4.3333\n",
      "Epoch 342/750\n",
      "122/122 [==============================] - 0s 722us/step - loss: 109.8524 - mae: 7.2147 - val_loss: 31.5653 - val_mae: 4.2270\n",
      "Epoch 343/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 106.0507 - mae: 7.2294 - val_loss: 26.4400 - val_mae: 3.6755\n",
      "Epoch 344/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 104.8372 - mae: 7.1255 - val_loss: 26.7232 - val_mae: 3.7602\n",
      "Epoch 345/750\n",
      "122/122 [==============================] - 0s 597us/step - loss: 100.8274 - mae: 7.0804 - val_loss: 26.4149 - val_mae: 3.6521\n",
      "Epoch 346/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 97.5666 - mae: 6.9295 - val_loss: 33.5365 - val_mae: 4.3822\n",
      "Epoch 347/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 109.1676 - mae: 7.1965 - val_loss: 28.0883 - val_mae: 3.8484\n",
      "Epoch 348/750\n",
      "122/122 [==============================] - 0s 705us/step - loss: 101.8513 - mae: 6.9840 - val_loss: 37.0128 - val_mae: 4.7661\n",
      "Epoch 349/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 104.3400 - mae: 7.1161 - val_loss: 55.2136 - val_mae: 5.8483\n",
      "Epoch 350/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 101.0611 - mae: 6.9225 - val_loss: 31.6471 - val_mae: 4.1530\n",
      "Epoch 351/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 102.8377 - mae: 7.0311 - val_loss: 26.8104 - val_mae: 3.7628\n",
      "Epoch 352/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 102.9173 - mae: 7.0509 - val_loss: 25.9433 - val_mae: 3.7112\n",
      "Epoch 353/750\n",
      "122/122 [==============================] - 0s 638us/step - loss: 97.2496 - mae: 6.9352 - val_loss: 29.0473 - val_mae: 3.9962\n",
      "Epoch 354/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 101.8608 - mae: 7.0589 - val_loss: 27.5322 - val_mae: 3.8269\n",
      "Epoch 355/750\n",
      "122/122 [==============================] - 0s 607us/step - loss: 102.7833 - mae: 7.0948 - val_loss: 26.3833 - val_mae: 3.7569\n",
      "Epoch 356/750\n",
      "122/122 [==============================] - 0s 665us/step - loss: 106.9887 - mae: 7.1731 - val_loss: 29.5928 - val_mae: 3.9851\n",
      "Epoch 357/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 103.1517 - mae: 7.0874 - val_loss: 28.0571 - val_mae: 3.8831\n",
      "Epoch 358/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 101.6772 - mae: 6.9874 - val_loss: 26.4960 - val_mae: 3.7361\n",
      "Epoch 359/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 103.5307 - mae: 6.9852 - val_loss: 28.0278 - val_mae: 3.9017\n",
      "Epoch 360/750\n",
      "122/122 [==============================] - 0s 700us/step - loss: 98.0498 - mae: 7.0070 - val_loss: 31.4725 - val_mae: 4.1969\n",
      "Epoch 361/750\n",
      "122/122 [==============================] - 0s 689us/step - loss: 99.0421 - mae: 6.9283 - val_loss: 30.4147 - val_mae: 4.0990\n",
      "Epoch 362/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 100.7136 - mae: 7.0039 - val_loss: 29.2821 - val_mae: 3.9603\n",
      "Epoch 363/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 95.0342 - mae: 7.0007 - val_loss: 36.3854 - val_mae: 4.5568\n",
      "Epoch 364/750\n",
      "122/122 [==============================] - 0s 623us/step - loss: 95.2897 - mae: 6.8695 - val_loss: 33.2182 - val_mae: 4.2321\n",
      "Epoch 365/750\n",
      "122/122 [==============================] - 0s 666us/step - loss: 107.0800 - mae: 7.1722 - val_loss: 29.4311 - val_mae: 3.8839\n",
      "Epoch 366/750\n",
      "122/122 [==============================] - 0s 647us/step - loss: 92.1786 - mae: 6.7391 - val_loss: 47.1474 - val_mae: 5.2840\n",
      "Epoch 367/750\n",
      "122/122 [==============================] - 0s 604us/step - loss: 98.6980 - mae: 7.0437 - val_loss: 29.7990 - val_mae: 3.8990\n",
      "Epoch 368/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 85.9520 - mae: 6.6387 - val_loss: 30.1710 - val_mae: 4.0454\n",
      "Epoch 369/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 93.1593 - mae: 6.7249 - val_loss: 29.1097 - val_mae: 4.0184\n",
      "Epoch 370/750\n",
      "122/122 [==============================] - 0s 724us/step - loss: 102.8433 - mae: 6.9984 - val_loss: 28.4344 - val_mae: 3.8643\n",
      "Epoch 371/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 92.8909 - mae: 6.8160 - val_loss: 28.9578 - val_mae: 3.8899\n",
      "Epoch 372/750\n",
      "122/122 [==============================] - 0s 587us/step - loss: 102.0314 - mae: 7.0075 - val_loss: 35.7799 - val_mae: 4.5236\n",
      "Epoch 373/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 98.8541 - mae: 6.9170 - val_loss: 37.9843 - val_mae: 4.6974\n",
      "Epoch 374/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 93.2011 - mae: 6.8115 - val_loss: 32.1853 - val_mae: 4.2354\n",
      "Epoch 375/750\n",
      "122/122 [==============================] - 0s 687us/step - loss: 100.0817 - mae: 6.9502 - val_loss: 28.4933 - val_mae: 3.9128\n",
      "Epoch 376/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 93.0084 - mae: 6.6333 - val_loss: 29.5267 - val_mae: 3.8541\n",
      "Epoch 377/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 92.6852 - mae: 6.7281 - val_loss: 31.9032 - val_mae: 4.1427\n",
      "Epoch 378/750\n",
      "122/122 [==============================] - 0s 604us/step - loss: 92.6517 - mae: 6.8010 - val_loss: 30.2909 - val_mae: 4.0417\n",
      "Epoch 379/750\n",
      "122/122 [==============================] - 0s 615us/step - loss: 93.8921 - mae: 6.8034 - val_loss: 29.2014 - val_mae: 3.8864\n",
      "Epoch 380/750\n",
      "122/122 [==============================] - 0s 719us/step - loss: 94.3025 - mae: 6.8911 - val_loss: 32.6366 - val_mae: 4.2367\n",
      "Epoch 381/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 89.1842 - mae: 6.7787 - val_loss: 31.7521 - val_mae: 4.1906\n",
      "Epoch 382/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 96.8417 - mae: 6.8589 - val_loss: 33.1807 - val_mae: 4.2194\n",
      "Epoch 383/750\n",
      "122/122 [==============================] - 0s 641us/step - loss: 100.9048 - mae: 7.0525 - val_loss: 32.7877 - val_mae: 4.2628\n",
      "Epoch 384/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 88.2470 - mae: 6.6166 - val_loss: 28.9049 - val_mae: 3.8335\n",
      "Epoch 385/750\n",
      "122/122 [==============================] - 0s 666us/step - loss: 90.7331 - mae: 6.7807 - val_loss: 31.3412 - val_mae: 4.1157\n",
      "Epoch 386/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 95.7009 - mae: 6.8377 - val_loss: 31.2958 - val_mae: 4.1369\n",
      "Epoch 387/750\n",
      "122/122 [==============================] - 0s 639us/step - loss: 90.8451 - mae: 6.6965 - val_loss: 30.7548 - val_mae: 3.9739\n",
      "Epoch 388/750\n",
      "122/122 [==============================] - 0s 603us/step - loss: 92.3130 - mae: 6.7664 - val_loss: 27.5784 - val_mae: 3.7148\n",
      "Epoch 389/750\n",
      "122/122 [==============================] - 0s 698us/step - loss: 102.5121 - mae: 6.9518 - val_loss: 47.9863 - val_mae: 5.2742\n",
      "Epoch 390/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 93.9138 - mae: 6.7863 - val_loss: 29.0710 - val_mae: 3.8446\n",
      "Epoch 391/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 94.4077 - mae: 6.7083 - val_loss: 36.1127 - val_mae: 4.5819\n",
      "Epoch 392/750\n",
      "122/122 [==============================] - 0s 597us/step - loss: 88.0844 - mae: 6.6645 - val_loss: 27.2920 - val_mae: 3.7545\n",
      "Epoch 393/750\n",
      "122/122 [==============================] - 0s 598us/step - loss: 84.1044 - mae: 6.4916 - val_loss: 36.9628 - val_mae: 4.6373\n",
      "Epoch 394/750\n",
      "122/122 [==============================] - 0s 656us/step - loss: 92.4405 - mae: 6.8273 - val_loss: 30.1676 - val_mae: 4.0570\n",
      "Epoch 395/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 88.8994 - mae: 6.6863 - val_loss: 43.2628 - val_mae: 4.9813\n",
      "Epoch 396/750\n",
      "122/122 [==============================] - 0s 625us/step - loss: 84.7210 - mae: 6.5175 - val_loss: 41.7753 - val_mae: 4.9531\n",
      "Epoch 397/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 85.8458 - mae: 6.6892 - val_loss: 31.2145 - val_mae: 4.0431\n",
      "Epoch 398/750\n",
      "122/122 [==============================] - 0s 645us/step - loss: 83.6381 - mae: 6.5164 - val_loss: 35.1846 - val_mae: 4.3148\n",
      "Epoch 399/750\n",
      "122/122 [==============================] - 0s 615us/step - loss: 90.1055 - mae: 6.6506 - val_loss: 42.3474 - val_mae: 5.0699\n",
      "Epoch 400/750\n",
      "122/122 [==============================] - 0s 734us/step - loss: 90.1603 - mae: 6.8009 - val_loss: 33.5662 - val_mae: 4.2741\n",
      "Epoch 401/750\n",
      "122/122 [==============================] - 0s 623us/step - loss: 86.8453 - mae: 6.5992 - val_loss: 29.3377 - val_mae: 3.9019\n",
      "Epoch 402/750\n",
      "122/122 [==============================] - 0s 657us/step - loss: 88.3569 - mae: 6.6915 - val_loss: 28.2164 - val_mae: 3.8182\n",
      "Epoch 403/750\n",
      "122/122 [==============================] - 0s 643us/step - loss: 88.8428 - mae: 6.6267 - val_loss: 31.4870 - val_mae: 3.9499\n",
      "Epoch 404/750\n",
      "122/122 [==============================] - 0s 705us/step - loss: 90.4284 - mae: 6.6495 - val_loss: 44.3323 - val_mae: 5.2434\n",
      "Epoch 405/750\n",
      "122/122 [==============================] - 0s 654us/step - loss: 88.0045 - mae: 6.7268 - val_loss: 37.6763 - val_mae: 4.6241\n",
      "Epoch 406/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 90.0423 - mae: 6.6967 - val_loss: 30.7561 - val_mae: 3.9596\n",
      "Epoch 407/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 87.8414 - mae: 6.5763 - val_loss: 28.2484 - val_mae: 3.8120\n",
      "Epoch 408/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 84.6900 - mae: 6.5590 - val_loss: 30.4107 - val_mae: 3.8625\n",
      "Epoch 409/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 95.3330 - mae: 6.8490 - val_loss: 30.8788 - val_mae: 3.9435\n",
      "Epoch 410/750\n",
      "122/122 [==============================] - 0s 687us/step - loss: 83.6047 - mae: 6.4153 - val_loss: 30.0478 - val_mae: 4.0209\n",
      "Epoch 411/750\n",
      "122/122 [==============================] - 0s 654us/step - loss: 88.4022 - mae: 6.5803 - val_loss: 30.1948 - val_mae: 3.9692\n",
      "Epoch 412/750\n",
      "122/122 [==============================] - 0s 603us/step - loss: 86.2332 - mae: 6.5421 - val_loss: 41.9904 - val_mae: 4.9070\n",
      "Epoch 413/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 86.9186 - mae: 6.5990 - val_loss: 32.2514 - val_mae: 4.0777\n",
      "Epoch 414/750\n",
      "122/122 [==============================] - 0s 652us/step - loss: 84.6118 - mae: 6.6260 - val_loss: 39.4070 - val_mae: 4.7931\n",
      "Epoch 415/750\n",
      "122/122 [==============================] - 0s 703us/step - loss: 82.3146 - mae: 6.4425 - val_loss: 45.8501 - val_mae: 5.2390\n",
      "Epoch 416/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 84.3839 - mae: 6.5140 - val_loss: 45.6437 - val_mae: 5.2258\n",
      "Epoch 417/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 83.0731 - mae: 6.4712 - val_loss: 35.6901 - val_mae: 4.5478\n",
      "Epoch 418/750\n",
      "122/122 [==============================] - 0s 654us/step - loss: 80.8810 - mae: 6.5048 - val_loss: 29.6560 - val_mae: 3.9330\n",
      "Epoch 419/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 79.9398 - mae: 6.4624 - val_loss: 34.3631 - val_mae: 4.2287\n",
      "Epoch 420/750\n",
      "122/122 [==============================] - 0s 746us/step - loss: 93.9247 - mae: 6.7656 - val_loss: 30.8754 - val_mae: 3.9351\n",
      "Epoch 421/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 79.7862 - mae: 6.4488 - val_loss: 30.6863 - val_mae: 4.0051\n",
      "Epoch 422/750\n",
      "122/122 [==============================] - 0s 658us/step - loss: 83.4938 - mae: 6.5437 - val_loss: 30.4594 - val_mae: 4.0650\n",
      "Epoch 423/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 93.0070 - mae: 6.7305 - val_loss: 30.4255 - val_mae: 3.9974\n",
      "Epoch 424/750\n",
      "122/122 [==============================] - 0s 643us/step - loss: 84.6801 - mae: 6.6104 - val_loss: 36.3662 - val_mae: 4.5198\n",
      "Epoch 425/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 87.4765 - mae: 6.5857 - val_loss: 30.2349 - val_mae: 4.0434\n",
      "Epoch 426/750\n",
      "122/122 [==============================] - 0s 660us/step - loss: 82.1496 - mae: 6.4539 - val_loss: 34.6226 - val_mae: 4.2399\n",
      "Epoch 427/750\n",
      "122/122 [==============================] - 0s 677us/step - loss: 83.4163 - mae: 6.5354 - val_loss: 30.9563 - val_mae: 4.0902\n",
      "Epoch 428/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 87.7299 - mae: 6.5864 - val_loss: 47.7096 - val_mae: 5.1839\n",
      "Epoch 429/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 90.5550 - mae: 6.6787 - val_loss: 30.8820 - val_mae: 3.9953\n",
      "Epoch 430/750\n",
      "122/122 [==============================] - 0s 650us/step - loss: 84.6569 - mae: 6.4979 - val_loss: 31.1983 - val_mae: 3.9513\n",
      "Epoch 431/750\n",
      "122/122 [==============================] - 0s 669us/step - loss: 88.9762 - mae: 6.6081 - val_loss: 38.1254 - val_mae: 4.6944\n",
      "Epoch 432/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 82.1596 - mae: 6.4404 - val_loss: 39.2199 - val_mae: 4.4361\n",
      "Epoch 433/750\n",
      "122/122 [==============================] - 0s 603us/step - loss: 80.9597 - mae: 6.3692 - val_loss: 33.3282 - val_mae: 4.2770\n",
      "Epoch 434/750\n",
      "122/122 [==============================] - 0s 635us/step - loss: 83.1183 - mae: 6.5566 - val_loss: 31.9290 - val_mae: 4.0877\n",
      "Epoch 435/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 85.6610 - mae: 6.6052 - val_loss: 32.6012 - val_mae: 4.0063\n",
      "Epoch 436/750\n",
      "122/122 [==============================] - 0s 706us/step - loss: 85.6673 - mae: 6.4767 - val_loss: 48.0107 - val_mae: 5.4601\n",
      "Epoch 437/750\n",
      "122/122 [==============================] - 0s 607us/step - loss: 88.1921 - mae: 6.6741 - val_loss: 32.7643 - val_mae: 4.3078\n",
      "Epoch 438/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 84.1691 - mae: 6.5341 - val_loss: 52.1361 - val_mae: 5.5044\n",
      "Epoch 439/750\n",
      "122/122 [==============================] - 0s 604us/step - loss: 83.4160 - mae: 6.5251 - val_loss: 30.8146 - val_mae: 4.0718\n",
      "Epoch 440/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 83.6179 - mae: 6.4526 - val_loss: 31.6465 - val_mae: 4.2188\n",
      "Epoch 441/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 75.7092 - mae: 6.3453 - val_loss: 34.7543 - val_mae: 4.4481\n",
      "Epoch 442/750\n",
      "122/122 [==============================] - 0s 587us/step - loss: 82.5373 - mae: 6.5264 - val_loss: 28.7379 - val_mae: 3.9771\n",
      "Epoch 443/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 77.3439 - mae: 6.3873 - val_loss: 29.6362 - val_mae: 4.1002\n",
      "Epoch 444/750\n",
      "122/122 [==============================] - 0s 644us/step - loss: 82.3686 - mae: 6.5548 - val_loss: 28.4510 - val_mae: 3.8599\n",
      "Epoch 445/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 80.6060 - mae: 6.4008 - val_loss: 34.5777 - val_mae: 4.2295\n",
      "Epoch 446/750\n",
      "122/122 [==============================] - 0s 615us/step - loss: 81.6477 - mae: 6.4718 - val_loss: 32.5714 - val_mae: 4.0666\n",
      "Epoch 447/750\n",
      "122/122 [==============================] - 0s 658us/step - loss: 81.2048 - mae: 6.4848 - val_loss: 27.0393 - val_mae: 3.7325\n",
      "Epoch 448/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 76.0092 - mae: 6.2947 - val_loss: 31.7000 - val_mae: 4.0796\n",
      "Epoch 449/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 76.4466 - mae: 6.3386 - val_loss: 35.2060 - val_mae: 4.4601\n",
      "Epoch 450/750\n",
      "122/122 [==============================] - 0s 726us/step - loss: 75.5632 - mae: 6.3038 - val_loss: 51.8053 - val_mae: 5.7166\n",
      "Epoch 451/750\n",
      "122/122 [==============================] - 0s 635us/step - loss: 76.0880 - mae: 6.2992 - val_loss: 27.2535 - val_mae: 3.7538\n",
      "Epoch 452/750\n",
      "122/122 [==============================] - 0s 640us/step - loss: 77.3345 - mae: 6.2743 - val_loss: 32.6909 - val_mae: 4.2374\n",
      "Epoch 453/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 79.8257 - mae: 6.4333 - val_loss: 37.2099 - val_mae: 4.5684\n",
      "Epoch 454/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 78.6640 - mae: 6.3528 - val_loss: 41.0004 - val_mae: 4.8835\n",
      "Epoch 455/750\n",
      "122/122 [==============================] - 0s 686us/step - loss: 81.1871 - mae: 6.4407 - val_loss: 35.0298 - val_mae: 4.4628\n",
      "Epoch 456/750\n",
      "122/122 [==============================] - 0s 655us/step - loss: 83.0631 - mae: 6.4995 - val_loss: 50.3203 - val_mae: 5.4980\n",
      "Epoch 457/750\n",
      "122/122 [==============================] - 0s 629us/step - loss: 75.7429 - mae: 6.2902 - val_loss: 30.2771 - val_mae: 3.9199\n",
      "Epoch 458/750\n",
      "122/122 [==============================] - 0s 633us/step - loss: 78.2679 - mae: 6.3590 - val_loss: 31.7143 - val_mae: 4.0789\n",
      "Epoch 459/750\n",
      "122/122 [==============================] - 0s 673us/step - loss: 78.1162 - mae: 6.3363 - val_loss: 39.6797 - val_mae: 4.7467\n",
      "Epoch 460/750\n",
      "122/122 [==============================] - 0s 716us/step - loss: 77.9614 - mae: 6.3501 - val_loss: 28.1655 - val_mae: 3.8531\n",
      "Epoch 461/750\n",
      "122/122 [==============================] - 0s 615us/step - loss: 82.6963 - mae: 6.4591 - val_loss: 31.7104 - val_mae: 4.2153\n",
      "Epoch 462/750\n",
      "122/122 [==============================] - 0s 637us/step - loss: 81.4981 - mae: 6.4275 - val_loss: 32.1939 - val_mae: 4.1792\n",
      "Epoch 463/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 80.9292 - mae: 6.3489 - val_loss: 31.7752 - val_mae: 4.0512\n",
      "Epoch 464/750\n",
      "122/122 [==============================] - 0s 668us/step - loss: 81.5982 - mae: 6.4785 - val_loss: 38.9392 - val_mae: 4.6718\n",
      "Epoch 465/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 79.1277 - mae: 6.3696 - val_loss: 31.4964 - val_mae: 4.0244\n",
      "Epoch 466/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 79.2505 - mae: 6.3741 - val_loss: 35.3577 - val_mae: 4.4356\n",
      "Epoch 467/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 77.8705 - mae: 6.4075 - val_loss: 31.0324 - val_mae: 3.8969\n",
      "Epoch 468/750\n",
      "122/122 [==============================] - 0s 607us/step - loss: 80.6214 - mae: 6.3727 - val_loss: 36.3310 - val_mae: 4.1775\n",
      "Epoch 469/750\n",
      "122/122 [==============================] - 0s 649us/step - loss: 80.7944 - mae: 6.3794 - val_loss: 35.3916 - val_mae: 4.5505\n",
      "Epoch 470/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 81.1620 - mae: 6.4580 - val_loss: 27.7315 - val_mae: 3.8007\n",
      "Epoch 471/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 74.2677 - mae: 6.1352 - val_loss: 28.7443 - val_mae: 3.9057\n",
      "Epoch 472/750\n",
      "122/122 [==============================] - 0s 607us/step - loss: 80.7248 - mae: 6.4194 - val_loss: 29.2167 - val_mae: 3.8909\n",
      "Epoch 473/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 80.5289 - mae: 6.3909 - val_loss: 28.3094 - val_mae: 3.7910\n",
      "Epoch 474/750\n",
      "122/122 [==============================] - 0s 604us/step - loss: 78.4577 - mae: 6.3216 - val_loss: 28.8232 - val_mae: 3.8886\n",
      "Epoch 475/750\n",
      "122/122 [==============================] - 0s 645us/step - loss: 75.6283 - mae: 6.3303 - val_loss: 33.5863 - val_mae: 4.2828\n",
      "Epoch 476/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 73.5778 - mae: 6.2005 - val_loss: 29.6414 - val_mae: 3.9722\n",
      "Epoch 477/750\n",
      "122/122 [==============================] - 0s 573us/step - loss: 76.6419 - mae: 6.2925 - val_loss: 30.8971 - val_mae: 4.0476\n",
      "Epoch 478/750\n",
      "122/122 [==============================] - 0s 578us/step - loss: 74.2026 - mae: 6.2757 - val_loss: 39.3133 - val_mae: 4.6852\n",
      "Epoch 479/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 75.3833 - mae: 6.3262 - val_loss: 34.6574 - val_mae: 4.2819\n",
      "Epoch 480/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 77.5851 - mae: 6.3245 - val_loss: 28.7017 - val_mae: 3.8243\n",
      "Epoch 481/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 71.7933 - mae: 6.1051 - val_loss: 37.5589 - val_mae: 4.5628\n",
      "Epoch 482/750\n",
      "122/122 [==============================] - 0s 649us/step - loss: 75.1564 - mae: 6.1958 - val_loss: 32.0502 - val_mae: 4.1972\n",
      "Epoch 483/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 78.5028 - mae: 6.3680 - val_loss: 31.8085 - val_mae: 4.0725\n",
      "Epoch 484/750\n",
      "122/122 [==============================] - 0s 629us/step - loss: 73.8527 - mae: 6.2689 - val_loss: 29.9898 - val_mae: 3.8761\n",
      "Epoch 485/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 75.9682 - mae: 6.2846 - val_loss: 44.1687 - val_mae: 5.0403\n",
      "Epoch 486/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 75.0684 - mae: 6.2250 - val_loss: 38.1778 - val_mae: 4.6965\n",
      "Epoch 487/750\n",
      "122/122 [==============================] - 0s 669us/step - loss: 71.2023 - mae: 6.1363 - val_loss: 31.2373 - val_mae: 3.9091\n",
      "Epoch 488/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 68.3597 - mae: 5.9956 - val_loss: 32.2401 - val_mae: 4.0721\n",
      "Epoch 489/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 75.5518 - mae: 6.2792 - val_loss: 31.8715 - val_mae: 4.1333\n",
      "Epoch 490/750\n",
      "122/122 [==============================] - 0s 627us/step - loss: 80.6542 - mae: 6.4201 - val_loss: 32.0581 - val_mae: 4.1124\n",
      "Epoch 491/750\n",
      "122/122 [==============================] - 0s 673us/step - loss: 76.8096 - mae: 6.3030 - val_loss: 33.4219 - val_mae: 4.2461\n",
      "Epoch 492/750\n",
      "122/122 [==============================] - 0s 643us/step - loss: 74.5707 - mae: 6.1599 - val_loss: 33.2623 - val_mae: 4.2191\n",
      "Epoch 493/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 81.0739 - mae: 6.3479 - val_loss: 28.7130 - val_mae: 3.8736\n",
      "Epoch 494/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 75.3345 - mae: 6.2578 - val_loss: 34.6140 - val_mae: 4.4627\n",
      "Epoch 495/750\n",
      "122/122 [==============================] - 0s 677us/step - loss: 72.1072 - mae: 6.1159 - val_loss: 30.8116 - val_mae: 4.0625\n",
      "Epoch 496/750\n",
      "122/122 [==============================] - 0s 627us/step - loss: 74.4627 - mae: 6.1442 - val_loss: 30.1907 - val_mae: 3.9739\n",
      "Epoch 497/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 77.0604 - mae: 6.2492 - val_loss: 30.3381 - val_mae: 4.0525\n",
      "Epoch 498/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 76.4105 - mae: 6.3073 - val_loss: 34.9044 - val_mae: 4.4992\n",
      "Epoch 499/750\n",
      "122/122 [==============================] - 0s 776us/step - loss: 74.0891 - mae: 6.1295 - val_loss: 29.9780 - val_mae: 3.9073\n",
      "Epoch 500/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 74.8251 - mae: 6.1719 - val_loss: 31.4550 - val_mae: 4.2133\n",
      "Epoch 501/750\n",
      "122/122 [==============================] - 0s 621us/step - loss: 82.0984 - mae: 6.3295 - val_loss: 28.9224 - val_mae: 3.8998\n",
      "Epoch 502/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 79.5036 - mae: 6.3466 - val_loss: 28.8326 - val_mae: 3.8212\n",
      "Epoch 503/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 73.2976 - mae: 6.2064 - val_loss: 33.0699 - val_mae: 4.1193\n",
      "Epoch 504/750\n",
      "122/122 [==============================] - 0s 575us/step - loss: 71.4029 - mae: 6.0827 - val_loss: 28.7331 - val_mae: 3.8125\n",
      "Epoch 505/750\n",
      "122/122 [==============================] - 0s 651us/step - loss: 79.5942 - mae: 6.4071 - val_loss: 34.5068 - val_mae: 4.3200\n",
      "Epoch 506/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 68.7064 - mae: 6.0591 - val_loss: 35.5037 - val_mae: 4.5456\n",
      "Epoch 507/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 73.1205 - mae: 6.2301 - val_loss: 44.7528 - val_mae: 5.2363\n",
      "Epoch 508/750\n",
      "122/122 [==============================] - 0s 612us/step - loss: 78.2505 - mae: 6.2871 - val_loss: 33.4535 - val_mae: 4.3112\n",
      "Epoch 509/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 73.7569 - mae: 6.2376 - val_loss: 34.2029 - val_mae: 4.3467\n",
      "Epoch 510/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 73.6986 - mae: 6.2278 - val_loss: 30.0735 - val_mae: 4.0875\n",
      "Epoch 511/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 72.9033 - mae: 6.1523 - val_loss: 31.1527 - val_mae: 4.1356\n",
      "Epoch 512/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 75.7191 - mae: 6.2388 - val_loss: 30.3344 - val_mae: 3.9633\n",
      "Epoch 513/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 74.7628 - mae: 6.1987 - val_loss: 42.4431 - val_mae: 5.0296\n",
      "Epoch 514/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 72.1910 - mae: 6.1158 - val_loss: 43.4369 - val_mae: 4.9812\n",
      "Epoch 515/750\n",
      "122/122 [==============================] - 0s 680us/step - loss: 76.8207 - mae: 6.2145 - val_loss: 30.3715 - val_mae: 4.0818\n",
      "Epoch 516/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 72.5074 - mae: 6.1753 - val_loss: 30.5238 - val_mae: 3.9959\n",
      "Epoch 517/750\n",
      "122/122 [==============================] - 0s 576us/step - loss: 78.2435 - mae: 6.2770 - val_loss: 34.9404 - val_mae: 4.3314\n",
      "Epoch 518/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 73.2714 - mae: 6.1340 - val_loss: 28.7661 - val_mae: 3.8557\n",
      "Epoch 519/750\n",
      "122/122 [==============================] - 0s 573us/step - loss: 75.0431 - mae: 6.2443 - val_loss: 37.3758 - val_mae: 4.6096\n",
      "Epoch 520/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 75.6591 - mae: 6.3043 - val_loss: 36.8111 - val_mae: 4.3032\n",
      "Epoch 521/750\n",
      "122/122 [==============================] - 0s 578us/step - loss: 79.8270 - mae: 6.3585 - val_loss: 45.3503 - val_mae: 5.0322\n",
      "Epoch 522/750\n",
      "122/122 [==============================] - 0s 688us/step - loss: 76.5286 - mae: 6.3104 - val_loss: 32.7170 - val_mae: 4.1624\n",
      "Epoch 523/750\n",
      "122/122 [==============================] - 0s 567us/step - loss: 74.1887 - mae: 6.1784 - val_loss: 36.2127 - val_mae: 4.3740\n",
      "Epoch 524/750\n",
      "122/122 [==============================] - 0s 608us/step - loss: 69.6408 - mae: 6.1255 - val_loss: 34.5070 - val_mae: 4.2728\n",
      "Epoch 525/750\n",
      "122/122 [==============================] - 0s 569us/step - loss: 77.2646 - mae: 6.2716 - val_loss: 36.5961 - val_mae: 4.5724\n",
      "Epoch 526/750\n",
      "122/122 [==============================] - 0s 573us/step - loss: 69.6448 - mae: 6.0293 - val_loss: 34.6705 - val_mae: 4.2623\n",
      "Epoch 527/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 71.7403 - mae: 6.1793 - val_loss: 38.7426 - val_mae: 4.6642\n",
      "Epoch 528/750\n",
      "122/122 [==============================] - 0s 633us/step - loss: 70.4921 - mae: 6.1337 - val_loss: 33.3165 - val_mae: 4.2071\n",
      "Epoch 529/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 77.2438 - mae: 6.2458 - val_loss: 32.0781 - val_mae: 4.0574\n",
      "Epoch 530/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 68.6174 - mae: 6.0329 - val_loss: 29.5381 - val_mae: 3.8799\n",
      "Epoch 531/750\n",
      "122/122 [==============================] - 0s 564us/step - loss: 72.4102 - mae: 6.2057 - val_loss: 36.9663 - val_mae: 4.5164\n",
      "Epoch 532/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 72.8768 - mae: 6.2228 - val_loss: 31.8988 - val_mae: 3.9880\n",
      "Epoch 533/750\n",
      "122/122 [==============================] - 0s 644us/step - loss: 68.3760 - mae: 6.1090 - val_loss: 31.1770 - val_mae: 4.0356\n",
      "Epoch 534/750\n",
      "122/122 [==============================] - 0s 596us/step - loss: 72.5499 - mae: 6.1766 - val_loss: 35.3424 - val_mae: 4.3869\n",
      "Epoch 535/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 67.3574 - mae: 6.0512 - val_loss: 47.2742 - val_mae: 5.2449\n",
      "Epoch 536/750\n",
      "122/122 [==============================] - 0s 564us/step - loss: 70.7400 - mae: 6.0809 - val_loss: 35.4617 - val_mae: 4.4002\n",
      "Epoch 537/750\n",
      "122/122 [==============================] - 0s 607us/step - loss: 72.1764 - mae: 6.1616 - val_loss: 41.2434 - val_mae: 4.7397\n",
      "Epoch 538/750\n",
      "122/122 [==============================] - 0s 567us/step - loss: 70.7939 - mae: 6.1603 - val_loss: 33.2702 - val_mae: 4.1767\n",
      "Epoch 539/750\n",
      "122/122 [==============================] - 0s 692us/step - loss: 74.1374 - mae: 6.2093 - val_loss: 43.6731 - val_mae: 4.8572\n",
      "Epoch 540/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 74.3859 - mae: 6.2833 - val_loss: 37.1086 - val_mae: 4.6951\n",
      "Epoch 541/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 76.8329 - mae: 6.2568 - val_loss: 32.3499 - val_mae: 4.1419\n",
      "Epoch 542/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 70.7029 - mae: 6.1046 - val_loss: 32.0301 - val_mae: 4.0827\n",
      "Epoch 543/750\n",
      "122/122 [==============================] - 0s 603us/step - loss: 76.3027 - mae: 6.2827 - val_loss: 30.2560 - val_mae: 3.9694\n",
      "Epoch 544/750\n",
      "122/122 [==============================] - 0s 640us/step - loss: 73.4728 - mae: 6.2263 - val_loss: 30.4826 - val_mae: 3.9369\n",
      "Epoch 545/750\n",
      "122/122 [==============================] - 0s 639us/step - loss: 67.6888 - mae: 6.0207 - val_loss: 36.8607 - val_mae: 4.5163\n",
      "Epoch 546/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 71.4696 - mae: 6.0724 - val_loss: 36.6624 - val_mae: 4.4585\n",
      "Epoch 547/750\n",
      "122/122 [==============================] - 0s 604us/step - loss: 71.3263 - mae: 6.0811 - val_loss: 30.4205 - val_mae: 3.9652\n",
      "Epoch 548/750\n",
      "122/122 [==============================] - 0s 664us/step - loss: 67.8369 - mae: 6.0144 - val_loss: 33.9911 - val_mae: 4.3675\n",
      "Epoch 549/750\n",
      "122/122 [==============================] - 0s 627us/step - loss: 68.8433 - mae: 5.9935 - val_loss: 32.4107 - val_mae: 4.0408\n",
      "Epoch 550/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 65.5661 - mae: 5.9632 - val_loss: 31.6655 - val_mae: 4.1129\n",
      "Epoch 551/750\n",
      "122/122 [==============================] - 0s 617us/step - loss: 70.6316 - mae: 6.0255 - val_loss: 33.9722 - val_mae: 4.2581\n",
      "Epoch 552/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 77.6726 - mae: 6.2027 - val_loss: 38.2591 - val_mae: 4.6874\n",
      "Epoch 553/750\n",
      "122/122 [==============================] - 0s 705us/step - loss: 71.5060 - mae: 6.2064 - val_loss: 30.5511 - val_mae: 4.0158\n",
      "Epoch 554/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 70.3565 - mae: 6.1201 - val_loss: 30.6575 - val_mae: 3.9995\n",
      "Epoch 555/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 71.1618 - mae: 6.0320 - val_loss: 43.3698 - val_mae: 5.0845\n",
      "Epoch 556/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 71.7070 - mae: 6.1625 - val_loss: 38.3431 - val_mae: 4.6137\n",
      "Epoch 557/750\n",
      "122/122 [==============================] - 0s 628us/step - loss: 72.0004 - mae: 6.0332 - val_loss: 38.3735 - val_mae: 4.5941\n",
      "Epoch 558/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 69.8904 - mae: 6.1370 - val_loss: 36.2736 - val_mae: 4.2801\n",
      "Epoch 559/750\n",
      "122/122 [==============================] - 0s 672us/step - loss: 73.9560 - mae: 6.2189 - val_loss: 32.3612 - val_mae: 4.0290\n",
      "Epoch 560/750\n",
      "122/122 [==============================] - 0s 586us/step - loss: 67.7988 - mae: 6.0844 - val_loss: 37.3306 - val_mae: 4.5462\n",
      "Epoch 561/750\n",
      "122/122 [==============================] - 0s 576us/step - loss: 68.5691 - mae: 6.0532 - val_loss: 29.3561 - val_mae: 3.8440\n",
      "Epoch 562/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 70.5760 - mae: 6.0549 - val_loss: 43.3225 - val_mae: 4.9746\n",
      "Epoch 563/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 70.3480 - mae: 6.1119 - val_loss: 35.7026 - val_mae: 4.4325\n",
      "Epoch 564/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 67.9727 - mae: 6.0281 - val_loss: 36.2618 - val_mae: 4.4179\n",
      "Epoch 565/750\n",
      "122/122 [==============================] - 0s 611us/step - loss: 72.1772 - mae: 6.0446 - val_loss: 36.5754 - val_mae: 4.5285\n",
      "Epoch 566/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 63.4386 - mae: 5.8571 - val_loss: 33.9947 - val_mae: 4.2875\n",
      "Epoch 567/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 73.2986 - mae: 6.1386 - val_loss: 57.1287 - val_mae: 5.8999\n",
      "Epoch 568/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 77.2868 - mae: 6.3232 - val_loss: 40.3574 - val_mae: 4.7870\n",
      "Epoch 569/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 69.9112 - mae: 6.1900 - val_loss: 46.2867 - val_mae: 5.1427\n",
      "Epoch 570/750\n",
      "122/122 [==============================] - 0s 561us/step - loss: 70.5577 - mae: 6.2034 - val_loss: 41.8854 - val_mae: 4.6857\n",
      "Epoch 571/750\n",
      "122/122 [==============================] - 0s 691us/step - loss: 69.8426 - mae: 6.1254 - val_loss: 32.0583 - val_mae: 3.9663\n",
      "Epoch 572/750\n",
      "122/122 [==============================] - 0s 572us/step - loss: 69.7088 - mae: 6.0676 - val_loss: 40.9577 - val_mae: 4.8142\n",
      "Epoch 573/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 69.4063 - mae: 6.0632 - val_loss: 30.0015 - val_mae: 3.9186\n",
      "Epoch 574/750\n",
      "122/122 [==============================] - 0s 576us/step - loss: 68.8140 - mae: 6.0214 - val_loss: 32.4272 - val_mae: 4.1668\n",
      "Epoch 575/750\n",
      "122/122 [==============================] - 0s 565us/step - loss: 66.7708 - mae: 5.9527 - val_loss: 39.6294 - val_mae: 4.1973\n",
      "Epoch 576/750\n",
      "122/122 [==============================] - 0s 565us/step - loss: 69.5426 - mae: 6.0684 - val_loss: 37.6060 - val_mae: 4.4940\n",
      "Epoch 577/750\n",
      "122/122 [==============================] - 0s 732us/step - loss: 67.6576 - mae: 6.0666 - val_loss: 33.5903 - val_mae: 4.1812\n",
      "Epoch 578/750\n",
      "122/122 [==============================] - 0s 572us/step - loss: 65.9672 - mae: 5.9814 - val_loss: 33.6551 - val_mae: 4.1701\n",
      "Epoch 579/750\n",
      "122/122 [==============================] - 0s 610us/step - loss: 67.0860 - mae: 5.9558 - val_loss: 34.2079 - val_mae: 4.1745\n",
      "Epoch 580/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 73.0458 - mae: 6.1847 - val_loss: 43.1992 - val_mae: 4.9030\n",
      "Epoch 581/750\n",
      "122/122 [==============================] - 0s 718us/step - loss: 65.6375 - mae: 5.9531 - val_loss: 35.7847 - val_mae: 4.1700\n",
      "Epoch 582/750\n",
      "122/122 [==============================] - 0s 569us/step - loss: 68.1364 - mae: 5.9905 - val_loss: 36.9519 - val_mae: 4.4614\n",
      "Epoch 583/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 74.7588 - mae: 6.1679 - val_loss: 40.5404 - val_mae: 4.7303\n",
      "Epoch 584/750\n",
      "122/122 [==============================] - 0s 569us/step - loss: 66.2313 - mae: 5.9658 - val_loss: 30.3494 - val_mae: 3.9610\n",
      "Epoch 585/750\n",
      "122/122 [==============================] - 0s 565us/step - loss: 67.2627 - mae: 6.0309 - val_loss: 33.8644 - val_mae: 4.1600\n",
      "Epoch 586/750\n",
      "122/122 [==============================] - 0s 791us/step - loss: 69.2663 - mae: 6.0622 - val_loss: 37.0566 - val_mae: 4.3857\n",
      "Epoch 587/750\n",
      "122/122 [==============================] - 0s 580us/step - loss: 69.7403 - mae: 6.1463 - val_loss: 36.5794 - val_mae: 4.5141\n",
      "Epoch 588/750\n",
      "122/122 [==============================] - 0s 562us/step - loss: 66.0512 - mae: 5.9349 - val_loss: 36.8069 - val_mae: 4.4711\n",
      "Epoch 589/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 72.7866 - mae: 6.1158 - val_loss: 32.6758 - val_mae: 4.0182\n",
      "Epoch 590/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 68.8863 - mae: 6.1995 - val_loss: 33.1724 - val_mae: 4.1123\n",
      "Epoch 591/750\n",
      "122/122 [==============================] - 0s 715us/step - loss: 65.2919 - mae: 5.9758 - val_loss: 34.9364 - val_mae: 4.1993\n",
      "Epoch 592/750\n",
      "122/122 [==============================] - 0s 583us/step - loss: 70.7018 - mae: 6.0569 - val_loss: 31.6875 - val_mae: 4.0127\n",
      "Epoch 593/750\n",
      "122/122 [==============================] - 0s 850us/step - loss: 69.5555 - mae: 6.0053 - val_loss: 40.2980 - val_mae: 4.7367\n",
      "Epoch 594/750\n",
      "122/122 [==============================] - 0s 717us/step - loss: 71.7903 - mae: 6.0273 - val_loss: 36.1858 - val_mae: 4.3115\n",
      "Epoch 595/750\n",
      "122/122 [==============================] - 0s 734us/step - loss: 64.1248 - mae: 5.8822 - val_loss: 33.5232 - val_mae: 4.1799\n",
      "Epoch 596/750\n",
      "122/122 [==============================] - 0s 580us/step - loss: 70.8680 - mae: 6.0719 - val_loss: 40.7840 - val_mae: 4.7807\n",
      "Epoch 597/750\n",
      "122/122 [==============================] - 0s 576us/step - loss: 67.5404 - mae: 5.9522 - val_loss: 43.0139 - val_mae: 4.9689\n",
      "Epoch 598/750\n",
      "122/122 [==============================] - 0s 608us/step - loss: 68.8492 - mae: 5.9577 - val_loss: 34.9471 - val_mae: 4.2818\n",
      "Epoch 599/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 65.2669 - mae: 5.8871 - val_loss: 35.6797 - val_mae: 4.4195\n",
      "Epoch 600/750\n",
      "122/122 [==============================] - 0s 662us/step - loss: 68.2026 - mae: 6.0414 - val_loss: 32.1290 - val_mae: 4.1111\n",
      "Epoch 601/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 72.5465 - mae: 6.1491 - val_loss: 32.3862 - val_mae: 4.2213\n",
      "Epoch 602/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 66.5564 - mae: 5.9301 - val_loss: 31.4600 - val_mae: 4.0169\n",
      "Epoch 603/750\n",
      "122/122 [==============================] - 0s 621us/step - loss: 67.9015 - mae: 6.0345 - val_loss: 31.5210 - val_mae: 4.1269\n",
      "Epoch 604/750\n",
      "122/122 [==============================] - 0s 575us/step - loss: 66.6980 - mae: 5.9230 - val_loss: 39.7237 - val_mae: 4.6128\n",
      "Epoch 605/750\n",
      "122/122 [==============================] - 0s 692us/step - loss: 65.5602 - mae: 5.9488 - val_loss: 39.0753 - val_mae: 4.6889\n",
      "Epoch 606/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 67.3780 - mae: 5.9398 - val_loss: 32.6281 - val_mae: 4.0446\n",
      "Epoch 607/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 69.3266 - mae: 6.1267 - val_loss: 32.9520 - val_mae: 4.0842\n",
      "Epoch 608/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 66.3525 - mae: 5.9201 - val_loss: 36.7049 - val_mae: 4.4887\n",
      "Epoch 609/750\n",
      "122/122 [==============================] - 0s 701us/step - loss: 64.7972 - mae: 5.9269 - val_loss: 36.6555 - val_mae: 4.4860\n",
      "Epoch 610/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 69.9741 - mae: 6.1066 - val_loss: 37.2692 - val_mae: 4.4254\n",
      "Epoch 611/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 65.8733 - mae: 5.9088 - val_loss: 42.2283 - val_mae: 4.6287\n",
      "Epoch 612/750\n",
      "122/122 [==============================] - 0s 681us/step - loss: 68.4740 - mae: 6.0160 - val_loss: 31.3202 - val_mae: 3.9555\n",
      "Epoch 613/750\n",
      "122/122 [==============================] - 0s 834us/step - loss: 70.5583 - mae: 6.0484 - val_loss: 38.2801 - val_mae: 4.5770\n",
      "Epoch 614/750\n",
      "122/122 [==============================] - 0s 686us/step - loss: 68.9639 - mae: 6.0414 - val_loss: 42.4491 - val_mae: 4.8996\n",
      "Epoch 615/750\n",
      "122/122 [==============================] - 0s 624us/step - loss: 65.7848 - mae: 5.9464 - val_loss: 36.1638 - val_mae: 4.3364\n",
      "Epoch 616/750\n",
      "122/122 [==============================] - 0s 639us/step - loss: 67.9071 - mae: 6.0406 - val_loss: 50.0261 - val_mae: 5.4759\n",
      "Epoch 617/750\n",
      "122/122 [==============================] - 0s 694us/step - loss: 67.7503 - mae: 5.9930 - val_loss: 34.2144 - val_mae: 4.1743\n",
      "Epoch 618/750\n",
      "122/122 [==============================] - 0s 653us/step - loss: 68.4690 - mae: 5.9548 - val_loss: 34.3193 - val_mae: 4.2693\n",
      "Epoch 619/750\n",
      "122/122 [==============================] - 0s 587us/step - loss: 66.9573 - mae: 5.8930 - val_loss: 34.2374 - val_mae: 4.1811\n",
      "Epoch 620/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 63.2843 - mae: 5.9039 - val_loss: 31.9232 - val_mae: 4.0875\n",
      "Epoch 621/750\n",
      "122/122 [==============================] - 0s 668us/step - loss: 70.5303 - mae: 6.0105 - val_loss: 45.5173 - val_mae: 5.0524\n",
      "Epoch 622/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 64.2330 - mae: 5.9299 - val_loss: 34.9034 - val_mae: 4.2513\n",
      "Epoch 623/750\n",
      "122/122 [==============================] - 0s 579us/step - loss: 67.7320 - mae: 6.0081 - val_loss: 38.7861 - val_mae: 4.6558\n",
      "Epoch 624/750\n",
      "122/122 [==============================] - 0s 585us/step - loss: 64.3508 - mae: 5.7452 - val_loss: 32.4340 - val_mae: 4.1210\n",
      "Epoch 625/750\n",
      "122/122 [==============================] - 0s 635us/step - loss: 64.9097 - mae: 5.8349 - val_loss: 36.9951 - val_mae: 4.4265\n",
      "Epoch 626/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 64.6027 - mae: 5.8557 - val_loss: 40.4499 - val_mae: 4.6276\n",
      "Epoch 627/750\n",
      "122/122 [==============================] - 0s 563us/step - loss: 65.2969 - mae: 5.8774 - val_loss: 32.3508 - val_mae: 4.0586\n",
      "Epoch 628/750\n",
      "122/122 [==============================] - 0s 577us/step - loss: 66.7041 - mae: 5.9082 - val_loss: 33.4886 - val_mae: 4.0815\n",
      "Epoch 629/750\n",
      "122/122 [==============================] - 0s 610us/step - loss: 67.3389 - mae: 5.9665 - val_loss: 36.9292 - val_mae: 4.4529\n",
      "Epoch 630/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 65.4095 - mae: 5.9037 - val_loss: 34.6356 - val_mae: 4.2498\n",
      "Epoch 631/750\n",
      "122/122 [==============================] - 0s 719us/step - loss: 66.0881 - mae: 6.0198 - val_loss: 32.6387 - val_mae: 4.0948\n",
      "Epoch 632/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 66.9997 - mae: 6.0469 - val_loss: 36.3639 - val_mae: 4.3144\n",
      "Epoch 633/750\n",
      "122/122 [==============================] - 0s 580us/step - loss: 63.6085 - mae: 5.8332 - val_loss: 45.5701 - val_mae: 5.1364\n",
      "Epoch 634/750\n",
      "122/122 [==============================] - 0s 586us/step - loss: 63.2808 - mae: 5.8076 - val_loss: 37.6252 - val_mae: 4.5446\n",
      "Epoch 635/750\n",
      "122/122 [==============================] - 0s 566us/step - loss: 63.4930 - mae: 5.8932 - val_loss: 41.1361 - val_mae: 4.7013\n",
      "Epoch 636/750\n",
      "122/122 [==============================] - 0s 574us/step - loss: 64.3586 - mae: 5.8052 - val_loss: 33.6816 - val_mae: 4.2084\n",
      "Epoch 637/750\n",
      "122/122 [==============================] - 0s 741us/step - loss: 63.4259 - mae: 5.8762 - val_loss: 43.0945 - val_mae: 5.0530\n",
      "Epoch 638/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 59.8685 - mae: 5.6388 - val_loss: 46.9402 - val_mae: 5.2798\n",
      "Epoch 639/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 62.6866 - mae: 5.8557 - val_loss: 34.0111 - val_mae: 4.0586\n",
      "Epoch 640/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 68.1836 - mae: 5.9902 - val_loss: 36.8952 - val_mae: 4.3103\n",
      "Epoch 641/750\n",
      "122/122 [==============================] - 0s 739us/step - loss: 67.6325 - mae: 5.9157 - val_loss: 35.0873 - val_mae: 4.0609\n",
      "Epoch 642/750\n",
      "122/122 [==============================] - 0s 581us/step - loss: 63.3174 - mae: 5.7483 - val_loss: 40.0598 - val_mae: 4.6940\n",
      "Epoch 643/750\n",
      "122/122 [==============================] - 0s 617us/step - loss: 64.1740 - mae: 5.8134 - val_loss: 31.0746 - val_mae: 3.9790\n",
      "Epoch 644/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 66.1800 - mae: 5.9578 - val_loss: 36.2611 - val_mae: 4.3644\n",
      "Epoch 645/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 66.7859 - mae: 5.9817 - val_loss: 34.5230 - val_mae: 4.1647\n",
      "Epoch 646/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 62.6954 - mae: 5.8355 - val_loss: 35.0428 - val_mae: 4.2529\n",
      "Epoch 647/750\n",
      "122/122 [==============================] - 0s 754us/step - loss: 66.5471 - mae: 5.9281 - val_loss: 32.9310 - val_mae: 4.2183\n",
      "Epoch 648/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 66.2569 - mae: 5.8913 - val_loss: 31.5104 - val_mae: 4.1386\n",
      "Epoch 649/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 59.5836 - mae: 5.6571 - val_loss: 28.6650 - val_mae: 3.8591\n",
      "Epoch 650/750\n",
      "122/122 [==============================] - 0s 595us/step - loss: 64.5923 - mae: 5.8095 - val_loss: 30.4795 - val_mae: 4.0788\n",
      "Epoch 651/750\n",
      "122/122 [==============================] - 0s 640us/step - loss: 70.3357 - mae: 6.0232 - val_loss: 34.5375 - val_mae: 4.2020\n",
      "Epoch 652/750\n",
      "122/122 [==============================] - 0s 699us/step - loss: 64.9159 - mae: 5.9044 - val_loss: 31.3034 - val_mae: 3.9873\n",
      "Epoch 653/750\n",
      "122/122 [==============================] - 0s 588us/step - loss: 67.2530 - mae: 5.9802 - val_loss: 34.5790 - val_mae: 4.1815\n",
      "Epoch 654/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 64.4499 - mae: 5.8817 - val_loss: 36.5782 - val_mae: 4.4367\n",
      "Epoch 655/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 64.1844 - mae: 5.8664 - val_loss: 38.8061 - val_mae: 4.5464\n",
      "Epoch 656/750\n",
      "122/122 [==============================] - 0s 728us/step - loss: 62.7468 - mae: 5.8431 - val_loss: 38.6178 - val_mae: 4.6102\n",
      "Epoch 657/750\n",
      "122/122 [==============================] - 0s 576us/step - loss: 65.9234 - mae: 5.9316 - val_loss: 36.4479 - val_mae: 4.3791\n",
      "Epoch 658/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 64.7032 - mae: 5.8100 - val_loss: 35.7403 - val_mae: 4.4059\n",
      "Epoch 659/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 67.2280 - mae: 5.9725 - val_loss: 39.9226 - val_mae: 4.6538\n",
      "Epoch 660/750\n",
      "122/122 [==============================] - 0s 684us/step - loss: 67.7858 - mae: 5.9984 - val_loss: 35.1590 - val_mae: 4.2605\n",
      "Epoch 661/750\n",
      "122/122 [==============================] - 0s 662us/step - loss: 65.5030 - mae: 5.9683 - val_loss: 32.2551 - val_mae: 4.1282\n",
      "Epoch 662/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 65.8198 - mae: 5.9311 - val_loss: 32.7059 - val_mae: 4.0351\n",
      "Epoch 663/750\n",
      "122/122 [==============================] - 0s 619us/step - loss: 64.3752 - mae: 5.9174 - val_loss: 37.5771 - val_mae: 4.4325\n",
      "Epoch 664/750\n",
      "122/122 [==============================] - 0s 653us/step - loss: 61.5303 - mae: 5.8255 - val_loss: 32.5370 - val_mae: 4.1186\n",
      "Epoch 665/750\n",
      "122/122 [==============================] - 0s 678us/step - loss: 63.8304 - mae: 5.7895 - val_loss: 37.8996 - val_mae: 4.4792\n",
      "Epoch 666/750\n",
      "122/122 [==============================] - 0s 652us/step - loss: 62.6733 - mae: 5.8130 - val_loss: 32.9812 - val_mae: 4.0601\n",
      "Epoch 667/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 64.4590 - mae: 5.8569 - val_loss: 41.5635 - val_mae: 4.7555\n",
      "Epoch 668/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 67.1196 - mae: 5.9087 - val_loss: 33.9923 - val_mae: 4.1455\n",
      "Epoch 669/750\n",
      "122/122 [==============================] - 0s 694us/step - loss: 64.8623 - mae: 5.9199 - val_loss: 31.3838 - val_mae: 4.0167\n",
      "Epoch 670/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 62.3801 - mae: 5.8244 - val_loss: 32.1180 - val_mae: 4.0622\n",
      "Epoch 671/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 63.8049 - mae: 5.8302 - val_loss: 37.6196 - val_mae: 4.4028\n",
      "Epoch 672/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 63.9560 - mae: 5.8612 - val_loss: 42.9377 - val_mae: 4.6873\n",
      "Epoch 673/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 62.6251 - mae: 5.7429 - val_loss: 35.5668 - val_mae: 4.2207\n",
      "Epoch 674/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 62.3622 - mae: 5.8381 - val_loss: 35.2562 - val_mae: 4.2675\n",
      "Epoch 675/750\n",
      "122/122 [==============================] - 0s 718us/step - loss: 66.2006 - mae: 5.8379 - val_loss: 35.9683 - val_mae: 4.3902\n",
      "Epoch 676/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 64.1486 - mae: 5.9109 - val_loss: 36.9711 - val_mae: 4.4658\n",
      "Epoch 677/750\n",
      "122/122 [==============================] - 0s 601us/step - loss: 64.8641 - mae: 5.9255 - val_loss: 50.2653 - val_mae: 5.4473\n",
      "Epoch 678/750\n",
      "122/122 [==============================] - 0s 582us/step - loss: 61.6564 - mae: 5.7768 - val_loss: 33.7888 - val_mae: 4.2322\n",
      "Epoch 679/750\n",
      "122/122 [==============================] - 0s 607us/step - loss: 60.1549 - mae: 5.7379 - val_loss: 37.2738 - val_mae: 4.4261\n",
      "Epoch 680/750\n",
      "122/122 [==============================] - 0s 626us/step - loss: 64.4094 - mae: 5.8639 - val_loss: 40.9613 - val_mae: 4.8178\n",
      "Epoch 681/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 62.2090 - mae: 5.7893 - val_loss: 37.0081 - val_mae: 4.4342\n",
      "Epoch 682/750\n",
      "122/122 [==============================] - 0s 589us/step - loss: 66.4620 - mae: 5.9783 - val_loss: 40.1628 - val_mae: 4.5624\n",
      "Epoch 683/750\n",
      "122/122 [==============================] - 0s 715us/step - loss: 63.8965 - mae: 5.8609 - val_loss: 32.6484 - val_mae: 4.1591\n",
      "Epoch 684/750\n",
      "122/122 [==============================] - 0s 606us/step - loss: 67.1497 - mae: 6.0285 - val_loss: 37.3329 - val_mae: 4.4367\n",
      "Epoch 685/750\n",
      "122/122 [==============================] - 0s 590us/step - loss: 64.3837 - mae: 5.8341 - val_loss: 32.0635 - val_mae: 4.0437\n",
      "Epoch 686/750\n",
      "122/122 [==============================] - 0s 587us/step - loss: 61.5473 - mae: 5.7895 - val_loss: 34.0065 - val_mae: 4.2458\n",
      "Epoch 687/750\n",
      "122/122 [==============================] - 0s 616us/step - loss: 64.4550 - mae: 5.9073 - val_loss: 36.0251 - val_mae: 4.3849\n",
      "Epoch 688/750\n",
      "122/122 [==============================] - 0s 591us/step - loss: 64.0332 - mae: 5.8304 - val_loss: 38.7395 - val_mae: 4.4478\n",
      "Epoch 689/750\n",
      "122/122 [==============================] - 0s 624us/step - loss: 67.6064 - mae: 6.0087 - val_loss: 35.1341 - val_mae: 4.2302\n",
      "Epoch 690/750\n",
      "122/122 [==============================] - 0s 645us/step - loss: 60.4019 - mae: 5.8073 - val_loss: 37.6412 - val_mae: 4.4733\n",
      "Epoch 691/750\n",
      "122/122 [==============================] - 0s 653us/step - loss: 66.1857 - mae: 5.8372 - val_loss: 34.3865 - val_mae: 4.2226\n",
      "Epoch 692/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 63.3405 - mae: 5.8435 - val_loss: 51.4557 - val_mae: 5.4830\n",
      "Epoch 693/750\n",
      "122/122 [==============================] - 0s 642us/step - loss: 63.9023 - mae: 5.8745 - val_loss: 45.9163 - val_mae: 5.0905\n",
      "Epoch 694/750\n",
      "122/122 [==============================] - 0s 605us/step - loss: 62.0613 - mae: 5.8170 - val_loss: 46.2966 - val_mae: 5.1657\n",
      "Epoch 695/750\n",
      "122/122 [==============================] - 0s 629us/step - loss: 64.9586 - mae: 5.9089 - val_loss: 34.5534 - val_mae: 4.0748\n",
      "Epoch 696/750\n",
      "122/122 [==============================] - 0s 580us/step - loss: 63.9468 - mae: 5.8000 - val_loss: 33.3389 - val_mae: 4.1379\n",
      "Epoch 697/750\n",
      "122/122 [==============================] - 0s 693us/step - loss: 64.9117 - mae: 5.8216 - val_loss: 34.4681 - val_mae: 4.2537\n",
      "Epoch 698/750\n",
      "122/122 [==============================] - 0s 613us/step - loss: 59.4129 - mae: 5.7063 - val_loss: 35.8231 - val_mae: 4.2943\n",
      "Epoch 699/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 62.9663 - mae: 5.8516 - val_loss: 32.8927 - val_mae: 4.2427\n",
      "Epoch 700/750\n",
      "122/122 [==============================] - 0s 599us/step - loss: 66.4468 - mae: 5.9492 - val_loss: 32.0317 - val_mae: 3.9920\n",
      "Epoch 701/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 62.4832 - mae: 5.8234 - val_loss: 36.1768 - val_mae: 4.4379\n",
      "Epoch 702/750\n",
      "122/122 [==============================] - 0s 696us/step - loss: 60.1200 - mae: 5.7556 - val_loss: 36.2057 - val_mae: 4.4091\n",
      "Epoch 703/750\n",
      "122/122 [==============================] - 0s 648us/step - loss: 65.5949 - mae: 5.9103 - val_loss: 37.4203 - val_mae: 4.4486\n",
      "Epoch 704/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 61.0786 - mae: 5.7973 - val_loss: 45.5445 - val_mae: 5.0735\n",
      "Epoch 705/750\n",
      "122/122 [==============================] - 0s 592us/step - loss: 61.0527 - mae: 5.7559 - val_loss: 36.4545 - val_mae: 4.3281\n",
      "Epoch 706/750\n",
      "122/122 [==============================] - 0s 709us/step - loss: 62.5652 - mae: 5.8119 - val_loss: 36.6391 - val_mae: 4.2962\n",
      "Epoch 707/750\n",
      "122/122 [==============================] - 0s 618us/step - loss: 62.0370 - mae: 5.6803 - val_loss: 33.3332 - val_mae: 4.0650\n",
      "Epoch 708/750\n",
      "122/122 [==============================] - 0s 624us/step - loss: 61.2814 - mae: 5.7616 - val_loss: 38.2721 - val_mae: 4.5494\n",
      "Epoch 709/750\n",
      "122/122 [==============================] - 0s 630us/step - loss: 67.4621 - mae: 5.9604 - val_loss: 36.1771 - val_mae: 4.2693\n",
      "Epoch 710/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 63.1259 - mae: 5.8839 - val_loss: 35.7236 - val_mae: 4.1157\n",
      "Epoch 711/750\n",
      "122/122 [==============================] - 0s 627us/step - loss: 61.8681 - mae: 5.7834 - val_loss: 43.5934 - val_mae: 4.8780\n",
      "Epoch 712/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 67.0708 - mae: 6.0192 - val_loss: 48.7107 - val_mae: 5.2422\n",
      "Epoch 713/750\n",
      "122/122 [==============================] - 0s 623us/step - loss: 61.6778 - mae: 5.8333 - val_loss: 39.5228 - val_mae: 4.7043\n",
      "Epoch 714/750\n",
      "122/122 [==============================] - 0s 608us/step - loss: 63.9307 - mae: 5.7908 - val_loss: 36.8254 - val_mae: 4.3589\n",
      "Epoch 715/750\n",
      "122/122 [==============================] - 0s 754us/step - loss: 62.7147 - mae: 5.8428 - val_loss: 36.2651 - val_mae: 4.2278\n",
      "Epoch 716/750\n",
      "122/122 [==============================] - 0s 621us/step - loss: 59.5825 - mae: 5.6971 - val_loss: 40.2869 - val_mae: 4.5937\n",
      "Epoch 717/750\n",
      "122/122 [==============================] - 0s 614us/step - loss: 62.3263 - mae: 5.8001 - val_loss: 34.0680 - val_mae: 4.2861\n",
      "Epoch 718/750\n",
      "122/122 [==============================] - 0s 643us/step - loss: 63.9047 - mae: 5.7191 - val_loss: 42.6714 - val_mae: 4.7965\n",
      "Epoch 719/750\n",
      "122/122 [==============================] - 0s 606us/step - loss: 66.0021 - mae: 5.9504 - val_loss: 32.0622 - val_mae: 4.1171\n",
      "Epoch 720/750\n",
      "122/122 [==============================] - 0s 689us/step - loss: 63.0492 - mae: 5.8458 - val_loss: 35.2609 - val_mae: 4.2930\n",
      "Epoch 721/750\n",
      "122/122 [==============================] - 0s 659us/step - loss: 66.4038 - mae: 5.9223 - val_loss: 33.0878 - val_mae: 4.0124\n",
      "Epoch 722/750\n",
      "122/122 [==============================] - 0s 608us/step - loss: 64.4120 - mae: 5.8148 - val_loss: 43.7519 - val_mae: 4.8786\n",
      "Epoch 723/750\n",
      "122/122 [==============================] - 0s 623us/step - loss: 66.1312 - mae: 5.8793 - val_loss: 39.5190 - val_mae: 4.4009\n",
      "Epoch 724/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 65.6071 - mae: 5.8426 - val_loss: 39.1279 - val_mae: 4.5031\n",
      "Epoch 725/750\n",
      "122/122 [==============================] - 0s 710us/step - loss: 62.8455 - mae: 5.8033 - val_loss: 35.4124 - val_mae: 4.2351\n",
      "Epoch 726/750\n",
      "122/122 [==============================] - 0s 638us/step - loss: 64.1552 - mae: 5.7971 - val_loss: 31.0315 - val_mae: 3.9581\n",
      "Epoch 727/750\n",
      "122/122 [==============================] - 0s 638us/step - loss: 61.9951 - mae: 5.7794 - val_loss: 42.4241 - val_mae: 4.8357\n",
      "Epoch 728/750\n",
      "122/122 [==============================] - 0s 602us/step - loss: 62.4908 - mae: 5.7330 - val_loss: 35.4254 - val_mae: 4.2736\n",
      "Epoch 729/750\n",
      "122/122 [==============================] - 0s 622us/step - loss: 61.1819 - mae: 5.7512 - val_loss: 32.7052 - val_mae: 4.0633\n",
      "Epoch 730/750\n",
      "122/122 [==============================] - 0s 584us/step - loss: 60.3605 - mae: 5.6619 - val_loss: 50.2921 - val_mae: 5.3321\n",
      "Epoch 731/750\n",
      "122/122 [==============================] - 0s 656us/step - loss: 68.9735 - mae: 6.0013 - val_loss: 39.4044 - val_mae: 4.5987\n",
      "Epoch 732/750\n",
      "122/122 [==============================] - 0s 646us/step - loss: 64.0364 - mae: 5.8642 - val_loss: 41.5632 - val_mae: 4.8176\n",
      "Epoch 733/750\n",
      "122/122 [==============================] - 0s 631us/step - loss: 63.1222 - mae: 5.8862 - val_loss: 36.2586 - val_mae: 4.3534\n",
      "Epoch 734/750\n",
      "122/122 [==============================] - 0s 629us/step - loss: 62.6106 - mae: 5.7924 - val_loss: 36.6613 - val_mae: 4.3035\n",
      "Epoch 735/750\n",
      "122/122 [==============================] - 0s 650us/step - loss: 61.0066 - mae: 5.7048 - val_loss: 37.0441 - val_mae: 4.3994\n",
      "Epoch 736/750\n",
      "122/122 [==============================] - 0s 663us/step - loss: 59.6018 - mae: 5.7061 - val_loss: 34.0250 - val_mae: 4.1435\n",
      "Epoch 737/750\n",
      "122/122 [==============================] - 0s 647us/step - loss: 65.8669 - mae: 5.7264 - val_loss: 36.6681 - val_mae: 4.3740\n",
      "Epoch 738/750\n",
      "122/122 [==============================] - 0s 593us/step - loss: 61.7956 - mae: 5.7607 - val_loss: 33.3133 - val_mae: 4.0908\n",
      "Epoch 739/750\n",
      "122/122 [==============================] - 0s 649us/step - loss: 65.4443 - mae: 5.8507 - val_loss: 36.8282 - val_mae: 4.3089\n",
      "Epoch 740/750\n",
      "122/122 [==============================] - 0s 671us/step - loss: 62.8470 - mae: 5.8377 - val_loss: 33.2945 - val_mae: 4.0973\n",
      "Epoch 741/750\n",
      "122/122 [==============================] - 0s 634us/step - loss: 61.2425 - mae: 5.7263 - val_loss: 37.1955 - val_mae: 4.4854\n",
      "Epoch 742/750\n",
      "122/122 [==============================] - 0s 600us/step - loss: 61.8584 - mae: 5.7634 - val_loss: 37.7037 - val_mae: 4.4013\n",
      "Epoch 743/750\n",
      "122/122 [==============================] - 0s 642us/step - loss: 62.8553 - mae: 5.7512 - val_loss: 33.5251 - val_mae: 4.1156\n",
      "Epoch 744/750\n",
      "122/122 [==============================] - 0s 609us/step - loss: 58.9118 - mae: 5.6625 - val_loss: 43.3913 - val_mae: 4.8600\n",
      "Epoch 745/750\n",
      "122/122 [==============================] - 0s 615us/step - loss: 62.0419 - mae: 5.7749 - val_loss: 36.8279 - val_mae: 4.4361\n",
      "Epoch 746/750\n",
      "122/122 [==============================] - 0s 852us/step - loss: 60.0755 - mae: 5.7191 - val_loss: 33.5843 - val_mae: 4.1298\n",
      "Epoch 747/750\n",
      "122/122 [==============================] - 0s 636us/step - loss: 61.4022 - mae: 5.7590 - val_loss: 31.8106 - val_mae: 3.9526\n",
      "Epoch 748/750\n",
      "122/122 [==============================] - 0s 594us/step - loss: 63.4975 - mae: 5.8102 - val_loss: 32.9736 - val_mae: 4.2232\n",
      "Epoch 749/750\n",
      "122/122 [==============================] - 0s 608us/step - loss: 61.3712 - mae: 5.7272 - val_loss: 31.3907 - val_mae: 4.0879\n",
      "Epoch 750/750\n",
      "122/122 [==============================] - 0s 632us/step - loss: 61.9817 - mae: 5.7778 - val_loss: 36.4415 - val_mae: 4.2953\n",
      "31/31 [==============================] - 0s 340us/step\n",
      "Epochs: 750 | MAE: 4.2953251377203525\n",
      "Training model with 800 epochs\n",
      "Epoch 1/800\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 65810.2969 - mae: 222.2995 - val_loss: 52485.7305 - val_mae: 198.1074\n",
      "Epoch 2/800\n",
      "122/122 [==============================] - 0s 601us/step - loss: 27594.0195 - mae: 136.6104 - val_loss: 6180.7363 - val_mae: 64.9131\n",
      "Epoch 3/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 4072.5901 - mae: 48.7397 - val_loss: 2383.5713 - val_mae: 36.6183\n",
      "Epoch 4/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 2585.9236 - mae: 38.1632 - val_loss: 1689.0178 - val_mae: 30.7182\n",
      "Epoch 5/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 2243.9595 - mae: 35.7133 - val_loss: 1386.2142 - val_mae: 27.6926\n",
      "Epoch 6/800\n",
      "122/122 [==============================] - 0s 689us/step - loss: 2108.2505 - mae: 34.3317 - val_loss: 1220.0460 - val_mae: 25.9972\n",
      "Epoch 7/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 1840.8884 - mae: 32.9412 - val_loss: 1114.7284 - val_mae: 24.9052\n",
      "Epoch 8/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 1675.5787 - mae: 31.4605 - val_loss: 1013.2903 - val_mae: 23.8783\n",
      "Epoch 9/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 1645.5131 - mae: 31.3165 - val_loss: 926.4903 - val_mae: 22.6933\n",
      "Epoch 10/800\n",
      "122/122 [==============================] - 0s 709us/step - loss: 1544.5509 - mae: 30.2281 - val_loss: 820.4689 - val_mae: 21.3705\n",
      "Epoch 11/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 1537.0649 - mae: 30.5764 - val_loss: 762.4094 - val_mae: 20.7882\n",
      "Epoch 12/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 1356.0292 - mae: 28.7299 - val_loss: 665.3035 - val_mae: 19.1276\n",
      "Epoch 13/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 1248.2493 - mae: 27.5864 - val_loss: 609.8949 - val_mae: 18.3322\n",
      "Epoch 14/800\n",
      "122/122 [==============================] - 0s 733us/step - loss: 1199.9778 - mae: 26.4902 - val_loss: 544.9894 - val_mae: 17.3202\n",
      "Epoch 15/800\n",
      "122/122 [==============================] - 0s 649us/step - loss: 1105.6304 - mae: 25.7845 - val_loss: 491.2184 - val_mae: 16.6559\n",
      "Epoch 16/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 1062.8138 - mae: 25.2733 - val_loss: 448.7185 - val_mae: 15.6342\n",
      "Epoch 17/800\n",
      "122/122 [==============================] - 0s 649us/step - loss: 995.0927 - mae: 24.5169 - val_loss: 393.2033 - val_mae: 14.7661\n",
      "Epoch 18/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 970.3111 - mae: 24.1099 - val_loss: 354.4810 - val_mae: 13.9297\n",
      "Epoch 19/800\n",
      "122/122 [==============================] - 0s 712us/step - loss: 936.0894 - mae: 23.7770 - val_loss: 314.5584 - val_mae: 12.9089\n",
      "Epoch 20/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 915.3024 - mae: 23.3131 - val_loss: 282.6858 - val_mae: 12.4646\n",
      "Epoch 21/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 876.7798 - mae: 22.7349 - val_loss: 259.8075 - val_mae: 12.3252\n",
      "Epoch 22/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 810.5205 - mae: 21.8139 - val_loss: 238.5651 - val_mae: 12.1386\n",
      "Epoch 23/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 807.4009 - mae: 21.5368 - val_loss: 212.6280 - val_mae: 10.8640\n",
      "Epoch 24/800\n",
      "122/122 [==============================] - 0s 689us/step - loss: 735.7916 - mae: 20.6731 - val_loss: 183.4346 - val_mae: 10.1393\n",
      "Epoch 25/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 733.0205 - mae: 20.5453 - val_loss: 167.6789 - val_mae: 9.7850\n",
      "Epoch 26/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 687.8945 - mae: 19.7944 - val_loss: 149.7554 - val_mae: 9.5022\n",
      "Epoch 27/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 684.1584 - mae: 19.8275 - val_loss: 125.4342 - val_mae: 8.3877\n",
      "Epoch 28/800\n",
      "122/122 [==============================] - 0s 689us/step - loss: 624.3302 - mae: 18.8961 - val_loss: 129.0200 - val_mae: 8.8548\n",
      "Epoch 29/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 654.8419 - mae: 18.9328 - val_loss: 112.6271 - val_mae: 7.8448\n",
      "Epoch 30/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 645.6710 - mae: 19.0385 - val_loss: 102.3323 - val_mae: 7.4634\n",
      "Epoch 31/800\n",
      "122/122 [==============================] - 0s 645us/step - loss: 615.8230 - mae: 18.6800 - val_loss: 90.4721 - val_mae: 7.4071\n",
      "Epoch 32/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 615.5527 - mae: 18.4486 - val_loss: 92.4662 - val_mae: 7.1255\n",
      "Epoch 33/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 612.8371 - mae: 18.2234 - val_loss: 77.8907 - val_mae: 6.5524\n",
      "Epoch 34/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 609.3121 - mae: 18.1664 - val_loss: 77.8378 - val_mae: 6.7128\n",
      "Epoch 35/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 575.0955 - mae: 17.7337 - val_loss: 72.8921 - val_mae: 6.5295\n",
      "Epoch 36/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 548.8295 - mae: 17.4050 - val_loss: 65.1541 - val_mae: 6.0357\n",
      "Epoch 37/800\n",
      "122/122 [==============================] - 0s 657us/step - loss: 530.1339 - mae: 16.8347 - val_loss: 64.2378 - val_mae: 5.9344\n",
      "Epoch 38/800\n",
      "122/122 [==============================] - 0s 656us/step - loss: 542.5764 - mae: 17.1063 - val_loss: 60.1875 - val_mae: 5.9241\n",
      "Epoch 39/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 570.7287 - mae: 17.2548 - val_loss: 62.5186 - val_mae: 5.9549\n",
      "Epoch 40/800\n",
      "122/122 [==============================] - 0s 600us/step - loss: 542.1174 - mae: 16.7143 - val_loss: 45.2838 - val_mae: 4.9100\n",
      "Epoch 41/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 513.5039 - mae: 16.6569 - val_loss: 65.1473 - val_mae: 6.3813\n",
      "Epoch 42/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 530.3857 - mae: 16.6043 - val_loss: 48.4219 - val_mae: 5.3276\n",
      "Epoch 43/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 553.6603 - mae: 16.8204 - val_loss: 50.2377 - val_mae: 5.6139\n",
      "Epoch 44/800\n",
      "122/122 [==============================] - 0s 673us/step - loss: 492.9730 - mae: 16.1474 - val_loss: 61.3499 - val_mae: 6.0421\n",
      "Epoch 45/800\n",
      "122/122 [==============================] - 0s 664us/step - loss: 515.8176 - mae: 16.2467 - val_loss: 39.8055 - val_mae: 4.5209\n",
      "Epoch 46/800\n",
      "122/122 [==============================] - 0s 617us/step - loss: 503.5434 - mae: 16.0750 - val_loss: 40.4536 - val_mae: 4.7553\n",
      "Epoch 47/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 492.5190 - mae: 15.9742 - val_loss: 43.4413 - val_mae: 5.1463\n",
      "Epoch 48/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 506.9314 - mae: 16.0440 - val_loss: 38.0892 - val_mae: 4.5507\n",
      "Epoch 49/800\n",
      "122/122 [==============================] - 0s 700us/step - loss: 497.4471 - mae: 16.0445 - val_loss: 33.6880 - val_mae: 4.2272\n",
      "Epoch 50/800\n",
      "122/122 [==============================] - 0s 632us/step - loss: 465.7989 - mae: 15.5300 - val_loss: 33.6769 - val_mae: 4.2863\n",
      "Epoch 51/800\n",
      "122/122 [==============================] - 0s 643us/step - loss: 519.6166 - mae: 16.1449 - val_loss: 51.5558 - val_mae: 5.6758\n",
      "Epoch 52/800\n",
      "122/122 [==============================] - 0s 633us/step - loss: 506.4759 - mae: 15.8194 - val_loss: 35.6190 - val_mae: 4.3539\n",
      "Epoch 53/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 483.8503 - mae: 15.6490 - val_loss: 37.3209 - val_mae: 4.4974\n",
      "Epoch 54/800\n",
      "122/122 [==============================] - 0s 664us/step - loss: 480.2952 - mae: 15.6147 - val_loss: 32.9312 - val_mae: 4.3251\n",
      "Epoch 55/800\n",
      "122/122 [==============================] - 0s 660us/step - loss: 437.4545 - mae: 14.9202 - val_loss: 59.3288 - val_mae: 5.8475\n",
      "Epoch 56/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 466.9507 - mae: 15.1522 - val_loss: 54.9339 - val_mae: 5.8708\n",
      "Epoch 57/800\n",
      "122/122 [==============================] - 0s 643us/step - loss: 454.7467 - mae: 14.9486 - val_loss: 36.2877 - val_mae: 4.5299\n",
      "Epoch 58/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 480.1400 - mae: 15.4035 - val_loss: 31.4711 - val_mae: 4.0029\n",
      "Epoch 59/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 459.3621 - mae: 14.9908 - val_loss: 32.3965 - val_mae: 3.9469\n",
      "Epoch 60/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 436.4318 - mae: 14.5620 - val_loss: 36.9156 - val_mae: 4.6603\n",
      "Epoch 61/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 463.9081 - mae: 15.0807 - val_loss: 29.7299 - val_mae: 3.7978\n",
      "Epoch 62/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 464.0538 - mae: 15.0180 - val_loss: 28.9039 - val_mae: 3.8763\n",
      "Epoch 63/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 437.7726 - mae: 14.4776 - val_loss: 29.0605 - val_mae: 3.7221\n",
      "Epoch 64/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 439.7889 - mae: 14.6805 - val_loss: 37.6287 - val_mae: 4.5862\n",
      "Epoch 65/800\n",
      "122/122 [==============================] - 0s 739us/step - loss: 441.9134 - mae: 14.4051 - val_loss: 33.1034 - val_mae: 4.0455\n",
      "Epoch 66/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 428.6249 - mae: 14.3903 - val_loss: 30.2002 - val_mae: 4.1043\n",
      "Epoch 67/800\n",
      "122/122 [==============================] - 0s 640us/step - loss: 392.0090 - mae: 13.9519 - val_loss: 37.5746 - val_mae: 4.7876\n",
      "Epoch 68/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 438.6626 - mae: 14.6014 - val_loss: 30.6586 - val_mae: 4.1439\n",
      "Epoch 69/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 421.2841 - mae: 14.2053 - val_loss: 34.6787 - val_mae: 4.2887\n",
      "Epoch 70/800\n",
      "122/122 [==============================] - 0s 727us/step - loss: 416.0543 - mae: 14.1075 - val_loss: 29.8017 - val_mae: 3.8564\n",
      "Epoch 71/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 426.4334 - mae: 14.2174 - val_loss: 31.6091 - val_mae: 4.1975\n",
      "Epoch 72/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 397.9054 - mae: 13.8717 - val_loss: 27.1563 - val_mae: 3.7083\n",
      "Epoch 73/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 404.4335 - mae: 13.9186 - val_loss: 33.5143 - val_mae: 4.3459\n",
      "Epoch 74/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 413.7409 - mae: 14.0341 - val_loss: 30.8580 - val_mae: 3.9053\n",
      "Epoch 75/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 399.9443 - mae: 13.9079 - val_loss: 27.2485 - val_mae: 3.6936\n",
      "Epoch 76/800\n",
      "122/122 [==============================] - 0s 678us/step - loss: 385.6108 - mae: 13.5415 - val_loss: 29.4195 - val_mae: 3.9245\n",
      "Epoch 77/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 393.8801 - mae: 13.5374 - val_loss: 37.1817 - val_mae: 4.6581\n",
      "Epoch 78/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 388.4883 - mae: 13.4873 - val_loss: 23.9215 - val_mae: 3.5060\n",
      "Epoch 79/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 387.0949 - mae: 13.5714 - val_loss: 27.2960 - val_mae: 3.7230\n",
      "Epoch 80/800\n",
      "122/122 [==============================] - 0s 711us/step - loss: 362.9378 - mae: 13.2486 - val_loss: 41.2924 - val_mae: 5.0708\n",
      "Epoch 81/800\n",
      "122/122 [==============================] - 0s 600us/step - loss: 405.5144 - mae: 13.7826 - val_loss: 29.4050 - val_mae: 3.7775\n",
      "Epoch 82/800\n",
      "122/122 [==============================] - 0s 641us/step - loss: 357.7652 - mae: 13.0557 - val_loss: 35.4057 - val_mae: 4.2720\n",
      "Epoch 83/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 356.0718 - mae: 13.1462 - val_loss: 25.0975 - val_mae: 3.5983\n",
      "Epoch 84/800\n",
      "122/122 [==============================] - 0s 632us/step - loss: 386.0536 - mae: 13.3797 - val_loss: 26.1154 - val_mae: 3.5864\n",
      "Epoch 85/800\n",
      "122/122 [==============================] - 0s 598us/step - loss: 353.8830 - mae: 12.9445 - val_loss: 31.6167 - val_mae: 3.9447\n",
      "Epoch 86/800\n",
      "122/122 [==============================] - 0s 687us/step - loss: 381.2042 - mae: 13.3454 - val_loss: 25.9338 - val_mae: 3.5086\n",
      "Epoch 87/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 378.6789 - mae: 13.3397 - val_loss: 25.3780 - val_mae: 3.6198\n",
      "Epoch 88/800\n",
      "122/122 [==============================] - 0s 619us/step - loss: 365.2385 - mae: 13.0149 - val_loss: 33.5349 - val_mae: 4.0614\n",
      "Epoch 89/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 378.1490 - mae: 13.2479 - val_loss: 24.0332 - val_mae: 3.4544\n",
      "Epoch 90/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 351.2008 - mae: 12.7417 - val_loss: 30.7888 - val_mae: 4.0255\n",
      "Epoch 91/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 342.3093 - mae: 12.4613 - val_loss: 26.3777 - val_mae: 3.6241\n",
      "Epoch 92/800\n",
      "122/122 [==============================] - 0s 661us/step - loss: 354.3969 - mae: 12.7861 - val_loss: 33.3653 - val_mae: 4.3641\n",
      "Epoch 93/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 348.3196 - mae: 12.6484 - val_loss: 64.7779 - val_mae: 5.9703\n",
      "Epoch 94/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 377.4272 - mae: 13.0252 - val_loss: 33.5768 - val_mae: 4.2404\n",
      "Epoch 95/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 338.5245 - mae: 12.5755 - val_loss: 23.6383 - val_mae: 3.5113\n",
      "Epoch 96/800\n",
      "122/122 [==============================] - 0s 692us/step - loss: 339.7086 - mae: 12.4937 - val_loss: 31.7874 - val_mae: 4.2677\n",
      "Epoch 97/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 358.7685 - mae: 12.7866 - val_loss: 25.0279 - val_mae: 3.7211\n",
      "Epoch 98/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 341.6294 - mae: 12.5179 - val_loss: 33.5938 - val_mae: 4.1231\n",
      "Epoch 99/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 354.7697 - mae: 12.7454 - val_loss: 29.6954 - val_mae: 4.1030\n",
      "Epoch 100/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 358.9180 - mae: 12.7389 - val_loss: 31.6946 - val_mae: 4.2633\n",
      "Epoch 101/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 342.5842 - mae: 12.3797 - val_loss: 23.1140 - val_mae: 3.4167\n",
      "Epoch 102/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 345.9698 - mae: 12.2895 - val_loss: 24.0266 - val_mae: 3.4626\n",
      "Epoch 103/800\n",
      "122/122 [==============================] - 0s 650us/step - loss: 348.2663 - mae: 12.4406 - val_loss: 23.1397 - val_mae: 3.3823\n",
      "Epoch 104/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 326.3959 - mae: 12.1865 - val_loss: 27.1906 - val_mae: 3.7008\n",
      "Epoch 105/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 348.6614 - mae: 12.3313 - val_loss: 29.8282 - val_mae: 3.9210\n",
      "Epoch 106/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 336.2043 - mae: 12.2472 - val_loss: 34.1400 - val_mae: 4.1050\n",
      "Epoch 107/800\n",
      "122/122 [==============================] - 0s 598us/step - loss: 352.7029 - mae: 12.5462 - val_loss: 29.2373 - val_mae: 3.9531\n",
      "Epoch 108/800\n",
      "122/122 [==============================] - 0s 641us/step - loss: 313.9574 - mae: 12.0341 - val_loss: 29.3606 - val_mae: 3.8623\n",
      "Epoch 109/800\n",
      "122/122 [==============================] - 0s 649us/step - loss: 346.0745 - mae: 12.3828 - val_loss: 24.3759 - val_mae: 3.4662\n",
      "Epoch 110/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 313.7437 - mae: 11.8534 - val_loss: 28.7643 - val_mae: 3.8424\n",
      "Epoch 111/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 327.1628 - mae: 12.1010 - val_loss: 24.8799 - val_mae: 3.6136\n",
      "Epoch 112/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 314.3045 - mae: 11.9707 - val_loss: 25.9737 - val_mae: 3.6304\n",
      "Epoch 113/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 331.4033 - mae: 11.9416 - val_loss: 24.4960 - val_mae: 3.5315\n",
      "Epoch 114/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 315.0676 - mae: 11.8397 - val_loss: 23.2672 - val_mae: 3.3426\n",
      "Epoch 115/800\n",
      "122/122 [==============================] - 0s 670us/step - loss: 304.4266 - mae: 11.7042 - val_loss: 26.3013 - val_mae: 3.8252\n",
      "Epoch 116/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 300.1311 - mae: 11.6099 - val_loss: 32.6193 - val_mae: 4.2933\n",
      "Epoch 117/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 293.4521 - mae: 11.4909 - val_loss: 24.9431 - val_mae: 3.4589\n",
      "Epoch 118/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 306.2092 - mae: 11.3820 - val_loss: 32.4680 - val_mae: 4.2370\n",
      "Epoch 119/800\n",
      "122/122 [==============================] - 0s 566us/step - loss: 292.8390 - mae: 11.5326 - val_loss: 37.3354 - val_mae: 4.5605\n",
      "Epoch 120/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 300.1433 - mae: 11.3959 - val_loss: 24.3826 - val_mae: 3.4535\n",
      "Epoch 121/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 309.0409 - mae: 11.5764 - val_loss: 30.2583 - val_mae: 4.1025\n",
      "Epoch 122/800\n",
      "122/122 [==============================] - 0s 674us/step - loss: 304.8201 - mae: 11.5354 - val_loss: 39.4007 - val_mae: 4.8250\n",
      "Epoch 123/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 274.3757 - mae: 11.0598 - val_loss: 36.4514 - val_mae: 4.2478\n",
      "Epoch 124/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 290.1277 - mae: 11.2988 - val_loss: 34.7325 - val_mae: 4.3809\n",
      "Epoch 125/800\n",
      "122/122 [==============================] - 0s 570us/step - loss: 298.1771 - mae: 11.5701 - val_loss: 26.4928 - val_mae: 3.7779\n",
      "Epoch 126/800\n",
      "122/122 [==============================] - 0s 703us/step - loss: 289.9856 - mae: 11.3045 - val_loss: 30.0983 - val_mae: 4.1336\n",
      "Epoch 127/800\n",
      "122/122 [==============================] - 0s 573us/step - loss: 260.2594 - mae: 10.8543 - val_loss: 45.8212 - val_mae: 5.0930\n",
      "Epoch 128/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 296.9858 - mae: 11.2424 - val_loss: 29.4804 - val_mae: 3.9159\n",
      "Epoch 129/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 276.6515 - mae: 11.0624 - val_loss: 33.3301 - val_mae: 4.2789\n",
      "Epoch 130/800\n",
      "122/122 [==============================] - 0s 573us/step - loss: 276.3304 - mae: 11.0942 - val_loss: 36.4902 - val_mae: 4.6097\n",
      "Epoch 131/800\n",
      "122/122 [==============================] - 0s 709us/step - loss: 284.5520 - mae: 11.1182 - val_loss: 25.0608 - val_mae: 3.6275\n",
      "Epoch 132/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 286.6134 - mae: 11.1062 - val_loss: 37.8725 - val_mae: 4.7204\n",
      "Epoch 133/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 279.6273 - mae: 11.1131 - val_loss: 68.4336 - val_mae: 6.6478\n",
      "Epoch 134/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 277.0378 - mae: 11.1240 - val_loss: 28.9727 - val_mae: 4.0759\n",
      "Epoch 135/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 250.3468 - mae: 10.6543 - val_loss: 23.3861 - val_mae: 3.4169\n",
      "Epoch 136/800\n",
      "122/122 [==============================] - 0s 710us/step - loss: 261.2219 - mae: 10.7600 - val_loss: 26.9800 - val_mae: 3.8359\n",
      "Epoch 137/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 266.9675 - mae: 10.8290 - val_loss: 51.9985 - val_mae: 5.3835\n",
      "Epoch 138/800\n",
      "122/122 [==============================] - 0s 593us/step - loss: 269.6842 - mae: 10.7396 - val_loss: 25.6637 - val_mae: 3.7340\n",
      "Epoch 139/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 262.7108 - mae: 10.7123 - val_loss: 28.5891 - val_mae: 3.8700\n",
      "Epoch 140/800\n",
      "122/122 [==============================] - 0s 564us/step - loss: 270.7531 - mae: 10.7567 - val_loss: 26.3402 - val_mae: 3.7531\n",
      "Epoch 141/800\n",
      "122/122 [==============================] - 0s 690us/step - loss: 260.5307 - mae: 10.6412 - val_loss: 32.0094 - val_mae: 4.3862\n",
      "Epoch 142/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 255.9444 - mae: 10.6060 - val_loss: 32.6330 - val_mae: 4.3726\n",
      "Epoch 143/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 251.5260 - mae: 10.6482 - val_loss: 23.4614 - val_mae: 3.4313\n",
      "Epoch 144/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 251.5848 - mae: 10.5825 - val_loss: 28.9549 - val_mae: 3.8205\n",
      "Epoch 145/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 250.6521 - mae: 10.4961 - val_loss: 34.7682 - val_mae: 4.5355\n",
      "Epoch 146/800\n",
      "122/122 [==============================] - 0s 592us/step - loss: 235.2289 - mae: 10.1985 - val_loss: 46.7257 - val_mae: 5.1957\n",
      "Epoch 147/800\n",
      "122/122 [==============================] - 0s 705us/step - loss: 235.4004 - mae: 10.1649 - val_loss: 29.4251 - val_mae: 3.7610\n",
      "Epoch 148/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 247.6496 - mae: 10.3583 - val_loss: 23.1051 - val_mae: 3.4353\n",
      "Epoch 149/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 252.5686 - mae: 10.4143 - val_loss: 23.3505 - val_mae: 3.4125\n",
      "Epoch 150/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 245.1877 - mae: 10.4060 - val_loss: 29.5768 - val_mae: 4.1453\n",
      "Epoch 151/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 254.9495 - mae: 10.4071 - val_loss: 27.1425 - val_mae: 3.6921\n",
      "Epoch 152/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 224.8946 - mae: 9.9590 - val_loss: 30.4724 - val_mae: 4.0298\n",
      "Epoch 153/800\n",
      "122/122 [==============================] - 0s 661us/step - loss: 231.6533 - mae: 10.1734 - val_loss: 23.5196 - val_mae: 3.4898\n",
      "Epoch 154/800\n",
      "122/122 [==============================] - 0s 668us/step - loss: 237.8797 - mae: 10.1100 - val_loss: 24.5713 - val_mae: 3.6090\n",
      "Epoch 155/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 258.2283 - mae: 10.5986 - val_loss: 25.0021 - val_mae: 3.6394\n",
      "Epoch 156/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 227.5953 - mae: 10.0243 - val_loss: 32.6960 - val_mae: 4.2680\n",
      "Epoch 157/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 229.9141 - mae: 10.0202 - val_loss: 26.5103 - val_mae: 3.6999\n",
      "Epoch 158/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 235.7861 - mae: 9.9908 - val_loss: 24.8584 - val_mae: 3.6044\n",
      "Epoch 159/800\n",
      "122/122 [==============================] - 0s 645us/step - loss: 218.6821 - mae: 9.8487 - val_loss: 25.3695 - val_mae: 3.7239\n",
      "Epoch 160/800\n",
      "122/122 [==============================] - 0s 650us/step - loss: 240.4268 - mae: 10.2208 - val_loss: 26.1795 - val_mae: 3.6810\n",
      "Epoch 161/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 224.5880 - mae: 9.9774 - val_loss: 32.9010 - val_mae: 4.4055\n",
      "Epoch 162/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 236.0889 - mae: 10.0247 - val_loss: 26.1074 - val_mae: 3.6980\n",
      "Epoch 163/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 227.7211 - mae: 9.9177 - val_loss: 35.6899 - val_mae: 4.5372\n",
      "Epoch 164/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 220.1187 - mae: 9.7256 - val_loss: 26.7848 - val_mae: 3.7804\n",
      "Epoch 165/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 213.6459 - mae: 9.6120 - val_loss: 26.4335 - val_mae: 3.8021\n",
      "Epoch 166/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 219.3445 - mae: 9.6479 - val_loss: 23.3205 - val_mae: 3.5151\n",
      "Epoch 167/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 208.0516 - mae: 9.5116 - val_loss: 23.4441 - val_mae: 3.4813\n",
      "Epoch 168/800\n",
      "122/122 [==============================] - 0s 682us/step - loss: 198.6780 - mae: 9.3712 - val_loss: 26.2297 - val_mae: 3.8227\n",
      "Epoch 169/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 216.3861 - mae: 9.5977 - val_loss: 26.4765 - val_mae: 3.7535\n",
      "Epoch 170/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 209.4751 - mae: 9.5933 - val_loss: 31.4863 - val_mae: 4.2829\n",
      "Epoch 171/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 210.3869 - mae: 9.4657 - val_loss: 23.0323 - val_mae: 3.4299\n",
      "Epoch 172/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 194.2000 - mae: 9.2069 - val_loss: 27.5541 - val_mae: 3.9677\n",
      "Epoch 173/800\n",
      "122/122 [==============================] - 0s 664us/step - loss: 199.2849 - mae: 9.3520 - val_loss: 23.8879 - val_mae: 3.5489\n",
      "Epoch 174/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 212.3289 - mae: 9.4148 - val_loss: 28.2727 - val_mae: 3.9048\n",
      "Epoch 175/800\n",
      "122/122 [==============================] - 0s 567us/step - loss: 218.5350 - mae: 9.5904 - val_loss: 30.0311 - val_mae: 4.0850\n",
      "Epoch 176/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 211.7662 - mae: 9.5175 - val_loss: 24.5655 - val_mae: 3.6077\n",
      "Epoch 177/800\n",
      "122/122 [==============================] - 0s 681us/step - loss: 194.5369 - mae: 9.1575 - val_loss: 29.6000 - val_mae: 4.1493\n",
      "Epoch 178/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 197.0606 - mae: 9.3503 - val_loss: 25.6780 - val_mae: 3.7238\n",
      "Epoch 179/800\n",
      "122/122 [==============================] - 0s 601us/step - loss: 196.4521 - mae: 9.1769 - val_loss: 24.4137 - val_mae: 3.5997\n",
      "Epoch 180/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 204.2452 - mae: 9.2993 - val_loss: 24.5899 - val_mae: 3.5813\n",
      "Epoch 181/800\n",
      "122/122 [==============================] - 0s 571us/step - loss: 182.3002 - mae: 8.9087 - val_loss: 23.0325 - val_mae: 3.4845\n",
      "Epoch 182/800\n",
      "122/122 [==============================] - 0s 713us/step - loss: 206.9414 - mae: 9.2836 - val_loss: 26.7815 - val_mae: 3.9303\n",
      "Epoch 183/800\n",
      "122/122 [==============================] - 0s 577us/step - loss: 198.9145 - mae: 9.2694 - val_loss: 31.2878 - val_mae: 4.1379\n",
      "Epoch 184/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 206.4194 - mae: 9.3389 - val_loss: 26.8851 - val_mae: 3.8547\n",
      "Epoch 185/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 200.9696 - mae: 9.2465 - val_loss: 22.8291 - val_mae: 3.4631\n",
      "Epoch 186/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 191.3675 - mae: 8.9461 - val_loss: 24.6980 - val_mae: 3.6486\n",
      "Epoch 187/800\n",
      "122/122 [==============================] - 0s 564us/step - loss: 183.6795 - mae: 8.9682 - val_loss: 26.0961 - val_mae: 3.7788\n",
      "Epoch 188/800\n",
      "122/122 [==============================] - 0s 704us/step - loss: 174.6808 - mae: 8.8233 - val_loss: 25.5969 - val_mae: 3.7352\n",
      "Epoch 189/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 190.8095 - mae: 9.1110 - val_loss: 25.1564 - val_mae: 3.6217\n",
      "Epoch 190/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 185.9922 - mae: 8.8869 - val_loss: 24.6532 - val_mae: 3.6433\n",
      "Epoch 191/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 194.3822 - mae: 9.0519 - val_loss: 26.6371 - val_mae: 3.8449\n",
      "Epoch 192/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 175.3132 - mae: 8.6457 - val_loss: 26.0438 - val_mae: 3.8383\n",
      "Epoch 193/800\n",
      "122/122 [==============================] - 0s 657us/step - loss: 192.2307 - mae: 8.9848 - val_loss: 29.6730 - val_mae: 4.0789\n",
      "Epoch 194/800\n",
      "122/122 [==============================] - 0s 609us/step - loss: 177.4308 - mae: 8.6640 - val_loss: 29.9679 - val_mae: 4.0942\n",
      "Epoch 195/800\n",
      "122/122 [==============================] - 0s 568us/step - loss: 183.7770 - mae: 8.9533 - val_loss: 24.1687 - val_mae: 3.5611\n",
      "Epoch 196/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 187.6342 - mae: 8.9385 - val_loss: 28.4497 - val_mae: 4.1213\n",
      "Epoch 197/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 177.1268 - mae: 8.6973 - val_loss: 29.8977 - val_mae: 4.1293\n",
      "Epoch 198/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 183.2732 - mae: 8.8515 - val_loss: 25.0591 - val_mae: 3.7280\n",
      "Epoch 199/800\n",
      "122/122 [==============================] - 0s 568us/step - loss: 191.0792 - mae: 8.9490 - val_loss: 29.2823 - val_mae: 4.0216\n",
      "Epoch 200/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 171.2824 - mae: 8.7242 - val_loss: 26.7967 - val_mae: 3.7690\n",
      "Epoch 201/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 176.9505 - mae: 8.6820 - val_loss: 23.9037 - val_mae: 3.6474\n",
      "Epoch 202/800\n",
      "122/122 [==============================] - 0s 648us/step - loss: 170.7810 - mae: 8.6384 - val_loss: 25.4085 - val_mae: 3.7438\n",
      "Epoch 203/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 161.5282 - mae: 8.4247 - val_loss: 27.0459 - val_mae: 3.7936\n",
      "Epoch 204/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 177.8090 - mae: 8.7121 - val_loss: 25.1899 - val_mae: 3.6687\n",
      "Epoch 205/800\n",
      "122/122 [==============================] - 0s 601us/step - loss: 183.9539 - mae: 8.6923 - val_loss: 30.0234 - val_mae: 4.0440\n",
      "Epoch 206/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 171.7566 - mae: 8.5566 - val_loss: 33.1072 - val_mae: 4.3621\n",
      "Epoch 207/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 170.5673 - mae: 8.5718 - val_loss: 25.7110 - val_mae: 3.6782\n",
      "Epoch 208/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 173.5757 - mae: 8.6771 - val_loss: 33.1303 - val_mae: 4.4332\n",
      "Epoch 209/800\n",
      "122/122 [==============================] - 0s 656us/step - loss: 160.7237 - mae: 8.4183 - val_loss: 29.0515 - val_mae: 4.1078\n",
      "Epoch 210/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 161.1267 - mae: 8.4671 - val_loss: 30.6209 - val_mae: 4.1329\n",
      "Epoch 211/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 173.5085 - mae: 8.8194 - val_loss: 25.6923 - val_mae: 3.7626\n",
      "Epoch 212/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 172.6916 - mae: 8.6389 - val_loss: 25.2620 - val_mae: 3.7487\n",
      "Epoch 213/800\n",
      "122/122 [==============================] - 0s 700us/step - loss: 153.9434 - mae: 8.1401 - val_loss: 28.4492 - val_mae: 4.0130\n",
      "Epoch 214/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 162.2072 - mae: 8.3589 - val_loss: 41.4047 - val_mae: 5.0349\n",
      "Epoch 215/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 171.1002 - mae: 8.5904 - val_loss: 25.3592 - val_mae: 3.6711\n",
      "Epoch 216/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 156.8181 - mae: 8.2543 - val_loss: 27.8038 - val_mae: 3.9726\n",
      "Epoch 217/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 160.1156 - mae: 8.3961 - val_loss: 23.6995 - val_mae: 3.5592\n",
      "Epoch 218/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 154.2216 - mae: 8.3103 - val_loss: 32.4623 - val_mae: 4.2492\n",
      "Epoch 219/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 162.3841 - mae: 8.4380 - val_loss: 25.1822 - val_mae: 3.6962\n",
      "Epoch 220/800\n",
      "122/122 [==============================] - 0s 645us/step - loss: 169.8660 - mae: 8.4944 - val_loss: 23.8794 - val_mae: 3.5669\n",
      "Epoch 221/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 151.9316 - mae: 8.2007 - val_loss: 29.0450 - val_mae: 4.1367\n",
      "Epoch 222/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 158.6392 - mae: 8.4288 - val_loss: 27.0797 - val_mae: 3.8697\n",
      "Epoch 223/800\n",
      "122/122 [==============================] - 0s 687us/step - loss: 153.8230 - mae: 8.2075 - val_loss: 30.2991 - val_mae: 4.2235\n",
      "Epoch 224/800\n",
      "122/122 [==============================] - 0s 573us/step - loss: 160.6277 - mae: 8.4002 - val_loss: 25.2573 - val_mae: 3.6599\n",
      "Epoch 225/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 146.6867 - mae: 8.0926 - val_loss: 54.5813 - val_mae: 5.6893\n",
      "Epoch 226/800\n",
      "122/122 [==============================] - 0s 578us/step - loss: 155.9863 - mae: 8.3153 - val_loss: 27.1055 - val_mae: 3.9528\n",
      "Epoch 227/800\n",
      "122/122 [==============================] - 0s 715us/step - loss: 147.8678 - mae: 8.1592 - val_loss: 36.0315 - val_mae: 4.5790\n",
      "Epoch 228/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 152.5225 - mae: 8.1826 - val_loss: 25.6906 - val_mae: 3.7488\n",
      "Epoch 229/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 155.8403 - mae: 8.3447 - val_loss: 31.2872 - val_mae: 4.2827\n",
      "Epoch 230/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 147.0431 - mae: 8.1710 - val_loss: 25.8569 - val_mae: 3.7484\n",
      "Epoch 231/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 162.5360 - mae: 8.4723 - val_loss: 33.2170 - val_mae: 4.3912\n",
      "Epoch 232/800\n",
      "122/122 [==============================] - 0s 721us/step - loss: 143.3106 - mae: 8.0431 - val_loss: 28.8679 - val_mae: 4.0242\n",
      "Epoch 233/800\n",
      "122/122 [==============================] - 0s 592us/step - loss: 144.7920 - mae: 8.1422 - val_loss: 27.0106 - val_mae: 3.8620\n",
      "Epoch 234/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 147.7139 - mae: 8.0992 - val_loss: 26.3969 - val_mae: 3.7323\n",
      "Epoch 235/800\n",
      "122/122 [==============================] - 0s 593us/step - loss: 155.6977 - mae: 8.2833 - val_loss: 29.0747 - val_mae: 4.0421\n",
      "Epoch 236/800\n",
      "122/122 [==============================] - 0s 571us/step - loss: 148.0242 - mae: 8.1922 - val_loss: 35.3680 - val_mae: 4.6033\n",
      "Epoch 237/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 151.5740 - mae: 8.1894 - val_loss: 24.9995 - val_mae: 3.6883\n",
      "Epoch 238/800\n",
      "122/122 [==============================] - 0s 575us/step - loss: 137.7165 - mae: 7.9360 - val_loss: 25.3144 - val_mae: 3.6817\n",
      "Epoch 239/800\n",
      "122/122 [==============================] - 0s 738us/step - loss: 140.2304 - mae: 7.9792 - val_loss: 28.3553 - val_mae: 3.8921\n",
      "Epoch 240/800\n",
      "122/122 [==============================] - 0s 574us/step - loss: 140.6575 - mae: 7.9447 - val_loss: 36.4047 - val_mae: 4.6435\n",
      "Epoch 241/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 156.6997 - mae: 8.3309 - val_loss: 25.5539 - val_mae: 3.6651\n",
      "Epoch 242/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 143.9834 - mae: 8.0364 - val_loss: 27.2942 - val_mae: 3.9015\n",
      "Epoch 243/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 137.1979 - mae: 7.9115 - val_loss: 30.5542 - val_mae: 4.1104\n",
      "Epoch 244/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 156.6691 - mae: 8.3127 - val_loss: 25.8041 - val_mae: 3.6991\n",
      "Epoch 245/800\n",
      "122/122 [==============================] - 0s 671us/step - loss: 138.5703 - mae: 7.9343 - val_loss: 26.0090 - val_mae: 3.7352\n",
      "Epoch 246/800\n",
      "122/122 [==============================] - 0s 574us/step - loss: 140.2616 - mae: 7.9400 - val_loss: 27.0279 - val_mae: 3.8704\n",
      "Epoch 247/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 144.7232 - mae: 8.0629 - val_loss: 26.2628 - val_mae: 3.8309\n",
      "Epoch 248/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 137.9640 - mae: 7.8705 - val_loss: 28.2729 - val_mae: 3.9413\n",
      "Epoch 249/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 142.8304 - mae: 8.0225 - val_loss: 26.8188 - val_mae: 3.8070\n",
      "Epoch 250/800\n",
      "122/122 [==============================] - 0s 577us/step - loss: 149.8358 - mae: 8.1831 - val_loss: 27.2542 - val_mae: 3.7921\n",
      "Epoch 251/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 138.6718 - mae: 7.8469 - val_loss: 27.4356 - val_mae: 3.8501\n",
      "Epoch 252/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 133.2145 - mae: 7.7926 - val_loss: 25.0766 - val_mae: 3.6534\n",
      "Epoch 253/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 134.5744 - mae: 7.9353 - val_loss: 28.6207 - val_mae: 4.0748\n",
      "Epoch 254/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 130.6879 - mae: 7.8224 - val_loss: 26.9741 - val_mae: 3.8714\n",
      "Epoch 255/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 147.0823 - mae: 8.0678 - val_loss: 27.0594 - val_mae: 3.8661\n",
      "Epoch 256/800\n",
      "122/122 [==============================] - 0s 706us/step - loss: 134.9681 - mae: 7.9151 - val_loss: 27.0699 - val_mae: 3.7940\n",
      "Epoch 257/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 139.3696 - mae: 7.9288 - val_loss: 29.0997 - val_mae: 4.0484\n",
      "Epoch 258/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 133.6819 - mae: 7.8590 - val_loss: 25.3178 - val_mae: 3.6405\n",
      "Epoch 259/800\n",
      "122/122 [==============================] - 0s 577us/step - loss: 138.0063 - mae: 7.9006 - val_loss: 26.1770 - val_mae: 3.7700\n",
      "Epoch 260/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 130.8161 - mae: 7.8022 - val_loss: 31.4063 - val_mae: 4.2803\n",
      "Epoch 261/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 134.8450 - mae: 7.9306 - val_loss: 27.7678 - val_mae: 3.9109\n",
      "Epoch 262/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 132.8595 - mae: 7.8085 - val_loss: 25.4800 - val_mae: 3.6012\n",
      "Epoch 263/800\n",
      "122/122 [==============================] - 0s 574us/step - loss: 131.1412 - mae: 7.6782 - val_loss: 27.0561 - val_mae: 3.7653\n",
      "Epoch 264/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 137.4608 - mae: 7.8463 - val_loss: 25.7250 - val_mae: 3.7048\n",
      "Epoch 265/800\n",
      "122/122 [==============================] - 0s 577us/step - loss: 132.5971 - mae: 7.7758 - val_loss: 28.1112 - val_mae: 3.9769\n",
      "Epoch 266/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 128.3606 - mae: 7.7256 - val_loss: 36.5414 - val_mae: 4.7025\n",
      "Epoch 267/800\n",
      "122/122 [==============================] - 0s 674us/step - loss: 133.3064 - mae: 7.7792 - val_loss: 24.8537 - val_mae: 3.7234\n",
      "Epoch 268/800\n",
      "122/122 [==============================] - 0s 724us/step - loss: 133.7180 - mae: 7.8351 - val_loss: 24.7038 - val_mae: 3.6399\n",
      "Epoch 269/800\n",
      "122/122 [==============================] - 0s 632us/step - loss: 126.5387 - mae: 7.6794 - val_loss: 25.2298 - val_mae: 3.7081\n",
      "Epoch 270/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 122.3756 - mae: 7.6097 - val_loss: 34.5107 - val_mae: 4.5314\n",
      "Epoch 271/800\n",
      "122/122 [==============================] - 0s 712us/step - loss: 126.5217 - mae: 7.7103 - val_loss: 26.0106 - val_mae: 3.7514\n",
      "Epoch 272/800\n",
      "122/122 [==============================] - 0s 673us/step - loss: 128.1093 - mae: 7.7434 - val_loss: 27.9593 - val_mae: 3.8815\n",
      "Epoch 273/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 124.0487 - mae: 7.6648 - val_loss: 25.1623 - val_mae: 3.7036\n",
      "Epoch 274/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 129.4403 - mae: 7.7300 - val_loss: 25.5235 - val_mae: 3.6468\n",
      "Epoch 275/800\n",
      "122/122 [==============================] - 0s 737us/step - loss: 119.4250 - mae: 7.4403 - val_loss: 29.6577 - val_mae: 4.0616\n",
      "Epoch 276/800\n",
      "122/122 [==============================] - 0s 612us/step - loss: 114.6945 - mae: 7.4421 - val_loss: 26.1520 - val_mae: 3.7123\n",
      "Epoch 277/800\n",
      "122/122 [==============================] - 0s 652us/step - loss: 122.2395 - mae: 7.6050 - val_loss: 28.9156 - val_mae: 4.0483\n",
      "Epoch 278/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 131.6875 - mae: 7.8146 - val_loss: 29.9202 - val_mae: 4.1083\n",
      "Epoch 279/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 116.0254 - mae: 7.4785 - val_loss: 27.7129 - val_mae: 3.8832\n",
      "Epoch 280/800\n",
      "122/122 [==============================] - 0s 711us/step - loss: 115.1659 - mae: 7.5621 - val_loss: 25.4680 - val_mae: 3.6718\n",
      "Epoch 281/800\n",
      "122/122 [==============================] - 0s 620us/step - loss: 115.9578 - mae: 7.3794 - val_loss: 26.1789 - val_mae: 3.7601\n",
      "Epoch 282/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 120.0198 - mae: 7.6241 - val_loss: 28.3908 - val_mae: 3.7801\n",
      "Epoch 283/800\n",
      "122/122 [==============================] - 0s 609us/step - loss: 118.7323 - mae: 7.4975 - val_loss: 26.3496 - val_mae: 3.7249\n",
      "Epoch 284/800\n",
      "122/122 [==============================] - 0s 657us/step - loss: 125.1609 - mae: 7.7225 - val_loss: 28.4961 - val_mae: 3.9590\n",
      "Epoch 285/800\n",
      "122/122 [==============================] - 0s 691us/step - loss: 123.8903 - mae: 7.5782 - val_loss: 34.9768 - val_mae: 4.5100\n",
      "Epoch 286/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 115.3716 - mae: 7.3581 - val_loss: 30.2824 - val_mae: 4.1250\n",
      "Epoch 287/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 120.0666 - mae: 7.5702 - val_loss: 25.0602 - val_mae: 3.6138\n",
      "Epoch 288/800\n",
      "122/122 [==============================] - 0s 703us/step - loss: 113.5174 - mae: 7.4929 - val_loss: 34.3877 - val_mae: 4.6727\n",
      "Epoch 289/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 122.0016 - mae: 7.5027 - val_loss: 25.4176 - val_mae: 3.6844\n",
      "Epoch 290/800\n",
      "122/122 [==============================] - 0s 756us/step - loss: 122.0660 - mae: 7.4964 - val_loss: 28.4990 - val_mae: 3.9756\n",
      "Epoch 291/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 111.8367 - mae: 7.3764 - val_loss: 26.6115 - val_mae: 3.7337\n",
      "Epoch 292/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 127.5245 - mae: 7.5748 - val_loss: 35.1786 - val_mae: 4.6049\n",
      "Epoch 293/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 119.3573 - mae: 7.5684 - val_loss: 26.2287 - val_mae: 3.7465\n",
      "Epoch 294/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 120.1925 - mae: 7.5188 - val_loss: 32.6816 - val_mae: 4.2926\n",
      "Epoch 295/800\n",
      "122/122 [==============================] - 0s 600us/step - loss: 112.0661 - mae: 7.3698 - val_loss: 25.6250 - val_mae: 3.7583\n",
      "Epoch 296/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 118.1146 - mae: 7.4851 - val_loss: 25.8438 - val_mae: 3.6548\n",
      "Epoch 297/800\n",
      "122/122 [==============================] - 0s 736us/step - loss: 123.0504 - mae: 7.6081 - val_loss: 27.3002 - val_mae: 3.7503\n",
      "Epoch 298/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 118.3348 - mae: 7.4350 - val_loss: 25.1371 - val_mae: 3.6223\n",
      "Epoch 299/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 119.7682 - mae: 7.5322 - val_loss: 24.3360 - val_mae: 3.6116\n",
      "Epoch 300/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 112.0662 - mae: 7.3806 - val_loss: 37.3732 - val_mae: 4.8273\n",
      "Epoch 301/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 112.5685 - mae: 7.3259 - val_loss: 28.9807 - val_mae: 4.0131\n",
      "Epoch 302/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 114.8081 - mae: 7.4615 - val_loss: 33.5459 - val_mae: 4.5113\n",
      "Epoch 303/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 112.2731 - mae: 7.4407 - val_loss: 25.5547 - val_mae: 3.6863\n",
      "Epoch 304/800\n",
      "122/122 [==============================] - 0s 710us/step - loss: 107.6735 - mae: 7.2004 - val_loss: 34.2845 - val_mae: 4.4608\n",
      "Epoch 305/800\n",
      "122/122 [==============================] - 0s 785us/step - loss: 113.4598 - mae: 7.3828 - val_loss: 25.8487 - val_mae: 3.8022\n",
      "Epoch 306/800\n",
      "122/122 [==============================] - 0s 629us/step - loss: 108.8987 - mae: 7.3216 - val_loss: 24.7310 - val_mae: 3.6182\n",
      "Epoch 307/800\n",
      "122/122 [==============================] - 0s 632us/step - loss: 111.2418 - mae: 7.4141 - val_loss: 29.6631 - val_mae: 4.1591\n",
      "Epoch 308/800\n",
      "122/122 [==============================] - 0s 612us/step - loss: 107.4077 - mae: 7.2149 - val_loss: 29.0579 - val_mae: 4.1426\n",
      "Epoch 309/800\n",
      "122/122 [==============================] - 0s 706us/step - loss: 107.4833 - mae: 7.2752 - val_loss: 30.8840 - val_mae: 4.2792\n",
      "Epoch 310/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 110.1241 - mae: 7.3430 - val_loss: 27.1676 - val_mae: 3.9181\n",
      "Epoch 311/800\n",
      "122/122 [==============================] - 0s 600us/step - loss: 108.9477 - mae: 7.3373 - val_loss: 32.5732 - val_mae: 4.4427\n",
      "Epoch 312/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 113.4443 - mae: 7.4030 - val_loss: 26.0483 - val_mae: 3.8038\n",
      "Epoch 313/800\n",
      "122/122 [==============================] - 0s 632us/step - loss: 119.8170 - mae: 7.5368 - val_loss: 25.4955 - val_mae: 3.6640\n",
      "Epoch 314/800\n",
      "122/122 [==============================] - 0s 648us/step - loss: 111.9626 - mae: 7.3283 - val_loss: 27.8448 - val_mae: 3.9333\n",
      "Epoch 315/800\n",
      "122/122 [==============================] - 0s 712us/step - loss: 110.7737 - mae: 7.3841 - val_loss: 29.9100 - val_mae: 4.1497\n",
      "Epoch 316/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 115.8538 - mae: 7.4692 - val_loss: 26.4467 - val_mae: 3.7945\n",
      "Epoch 317/800\n",
      "122/122 [==============================] - 0s 612us/step - loss: 112.0546 - mae: 7.4061 - val_loss: 25.2138 - val_mae: 3.6873\n",
      "Epoch 318/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 113.3812 - mae: 7.4286 - val_loss: 26.3542 - val_mae: 3.7852\n",
      "Epoch 319/800\n",
      "122/122 [==============================] - 0s 646us/step - loss: 109.9604 - mae: 7.3915 - val_loss: 25.7947 - val_mae: 3.6759\n",
      "Epoch 320/800\n",
      "122/122 [==============================] - 0s 600us/step - loss: 112.9706 - mae: 7.4117 - val_loss: 30.3743 - val_mae: 4.1851\n",
      "Epoch 321/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 108.6851 - mae: 7.3250 - val_loss: 29.5723 - val_mae: 4.0239\n",
      "Epoch 322/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 106.7189 - mae: 7.2920 - val_loss: 36.2658 - val_mae: 4.6993\n",
      "Epoch 323/800\n",
      "122/122 [==============================] - 0s 710us/step - loss: 110.0504 - mae: 7.3207 - val_loss: 35.7946 - val_mae: 4.5884\n",
      "Epoch 324/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 111.2450 - mae: 7.4426 - val_loss: 26.7977 - val_mae: 3.7823\n",
      "Epoch 325/800\n",
      "122/122 [==============================] - 0s 578us/step - loss: 110.1012 - mae: 7.3431 - val_loss: 38.0227 - val_mae: 4.8217\n",
      "Epoch 326/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 107.7082 - mae: 7.3381 - val_loss: 29.6281 - val_mae: 4.1311\n",
      "Epoch 327/800\n",
      "122/122 [==============================] - 0s 592us/step - loss: 109.6274 - mae: 7.3489 - val_loss: 30.5733 - val_mae: 4.2231\n",
      "Epoch 328/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 106.8809 - mae: 7.3585 - val_loss: 26.5180 - val_mae: 3.7766\n",
      "Epoch 329/800\n",
      "122/122 [==============================] - 0s 682us/step - loss: 110.8102 - mae: 7.3323 - val_loss: 28.1303 - val_mae: 3.8434\n",
      "Epoch 330/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 109.1744 - mae: 7.3652 - val_loss: 30.8946 - val_mae: 4.1362\n",
      "Epoch 331/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 109.5486 - mae: 7.3176 - val_loss: 30.9137 - val_mae: 4.1790\n",
      "Epoch 332/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 105.0138 - mae: 7.2241 - val_loss: 34.2469 - val_mae: 4.4655\n",
      "Epoch 333/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 103.9848 - mae: 7.2361 - val_loss: 27.6275 - val_mae: 3.9352\n",
      "Epoch 334/800\n",
      "122/122 [==============================] - 0s 593us/step - loss: 104.9148 - mae: 7.1465 - val_loss: 27.7864 - val_mae: 3.9423\n",
      "Epoch 335/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 111.8564 - mae: 7.4296 - val_loss: 27.7541 - val_mae: 3.9408\n",
      "Epoch 336/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 109.5151 - mae: 7.2880 - val_loss: 28.0490 - val_mae: 4.0188\n",
      "Epoch 337/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 103.0469 - mae: 7.1779 - val_loss: 33.0603 - val_mae: 4.3435\n",
      "Epoch 338/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 105.5270 - mae: 7.1784 - val_loss: 61.3249 - val_mae: 6.1574\n",
      "Epoch 339/800\n",
      "122/122 [==============================] - 0s 715us/step - loss: 103.3871 - mae: 7.2045 - val_loss: 48.8470 - val_mae: 5.4865\n",
      "Epoch 340/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 103.8432 - mae: 7.1847 - val_loss: 35.2460 - val_mae: 4.5871\n",
      "Epoch 341/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 106.1116 - mae: 7.3454 - val_loss: 53.5396 - val_mae: 5.7540\n",
      "Epoch 342/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 106.5006 - mae: 7.3021 - val_loss: 28.4485 - val_mae: 3.8760\n",
      "Epoch 343/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 107.2100 - mae: 7.4174 - val_loss: 26.9026 - val_mae: 3.7728\n",
      "Epoch 344/800\n",
      "122/122 [==============================] - 0s 573us/step - loss: 102.9986 - mae: 7.2677 - val_loss: 31.3297 - val_mae: 4.1924\n",
      "Epoch 345/800\n",
      "122/122 [==============================] - 0s 668us/step - loss: 100.7679 - mae: 7.1984 - val_loss: 26.6782 - val_mae: 3.8385\n",
      "Epoch 346/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 104.1765 - mae: 7.2858 - val_loss: 27.5024 - val_mae: 3.7947\n",
      "Epoch 347/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 97.7725 - mae: 7.0306 - val_loss: 31.1524 - val_mae: 4.2113\n",
      "Epoch 348/800\n",
      "122/122 [==============================] - 0s 575us/step - loss: 96.4762 - mae: 6.9706 - val_loss: 27.3346 - val_mae: 3.7994\n",
      "Epoch 349/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 105.3702 - mae: 7.2606 - val_loss: 30.1539 - val_mae: 4.1402\n",
      "Epoch 350/800\n",
      "122/122 [==============================] - 0s 574us/step - loss: 103.8725 - mae: 7.2119 - val_loss: 35.0355 - val_mae: 4.5293\n",
      "Epoch 351/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 101.7978 - mae: 7.0748 - val_loss: 33.9649 - val_mae: 4.5154\n",
      "Epoch 352/800\n",
      "122/122 [==============================] - 0s 695us/step - loss: 94.1633 - mae: 6.9564 - val_loss: 26.2752 - val_mae: 3.7805\n",
      "Epoch 353/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 93.4681 - mae: 6.9810 - val_loss: 30.7005 - val_mae: 4.2059\n",
      "Epoch 354/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 103.0754 - mae: 7.1185 - val_loss: 33.4654 - val_mae: 4.4564\n",
      "Epoch 355/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 103.7153 - mae: 7.1525 - val_loss: 29.4282 - val_mae: 4.0917\n",
      "Epoch 356/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 108.4751 - mae: 7.2783 - val_loss: 36.8691 - val_mae: 4.7600\n",
      "Epoch 357/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 107.7948 - mae: 7.2819 - val_loss: 37.1490 - val_mae: 4.7021\n",
      "Epoch 358/800\n",
      "122/122 [==============================] - 0s 730us/step - loss: 93.4583 - mae: 6.9333 - val_loss: 27.0759 - val_mae: 3.8761\n",
      "Epoch 359/800\n",
      "122/122 [==============================] - 0s 657us/step - loss: 102.3835 - mae: 7.1207 - val_loss: 25.9317 - val_mae: 3.8002\n",
      "Epoch 360/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 92.8056 - mae: 6.9823 - val_loss: 37.7442 - val_mae: 4.6917\n",
      "Epoch 361/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 96.9862 - mae: 7.0720 - val_loss: 29.5776 - val_mae: 3.9950\n",
      "Epoch 362/800\n",
      "122/122 [==============================] - 0s 647us/step - loss: 104.8395 - mae: 7.2852 - val_loss: 28.8541 - val_mae: 4.0004\n",
      "Epoch 363/800\n",
      "122/122 [==============================] - 0s 798us/step - loss: 101.9002 - mae: 7.1996 - val_loss: 29.9976 - val_mae: 4.0711\n",
      "Epoch 364/800\n",
      "122/122 [==============================] - 0s 775us/step - loss: 92.0437 - mae: 6.8947 - val_loss: 29.7436 - val_mae: 4.0881\n",
      "Epoch 365/800\n",
      "122/122 [==============================] - 0s 712us/step - loss: 93.7439 - mae: 6.9896 - val_loss: 29.9165 - val_mae: 4.0228\n",
      "Epoch 366/800\n",
      "122/122 [==============================] - 0s 731us/step - loss: 98.2163 - mae: 7.1188 - val_loss: 32.8392 - val_mae: 4.4513\n",
      "Epoch 367/800\n",
      "122/122 [==============================] - 0s 683us/step - loss: 101.1986 - mae: 7.1022 - val_loss: 35.0340 - val_mae: 4.5737\n",
      "Epoch 368/800\n",
      "122/122 [==============================] - 0s 761us/step - loss: 94.7076 - mae: 6.9388 - val_loss: 26.9906 - val_mae: 3.8421\n",
      "Epoch 369/800\n",
      "122/122 [==============================] - 0s 805us/step - loss: 96.2791 - mae: 7.0278 - val_loss: 35.4428 - val_mae: 4.6199\n",
      "Epoch 370/800\n",
      "122/122 [==============================] - 0s 677us/step - loss: 99.1870 - mae: 7.0972 - val_loss: 26.4803 - val_mae: 3.8215\n",
      "Epoch 371/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 98.0921 - mae: 7.0413 - val_loss: 30.0783 - val_mae: 4.1633\n",
      "Epoch 372/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 97.1367 - mae: 7.0839 - val_loss: 30.3626 - val_mae: 4.0520\n",
      "Epoch 373/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 89.9297 - mae: 6.9423 - val_loss: 26.2232 - val_mae: 3.7276\n",
      "Epoch 374/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 96.0074 - mae: 6.9572 - val_loss: 28.8871 - val_mae: 3.9654\n",
      "Epoch 375/800\n",
      "122/122 [==============================] - 0s 700us/step - loss: 98.1718 - mae: 7.0939 - val_loss: 26.8302 - val_mae: 3.8009\n",
      "Epoch 376/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 97.2577 - mae: 7.0131 - val_loss: 32.7299 - val_mae: 4.3835\n",
      "Epoch 377/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 94.2511 - mae: 6.9446 - val_loss: 31.5614 - val_mae: 4.3286\n",
      "Epoch 378/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 92.8454 - mae: 6.9665 - val_loss: 28.9729 - val_mae: 4.0628\n",
      "Epoch 379/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 97.1935 - mae: 7.0315 - val_loss: 38.6781 - val_mae: 4.6963\n",
      "Epoch 380/800\n",
      "122/122 [==============================] - 0s 674us/step - loss: 89.5421 - mae: 6.8300 - val_loss: 27.3318 - val_mae: 3.8257\n",
      "Epoch 381/800\n",
      "122/122 [==============================] - 0s 679us/step - loss: 90.4911 - mae: 6.9429 - val_loss: 28.0891 - val_mae: 3.8870\n",
      "Epoch 382/800\n",
      "122/122 [==============================] - 0s 640us/step - loss: 100.3195 - mae: 7.1031 - val_loss: 39.0865 - val_mae: 4.7973\n",
      "Epoch 383/800\n",
      "122/122 [==============================] - 0s 693us/step - loss: 94.7438 - mae: 6.9583 - val_loss: 29.4537 - val_mae: 4.0279\n",
      "Epoch 384/800\n",
      "122/122 [==============================] - 0s 699us/step - loss: 99.7738 - mae: 7.0622 - val_loss: 27.7263 - val_mae: 3.8809\n",
      "Epoch 385/800\n",
      "122/122 [==============================] - 0s 688us/step - loss: 88.9156 - mae: 6.8391 - val_loss: 29.6558 - val_mae: 4.0494\n",
      "Epoch 386/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 96.5559 - mae: 7.0631 - val_loss: 31.1116 - val_mae: 4.1917\n",
      "Epoch 387/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 97.3210 - mae: 6.9933 - val_loss: 32.4115 - val_mae: 4.2420\n",
      "Epoch 388/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 92.8172 - mae: 6.9319 - val_loss: 26.8691 - val_mae: 3.8221\n",
      "Epoch 389/800\n",
      "122/122 [==============================] - 0s 629us/step - loss: 102.8086 - mae: 7.1971 - val_loss: 37.4572 - val_mae: 4.7099\n",
      "Epoch 390/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 98.1311 - mae: 7.0016 - val_loss: 35.4294 - val_mae: 4.6165\n",
      "Epoch 391/800\n",
      "122/122 [==============================] - 0s 673us/step - loss: 93.6677 - mae: 6.9978 - val_loss: 30.5140 - val_mae: 4.1217\n",
      "Epoch 392/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 95.1205 - mae: 6.9566 - val_loss: 31.2800 - val_mae: 4.2115\n",
      "Epoch 393/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 92.9910 - mae: 6.8584 - val_loss: 31.1361 - val_mae: 4.1406\n",
      "Epoch 394/800\n",
      "122/122 [==============================] - 0s 577us/step - loss: 94.9846 - mae: 7.0040 - val_loss: 40.9490 - val_mae: 5.0236\n",
      "Epoch 395/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 97.9441 - mae: 7.0944 - val_loss: 37.7754 - val_mae: 4.7749\n",
      "Epoch 396/800\n",
      "122/122 [==============================] - 0s 598us/step - loss: 92.4667 - mae: 6.9105 - val_loss: 28.0460 - val_mae: 3.8473\n",
      "Epoch 397/800\n",
      "122/122 [==============================] - 0s 598us/step - loss: 96.2104 - mae: 7.0261 - val_loss: 29.1568 - val_mae: 3.9020\n",
      "Epoch 398/800\n",
      "122/122 [==============================] - 0s 707us/step - loss: 98.2353 - mae: 7.0900 - val_loss: 32.1531 - val_mae: 4.2799\n",
      "Epoch 399/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 96.1883 - mae: 7.0054 - val_loss: 29.7858 - val_mae: 4.0293\n",
      "Epoch 400/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 93.0225 - mae: 6.9883 - val_loss: 29.4310 - val_mae: 4.0300\n",
      "Epoch 401/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 92.6217 - mae: 6.9386 - val_loss: 30.2389 - val_mae: 4.0956\n",
      "Epoch 402/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 93.5430 - mae: 6.9300 - val_loss: 28.7087 - val_mae: 3.9631\n",
      "Epoch 403/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 94.3980 - mae: 7.0214 - val_loss: 28.7201 - val_mae: 3.9201\n",
      "Epoch 404/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 100.1614 - mae: 7.1245 - val_loss: 30.0701 - val_mae: 3.9932\n",
      "Epoch 405/800\n",
      "122/122 [==============================] - 0s 653us/step - loss: 97.7254 - mae: 6.9906 - val_loss: 74.5955 - val_mae: 6.8740\n",
      "Epoch 406/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 94.5240 - mae: 7.0074 - val_loss: 29.1663 - val_mae: 3.9607\n",
      "Epoch 407/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 93.7206 - mae: 6.9759 - val_loss: 34.6304 - val_mae: 4.3886\n",
      "Epoch 408/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 92.0904 - mae: 7.0032 - val_loss: 31.4905 - val_mae: 4.1208\n",
      "Epoch 409/800\n",
      "122/122 [==============================] - 0s 676us/step - loss: 92.7380 - mae: 6.8353 - val_loss: 32.5274 - val_mae: 4.2881\n",
      "Epoch 410/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 91.1540 - mae: 6.9147 - val_loss: 40.1147 - val_mae: 4.9741\n",
      "Epoch 411/800\n",
      "122/122 [==============================] - 0s 708us/step - loss: 93.0625 - mae: 7.0359 - val_loss: 31.0158 - val_mae: 4.2534\n",
      "Epoch 412/800\n",
      "122/122 [==============================] - 0s 646us/step - loss: 100.9114 - mae: 7.1968 - val_loss: 33.0765 - val_mae: 4.3722\n",
      "Epoch 413/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 87.5046 - mae: 6.8063 - val_loss: 30.5283 - val_mae: 4.0238\n",
      "Epoch 414/800\n",
      "122/122 [==============================] - 0s 620us/step - loss: 97.7194 - mae: 7.1116 - val_loss: 30.7011 - val_mae: 4.0460\n",
      "Epoch 415/800\n",
      "122/122 [==============================] - 0s 609us/step - loss: 88.9964 - mae: 6.8267 - val_loss: 25.4307 - val_mae: 3.7289\n",
      "Epoch 416/800\n",
      "122/122 [==============================] - 0s 669us/step - loss: 87.4115 - mae: 6.8310 - val_loss: 30.9693 - val_mae: 4.2719\n",
      "Epoch 417/800\n",
      "122/122 [==============================] - 0s 704us/step - loss: 91.6643 - mae: 6.9135 - val_loss: 26.4260 - val_mae: 3.7665\n",
      "Epoch 418/800\n",
      "122/122 [==============================] - 0s 698us/step - loss: 82.0485 - mae: 6.6173 - val_loss: 37.3811 - val_mae: 4.7581\n",
      "Epoch 419/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 89.9114 - mae: 6.8616 - val_loss: 34.3133 - val_mae: 4.5070\n",
      "Epoch 420/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 95.7702 - mae: 7.0701 - val_loss: 26.9849 - val_mae: 3.8123\n",
      "Epoch 421/800\n",
      "122/122 [==============================] - 0s 679us/step - loss: 86.0200 - mae: 6.7205 - val_loss: 32.3760 - val_mae: 4.3253\n",
      "Epoch 422/800\n",
      "122/122 [==============================] - 0s 709us/step - loss: 87.2871 - mae: 6.7793 - val_loss: 39.6488 - val_mae: 4.8712\n",
      "Epoch 423/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 83.9866 - mae: 6.6826 - val_loss: 33.7374 - val_mae: 4.3968\n",
      "Epoch 424/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 90.3649 - mae: 6.9369 - val_loss: 28.7225 - val_mae: 3.9701\n",
      "Epoch 425/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 93.3971 - mae: 6.9693 - val_loss: 28.1375 - val_mae: 3.9065\n",
      "Epoch 426/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 85.4858 - mae: 6.7950 - val_loss: 31.6751 - val_mae: 4.2286\n",
      "Epoch 427/800\n",
      "122/122 [==============================] - 0s 592us/step - loss: 94.2174 - mae: 6.9805 - val_loss: 39.0132 - val_mae: 4.8697\n",
      "Epoch 428/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 81.7105 - mae: 6.5816 - val_loss: 26.7239 - val_mae: 3.7592\n",
      "Epoch 429/800\n",
      "122/122 [==============================] - 0s 645us/step - loss: 90.0897 - mae: 6.8888 - val_loss: 36.2049 - val_mae: 4.5853\n",
      "Epoch 430/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 91.2473 - mae: 6.8465 - val_loss: 30.5343 - val_mae: 4.0285\n",
      "Epoch 431/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 92.9121 - mae: 6.9093 - val_loss: 31.8044 - val_mae: 4.2722\n",
      "Epoch 432/800\n",
      "122/122 [==============================] - 0s 592us/step - loss: 90.5814 - mae: 6.9629 - val_loss: 34.5263 - val_mae: 4.3601\n",
      "Epoch 433/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 86.3399 - mae: 6.6924 - val_loss: 26.2920 - val_mae: 3.7644\n",
      "Epoch 434/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 87.2683 - mae: 6.7699 - val_loss: 37.8604 - val_mae: 4.7480\n",
      "Epoch 435/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 90.2854 - mae: 6.9022 - val_loss: 32.6437 - val_mae: 4.3902\n",
      "Epoch 436/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 85.5030 - mae: 6.7118 - val_loss: 31.3200 - val_mae: 4.1488\n",
      "Epoch 437/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 89.0261 - mae: 6.8408 - val_loss: 30.2143 - val_mae: 4.0328\n",
      "Epoch 438/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 87.2919 - mae: 6.8268 - val_loss: 25.8677 - val_mae: 3.7280\n",
      "Epoch 439/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 83.5206 - mae: 6.6410 - val_loss: 31.5310 - val_mae: 4.1868\n",
      "Epoch 440/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 88.5323 - mae: 6.7973 - val_loss: 35.8124 - val_mae: 4.5991\n",
      "Epoch 441/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 91.0675 - mae: 6.8621 - val_loss: 33.5797 - val_mae: 4.3738\n",
      "Epoch 442/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 83.5999 - mae: 6.6666 - val_loss: 33.9830 - val_mae: 4.4414\n",
      "Epoch 443/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 88.3701 - mae: 6.8710 - val_loss: 29.7268 - val_mae: 3.9893\n",
      "Epoch 444/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 85.5692 - mae: 6.7142 - val_loss: 29.0113 - val_mae: 3.9820\n",
      "Epoch 445/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 85.7048 - mae: 6.8183 - val_loss: 34.2816 - val_mae: 4.4864\n",
      "Epoch 446/800\n",
      "122/122 [==============================] - 0s 674us/step - loss: 87.7675 - mae: 6.7982 - val_loss: 32.2518 - val_mae: 4.3120\n",
      "Epoch 447/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 83.6139 - mae: 6.6581 - val_loss: 27.0989 - val_mae: 3.7941\n",
      "Epoch 448/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 86.5310 - mae: 6.7953 - val_loss: 31.2630 - val_mae: 4.1562\n",
      "Epoch 449/800\n",
      "122/122 [==============================] - 0s 564us/step - loss: 87.4261 - mae: 6.7973 - val_loss: 34.2690 - val_mae: 4.5303\n",
      "Epoch 450/800\n",
      "122/122 [==============================] - 0s 612us/step - loss: 85.9286 - mae: 6.7473 - val_loss: 25.8358 - val_mae: 3.7233\n",
      "Epoch 451/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 92.4201 - mae: 6.9629 - val_loss: 29.6683 - val_mae: 4.0479\n",
      "Epoch 452/800\n",
      "122/122 [==============================] - 0s 691us/step - loss: 83.9175 - mae: 6.7567 - val_loss: 31.1993 - val_mae: 4.1705\n",
      "Epoch 453/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 86.1735 - mae: 6.8453 - val_loss: 37.6085 - val_mae: 4.6700\n",
      "Epoch 454/800\n",
      "122/122 [==============================] - 0s 558us/step - loss: 85.8545 - mae: 6.7687 - val_loss: 27.1111 - val_mae: 3.8274\n",
      "Epoch 455/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 85.2424 - mae: 6.8061 - val_loss: 27.7628 - val_mae: 3.7932\n",
      "Epoch 456/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 88.9860 - mae: 6.8991 - val_loss: 35.4075 - val_mae: 4.6384\n",
      "Epoch 457/800\n",
      "122/122 [==============================] - 0s 609us/step - loss: 85.8469 - mae: 6.8183 - val_loss: 31.4017 - val_mae: 4.1449\n",
      "Epoch 458/800\n",
      "122/122 [==============================] - 0s 684us/step - loss: 88.6435 - mae: 6.8335 - val_loss: 39.5988 - val_mae: 4.8445\n",
      "Epoch 459/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 87.4295 - mae: 6.8725 - val_loss: 35.2684 - val_mae: 4.5984\n",
      "Epoch 460/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 85.6415 - mae: 6.7170 - val_loss: 28.2125 - val_mae: 3.9766\n",
      "Epoch 461/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 83.1993 - mae: 6.6639 - val_loss: 26.3780 - val_mae: 3.7966\n",
      "Epoch 462/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 83.1905 - mae: 6.6927 - val_loss: 28.5378 - val_mae: 3.9422\n",
      "Epoch 463/800\n",
      "122/122 [==============================] - 0s 572us/step - loss: 88.6117 - mae: 6.8315 - val_loss: 28.4551 - val_mae: 4.0475\n",
      "Epoch 464/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 80.0374 - mae: 6.6312 - val_loss: 29.4746 - val_mae: 4.1324\n",
      "Epoch 465/800\n",
      "122/122 [==============================] - 0s 655us/step - loss: 83.4129 - mae: 6.6115 - val_loss: 28.0298 - val_mae: 3.9271\n",
      "Epoch 466/800\n",
      "122/122 [==============================] - 0s 655us/step - loss: 92.2357 - mae: 6.8986 - val_loss: 29.0981 - val_mae: 3.9085\n",
      "Epoch 467/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 87.9898 - mae: 6.7916 - val_loss: 26.7232 - val_mae: 3.7699\n",
      "Epoch 468/800\n",
      "122/122 [==============================] - 0s 617us/step - loss: 79.5325 - mae: 6.6352 - val_loss: 35.4842 - val_mae: 4.6253\n",
      "Epoch 469/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 82.1121 - mae: 6.6188 - val_loss: 28.0393 - val_mae: 3.9051\n",
      "Epoch 470/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 86.5578 - mae: 6.7681 - val_loss: 28.7342 - val_mae: 4.0621\n",
      "Epoch 471/800\n",
      "122/122 [==============================] - 0s 668us/step - loss: 88.3923 - mae: 6.8194 - val_loss: 26.1426 - val_mae: 3.8499\n",
      "Epoch 472/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 85.5390 - mae: 6.7510 - val_loss: 32.9134 - val_mae: 4.4794\n",
      "Epoch 473/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 83.2589 - mae: 6.6966 - val_loss: 25.4937 - val_mae: 3.7519\n",
      "Epoch 474/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 83.8278 - mae: 6.7122 - val_loss: 29.8031 - val_mae: 4.0146\n",
      "Epoch 475/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 85.1811 - mae: 6.7704 - val_loss: 31.4417 - val_mae: 4.2893\n",
      "Epoch 476/800\n",
      "122/122 [==============================] - 0s 655us/step - loss: 87.7686 - mae: 6.7763 - val_loss: 26.0468 - val_mae: 3.7654\n",
      "Epoch 477/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 89.0104 - mae: 6.8162 - val_loss: 26.9801 - val_mae: 3.8794\n",
      "Epoch 478/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 81.7580 - mae: 6.6757 - val_loss: 30.6395 - val_mae: 4.2349\n",
      "Epoch 479/800\n",
      "122/122 [==============================] - 0s 577us/step - loss: 89.3454 - mae: 6.9051 - val_loss: 28.9030 - val_mae: 3.9965\n",
      "Epoch 480/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 84.1065 - mae: 6.7151 - val_loss: 37.6588 - val_mae: 4.7613\n",
      "Epoch 481/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 85.2612 - mae: 6.7015 - val_loss: 30.9188 - val_mae: 4.0044\n",
      "Epoch 482/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 85.6890 - mae: 6.7743 - val_loss: 51.9092 - val_mae: 5.4693\n",
      "Epoch 483/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 82.9368 - mae: 6.6574 - val_loss: 41.5924 - val_mae: 4.9019\n",
      "Epoch 484/800\n",
      "122/122 [==============================] - 0s 682us/step - loss: 85.8464 - mae: 6.7595 - val_loss: 31.7288 - val_mae: 4.0647\n",
      "Epoch 485/800\n",
      "122/122 [==============================] - 0s 629us/step - loss: 86.3287 - mae: 6.8093 - val_loss: 31.1812 - val_mae: 4.0149\n",
      "Epoch 486/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 86.0780 - mae: 6.7935 - val_loss: 31.6243 - val_mae: 4.1801\n",
      "Epoch 487/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 80.5247 - mae: 6.6203 - val_loss: 28.6652 - val_mae: 3.9045\n",
      "Epoch 488/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 86.2365 - mae: 6.7783 - val_loss: 29.7862 - val_mae: 4.1799\n",
      "Epoch 489/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 81.8194 - mae: 6.6216 - val_loss: 47.7731 - val_mae: 5.4275\n",
      "Epoch 490/800\n",
      "122/122 [==============================] - 0s 649us/step - loss: 80.7796 - mae: 6.5748 - val_loss: 27.7563 - val_mae: 3.8859\n",
      "Epoch 491/800\n",
      "122/122 [==============================] - 0s 720us/step - loss: 82.1449 - mae: 6.6644 - val_loss: 29.0537 - val_mae: 4.0082\n",
      "Epoch 492/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 84.1177 - mae: 6.7087 - val_loss: 27.9574 - val_mae: 4.0211\n",
      "Epoch 493/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 86.1067 - mae: 6.7152 - val_loss: 28.7830 - val_mae: 3.9067\n",
      "Epoch 494/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 81.5660 - mae: 6.7037 - val_loss: 42.7357 - val_mae: 5.1138\n",
      "Epoch 495/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 84.9402 - mae: 6.6722 - val_loss: 31.6500 - val_mae: 4.2680\n",
      "Epoch 496/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 81.2284 - mae: 6.6113 - val_loss: 27.8649 - val_mae: 3.9061\n",
      "Epoch 497/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 78.0193 - mae: 6.5003 - val_loss: 27.7398 - val_mae: 3.8684\n",
      "Epoch 498/800\n",
      "122/122 [==============================] - 0s 656us/step - loss: 85.0424 - mae: 6.5747 - val_loss: 27.1136 - val_mae: 3.8366\n",
      "Epoch 499/800\n",
      "122/122 [==============================] - 0s 593us/step - loss: 83.2942 - mae: 6.6767 - val_loss: 28.4328 - val_mae: 3.9750\n",
      "Epoch 500/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 75.5308 - mae: 6.4505 - val_loss: 30.9072 - val_mae: 4.2079\n",
      "Epoch 501/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 80.1199 - mae: 6.6268 - val_loss: 30.5043 - val_mae: 4.1324\n",
      "Epoch 502/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 80.8459 - mae: 6.5885 - val_loss: 32.3300 - val_mae: 4.1259\n",
      "Epoch 503/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 80.3407 - mae: 6.6062 - val_loss: 29.6100 - val_mae: 4.0622\n",
      "Epoch 504/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 80.5973 - mae: 6.5636 - val_loss: 31.9925 - val_mae: 4.2285\n",
      "Epoch 505/800\n",
      "122/122 [==============================] - 0s 655us/step - loss: 88.8745 - mae: 6.8076 - val_loss: 28.2253 - val_mae: 3.8806\n",
      "Epoch 506/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 78.9465 - mae: 6.5799 - val_loss: 26.2502 - val_mae: 3.7176\n",
      "Epoch 507/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 78.2067 - mae: 6.4589 - val_loss: 26.7342 - val_mae: 3.7748\n",
      "Epoch 508/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 74.5280 - mae: 6.4279 - val_loss: 27.3561 - val_mae: 3.9169\n",
      "Epoch 509/800\n",
      "122/122 [==============================] - 0s 641us/step - loss: 77.4124 - mae: 6.5593 - val_loss: 26.6995 - val_mae: 3.7811\n",
      "Epoch 510/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 80.8426 - mae: 6.5495 - val_loss: 31.9459 - val_mae: 4.3191\n",
      "Epoch 511/800\n",
      "122/122 [==============================] - 0s 737us/step - loss: 81.4419 - mae: 6.6233 - val_loss: 27.4095 - val_mae: 3.8556\n",
      "Epoch 512/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 78.4964 - mae: 6.5189 - val_loss: 26.7052 - val_mae: 3.7644\n",
      "Epoch 513/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 78.9059 - mae: 6.5383 - val_loss: 30.3215 - val_mae: 4.1866\n",
      "Epoch 514/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 80.9264 - mae: 6.6341 - val_loss: 26.9501 - val_mae: 3.8308\n",
      "Epoch 515/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 83.7079 - mae: 6.6884 - val_loss: 30.7189 - val_mae: 4.2242\n",
      "Epoch 516/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 77.0380 - mae: 6.4669 - val_loss: 29.0322 - val_mae: 3.8428\n",
      "Epoch 517/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 80.3433 - mae: 6.5409 - val_loss: 40.0589 - val_mae: 4.8844\n",
      "Epoch 518/800\n",
      "122/122 [==============================] - 0s 696us/step - loss: 76.5990 - mae: 6.5800 - val_loss: 30.4861 - val_mae: 3.9934\n",
      "Epoch 519/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 77.8722 - mae: 6.5297 - val_loss: 30.8075 - val_mae: 4.0153\n",
      "Epoch 520/800\n",
      "122/122 [==============================] - 0s 629us/step - loss: 82.1021 - mae: 6.6786 - val_loss: 40.1027 - val_mae: 4.8713\n",
      "Epoch 521/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 79.8413 - mae: 6.6407 - val_loss: 27.9150 - val_mae: 3.9554\n",
      "Epoch 522/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 80.2715 - mae: 6.5805 - val_loss: 27.4460 - val_mae: 3.9097\n",
      "Epoch 523/800\n",
      "122/122 [==============================] - 0s 643us/step - loss: 74.9785 - mae: 6.3957 - val_loss: 26.0650 - val_mae: 3.6871\n",
      "Epoch 524/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 81.7698 - mae: 6.5845 - val_loss: 30.5639 - val_mae: 4.0951\n",
      "Epoch 525/800\n",
      "122/122 [==============================] - 0s 619us/step - loss: 73.4072 - mae: 6.3217 - val_loss: 31.9630 - val_mae: 4.3072\n",
      "Epoch 526/800\n",
      "122/122 [==============================] - 0s 677us/step - loss: 76.6312 - mae: 6.5345 - val_loss: 36.3356 - val_mae: 4.5874\n",
      "Epoch 527/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 80.1302 - mae: 6.5917 - val_loss: 28.4349 - val_mae: 3.9118\n",
      "Epoch 528/800\n",
      "122/122 [==============================] - 0s 628us/step - loss: 75.7988 - mae: 6.4323 - val_loss: 42.3757 - val_mae: 4.9795\n",
      "Epoch 529/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 82.2314 - mae: 6.6224 - val_loss: 30.1504 - val_mae: 3.9893\n",
      "Epoch 530/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 83.8420 - mae: 6.6502 - val_loss: 32.5060 - val_mae: 4.3348\n",
      "Epoch 531/800\n",
      "122/122 [==============================] - 0s 675us/step - loss: 78.5099 - mae: 6.5119 - val_loss: 37.0650 - val_mae: 4.5702\n",
      "Epoch 532/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 79.0000 - mae: 6.4751 - val_loss: 32.1012 - val_mae: 4.1400\n",
      "Epoch 533/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 77.8298 - mae: 6.4999 - val_loss: 26.9714 - val_mae: 3.7728\n",
      "Epoch 534/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 76.3345 - mae: 6.4223 - val_loss: 28.4044 - val_mae: 3.8772\n",
      "Epoch 535/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 78.7373 - mae: 6.4617 - val_loss: 28.0529 - val_mae: 3.9323\n",
      "Epoch 536/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 76.2755 - mae: 6.4756 - val_loss: 34.2692 - val_mae: 4.4542\n",
      "Epoch 537/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 77.3452 - mae: 6.5873 - val_loss: 29.5349 - val_mae: 3.9982\n",
      "Epoch 538/800\n",
      "122/122 [==============================] - 0s 617us/step - loss: 77.3390 - mae: 6.5255 - val_loss: 27.5228 - val_mae: 3.8834\n",
      "Epoch 539/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 73.5697 - mae: 6.3735 - val_loss: 29.1599 - val_mae: 3.9544\n",
      "Epoch 540/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 77.7973 - mae: 6.4484 - val_loss: 28.4192 - val_mae: 3.9658\n",
      "Epoch 541/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 77.9224 - mae: 6.4515 - val_loss: 29.9555 - val_mae: 3.9725\n",
      "Epoch 542/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 75.5003 - mae: 6.3900 - val_loss: 35.9493 - val_mae: 4.3353\n",
      "Epoch 543/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 75.1116 - mae: 6.3954 - val_loss: 29.7249 - val_mae: 3.8150\n",
      "Epoch 544/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 74.4348 - mae: 6.3885 - val_loss: 31.5044 - val_mae: 4.1523\n",
      "Epoch 545/800\n",
      "122/122 [==============================] - 0s 661us/step - loss: 76.1023 - mae: 6.3434 - val_loss: 29.1272 - val_mae: 3.9551\n",
      "Epoch 546/800\n",
      "122/122 [==============================] - 0s 617us/step - loss: 79.7864 - mae: 6.5400 - val_loss: 27.5125 - val_mae: 3.7925\n",
      "Epoch 547/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 75.5453 - mae: 6.3924 - val_loss: 30.0012 - val_mae: 4.1646\n",
      "Epoch 548/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 80.6224 - mae: 6.6375 - val_loss: 27.5547 - val_mae: 3.8212\n",
      "Epoch 549/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 74.1512 - mae: 6.3668 - val_loss: 27.7738 - val_mae: 3.8694\n",
      "Epoch 550/800\n",
      "122/122 [==============================] - 0s 571us/step - loss: 78.4785 - mae: 6.5697 - val_loss: 28.2250 - val_mae: 3.8396\n",
      "Epoch 551/800\n",
      "122/122 [==============================] - 0s 675us/step - loss: 79.3157 - mae: 6.5172 - val_loss: 29.0745 - val_mae: 3.9515\n",
      "Epoch 552/800\n",
      "122/122 [==============================] - 0s 587us/step - loss: 81.7045 - mae: 6.5380 - val_loss: 25.8831 - val_mae: 3.7043\n",
      "Epoch 553/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 78.7204 - mae: 6.5101 - val_loss: 30.2948 - val_mae: 3.9848\n",
      "Epoch 554/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 73.9513 - mae: 6.3682 - val_loss: 31.0248 - val_mae: 4.0928\n",
      "Epoch 555/800\n",
      "122/122 [==============================] - 0s 599us/step - loss: 77.4713 - mae: 6.4376 - val_loss: 30.6277 - val_mae: 4.0204\n",
      "Epoch 556/800\n",
      "122/122 [==============================] - 0s 584us/step - loss: 77.6879 - mae: 6.5466 - val_loss: 28.1881 - val_mae: 3.9033\n",
      "Epoch 557/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 77.0463 - mae: 6.4480 - val_loss: 28.9909 - val_mae: 3.9628\n",
      "Epoch 558/800\n",
      "122/122 [==============================] - 0s 583us/step - loss: 77.8965 - mae: 6.4780 - val_loss: 31.1496 - val_mae: 4.1675\n",
      "Epoch 559/800\n",
      "122/122 [==============================] - 0s 569us/step - loss: 74.5775 - mae: 6.3299 - val_loss: 28.0687 - val_mae: 3.8308\n",
      "Epoch 560/800\n",
      "122/122 [==============================] - 0s 782us/step - loss: 79.0024 - mae: 6.4992 - val_loss: 40.5338 - val_mae: 4.8425\n",
      "Epoch 561/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 75.4391 - mae: 6.3838 - val_loss: 30.9826 - val_mae: 4.0746\n",
      "Epoch 562/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 76.4929 - mae: 6.5005 - val_loss: 30.1587 - val_mae: 3.9666\n",
      "Epoch 563/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 77.0227 - mae: 6.4100 - val_loss: 29.1521 - val_mae: 3.8984\n",
      "Epoch 564/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 76.3667 - mae: 6.4533 - val_loss: 29.3212 - val_mae: 3.9894\n",
      "Epoch 565/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 75.6468 - mae: 6.4113 - val_loss: 29.5218 - val_mae: 3.9520\n",
      "Epoch 566/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 75.1829 - mae: 6.3544 - val_loss: 28.3176 - val_mae: 3.8416\n",
      "Epoch 567/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 72.2822 - mae: 6.3314 - val_loss: 30.2060 - val_mae: 4.1390\n",
      "Epoch 568/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 77.1367 - mae: 6.4373 - val_loss: 27.6420 - val_mae: 3.8544\n",
      "Epoch 569/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 77.6398 - mae: 6.5199 - val_loss: 30.3485 - val_mae: 4.2097\n",
      "Epoch 570/800\n",
      "122/122 [==============================] - 0s 663us/step - loss: 77.3652 - mae: 6.4780 - val_loss: 29.2805 - val_mae: 4.0157\n",
      "Epoch 571/800\n",
      "122/122 [==============================] - 0s 645us/step - loss: 76.8668 - mae: 6.4850 - val_loss: 27.9192 - val_mae: 3.9600\n",
      "Epoch 572/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 71.6401 - mae: 6.2578 - val_loss: 28.6085 - val_mae: 3.9444\n",
      "Epoch 573/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 74.0149 - mae: 6.3668 - val_loss: 28.2464 - val_mae: 3.8881\n",
      "Epoch 574/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 73.5275 - mae: 6.3202 - val_loss: 28.1873 - val_mae: 3.8786\n",
      "Epoch 575/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 75.6151 - mae: 6.4108 - val_loss: 29.3944 - val_mae: 4.1106\n",
      "Epoch 576/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 80.4251 - mae: 6.4684 - val_loss: 29.2667 - val_mae: 4.0295\n",
      "Epoch 577/800\n",
      "122/122 [==============================] - 0s 687us/step - loss: 73.7569 - mae: 6.3090 - val_loss: 31.4705 - val_mae: 4.2301\n",
      "Epoch 578/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 78.0820 - mae: 6.5208 - val_loss: 26.7444 - val_mae: 3.8308\n",
      "Epoch 579/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 75.3176 - mae: 6.2851 - val_loss: 29.5698 - val_mae: 4.0069\n",
      "Epoch 580/800\n",
      "122/122 [==============================] - 0s 689us/step - loss: 80.1691 - mae: 6.5992 - val_loss: 28.1945 - val_mae: 3.8862\n",
      "Epoch 581/800\n",
      "122/122 [==============================] - 0s 660us/step - loss: 71.9447 - mae: 6.2820 - val_loss: 29.8016 - val_mae: 4.0216\n",
      "Epoch 582/800\n",
      "122/122 [==============================] - 0s 709us/step - loss: 72.0328 - mae: 6.2649 - val_loss: 30.6354 - val_mae: 4.0354\n",
      "Epoch 583/800\n",
      "122/122 [==============================] - 0s 656us/step - loss: 74.8709 - mae: 6.3584 - val_loss: 32.3317 - val_mae: 4.3537\n",
      "Epoch 584/800\n",
      "122/122 [==============================] - 0s 699us/step - loss: 72.1143 - mae: 6.3702 - val_loss: 36.5005 - val_mae: 4.6352\n",
      "Epoch 585/800\n",
      "122/122 [==============================] - 0s 658us/step - loss: 75.6523 - mae: 6.3965 - val_loss: 26.9388 - val_mae: 3.7729\n",
      "Epoch 586/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 71.4341 - mae: 6.2303 - val_loss: 26.3788 - val_mae: 3.7165\n",
      "Epoch 587/800\n",
      "122/122 [==============================] - 0s 619us/step - loss: 70.9785 - mae: 6.2522 - val_loss: 27.2124 - val_mae: 3.8750\n",
      "Epoch 588/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 73.3956 - mae: 6.2667 - val_loss: 28.5431 - val_mae: 4.0083\n",
      "Epoch 589/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 72.1370 - mae: 6.3058 - val_loss: 29.0647 - val_mae: 3.9236\n",
      "Epoch 590/800\n",
      "122/122 [==============================] - 0s 652us/step - loss: 72.1594 - mae: 6.2280 - val_loss: 29.2321 - val_mae: 4.0832\n",
      "Epoch 591/800\n",
      "122/122 [==============================] - 0s 685us/step - loss: 76.4060 - mae: 6.5243 - val_loss: 29.6923 - val_mae: 4.0175\n",
      "Epoch 592/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 74.0014 - mae: 6.3235 - val_loss: 35.2392 - val_mae: 4.5440\n",
      "Epoch 593/800\n",
      "122/122 [==============================] - 0s 651us/step - loss: 73.9821 - mae: 6.3642 - val_loss: 30.7200 - val_mae: 4.0857\n",
      "Epoch 594/800\n",
      "122/122 [==============================] - 0s 667us/step - loss: 71.7569 - mae: 6.2565 - val_loss: 28.7370 - val_mae: 3.8946\n",
      "Epoch 595/800\n",
      "122/122 [==============================] - 0s 692us/step - loss: 75.2872 - mae: 6.3821 - val_loss: 35.6617 - val_mae: 4.6840\n",
      "Epoch 596/800\n",
      "122/122 [==============================] - 0s 713us/step - loss: 72.4029 - mae: 6.2651 - val_loss: 27.9991 - val_mae: 3.8449\n",
      "Epoch 597/800\n",
      "122/122 [==============================] - 0s 672us/step - loss: 71.1941 - mae: 6.2543 - val_loss: 29.4667 - val_mae: 4.0024\n",
      "Epoch 598/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 70.1627 - mae: 6.2191 - val_loss: 32.3167 - val_mae: 4.4528\n",
      "Epoch 599/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 71.7355 - mae: 6.2460 - val_loss: 29.9638 - val_mae: 4.0832\n",
      "Epoch 600/800\n",
      "122/122 [==============================] - 0s 617us/step - loss: 70.0760 - mae: 6.2217 - val_loss: 30.6424 - val_mae: 4.1106\n",
      "Epoch 601/800\n",
      "122/122 [==============================] - 0s 705us/step - loss: 75.8621 - mae: 6.3543 - val_loss: 31.0367 - val_mae: 4.0594\n",
      "Epoch 602/800\n",
      "122/122 [==============================] - 0s 684us/step - loss: 70.9254 - mae: 6.2346 - val_loss: 28.1747 - val_mae: 3.9390\n",
      "Epoch 603/800\n",
      "122/122 [==============================] - 0s 682us/step - loss: 79.2894 - mae: 6.5569 - val_loss: 32.7025 - val_mae: 4.2861\n",
      "Epoch 604/800\n",
      "122/122 [==============================] - 0s 772us/step - loss: 71.5796 - mae: 6.1807 - val_loss: 26.8065 - val_mae: 3.7599\n",
      "Epoch 605/800\n",
      "122/122 [==============================] - 0s 759us/step - loss: 68.9065 - mae: 6.1692 - val_loss: 34.7804 - val_mae: 4.4381\n",
      "Epoch 606/800\n",
      "122/122 [==============================] - 0s 883us/step - loss: 74.7335 - mae: 6.3701 - val_loss: 32.7822 - val_mae: 4.3295\n",
      "Epoch 607/800\n",
      "122/122 [==============================] - 0s 781us/step - loss: 73.7527 - mae: 6.2863 - val_loss: 30.9621 - val_mae: 4.0514\n",
      "Epoch 608/800\n",
      "122/122 [==============================] - 0s 689us/step - loss: 71.6109 - mae: 6.2281 - val_loss: 30.1253 - val_mae: 4.0826\n",
      "Epoch 609/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 76.2344 - mae: 6.4680 - val_loss: 30.6406 - val_mae: 4.1892\n",
      "Epoch 610/800\n",
      "122/122 [==============================] - 0s 746us/step - loss: 73.6946 - mae: 6.4149 - val_loss: 33.1693 - val_mae: 4.2387\n",
      "Epoch 611/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 73.4492 - mae: 6.3233 - val_loss: 27.8029 - val_mae: 3.8792\n",
      "Epoch 612/800\n",
      "122/122 [==============================] - 0s 657us/step - loss: 75.7231 - mae: 6.3458 - val_loss: 34.0301 - val_mae: 4.4831\n",
      "Epoch 613/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 72.5250 - mae: 6.2655 - val_loss: 26.7963 - val_mae: 3.7694\n",
      "Epoch 614/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 74.7111 - mae: 6.4017 - val_loss: 30.3969 - val_mae: 4.1182\n",
      "Epoch 615/800\n",
      "122/122 [==============================] - 0s 673us/step - loss: 68.6325 - mae: 6.0674 - val_loss: 27.1867 - val_mae: 3.8213\n",
      "Epoch 616/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 75.6332 - mae: 6.3356 - val_loss: 28.7872 - val_mae: 3.9453\n",
      "Epoch 617/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 75.3570 - mae: 6.4328 - val_loss: 35.1987 - val_mae: 4.3873\n",
      "Epoch 618/800\n",
      "122/122 [==============================] - 0s 670us/step - loss: 77.6795 - mae: 6.5604 - val_loss: 29.6958 - val_mae: 4.0124\n",
      "Epoch 619/800\n",
      "122/122 [==============================] - 0s 656us/step - loss: 76.5240 - mae: 6.4733 - val_loss: 29.8177 - val_mae: 4.0278\n",
      "Epoch 620/800\n",
      "122/122 [==============================] - 0s 682us/step - loss: 73.5339 - mae: 6.2791 - val_loss: 29.4912 - val_mae: 3.9638\n",
      "Epoch 621/800\n",
      "122/122 [==============================] - 0s 657us/step - loss: 75.6272 - mae: 6.3966 - val_loss: 28.6852 - val_mae: 3.9096\n",
      "Epoch 622/800\n",
      "122/122 [==============================] - 0s 744us/step - loss: 71.4961 - mae: 6.2504 - val_loss: 29.1337 - val_mae: 3.8247\n",
      "Epoch 623/800\n",
      "122/122 [==============================] - 0s 653us/step - loss: 79.5325 - mae: 6.4558 - val_loss: 30.0777 - val_mae: 4.1191\n",
      "Epoch 624/800\n",
      "122/122 [==============================] - 0s 691us/step - loss: 71.8517 - mae: 6.3102 - val_loss: 33.1190 - val_mae: 4.2180\n",
      "Epoch 625/800\n",
      "122/122 [==============================] - 0s 690us/step - loss: 71.9180 - mae: 6.2821 - val_loss: 31.8411 - val_mae: 4.0862\n",
      "Epoch 626/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 74.0203 - mae: 6.3226 - val_loss: 29.1843 - val_mae: 3.9117\n",
      "Epoch 627/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 72.8434 - mae: 6.2817 - val_loss: 29.3108 - val_mae: 3.9589\n",
      "Epoch 628/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 71.9456 - mae: 6.2805 - val_loss: 27.5027 - val_mae: 3.9203\n",
      "Epoch 629/800\n",
      "122/122 [==============================] - 0s 633us/step - loss: 79.7088 - mae: 6.4437 - val_loss: 28.3587 - val_mae: 3.8976\n",
      "Epoch 630/800\n",
      "122/122 [==============================] - 0s 625us/step - loss: 75.3012 - mae: 6.3105 - val_loss: 26.6554 - val_mae: 3.7821\n",
      "Epoch 631/800\n",
      "122/122 [==============================] - 0s 573us/step - loss: 76.7514 - mae: 6.3670 - val_loss: 28.2375 - val_mae: 3.8614\n",
      "Epoch 632/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 74.2354 - mae: 6.2800 - val_loss: 35.7429 - val_mae: 4.5917\n",
      "Epoch 633/800\n",
      "122/122 [==============================] - 0s 715us/step - loss: 74.5140 - mae: 6.4314 - val_loss: 28.1589 - val_mae: 4.0311\n",
      "Epoch 634/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 70.8344 - mae: 6.2009 - val_loss: 29.6152 - val_mae: 4.0718\n",
      "Epoch 635/800\n",
      "122/122 [==============================] - 0s 619us/step - loss: 69.0127 - mae: 6.1685 - val_loss: 31.4451 - val_mae: 4.0996\n",
      "Epoch 636/800\n",
      "122/122 [==============================] - 0s 590us/step - loss: 73.6298 - mae: 6.2706 - val_loss: 30.1553 - val_mae: 4.0326\n",
      "Epoch 637/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 70.9587 - mae: 6.2803 - val_loss: 31.0481 - val_mae: 4.1123\n",
      "Epoch 638/800\n",
      "122/122 [==============================] - 0s 592us/step - loss: 76.1849 - mae: 6.4405 - val_loss: 33.0847 - val_mae: 4.2575\n",
      "Epoch 639/800\n",
      "122/122 [==============================] - 0s 665us/step - loss: 75.7002 - mae: 6.4468 - val_loss: 32.5099 - val_mae: 4.1255\n",
      "Epoch 640/800\n",
      "122/122 [==============================] - 0s 585us/step - loss: 72.9906 - mae: 6.2974 - val_loss: 29.9854 - val_mae: 4.0942\n",
      "Epoch 641/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 71.4121 - mae: 6.2599 - val_loss: 30.7849 - val_mae: 4.1912\n",
      "Epoch 642/800\n",
      "122/122 [==============================] - 0s 593us/step - loss: 72.9104 - mae: 6.2938 - val_loss: 27.5326 - val_mae: 3.7986\n",
      "Epoch 643/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 73.4181 - mae: 6.3399 - val_loss: 34.6360 - val_mae: 4.5304\n",
      "Epoch 644/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 74.4644 - mae: 6.3032 - val_loss: 29.2402 - val_mae: 3.8984\n",
      "Epoch 645/800\n",
      "122/122 [==============================] - 0s 572us/step - loss: 70.1297 - mae: 6.2374 - val_loss: 26.1139 - val_mae: 3.6998\n",
      "Epoch 646/800\n",
      "122/122 [==============================] - 0s 715us/step - loss: 71.6400 - mae: 6.1882 - val_loss: 26.8973 - val_mae: 3.7391\n",
      "Epoch 647/800\n",
      "122/122 [==============================] - 0s 582us/step - loss: 74.4700 - mae: 6.3790 - val_loss: 29.8172 - val_mae: 4.0188\n",
      "Epoch 648/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 79.5247 - mae: 6.4688 - val_loss: 33.7778 - val_mae: 4.5208\n",
      "Epoch 649/800\n",
      "122/122 [==============================] - 0s 640us/step - loss: 71.5121 - mae: 6.2801 - val_loss: 27.9790 - val_mae: 3.9075\n",
      "Epoch 650/800\n",
      "122/122 [==============================] - 0s 580us/step - loss: 71.9954 - mae: 6.2989 - val_loss: 30.2531 - val_mae: 4.1504\n",
      "Epoch 651/800\n",
      "122/122 [==============================] - 0s 602us/step - loss: 73.0611 - mae: 6.3081 - val_loss: 37.6631 - val_mae: 4.7499\n",
      "Epoch 652/800\n",
      "122/122 [==============================] - 0s 682us/step - loss: 79.5435 - mae: 6.5108 - val_loss: 27.1859 - val_mae: 3.8116\n",
      "Epoch 653/800\n",
      "122/122 [==============================] - 0s 647us/step - loss: 75.1802 - mae: 6.2728 - val_loss: 32.3196 - val_mae: 4.1545\n",
      "Epoch 654/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 77.4620 - mae: 6.5145 - val_loss: 27.3407 - val_mae: 3.6994\n",
      "Epoch 655/800\n",
      "122/122 [==============================] - 0s 663us/step - loss: 71.2262 - mae: 6.2423 - val_loss: 28.0487 - val_mae: 3.8580\n",
      "Epoch 656/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 71.3808 - mae: 6.2542 - val_loss: 35.7010 - val_mae: 4.4428\n",
      "Epoch 657/800\n",
      "122/122 [==============================] - 0s 597us/step - loss: 76.5230 - mae: 6.4670 - val_loss: 31.2905 - val_mae: 4.1457\n",
      "Epoch 658/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 70.3424 - mae: 6.2322 - val_loss: 27.8689 - val_mae: 3.8580\n",
      "Epoch 659/800\n",
      "122/122 [==============================] - 0s 651us/step - loss: 71.0044 - mae: 6.1861 - val_loss: 30.7166 - val_mae: 4.1369\n",
      "Epoch 660/800\n",
      "122/122 [==============================] - 0s 612us/step - loss: 71.0144 - mae: 6.2446 - val_loss: 31.5029 - val_mae: 4.2444\n",
      "Epoch 661/800\n",
      "122/122 [==============================] - 0s 618us/step - loss: 75.0787 - mae: 6.3916 - val_loss: 34.7783 - val_mae: 4.4068\n",
      "Epoch 662/800\n",
      "122/122 [==============================] - 0s 588us/step - loss: 73.6522 - mae: 6.3176 - val_loss: 31.8997 - val_mae: 4.1846\n",
      "Epoch 663/800\n",
      "122/122 [==============================] - 0s 612us/step - loss: 73.5911 - mae: 6.2580 - val_loss: 32.8821 - val_mae: 4.2864\n",
      "Epoch 664/800\n",
      "122/122 [==============================] - 0s 576us/step - loss: 73.6930 - mae: 6.3683 - val_loss: 32.7175 - val_mae: 4.2333\n",
      "Epoch 665/800\n",
      "122/122 [==============================] - 0s 607us/step - loss: 73.4972 - mae: 6.1971 - val_loss: 37.4544 - val_mae: 4.6810\n",
      "Epoch 666/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 72.6133 - mae: 6.2448 - val_loss: 31.0406 - val_mae: 4.1385\n",
      "Epoch 667/800\n",
      "122/122 [==============================] - 0s 895us/step - loss: 72.3878 - mae: 6.2779 - val_loss: 34.9556 - val_mae: 4.4415\n",
      "Epoch 668/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 73.6325 - mae: 6.2799 - val_loss: 31.7997 - val_mae: 4.0658\n",
      "Epoch 669/800\n",
      "122/122 [==============================] - 0s 629us/step - loss: 74.7402 - mae: 6.2851 - val_loss: 35.4812 - val_mae: 4.4938\n",
      "Epoch 670/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 71.5657 - mae: 6.2964 - val_loss: 30.5141 - val_mae: 4.0231\n",
      "Epoch 671/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 71.7426 - mae: 6.2666 - val_loss: 31.1939 - val_mae: 3.9871\n",
      "Epoch 672/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 69.4291 - mae: 6.1152 - val_loss: 30.2023 - val_mae: 3.9863\n",
      "Epoch 673/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 73.4892 - mae: 6.3243 - val_loss: 33.1919 - val_mae: 4.3451\n",
      "Epoch 674/800\n",
      "122/122 [==============================] - 0s 579us/step - loss: 72.0102 - mae: 6.2777 - val_loss: 29.0690 - val_mae: 3.7942\n",
      "Epoch 675/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 74.9302 - mae: 6.2613 - val_loss: 32.0624 - val_mae: 4.1832\n",
      "Epoch 676/800\n",
      "122/122 [==============================] - 0s 571us/step - loss: 71.5817 - mae: 6.1786 - val_loss: 39.8024 - val_mae: 4.9228\n",
      "Epoch 677/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 72.3453 - mae: 6.3201 - val_loss: 34.5103 - val_mae: 4.4628\n",
      "Epoch 678/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 70.4228 - mae: 6.1992 - val_loss: 32.7666 - val_mae: 4.1769\n",
      "Epoch 679/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 75.3287 - mae: 6.4809 - val_loss: 32.4450 - val_mae: 4.3188\n",
      "Epoch 680/800\n",
      "122/122 [==============================] - 0s 589us/step - loss: 74.1456 - mae: 6.3015 - val_loss: 30.6175 - val_mae: 3.9696\n",
      "Epoch 681/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 77.0526 - mae: 6.5396 - val_loss: 29.7749 - val_mae: 3.9505\n",
      "Epoch 682/800\n",
      "122/122 [==============================] - 0s 581us/step - loss: 76.1652 - mae: 6.3134 - val_loss: 29.9107 - val_mae: 3.9561\n",
      "Epoch 683/800\n",
      "122/122 [==============================] - 0s 598us/step - loss: 70.6877 - mae: 6.2446 - val_loss: 28.3267 - val_mae: 3.8308\n",
      "Epoch 684/800\n",
      "122/122 [==============================] - 0s 573us/step - loss: 71.5040 - mae: 6.2811 - val_loss: 27.6767 - val_mae: 3.9563\n",
      "Epoch 685/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 68.7206 - mae: 6.1674 - val_loss: 33.3385 - val_mae: 4.3030\n",
      "Epoch 686/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 77.2220 - mae: 6.3445 - val_loss: 37.0654 - val_mae: 4.6230\n",
      "Epoch 687/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 69.6266 - mae: 6.1550 - val_loss: 30.5984 - val_mae: 3.8282\n",
      "Epoch 688/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 77.8072 - mae: 6.4250 - val_loss: 32.1135 - val_mae: 4.0739\n",
      "Epoch 689/800\n",
      "122/122 [==============================] - 0s 564us/step - loss: 71.0102 - mae: 6.2415 - val_loss: 36.1094 - val_mae: 4.5839\n",
      "Epoch 690/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 71.6226 - mae: 6.2860 - val_loss: 29.5505 - val_mae: 3.9732\n",
      "Epoch 691/800\n",
      "122/122 [==============================] - 0s 713us/step - loss: 75.7930 - mae: 6.4306 - val_loss: 32.6278 - val_mae: 4.2515\n",
      "Epoch 692/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 70.5772 - mae: 6.2195 - val_loss: 28.9890 - val_mae: 4.0694\n",
      "Epoch 693/800\n",
      "122/122 [==============================] - 0s 684us/step - loss: 73.7583 - mae: 6.2729 - val_loss: 29.4760 - val_mae: 4.1301\n",
      "Epoch 694/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 72.1598 - mae: 6.3038 - val_loss: 33.2287 - val_mae: 4.3783\n",
      "Epoch 695/800\n",
      "122/122 [==============================] - 0s 652us/step - loss: 72.8452 - mae: 6.2093 - val_loss: 31.5113 - val_mae: 4.1873\n",
      "Epoch 696/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 76.9862 - mae: 6.4303 - val_loss: 28.2079 - val_mae: 3.8776\n",
      "Epoch 697/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 70.3690 - mae: 6.1497 - val_loss: 32.6933 - val_mae: 4.2812\n",
      "Epoch 698/800\n",
      "122/122 [==============================] - 0s 678us/step - loss: 71.8615 - mae: 6.1323 - val_loss: 29.9795 - val_mae: 3.8675\n",
      "Epoch 699/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 69.3654 - mae: 6.1874 - val_loss: 35.0199 - val_mae: 4.2894\n",
      "Epoch 700/800\n",
      "122/122 [==============================] - 0s 650us/step - loss: 70.4067 - mae: 6.1709 - val_loss: 28.1325 - val_mae: 3.9477\n",
      "Epoch 701/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 75.8332 - mae: 6.4110 - val_loss: 28.7589 - val_mae: 3.9603\n",
      "Epoch 702/800\n",
      "122/122 [==============================] - 0s 633us/step - loss: 75.1564 - mae: 6.3368 - val_loss: 29.4632 - val_mae: 4.0739\n",
      "Epoch 703/800\n",
      "122/122 [==============================] - 0s 637us/step - loss: 71.2371 - mae: 6.2333 - val_loss: 28.2670 - val_mae: 3.8508\n",
      "Epoch 704/800\n",
      "122/122 [==============================] - 0s 595us/step - loss: 74.6958 - mae: 6.3364 - val_loss: 35.2909 - val_mae: 4.5683\n",
      "Epoch 705/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 75.2448 - mae: 6.4222 - val_loss: 35.9358 - val_mae: 4.4935\n",
      "Epoch 706/800\n",
      "122/122 [==============================] - 0s 593us/step - loss: 76.4446 - mae: 6.4686 - val_loss: 32.0280 - val_mae: 4.2245\n",
      "Epoch 707/800\n",
      "122/122 [==============================] - 0s 616us/step - loss: 73.6838 - mae: 6.3243 - val_loss: 26.8323 - val_mae: 3.7945\n",
      "Epoch 708/800\n",
      "122/122 [==============================] - 0s 683us/step - loss: 75.3052 - mae: 6.3661 - val_loss: 29.2889 - val_mae: 3.9776\n",
      "Epoch 709/800\n",
      "122/122 [==============================] - 0s 656us/step - loss: 72.2272 - mae: 6.3234 - val_loss: 29.5322 - val_mae: 4.0500\n",
      "Epoch 710/800\n",
      "122/122 [==============================] - 0s 596us/step - loss: 70.8740 - mae: 6.2203 - val_loss: 29.3659 - val_mae: 3.9091\n",
      "Epoch 711/800\n",
      "122/122 [==============================] - 0s 596us/step - loss: 74.3381 - mae: 6.2448 - val_loss: 29.6257 - val_mae: 3.9632\n",
      "Epoch 712/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 70.3079 - mae: 6.2030 - val_loss: 33.5557 - val_mae: 4.2807\n",
      "Epoch 713/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 74.7585 - mae: 6.3594 - val_loss: 38.6356 - val_mae: 4.7856\n",
      "Epoch 714/800\n",
      "122/122 [==============================] - 0s 653us/step - loss: 69.4912 - mae: 6.2071 - val_loss: 29.9011 - val_mae: 4.0475\n",
      "Epoch 715/800\n",
      "122/122 [==============================] - 0s 700us/step - loss: 68.5359 - mae: 6.1856 - val_loss: 28.1618 - val_mae: 3.8062\n",
      "Epoch 716/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 70.9696 - mae: 6.2116 - val_loss: 32.0110 - val_mae: 4.3000\n",
      "Epoch 717/800\n",
      "122/122 [==============================] - 0s 641us/step - loss: 70.9575 - mae: 6.2226 - val_loss: 39.1795 - val_mae: 4.7664\n",
      "Epoch 718/800\n",
      "122/122 [==============================] - 0s 765us/step - loss: 71.1212 - mae: 6.1759 - val_loss: 32.8889 - val_mae: 4.3637\n",
      "Epoch 719/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 77.2268 - mae: 6.4070 - val_loss: 34.7929 - val_mae: 4.5306\n",
      "Epoch 720/800\n",
      "122/122 [==============================] - 0s 652us/step - loss: 73.3156 - mae: 6.2272 - val_loss: 30.1564 - val_mae: 4.1003\n",
      "Epoch 721/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 71.6024 - mae: 6.2522 - val_loss: 30.3321 - val_mae: 4.1249\n",
      "Epoch 722/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 74.0775 - mae: 6.3425 - val_loss: 30.3370 - val_mae: 3.9250\n",
      "Epoch 723/800\n",
      "122/122 [==============================] - 0s 640us/step - loss: 69.6691 - mae: 6.1670 - val_loss: 32.5866 - val_mae: 4.2245\n",
      "Epoch 724/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 73.8222 - mae: 6.2402 - val_loss: 31.5730 - val_mae: 4.0419\n",
      "Epoch 725/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 68.7617 - mae: 6.1314 - val_loss: 30.2184 - val_mae: 4.0893\n",
      "Epoch 726/800\n",
      "122/122 [==============================] - 0s 638us/step - loss: 73.0003 - mae: 6.2217 - val_loss: 29.5673 - val_mae: 3.9494\n",
      "Epoch 727/800\n",
      "122/122 [==============================] - 0s 621us/step - loss: 65.8564 - mae: 6.0363 - val_loss: 28.1067 - val_mae: 3.8596\n",
      "Epoch 728/800\n",
      "122/122 [==============================] - 0s 603us/step - loss: 73.3090 - mae: 6.3247 - val_loss: 33.2953 - val_mae: 4.1440\n",
      "Epoch 729/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 70.4361 - mae: 6.3285 - val_loss: 30.9264 - val_mae: 4.0699\n",
      "Epoch 730/800\n",
      "122/122 [==============================] - 0s 710us/step - loss: 69.6165 - mae: 6.1176 - val_loss: 29.7315 - val_mae: 3.9211\n",
      "Epoch 731/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 74.8205 - mae: 6.3276 - val_loss: 33.5757 - val_mae: 4.3566\n",
      "Epoch 732/800\n",
      "122/122 [==============================] - 0s 679us/step - loss: 71.9101 - mae: 6.1978 - val_loss: 33.8046 - val_mae: 4.4293\n",
      "Epoch 733/800\n",
      "122/122 [==============================] - 0s 605us/step - loss: 72.9253 - mae: 6.3070 - val_loss: 29.4817 - val_mae: 3.8543\n",
      "Epoch 734/800\n",
      "122/122 [==============================] - 0s 644us/step - loss: 70.7775 - mae: 6.2734 - val_loss: 28.8088 - val_mae: 3.8835\n",
      "Epoch 735/800\n",
      "122/122 [==============================] - 0s 574us/step - loss: 73.7313 - mae: 6.2393 - val_loss: 29.9386 - val_mae: 3.7987\n",
      "Epoch 736/800\n",
      "122/122 [==============================] - 0s 611us/step - loss: 71.9979 - mae: 6.2635 - val_loss: 30.2028 - val_mae: 3.9077\n",
      "Epoch 737/800\n",
      "122/122 [==============================] - 0s 681us/step - loss: 69.3995 - mae: 6.2067 - val_loss: 31.3419 - val_mae: 3.9505\n",
      "Epoch 738/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 70.1556 - mae: 6.1724 - val_loss: 31.2907 - val_mae: 4.0400\n",
      "Epoch 739/800\n",
      "122/122 [==============================] - 0s 643us/step - loss: 71.2515 - mae: 6.3430 - val_loss: 28.4480 - val_mae: 3.8039\n",
      "Epoch 740/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 67.5676 - mae: 6.1388 - val_loss: 30.2478 - val_mae: 3.9405\n",
      "Epoch 741/800\n",
      "122/122 [==============================] - 0s 626us/step - loss: 72.1151 - mae: 6.2547 - val_loss: 35.1275 - val_mae: 4.4984\n",
      "Epoch 742/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 71.9077 - mae: 6.1860 - val_loss: 31.3857 - val_mae: 4.0255\n",
      "Epoch 743/800\n",
      "122/122 [==============================] - 0s 586us/step - loss: 70.4085 - mae: 6.2998 - val_loss: 29.4091 - val_mae: 3.8923\n",
      "Epoch 744/800\n",
      "122/122 [==============================] - 0s 640us/step - loss: 69.5081 - mae: 6.1206 - val_loss: 31.0179 - val_mae: 4.1384\n",
      "Epoch 745/800\n",
      "122/122 [==============================] - 0s 727us/step - loss: 73.2649 - mae: 6.2928 - val_loss: 39.7833 - val_mae: 4.7908\n",
      "Epoch 746/800\n",
      "122/122 [==============================] - 0s 671us/step - loss: 68.5064 - mae: 6.2046 - val_loss: 30.7986 - val_mae: 4.0267\n",
      "Epoch 747/800\n",
      "122/122 [==============================] - 0s 659us/step - loss: 72.0504 - mae: 6.2824 - val_loss: 30.7572 - val_mae: 4.0516\n",
      "Epoch 748/800\n",
      "122/122 [==============================] - 0s 669us/step - loss: 68.9602 - mae: 6.1340 - val_loss: 29.9189 - val_mae: 3.9828\n",
      "Epoch 749/800\n",
      "122/122 [==============================] - 0s 646us/step - loss: 69.9928 - mae: 6.1787 - val_loss: 27.6759 - val_mae: 3.8289\n",
      "Epoch 750/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 70.3567 - mae: 6.1412 - val_loss: 28.5678 - val_mae: 3.8467\n",
      "Epoch 751/800\n",
      "122/122 [==============================] - 0s 627us/step - loss: 74.2826 - mae: 6.2951 - val_loss: 28.6053 - val_mae: 3.8449\n",
      "Epoch 752/800\n",
      "122/122 [==============================] - 0s 708us/step - loss: 71.8039 - mae: 6.2943 - val_loss: 40.7829 - val_mae: 4.8362\n",
      "Epoch 753/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 72.8394 - mae: 6.1979 - val_loss: 36.3572 - val_mae: 4.3515\n",
      "Epoch 754/800\n",
      "122/122 [==============================] - 0s 614us/step - loss: 73.4175 - mae: 6.3314 - val_loss: 27.3356 - val_mae: 3.7624\n",
      "Epoch 755/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 76.7869 - mae: 6.4620 - val_loss: 34.0828 - val_mae: 4.0214\n",
      "Epoch 756/800\n",
      "122/122 [==============================] - 0s 650us/step - loss: 66.9705 - mae: 6.1385 - val_loss: 30.7318 - val_mae: 4.1064\n",
      "Epoch 757/800\n",
      "122/122 [==============================] - 0s 698us/step - loss: 76.8139 - mae: 6.5533 - val_loss: 34.0680 - val_mae: 4.2566\n",
      "Epoch 758/800\n",
      "122/122 [==============================] - 0s 620us/step - loss: 74.2396 - mae: 6.2684 - val_loss: 32.6283 - val_mae: 4.0948\n",
      "Epoch 759/800\n",
      "122/122 [==============================] - 0s 609us/step - loss: 72.1575 - mae: 6.1314 - val_loss: 27.8765 - val_mae: 3.8103\n",
      "Epoch 760/800\n",
      "122/122 [==============================] - 0s 663us/step - loss: 66.6371 - mae: 6.0401 - val_loss: 33.5870 - val_mae: 4.1394\n",
      "Epoch 761/800\n",
      "122/122 [==============================] - 0s 598us/step - loss: 68.5564 - mae: 6.2121 - val_loss: 31.0940 - val_mae: 3.9819\n",
      "Epoch 762/800\n",
      "122/122 [==============================] - 0s 639us/step - loss: 70.5824 - mae: 6.2656 - val_loss: 35.0596 - val_mae: 4.2045\n",
      "Epoch 763/800\n",
      "122/122 [==============================] - 0s 747us/step - loss: 74.1458 - mae: 6.3061 - val_loss: 34.2275 - val_mae: 4.1480\n",
      "Epoch 764/800\n",
      "122/122 [==============================] - 0s 655us/step - loss: 75.3892 - mae: 6.3913 - val_loss: 28.4400 - val_mae: 3.8515\n",
      "Epoch 765/800\n",
      "122/122 [==============================] - 0s 666us/step - loss: 73.5174 - mae: 6.2972 - val_loss: 30.9129 - val_mae: 4.0387\n",
      "Epoch 766/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 71.3976 - mae: 6.2393 - val_loss: 29.8856 - val_mae: 4.1039\n",
      "Epoch 767/800\n",
      "122/122 [==============================] - 0s 674us/step - loss: 69.2252 - mae: 6.1930 - val_loss: 27.9345 - val_mae: 3.7510\n",
      "Epoch 768/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 72.2491 - mae: 6.2156 - val_loss: 33.1248 - val_mae: 4.0837\n",
      "Epoch 769/800\n",
      "122/122 [==============================] - 0s 599us/step - loss: 72.7354 - mae: 6.2676 - val_loss: 38.4182 - val_mae: 4.7637\n",
      "Epoch 770/800\n",
      "122/122 [==============================] - 0s 685us/step - loss: 71.6642 - mae: 6.2343 - val_loss: 29.6565 - val_mae: 4.0011\n",
      "Epoch 771/800\n",
      "122/122 [==============================] - 0s 663us/step - loss: 69.5239 - mae: 6.1762 - val_loss: 31.7578 - val_mae: 4.0655\n",
      "Epoch 772/800\n",
      "122/122 [==============================] - 0s 649us/step - loss: 70.9172 - mae: 6.2434 - val_loss: 30.2159 - val_mae: 3.9653\n",
      "Epoch 773/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 74.4968 - mae: 6.2489 - val_loss: 38.5008 - val_mae: 4.7424\n",
      "Epoch 774/800\n",
      "122/122 [==============================] - 0s 654us/step - loss: 68.2249 - mae: 6.1990 - val_loss: 30.6692 - val_mae: 4.0701\n",
      "Epoch 775/800\n",
      "122/122 [==============================] - 0s 641us/step - loss: 71.6063 - mae: 6.2619 - val_loss: 30.4448 - val_mae: 3.9520\n",
      "Epoch 776/800\n",
      "122/122 [==============================] - 0s 600us/step - loss: 68.9421 - mae: 6.2005 - val_loss: 38.3839 - val_mae: 4.7529\n",
      "Epoch 777/800\n",
      "122/122 [==============================] - 0s 608us/step - loss: 70.0090 - mae: 6.1690 - val_loss: 30.3209 - val_mae: 3.9452\n",
      "Epoch 778/800\n",
      "122/122 [==============================] - 0s 691us/step - loss: 75.1943 - mae: 6.3753 - val_loss: 29.3112 - val_mae: 3.9257\n",
      "Epoch 779/800\n",
      "122/122 [==============================] - 0s 624us/step - loss: 76.2225 - mae: 6.4633 - val_loss: 34.5441 - val_mae: 4.3930\n",
      "Epoch 780/800\n",
      "122/122 [==============================] - 0s 631us/step - loss: 76.2500 - mae: 6.2604 - val_loss: 30.2627 - val_mae: 4.0054\n",
      "Epoch 781/800\n",
      "122/122 [==============================] - 0s 680us/step - loss: 72.1626 - mae: 6.2626 - val_loss: 35.5067 - val_mae: 4.1673\n",
      "Epoch 782/800\n",
      "122/122 [==============================] - 0s 632us/step - loss: 72.9381 - mae: 6.2503 - val_loss: 33.6069 - val_mae: 4.2287\n",
      "Epoch 783/800\n",
      "122/122 [==============================] - 0s 630us/step - loss: 68.7151 - mae: 6.1616 - val_loss: 29.2423 - val_mae: 3.8557\n",
      "Epoch 784/800\n",
      "122/122 [==============================] - 0s 647us/step - loss: 71.9946 - mae: 6.2425 - val_loss: 31.4985 - val_mae: 4.1309\n",
      "Epoch 785/800\n",
      "122/122 [==============================] - 0s 722us/step - loss: 70.8778 - mae: 6.2158 - val_loss: 36.3454 - val_mae: 4.4694\n",
      "Epoch 786/800\n",
      "122/122 [==============================] - 0s 642us/step - loss: 72.1470 - mae: 6.2020 - val_loss: 29.1221 - val_mae: 3.9227\n",
      "Epoch 787/800\n",
      "122/122 [==============================] - 0s 623us/step - loss: 77.2858 - mae: 6.3427 - val_loss: 35.6822 - val_mae: 4.5257\n",
      "Epoch 788/800\n",
      "122/122 [==============================] - 0s 606us/step - loss: 71.2075 - mae: 6.1910 - val_loss: 34.5792 - val_mae: 4.4172\n",
      "Epoch 789/800\n",
      "122/122 [==============================] - 0s 636us/step - loss: 72.3300 - mae: 6.2683 - val_loss: 33.9564 - val_mae: 4.2342\n",
      "Epoch 790/800\n",
      "122/122 [==============================] - 0s 610us/step - loss: 72.9472 - mae: 6.2689 - val_loss: 29.4773 - val_mae: 3.8602\n",
      "Epoch 791/800\n",
      "122/122 [==============================] - 0s 729us/step - loss: 69.6443 - mae: 6.1698 - val_loss: 35.5421 - val_mae: 4.3326\n",
      "Epoch 792/800\n",
      "122/122 [==============================] - 0s 622us/step - loss: 72.3912 - mae: 6.3013 - val_loss: 32.1177 - val_mae: 4.1087\n",
      "Epoch 793/800\n",
      "122/122 [==============================] - 0s 634us/step - loss: 69.0303 - mae: 6.1417 - val_loss: 32.5692 - val_mae: 3.9564\n",
      "Epoch 794/800\n",
      "122/122 [==============================] - 0s 604us/step - loss: 71.8207 - mae: 6.2331 - val_loss: 32.7894 - val_mae: 3.9298\n",
      "Epoch 795/800\n",
      "122/122 [==============================] - 0s 591us/step - loss: 70.9518 - mae: 6.2195 - val_loss: 30.1988 - val_mae: 3.8551\n",
      "Epoch 796/800\n",
      "122/122 [==============================] - 0s 615us/step - loss: 70.6828 - mae: 6.2086 - val_loss: 31.1534 - val_mae: 3.9862\n",
      "Epoch 797/800\n",
      "122/122 [==============================] - 0s 683us/step - loss: 67.3566 - mae: 6.1667 - val_loss: 32.8974 - val_mae: 4.1201\n",
      "Epoch 798/800\n",
      "122/122 [==============================] - 0s 635us/step - loss: 72.9839 - mae: 6.2807 - val_loss: 30.0247 - val_mae: 3.9544\n",
      "Epoch 799/800\n",
      "122/122 [==============================] - 0s 613us/step - loss: 68.5459 - mae: 6.1302 - val_loss: 31.2359 - val_mae: 3.9771\n",
      "Epoch 800/800\n",
      "122/122 [==============================] - 0s 594us/step - loss: 77.2955 - mae: 6.4027 - val_loss: 37.0017 - val_mae: 4.4858\n",
      "31/31 [==============================] - 0s 311us/step\n",
      "Epochs: 800 | MAE: 4.485844051889273\n",
      "Training model with 850 epochs\n",
      "Epoch 1/850\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68011.7266 - mae: 225.7964 - val_loss: 59246.4141 - val_mae: 210.1987\n",
      "Epoch 2/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 39133.8672 - mae: 166.4258 - val_loss: 15541.2500 - val_mae: 106.2694\n",
      "Epoch 3/850\n",
      "122/122 [==============================] - 0s 666us/step - loss: 7324.5229 - mae: 67.5348 - val_loss: 3712.8389 - val_mae: 46.0445\n",
      "Epoch 4/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 3531.3560 - mae: 45.6058 - val_loss: 2347.8325 - val_mae: 36.6682\n",
      "Epoch 5/850\n",
      "122/122 [==============================] - 0s 575us/step - loss: 2615.0920 - mae: 38.9769 - val_loss: 1734.7058 - val_mae: 31.4972\n",
      "Epoch 6/850\n",
      "122/122 [==============================] - 0s 661us/step - loss: 2248.1746 - mae: 36.3356 - val_loss: 1413.1769 - val_mae: 28.1676\n",
      "Epoch 7/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 1938.4336 - mae: 33.5382 - val_loss: 1197.5734 - val_mae: 25.8796\n",
      "Epoch 8/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 1842.5001 - mae: 32.4443 - val_loss: 1053.3331 - val_mae: 24.3096\n",
      "Epoch 9/850\n",
      "122/122 [==============================] - 0s 648us/step - loss: 1653.7571 - mae: 31.3650 - val_loss: 955.4039 - val_mae: 22.9897\n",
      "Epoch 10/850\n",
      "122/122 [==============================] - 0s 594us/step - loss: 1545.9227 - mae: 30.1345 - val_loss: 852.2769 - val_mae: 21.7308\n",
      "Epoch 11/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 1360.3146 - mae: 29.0159 - val_loss: 752.6031 - val_mae: 20.3934\n",
      "Epoch 12/850\n",
      "122/122 [==============================] - 0s 589us/step - loss: 1341.5450 - mae: 28.5098 - val_loss: 662.5269 - val_mae: 19.2949\n",
      "Epoch 13/850\n",
      "122/122 [==============================] - 0s 651us/step - loss: 1247.8733 - mae: 27.3198 - val_loss: 606.4758 - val_mae: 18.3697\n",
      "Epoch 14/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 1171.2223 - mae: 26.5115 - val_loss: 557.3584 - val_mae: 17.6571\n",
      "Epoch 15/850\n",
      "122/122 [==============================] - 0s 646us/step - loss: 1100.6873 - mae: 25.9727 - val_loss: 481.6012 - val_mae: 16.3776\n",
      "Epoch 16/850\n",
      "122/122 [==============================] - 0s 705us/step - loss: 1029.6078 - mae: 25.0509 - val_loss: 443.4626 - val_mae: 15.7269\n",
      "Epoch 17/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 1009.0812 - mae: 24.7996 - val_loss: 392.3553 - val_mae: 14.9249\n",
      "Epoch 18/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 982.6749 - mae: 24.3611 - val_loss: 364.5020 - val_mae: 14.2395\n",
      "Epoch 19/850\n",
      "122/122 [==============================] - 0s 649us/step - loss: 969.0809 - mae: 24.0374 - val_loss: 311.6536 - val_mae: 13.2670\n",
      "Epoch 20/850\n",
      "122/122 [==============================] - 0s 686us/step - loss: 928.8756 - mae: 23.4429 - val_loss: 287.1407 - val_mae: 12.8722\n",
      "Epoch 21/850\n",
      "122/122 [==============================] - 0s 640us/step - loss: 849.2314 - mae: 22.5228 - val_loss: 272.0995 - val_mae: 12.7184\n",
      "Epoch 22/850\n",
      "122/122 [==============================] - 0s 590us/step - loss: 862.5324 - mae: 22.4783 - val_loss: 243.7854 - val_mae: 12.1398\n",
      "Epoch 23/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 824.5992 - mae: 21.8935 - val_loss: 203.9847 - val_mae: 10.6127\n",
      "Epoch 24/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 789.0447 - mae: 21.4937 - val_loss: 202.0318 - val_mae: 11.2117\n",
      "Epoch 25/850\n",
      "122/122 [==============================] - 0s 665us/step - loss: 735.7280 - mae: 20.8723 - val_loss: 166.5463 - val_mae: 9.8759\n",
      "Epoch 26/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 736.2639 - mae: 20.4591 - val_loss: 150.1525 - val_mae: 9.1435\n",
      "Epoch 27/850\n",
      "122/122 [==============================] - 0s 729us/step - loss: 717.1790 - mae: 20.3072 - val_loss: 135.3418 - val_mae: 9.2263\n",
      "Epoch 28/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 695.8934 - mae: 19.9838 - val_loss: 114.7895 - val_mae: 7.9110\n",
      "Epoch 29/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 658.2562 - mae: 19.5287 - val_loss: 99.9022 - val_mae: 7.4213\n",
      "Epoch 30/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 662.8557 - mae: 19.3617 - val_loss: 98.5605 - val_mae: 7.3039\n",
      "Epoch 31/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 681.8015 - mae: 19.3994 - val_loss: 93.8559 - val_mae: 7.1576\n",
      "Epoch 32/850\n",
      "122/122 [==============================] - 0s 688us/step - loss: 616.8608 - mae: 18.5114 - val_loss: 82.0709 - val_mae: 7.1401\n",
      "Epoch 33/850\n",
      "122/122 [==============================] - 0s 629us/step - loss: 618.0724 - mae: 18.5296 - val_loss: 72.0527 - val_mae: 6.6355\n",
      "Epoch 34/850\n",
      "122/122 [==============================] - 0s 691us/step - loss: 625.0518 - mae: 18.4978 - val_loss: 62.5666 - val_mae: 5.7566\n",
      "Epoch 35/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 608.3465 - mae: 18.2598 - val_loss: 57.4604 - val_mae: 5.6217\n",
      "Epoch 36/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 593.1229 - mae: 17.8583 - val_loss: 47.8680 - val_mae: 5.0835\n",
      "Epoch 37/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 574.4374 - mae: 17.5677 - val_loss: 50.7613 - val_mae: 5.2141\n",
      "Epoch 38/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 582.3357 - mae: 17.5212 - val_loss: 46.8019 - val_mae: 5.0480\n",
      "Epoch 39/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 544.1545 - mae: 16.9841 - val_loss: 36.5995 - val_mae: 4.4405\n",
      "Epoch 40/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 588.6357 - mae: 17.3746 - val_loss: 48.6163 - val_mae: 5.3335\n",
      "Epoch 41/850\n",
      "122/122 [==============================] - 0s 684us/step - loss: 559.2410 - mae: 17.1962 - val_loss: 38.6974 - val_mae: 4.7818\n",
      "Epoch 42/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 507.8773 - mae: 16.3892 - val_loss: 41.6521 - val_mae: 5.0680\n",
      "Epoch 43/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 541.6265 - mae: 16.8954 - val_loss: 36.9477 - val_mae: 4.5567\n",
      "Epoch 44/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 510.6973 - mae: 16.3660 - val_loss: 33.0436 - val_mae: 4.2014\n",
      "Epoch 45/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 528.8338 - mae: 16.5259 - val_loss: 33.6828 - val_mae: 4.1656\n",
      "Epoch 46/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 510.4669 - mae: 16.3999 - val_loss: 27.2259 - val_mae: 3.7502\n",
      "Epoch 47/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 524.2660 - mae: 16.4391 - val_loss: 35.0219 - val_mae: 4.4554\n",
      "Epoch 48/850\n",
      "122/122 [==============================] - 0s 629us/step - loss: 517.6331 - mae: 16.4576 - val_loss: 32.5219 - val_mae: 4.3471\n",
      "Epoch 49/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 482.4486 - mae: 15.7180 - val_loss: 27.4125 - val_mae: 3.7714\n",
      "Epoch 50/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 517.7781 - mae: 16.2526 - val_loss: 47.9149 - val_mae: 5.3983\n",
      "Epoch 51/850\n",
      "122/122 [==============================] - 0s 592us/step - loss: 509.6205 - mae: 15.9927 - val_loss: 41.9679 - val_mae: 4.9697\n",
      "Epoch 52/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 509.3023 - mae: 15.9918 - val_loss: 30.7465 - val_mae: 4.2523\n",
      "Epoch 53/850\n",
      "122/122 [==============================] - 0s 621us/step - loss: 514.7354 - mae: 16.0115 - val_loss: 30.0565 - val_mae: 3.9847\n",
      "Epoch 54/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 491.8640 - mae: 15.8688 - val_loss: 30.9275 - val_mae: 4.1210\n",
      "Epoch 55/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 484.1912 - mae: 15.5661 - val_loss: 23.3836 - val_mae: 3.4091\n",
      "Epoch 56/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 491.0066 - mae: 15.7183 - val_loss: 33.2882 - val_mae: 4.5942\n",
      "Epoch 57/850\n",
      "122/122 [==============================] - 0s 586us/step - loss: 514.4954 - mae: 15.9713 - val_loss: 29.6626 - val_mae: 3.9472\n",
      "Epoch 58/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 496.0417 - mae: 15.6547 - val_loss: 38.6423 - val_mae: 4.8296\n",
      "Epoch 59/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 504.3762 - mae: 15.6852 - val_loss: 25.5463 - val_mae: 3.5492\n",
      "Epoch 60/850\n",
      "122/122 [==============================] - 0s 670us/step - loss: 473.0929 - mae: 15.3641 - val_loss: 26.7694 - val_mae: 3.6839\n",
      "Epoch 61/850\n",
      "122/122 [==============================] - 0s 593us/step - loss: 503.3155 - mae: 15.7357 - val_loss: 42.3256 - val_mae: 4.9688\n",
      "Epoch 62/850\n",
      "122/122 [==============================] - 0s 567us/step - loss: 482.7222 - mae: 15.3226 - val_loss: 29.6978 - val_mae: 3.9325\n",
      "Epoch 63/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 464.9839 - mae: 15.1976 - val_loss: 25.2423 - val_mae: 3.6742\n",
      "Epoch 64/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 451.7997 - mae: 14.9844 - val_loss: 25.4644 - val_mae: 3.6212\n",
      "Epoch 65/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 482.2867 - mae: 15.1637 - val_loss: 22.7574 - val_mae: 3.4227\n",
      "Epoch 66/850\n",
      "122/122 [==============================] - 0s 671us/step - loss: 482.8519 - mae: 15.3978 - val_loss: 30.5046 - val_mae: 4.0665\n",
      "Epoch 67/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 476.7881 - mae: 15.2371 - val_loss: 28.6545 - val_mae: 3.8090\n",
      "Epoch 68/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 496.3254 - mae: 15.3899 - val_loss: 26.1358 - val_mae: 3.7832\n",
      "Epoch 69/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 495.1006 - mae: 15.2082 - val_loss: 34.0402 - val_mae: 4.3916\n",
      "Epoch 70/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 473.5052 - mae: 14.9922 - val_loss: 50.6251 - val_mae: 5.6288\n",
      "Epoch 71/850\n",
      "122/122 [==============================] - 0s 595us/step - loss: 480.1515 - mae: 15.3275 - val_loss: 26.5842 - val_mae: 3.8620\n",
      "Epoch 72/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 460.9816 - mae: 14.7468 - val_loss: 22.7445 - val_mae: 3.5269\n",
      "Epoch 73/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 462.6695 - mae: 15.0266 - val_loss: 34.9302 - val_mae: 4.5604\n",
      "Epoch 74/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 451.5415 - mae: 14.6873 - val_loss: 29.1367 - val_mae: 4.0593\n",
      "Epoch 75/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 474.5202 - mae: 14.9532 - val_loss: 25.2006 - val_mae: 3.5966\n",
      "Epoch 76/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 462.9007 - mae: 14.8518 - val_loss: 46.2584 - val_mae: 5.1975\n",
      "Epoch 77/850\n",
      "122/122 [==============================] - 0s 570us/step - loss: 445.6629 - mae: 14.4963 - val_loss: 29.2885 - val_mae: 3.7894\n",
      "Epoch 78/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 442.5536 - mae: 14.6563 - val_loss: 46.0432 - val_mae: 5.2353\n",
      "Epoch 79/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 421.5299 - mae: 14.3387 - val_loss: 24.4711 - val_mae: 3.5329\n",
      "Epoch 80/850\n",
      "122/122 [==============================] - 0s 589us/step - loss: 440.7086 - mae: 14.4875 - val_loss: 34.8801 - val_mae: 4.6133\n",
      "Epoch 81/850\n",
      "122/122 [==============================] - 0s 572us/step - loss: 442.5829 - mae: 14.2860 - val_loss: 31.9012 - val_mae: 4.3505\n",
      "Epoch 82/850\n",
      "122/122 [==============================] - 0s 601us/step - loss: 430.7016 - mae: 14.4306 - val_loss: 22.5741 - val_mae: 3.4486\n",
      "Epoch 83/850\n",
      "122/122 [==============================] - 0s 575us/step - loss: 446.9872 - mae: 14.6050 - val_loss: 44.6160 - val_mae: 5.1135\n",
      "Epoch 84/850\n",
      "122/122 [==============================] - 0s 680us/step - loss: 464.9302 - mae: 14.6471 - val_loss: 45.5266 - val_mae: 5.3582\n",
      "Epoch 85/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 444.8104 - mae: 14.5272 - val_loss: 25.1747 - val_mae: 3.5871\n",
      "Epoch 86/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 463.5918 - mae: 14.7620 - val_loss: 22.7465 - val_mae: 3.3652\n",
      "Epoch 87/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 433.3290 - mae: 14.2772 - val_loss: 30.2403 - val_mae: 4.2020\n",
      "Epoch 88/850\n",
      "122/122 [==============================] - 0s 594us/step - loss: 433.8936 - mae: 14.3260 - val_loss: 25.8576 - val_mae: 3.6784\n",
      "Epoch 89/850\n",
      "122/122 [==============================] - 0s 570us/step - loss: 438.4301 - mae: 14.2849 - val_loss: 29.4097 - val_mae: 4.0974\n",
      "Epoch 90/850\n",
      "122/122 [==============================] - 0s 616us/step - loss: 414.6656 - mae: 13.8735 - val_loss: 27.0759 - val_mae: 3.8062\n",
      "Epoch 91/850\n",
      "122/122 [==============================] - 0s 579us/step - loss: 423.9687 - mae: 14.1412 - val_loss: 26.5527 - val_mae: 3.5776\n",
      "Epoch 92/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 412.1022 - mae: 13.9399 - val_loss: 24.1081 - val_mae: 3.5486\n",
      "Epoch 93/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 405.7587 - mae: 13.7448 - val_loss: 21.9403 - val_mae: 3.3212\n",
      "Epoch 94/850\n",
      "122/122 [==============================] - 0s 570us/step - loss: 420.3445 - mae: 14.1090 - val_loss: 27.5779 - val_mae: 3.6986\n",
      "Epoch 95/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 411.1634 - mae: 13.7948 - val_loss: 38.0051 - val_mae: 4.6356\n",
      "Epoch 96/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 422.5557 - mae: 13.9079 - val_loss: 29.4045 - val_mae: 3.7847\n",
      "Epoch 97/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 406.5149 - mae: 13.7240 - val_loss: 33.1013 - val_mae: 4.4083\n",
      "Epoch 98/850\n",
      "122/122 [==============================] - 0s 564us/step - loss: 412.6458 - mae: 13.8747 - val_loss: 25.8238 - val_mae: 3.7238\n",
      "Epoch 99/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 410.7995 - mae: 13.7334 - val_loss: 25.9872 - val_mae: 3.6469\n",
      "Epoch 100/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 382.3708 - mae: 13.3793 - val_loss: 27.7594 - val_mae: 3.7733\n",
      "Epoch 101/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 387.0090 - mae: 13.3744 - val_loss: 24.5137 - val_mae: 3.5837\n",
      "Epoch 102/850\n",
      "122/122 [==============================] - 0s 599us/step - loss: 414.5351 - mae: 13.7802 - val_loss: 34.7535 - val_mae: 4.5921\n",
      "Epoch 103/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 410.1445 - mae: 13.7439 - val_loss: 22.6100 - val_mae: 3.2650\n",
      "Epoch 104/850\n",
      "122/122 [==============================] - 0s 667us/step - loss: 379.8802 - mae: 13.1829 - val_loss: 29.9320 - val_mae: 4.1179\n",
      "Epoch 105/850\n",
      "122/122 [==============================] - 0s 570us/step - loss: 396.5938 - mae: 13.5141 - val_loss: 28.1021 - val_mae: 3.9273\n",
      "Epoch 106/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 401.3936 - mae: 13.5488 - val_loss: 24.2706 - val_mae: 3.6140\n",
      "Epoch 107/850\n",
      "122/122 [==============================] - 0s 574us/step - loss: 412.4237 - mae: 13.5883 - val_loss: 26.1610 - val_mae: 3.6104\n",
      "Epoch 108/850\n",
      "122/122 [==============================] - 0s 701us/step - loss: 387.8454 - mae: 13.3721 - val_loss: 24.7588 - val_mae: 3.5199\n",
      "Epoch 109/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 388.8458 - mae: 13.4296 - val_loss: 29.8456 - val_mae: 4.0456\n",
      "Epoch 110/850\n",
      "122/122 [==============================] - 0s 600us/step - loss: 380.2831 - mae: 13.2641 - val_loss: 25.6770 - val_mae: 3.6774\n",
      "Epoch 111/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 371.5294 - mae: 13.0521 - val_loss: 31.6654 - val_mae: 4.0824\n",
      "Epoch 112/850\n",
      "122/122 [==============================] - 0s 567us/step - loss: 385.2267 - mae: 13.3693 - val_loss: 30.0272 - val_mae: 3.9660\n",
      "Epoch 113/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 388.0229 - mae: 13.2516 - val_loss: 23.1287 - val_mae: 3.3981\n",
      "Epoch 114/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 381.2320 - mae: 13.4296 - val_loss: 57.0201 - val_mae: 5.6372\n",
      "Epoch 115/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 409.5391 - mae: 13.5150 - val_loss: 26.5750 - val_mae: 3.6853\n",
      "Epoch 116/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 366.4109 - mae: 12.8956 - val_loss: 22.8046 - val_mae: 3.4351\n",
      "Epoch 117/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 382.2898 - mae: 12.9203 - val_loss: 29.0880 - val_mae: 3.9234\n",
      "Epoch 118/850\n",
      "122/122 [==============================] - 0s 572us/step - loss: 385.4937 - mae: 13.0576 - val_loss: 27.8323 - val_mae: 3.7129\n",
      "Epoch 119/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 375.3717 - mae: 12.9996 - val_loss: 22.8758 - val_mae: 3.3861\n",
      "Epoch 120/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 354.6682 - mae: 12.5019 - val_loss: 23.4081 - val_mae: 3.4763\n",
      "Epoch 121/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 382.2490 - mae: 12.9074 - val_loss: 26.7706 - val_mae: 3.7306\n",
      "Epoch 122/850\n",
      "122/122 [==============================] - 0s 729us/step - loss: 373.0966 - mae: 13.1040 - val_loss: 44.2790 - val_mae: 4.9698\n",
      "Epoch 123/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 365.9074 - mae: 12.7684 - val_loss: 27.2542 - val_mae: 3.6990\n",
      "Epoch 124/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 350.8170 - mae: 12.5978 - val_loss: 25.5300 - val_mae: 3.5689\n",
      "Epoch 125/850\n",
      "122/122 [==============================] - 0s 606us/step - loss: 370.9829 - mae: 12.7852 - val_loss: 21.5366 - val_mae: 3.2916\n",
      "Epoch 126/850\n",
      "122/122 [==============================] - 0s 574us/step - loss: 377.1356 - mae: 12.5710 - val_loss: 27.4305 - val_mae: 3.7299\n",
      "Epoch 127/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 381.1648 - mae: 12.8919 - val_loss: 27.7411 - val_mae: 3.9027\n",
      "Epoch 128/850\n",
      "122/122 [==============================] - 0s 573us/step - loss: 364.4775 - mae: 12.8647 - val_loss: 28.2607 - val_mae: 4.0314\n",
      "Epoch 129/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 367.7966 - mae: 12.6956 - val_loss: 26.0492 - val_mae: 3.7673\n",
      "Epoch 130/850\n",
      "122/122 [==============================] - 0s 574us/step - loss: 363.0423 - mae: 12.5583 - val_loss: 23.4130 - val_mae: 3.4850\n",
      "Epoch 131/850\n",
      "122/122 [==============================] - 0s 686us/step - loss: 346.0815 - mae: 12.4484 - val_loss: 32.7257 - val_mae: 4.3574\n",
      "Epoch 132/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 346.8516 - mae: 12.4652 - val_loss: 35.1367 - val_mae: 4.5307\n",
      "Epoch 133/850\n",
      "122/122 [==============================] - 0s 583us/step - loss: 388.4179 - mae: 12.9111 - val_loss: 24.2405 - val_mae: 3.5127\n",
      "Epoch 134/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 344.0495 - mae: 12.3067 - val_loss: 24.3601 - val_mae: 3.6360\n",
      "Epoch 135/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 334.8607 - mae: 12.3690 - val_loss: 22.2276 - val_mae: 3.3305\n",
      "Epoch 136/850\n",
      "122/122 [==============================] - 0s 589us/step - loss: 336.0135 - mae: 12.3790 - val_loss: 23.5065 - val_mae: 3.3500\n",
      "Epoch 137/850\n",
      "122/122 [==============================] - 0s 591us/step - loss: 332.8985 - mae: 12.1671 - val_loss: 25.3411 - val_mae: 3.6607\n",
      "Epoch 138/850\n",
      "122/122 [==============================] - 0s 572us/step - loss: 326.3324 - mae: 12.0967 - val_loss: 25.4671 - val_mae: 3.6651\n",
      "Epoch 139/850\n",
      "122/122 [==============================] - 0s 630us/step - loss: 348.5840 - mae: 12.1066 - val_loss: 30.5739 - val_mae: 4.1656\n",
      "Epoch 140/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 343.5545 - mae: 12.2575 - val_loss: 23.7678 - val_mae: 3.4103\n",
      "Epoch 141/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 341.5164 - mae: 12.2997 - val_loss: 26.3373 - val_mae: 3.7637\n",
      "Epoch 142/850\n",
      "122/122 [==============================] - 0s 600us/step - loss: 349.5151 - mae: 12.4488 - val_loss: 22.5854 - val_mae: 3.4116\n",
      "Epoch 143/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 347.6120 - mae: 12.1132 - val_loss: 29.3875 - val_mae: 3.7546\n",
      "Epoch 144/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 352.3167 - mae: 12.2797 - val_loss: 24.6834 - val_mae: 3.5882\n",
      "Epoch 145/850\n",
      "122/122 [==============================] - 0s 583us/step - loss: 330.7189 - mae: 12.0044 - val_loss: 30.7936 - val_mae: 4.1055\n",
      "Epoch 146/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 314.4861 - mae: 11.7180 - val_loss: 29.8048 - val_mae: 3.9583\n",
      "Epoch 147/850\n",
      "122/122 [==============================] - 0s 688us/step - loss: 317.0820 - mae: 11.7577 - val_loss: 29.4536 - val_mae: 3.9615\n",
      "Epoch 148/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 343.6948 - mae: 12.1442 - val_loss: 23.0317 - val_mae: 3.4271\n",
      "Epoch 149/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 330.5934 - mae: 11.9781 - val_loss: 24.9363 - val_mae: 3.5982\n",
      "Epoch 150/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 325.1485 - mae: 12.0375 - val_loss: 27.1399 - val_mae: 3.6350\n",
      "Epoch 151/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 332.0110 - mae: 12.0155 - val_loss: 28.1767 - val_mae: 3.8257\n",
      "Epoch 152/850\n",
      "122/122 [==============================] - 0s 567us/step - loss: 341.1541 - mae: 12.0278 - val_loss: 22.4328 - val_mae: 3.3579\n",
      "Epoch 153/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 293.0309 - mae: 11.4069 - val_loss: 24.4730 - val_mae: 3.5403\n",
      "Epoch 154/850\n",
      "122/122 [==============================] - 0s 594us/step - loss: 328.7072 - mae: 11.8127 - val_loss: 26.4649 - val_mae: 3.7107\n",
      "Epoch 155/850\n",
      "122/122 [==============================] - 0s 701us/step - loss: 325.7803 - mae: 11.7609 - val_loss: 25.1121 - val_mae: 3.5901\n",
      "Epoch 156/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 305.5827 - mae: 11.5864 - val_loss: 24.2464 - val_mae: 3.5188\n",
      "Epoch 157/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 310.7813 - mae: 11.4725 - val_loss: 22.3802 - val_mae: 3.3627\n",
      "Epoch 158/850\n",
      "122/122 [==============================] - 0s 677us/step - loss: 304.4101 - mae: 11.5359 - val_loss: 27.9893 - val_mae: 3.8790\n",
      "Epoch 159/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 294.4467 - mae: 11.2103 - val_loss: 28.9388 - val_mae: 3.9980\n",
      "Epoch 160/850\n",
      "122/122 [==============================] - 0s 591us/step - loss: 300.5653 - mae: 11.3179 - val_loss: 21.9546 - val_mae: 3.3479\n",
      "Epoch 161/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 322.0022 - mae: 11.7748 - val_loss: 25.5337 - val_mae: 3.6093\n",
      "Epoch 162/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 301.1522 - mae: 11.2416 - val_loss: 31.8299 - val_mae: 4.2706\n",
      "Epoch 163/850\n",
      "122/122 [==============================] - 0s 598us/step - loss: 314.9211 - mae: 11.4717 - val_loss: 25.5448 - val_mae: 3.7209\n",
      "Epoch 164/850\n",
      "122/122 [==============================] - 0s 584us/step - loss: 304.5055 - mae: 11.4145 - val_loss: 29.1195 - val_mae: 3.8853\n",
      "Epoch 165/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 289.4635 - mae: 11.1578 - val_loss: 23.8524 - val_mae: 3.5051\n",
      "Epoch 166/850\n",
      "122/122 [==============================] - 0s 666us/step - loss: 310.4045 - mae: 11.5100 - val_loss: 22.3776 - val_mae: 3.3501\n",
      "Epoch 167/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 301.3391 - mae: 11.1785 - val_loss: 26.3331 - val_mae: 3.5753\n",
      "Epoch 168/850\n",
      "122/122 [==============================] - 0s 573us/step - loss: 292.8023 - mae: 11.2038 - val_loss: 30.2500 - val_mae: 4.1245\n",
      "Epoch 169/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 315.5910 - mae: 11.5130 - val_loss: 22.6531 - val_mae: 3.4333\n",
      "Epoch 170/850\n",
      "122/122 [==============================] - 0s 586us/step - loss: 281.8838 - mae: 10.9947 - val_loss: 25.8475 - val_mae: 3.6146\n",
      "Epoch 171/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 284.8812 - mae: 11.0044 - val_loss: 32.2806 - val_mae: 4.2919\n",
      "Epoch 172/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 267.0187 - mae: 10.7609 - val_loss: 26.9743 - val_mae: 3.8161\n",
      "Epoch 173/850\n",
      "122/122 [==============================] - 0s 571us/step - loss: 285.0560 - mae: 10.9570 - val_loss: 22.5284 - val_mae: 3.3715\n",
      "Epoch 174/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 279.8235 - mae: 10.7423 - val_loss: 28.6084 - val_mae: 3.9676\n",
      "Epoch 175/850\n",
      "122/122 [==============================] - 0s 824us/step - loss: 258.0021 - mae: 10.5550 - val_loss: 25.1542 - val_mae: 3.5612\n",
      "Epoch 176/850\n",
      "122/122 [==============================] - 0s 667us/step - loss: 282.3053 - mae: 11.1101 - val_loss: 29.5374 - val_mae: 3.9673\n",
      "Epoch 177/850\n",
      "122/122 [==============================] - 0s 648us/step - loss: 280.3780 - mae: 10.8765 - val_loss: 22.5746 - val_mae: 3.3943\n",
      "Epoch 178/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 279.1310 - mae: 10.8472 - val_loss: 27.9647 - val_mae: 3.9474\n",
      "Epoch 179/850\n",
      "122/122 [==============================] - 0s 655us/step - loss: 278.4453 - mae: 10.7741 - val_loss: 29.3767 - val_mae: 4.0150\n",
      "Epoch 180/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 267.9741 - mae: 10.6894 - val_loss: 27.2161 - val_mae: 3.8482\n",
      "Epoch 181/850\n",
      "122/122 [==============================] - 0s 684us/step - loss: 295.5601 - mae: 11.0598 - val_loss: 22.5821 - val_mae: 3.3556\n",
      "Epoch 182/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 272.7603 - mae: 10.7862 - val_loss: 23.7246 - val_mae: 3.4029\n",
      "Epoch 183/850\n",
      "122/122 [==============================] - 0s 760us/step - loss: 262.9181 - mae: 10.5673 - val_loss: 22.9548 - val_mae: 3.4615\n",
      "Epoch 184/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 274.7241 - mae: 10.6621 - val_loss: 27.5886 - val_mae: 3.6747\n",
      "Epoch 185/850\n",
      "122/122 [==============================] - 0s 632us/step - loss: 269.7828 - mae: 10.6999 - val_loss: 25.8539 - val_mae: 3.7830\n",
      "Epoch 186/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 275.2548 - mae: 10.5573 - val_loss: 27.9906 - val_mae: 3.8566\n",
      "Epoch 187/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 287.5136 - mae: 10.8921 - val_loss: 33.5264 - val_mae: 4.4333\n",
      "Epoch 188/850\n",
      "122/122 [==============================] - 0s 719us/step - loss: 271.4621 - mae: 10.4920 - val_loss: 28.9027 - val_mae: 3.9333\n",
      "Epoch 189/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 283.1324 - mae: 10.6770 - val_loss: 27.3380 - val_mae: 3.8213\n",
      "Epoch 190/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 266.6160 - mae: 10.4670 - val_loss: 24.6269 - val_mae: 3.6049\n",
      "Epoch 191/850\n",
      "122/122 [==============================] - 0s 654us/step - loss: 278.4449 - mae: 10.5392 - val_loss: 34.1696 - val_mae: 4.3739\n",
      "Epoch 192/850\n",
      "122/122 [==============================] - 0s 681us/step - loss: 262.2390 - mae: 10.5432 - val_loss: 23.5866 - val_mae: 3.5134\n",
      "Epoch 193/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 246.1166 - mae: 10.1892 - val_loss: 37.0165 - val_mae: 4.6113\n",
      "Epoch 194/850\n",
      "122/122 [==============================] - 0s 649us/step - loss: 276.1789 - mae: 10.5012 - val_loss: 22.2060 - val_mae: 3.3229\n",
      "Epoch 195/850\n",
      "122/122 [==============================] - 0s 719us/step - loss: 252.9585 - mae: 10.2890 - val_loss: 32.3422 - val_mae: 4.3114\n",
      "Epoch 196/850\n",
      "122/122 [==============================] - 0s 737us/step - loss: 260.0721 - mae: 10.3077 - val_loss: 30.9073 - val_mae: 4.1729\n",
      "Epoch 197/850\n",
      "122/122 [==============================] - 0s 643us/step - loss: 259.8213 - mae: 10.3751 - val_loss: 23.6170 - val_mae: 3.4518\n",
      "Epoch 198/850\n",
      "122/122 [==============================] - 0s 679us/step - loss: 260.0910 - mae: 10.3514 - val_loss: 22.6745 - val_mae: 3.4343\n",
      "Epoch 199/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 247.3087 - mae: 10.2303 - val_loss: 25.0021 - val_mae: 3.6937\n",
      "Epoch 200/850\n",
      "122/122 [==============================] - 0s 623us/step - loss: 256.7666 - mae: 10.2971 - val_loss: 49.7930 - val_mae: 5.2137\n",
      "Epoch 201/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 270.5522 - mae: 10.3027 - val_loss: 34.3067 - val_mae: 4.4646\n",
      "Epoch 202/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 244.3407 - mae: 10.1507 - val_loss: 30.0836 - val_mae: 4.0605\n",
      "Epoch 203/850\n",
      "122/122 [==============================] - 0s 699us/step - loss: 238.1646 - mae: 10.0172 - val_loss: 24.5434 - val_mae: 3.5084\n",
      "Epoch 204/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 248.5989 - mae: 10.0587 - val_loss: 55.1169 - val_mae: 5.5513\n",
      "Epoch 205/850\n",
      "122/122 [==============================] - 0s 640us/step - loss: 245.3684 - mae: 9.9827 - val_loss: 24.2290 - val_mae: 3.5925\n",
      "Epoch 206/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 241.5649 - mae: 9.9819 - val_loss: 22.1719 - val_mae: 3.3186\n",
      "Epoch 207/850\n",
      "122/122 [==============================] - 0s 670us/step - loss: 257.1870 - mae: 10.1904 - val_loss: 26.3477 - val_mae: 3.7915\n",
      "Epoch 208/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 234.6316 - mae: 9.7880 - val_loss: 31.6425 - val_mae: 4.2289\n",
      "Epoch 209/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 246.3183 - mae: 10.0098 - val_loss: 23.7164 - val_mae: 3.4926\n",
      "Epoch 210/850\n",
      "122/122 [==============================] - 0s 757us/step - loss: 236.2070 - mae: 9.8869 - val_loss: 27.0300 - val_mae: 3.6846\n",
      "Epoch 211/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 239.7264 - mae: 9.8949 - val_loss: 45.8101 - val_mae: 5.1724\n",
      "Epoch 212/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 243.7303 - mae: 9.8271 - val_loss: 25.4995 - val_mae: 3.6143\n",
      "Epoch 213/850\n",
      "122/122 [==============================] - 0s 743us/step - loss: 253.8558 - mae: 10.1069 - val_loss: 32.3096 - val_mae: 4.2925\n",
      "Epoch 214/850\n",
      "122/122 [==============================] - 0s 683us/step - loss: 241.2344 - mae: 9.9749 - val_loss: 23.0750 - val_mae: 3.3945\n",
      "Epoch 215/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 247.2175 - mae: 9.9281 - val_loss: 23.1993 - val_mae: 3.4191\n",
      "Epoch 216/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 233.2948 - mae: 9.7545 - val_loss: 24.6362 - val_mae: 3.6294\n",
      "Epoch 217/850\n",
      "122/122 [==============================] - 0s 661us/step - loss: 228.9362 - mae: 9.5865 - val_loss: 26.4815 - val_mae: 3.8190\n",
      "Epoch 218/850\n",
      "122/122 [==============================] - 0s 599us/step - loss: 232.3089 - mae: 9.6656 - val_loss: 24.1223 - val_mae: 3.5328\n",
      "Epoch 219/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 232.1596 - mae: 9.6902 - val_loss: 22.9134 - val_mae: 3.3485\n",
      "Epoch 220/850\n",
      "122/122 [==============================] - 0s 671us/step - loss: 231.5399 - mae: 9.6529 - val_loss: 24.5577 - val_mae: 3.5209\n",
      "Epoch 221/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 208.0414 - mae: 9.3378 - val_loss: 34.8111 - val_mae: 4.5144\n",
      "Epoch 222/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 230.2150 - mae: 9.5799 - val_loss: 22.8962 - val_mae: 3.4165\n",
      "Epoch 223/850\n",
      "122/122 [==============================] - 0s 616us/step - loss: 221.3889 - mae: 9.4916 - val_loss: 29.3417 - val_mae: 4.0800\n",
      "Epoch 224/850\n",
      "122/122 [==============================] - 0s 632us/step - loss: 217.1718 - mae: 9.3545 - val_loss: 33.9676 - val_mae: 4.4655\n",
      "Epoch 225/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 217.5087 - mae: 9.4388 - val_loss: 26.1161 - val_mae: 3.6976\n",
      "Epoch 226/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 232.9102 - mae: 9.6503 - val_loss: 21.9493 - val_mae: 3.3430\n",
      "Epoch 227/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 223.0973 - mae: 9.5339 - val_loss: 41.8886 - val_mae: 4.9615\n",
      "Epoch 228/850\n",
      "122/122 [==============================] - 0s 572us/step - loss: 230.2987 - mae: 9.6276 - val_loss: 25.9017 - val_mae: 3.6970\n",
      "Epoch 229/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 230.4515 - mae: 9.6037 - val_loss: 25.3127 - val_mae: 3.5253\n",
      "Epoch 230/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 212.7433 - mae: 9.3601 - val_loss: 39.4260 - val_mae: 4.8303\n",
      "Epoch 231/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 222.4551 - mae: 9.3647 - val_loss: 36.2667 - val_mae: 4.5742\n",
      "Epoch 232/850\n",
      "122/122 [==============================] - 0s 673us/step - loss: 209.9924 - mae: 9.1730 - val_loss: 25.3383 - val_mae: 3.5319\n",
      "Epoch 233/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 212.5984 - mae: 9.2181 - val_loss: 24.7325 - val_mae: 3.4951\n",
      "Epoch 234/850\n",
      "122/122 [==============================] - 0s 629us/step - loss: 217.3058 - mae: 9.2731 - val_loss: 24.4715 - val_mae: 3.5054\n",
      "Epoch 235/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 205.4111 - mae: 9.1545 - val_loss: 23.4837 - val_mae: 3.4155\n",
      "Epoch 236/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 204.2743 - mae: 9.1225 - val_loss: 24.8263 - val_mae: 3.6433\n",
      "Epoch 237/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 200.9331 - mae: 9.1334 - val_loss: 25.5448 - val_mae: 3.6522\n",
      "Epoch 238/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 212.4950 - mae: 9.3086 - val_loss: 28.2550 - val_mae: 3.9442\n",
      "Epoch 239/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 199.5077 - mae: 9.0321 - val_loss: 26.0517 - val_mae: 3.7475\n",
      "Epoch 240/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 199.8946 - mae: 9.0338 - val_loss: 37.0417 - val_mae: 4.6462\n",
      "Epoch 241/850\n",
      "122/122 [==============================] - 0s 571us/step - loss: 206.7286 - mae: 9.2244 - val_loss: 30.9140 - val_mae: 4.1134\n",
      "Epoch 242/850\n",
      "122/122 [==============================] - 0s 744us/step - loss: 202.8299 - mae: 9.0028 - val_loss: 28.3869 - val_mae: 3.9148\n",
      "Epoch 243/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 197.8116 - mae: 8.9864 - val_loss: 24.8228 - val_mae: 3.4452\n",
      "Epoch 244/850\n",
      "122/122 [==============================] - 0s 592us/step - loss: 190.2717 - mae: 8.9479 - val_loss: 25.4858 - val_mae: 3.6879\n",
      "Epoch 245/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 209.7765 - mae: 9.1979 - val_loss: 24.1205 - val_mae: 3.4637\n",
      "Epoch 246/850\n",
      "122/122 [==============================] - 0s 621us/step - loss: 215.9488 - mae: 9.2200 - val_loss: 26.9729 - val_mae: 3.8442\n",
      "Epoch 247/850\n",
      "122/122 [==============================] - 0s 583us/step - loss: 198.6761 - mae: 9.0713 - val_loss: 25.8574 - val_mae: 3.6414\n",
      "Epoch 248/850\n",
      "122/122 [==============================] - 0s 661us/step - loss: 204.9192 - mae: 9.0527 - val_loss: 23.2166 - val_mae: 3.4073\n",
      "Epoch 249/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 197.8841 - mae: 9.0265 - val_loss: 25.7877 - val_mae: 3.6542\n",
      "Epoch 250/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 189.3926 - mae: 8.8652 - val_loss: 31.2027 - val_mae: 4.0525\n",
      "Epoch 251/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 189.1797 - mae: 8.9296 - val_loss: 24.7966 - val_mae: 3.5746\n",
      "Epoch 252/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 196.6601 - mae: 8.8936 - val_loss: 28.8214 - val_mae: 3.7291\n",
      "Epoch 253/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 194.1842 - mae: 8.9236 - val_loss: 28.2841 - val_mae: 3.8977\n",
      "Epoch 254/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 191.1982 - mae: 8.8856 - val_loss: 39.2510 - val_mae: 4.7366\n",
      "Epoch 255/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 185.4061 - mae: 8.8094 - val_loss: 27.8759 - val_mae: 3.7765\n",
      "Epoch 256/850\n",
      "122/122 [==============================] - 0s 690us/step - loss: 191.2150 - mae: 8.8188 - val_loss: 28.8319 - val_mae: 3.8704\n",
      "Epoch 257/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 186.9466 - mae: 8.8765 - val_loss: 24.6042 - val_mae: 3.3882\n",
      "Epoch 258/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 189.3530 - mae: 8.7717 - val_loss: 29.7617 - val_mae: 3.8384\n",
      "Epoch 259/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 190.9183 - mae: 8.8462 - val_loss: 27.9679 - val_mae: 3.7529\n",
      "Epoch 260/850\n",
      "122/122 [==============================] - 0s 595us/step - loss: 184.1450 - mae: 8.7554 - val_loss: 28.4229 - val_mae: 3.7238\n",
      "Epoch 261/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 194.4562 - mae: 8.8706 - val_loss: 27.5433 - val_mae: 3.7142\n",
      "Epoch 262/850\n",
      "122/122 [==============================] - 0s 638us/step - loss: 199.8349 - mae: 9.0406 - val_loss: 26.0500 - val_mae: 3.6110\n",
      "Epoch 263/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 189.9044 - mae: 8.7497 - val_loss: 27.7968 - val_mae: 3.8373\n",
      "Epoch 264/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 184.0027 - mae: 8.6760 - val_loss: 24.6502 - val_mae: 3.3844\n",
      "Epoch 265/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 180.7032 - mae: 8.5356 - val_loss: 33.2129 - val_mae: 4.1768\n",
      "Epoch 266/850\n",
      "122/122 [==============================] - 0s 593us/step - loss: 193.2033 - mae: 8.8634 - val_loss: 24.6849 - val_mae: 3.5811\n",
      "Epoch 267/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 181.7836 - mae: 8.6513 - val_loss: 25.9437 - val_mae: 3.7219\n",
      "Epoch 268/850\n",
      "122/122 [==============================] - 0s 662us/step - loss: 170.8600 - mae: 8.5048 - val_loss: 27.9612 - val_mae: 3.8743\n",
      "Epoch 269/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 171.8195 - mae: 8.6019 - val_loss: 23.3846 - val_mae: 3.4141\n",
      "Epoch 270/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 187.6394 - mae: 8.9149 - val_loss: 40.1404 - val_mae: 4.9425\n",
      "Epoch 271/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 187.0040 - mae: 8.6811 - val_loss: 25.7385 - val_mae: 3.7322\n",
      "Epoch 272/850\n",
      "122/122 [==============================] - 0s 623us/step - loss: 189.0792 - mae: 8.7407 - val_loss: 26.7572 - val_mae: 3.7757\n",
      "Epoch 273/850\n",
      "122/122 [==============================] - 0s 583us/step - loss: 190.6136 - mae: 8.7659 - val_loss: 30.0592 - val_mae: 4.1410\n",
      "Epoch 274/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 189.4615 - mae: 8.8619 - val_loss: 26.0604 - val_mae: 3.6875\n",
      "Epoch 275/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 176.7191 - mae: 8.5666 - val_loss: 26.2165 - val_mae: 3.6821\n",
      "Epoch 276/850\n",
      "122/122 [==============================] - 0s 688us/step - loss: 171.6315 - mae: 8.3783 - val_loss: 34.2696 - val_mae: 4.4145\n",
      "Epoch 277/850\n",
      "122/122 [==============================] - 0s 579us/step - loss: 172.6772 - mae: 8.4914 - val_loss: 34.4133 - val_mae: 4.4195\n",
      "Epoch 278/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 177.3786 - mae: 8.5242 - val_loss: 38.3308 - val_mae: 4.7944\n",
      "Epoch 279/850\n",
      "122/122 [==============================] - 0s 589us/step - loss: 171.0346 - mae: 8.4473 - val_loss: 24.9747 - val_mae: 3.6108\n",
      "Epoch 280/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 167.9422 - mae: 8.4016 - val_loss: 27.3676 - val_mae: 3.9187\n",
      "Epoch 281/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 180.9245 - mae: 8.5789 - val_loss: 25.1901 - val_mae: 3.6210\n",
      "Epoch 282/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 172.1029 - mae: 8.2857 - val_loss: 27.1171 - val_mae: 3.9368\n",
      "Epoch 283/850\n",
      "122/122 [==============================] - 0s 653us/step - loss: 162.1870 - mae: 8.2413 - val_loss: 43.4709 - val_mae: 5.1370\n",
      "Epoch 284/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 166.4610 - mae: 8.2679 - val_loss: 24.8942 - val_mae: 3.6070\n",
      "Epoch 285/850\n",
      "122/122 [==============================] - 0s 616us/step - loss: 154.9327 - mae: 8.1519 - val_loss: 27.5003 - val_mae: 3.9192\n",
      "Epoch 286/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 157.2087 - mae: 8.1991 - val_loss: 27.0125 - val_mae: 3.7915\n",
      "Epoch 287/850\n",
      "122/122 [==============================] - 0s 746us/step - loss: 172.8069 - mae: 8.5214 - val_loss: 25.5738 - val_mae: 3.6568\n",
      "Epoch 288/850\n",
      "122/122 [==============================] - 0s 678us/step - loss: 159.2201 - mae: 8.3247 - val_loss: 25.3917 - val_mae: 3.6539\n",
      "Epoch 289/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 166.4362 - mae: 8.3313 - val_loss: 24.5188 - val_mae: 3.5549\n",
      "Epoch 290/850\n",
      "122/122 [==============================] - 0s 760us/step - loss: 163.4580 - mae: 8.1551 - val_loss: 25.2774 - val_mae: 3.6514\n",
      "Epoch 291/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 166.1745 - mae: 8.3475 - val_loss: 24.2655 - val_mae: 3.4535\n",
      "Epoch 292/850\n",
      "122/122 [==============================] - 0s 653us/step - loss: 154.3066 - mae: 8.0497 - val_loss: 24.6218 - val_mae: 3.5127\n",
      "Epoch 293/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 149.0472 - mae: 8.0788 - val_loss: 33.4759 - val_mae: 4.3332\n",
      "Epoch 294/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 154.9065 - mae: 8.1607 - val_loss: 27.4312 - val_mae: 3.8456\n",
      "Epoch 295/850\n",
      "122/122 [==============================] - 0s 654us/step - loss: 166.3867 - mae: 8.4065 - val_loss: 24.2644 - val_mae: 3.5158\n",
      "Epoch 296/850\n",
      "122/122 [==============================] - 0s 631us/step - loss: 163.1619 - mae: 8.2546 - val_loss: 25.3042 - val_mae: 3.6121\n",
      "Epoch 297/850\n",
      "122/122 [==============================] - 0s 654us/step - loss: 144.6206 - mae: 8.0259 - val_loss: 25.8979 - val_mae: 3.6971\n",
      "Epoch 298/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 179.3010 - mae: 8.4784 - val_loss: 25.3300 - val_mae: 3.6303\n",
      "Epoch 299/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 159.6722 - mae: 8.2376 - val_loss: 26.5593 - val_mae: 3.8115\n",
      "Epoch 300/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 155.4782 - mae: 8.1097 - val_loss: 26.4203 - val_mae: 3.6577\n",
      "Epoch 301/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 143.1038 - mae: 7.8531 - val_loss: 27.3246 - val_mae: 3.7669\n",
      "Epoch 302/850\n",
      "122/122 [==============================] - 0s 672us/step - loss: 150.6335 - mae: 8.0323 - val_loss: 23.3975 - val_mae: 3.3867\n",
      "Epoch 303/850\n",
      "122/122 [==============================] - 0s 665us/step - loss: 149.3553 - mae: 8.0645 - val_loss: 41.8853 - val_mae: 4.9038\n",
      "Epoch 304/850\n",
      "122/122 [==============================] - 0s 647us/step - loss: 158.1751 - mae: 8.2087 - val_loss: 35.9540 - val_mae: 4.4873\n",
      "Epoch 305/850\n",
      "122/122 [==============================] - 0s 651us/step - loss: 143.0486 - mae: 7.9230 - val_loss: 27.8525 - val_mae: 3.7799\n",
      "Epoch 306/850\n",
      "122/122 [==============================] - 0s 674us/step - loss: 160.6855 - mae: 8.2308 - val_loss: 25.5562 - val_mae: 3.6008\n",
      "Epoch 307/850\n",
      "122/122 [==============================] - 0s 670us/step - loss: 151.9738 - mae: 8.1088 - val_loss: 25.3714 - val_mae: 3.6056\n",
      "Epoch 308/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 141.3358 - mae: 7.8386 - val_loss: 25.8872 - val_mae: 3.5836\n",
      "Epoch 309/850\n",
      "122/122 [==============================] - 0s 718us/step - loss: 152.6624 - mae: 8.0146 - val_loss: 28.0069 - val_mae: 3.9125\n",
      "Epoch 310/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 162.9814 - mae: 8.3462 - val_loss: 24.2251 - val_mae: 3.5109\n",
      "Epoch 311/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 151.8436 - mae: 8.0394 - val_loss: 26.0553 - val_mae: 3.6819\n",
      "Epoch 312/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 139.8434 - mae: 7.8200 - val_loss: 33.3218 - val_mae: 4.3775\n",
      "Epoch 313/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 133.5982 - mae: 7.7041 - val_loss: 29.2068 - val_mae: 4.0822\n",
      "Epoch 314/850\n",
      "122/122 [==============================] - 0s 599us/step - loss: 155.1236 - mae: 8.1676 - val_loss: 26.9904 - val_mae: 3.7118\n",
      "Epoch 315/850\n",
      "122/122 [==============================] - 0s 749us/step - loss: 157.4992 - mae: 8.1435 - val_loss: 24.8449 - val_mae: 3.5951\n",
      "Epoch 316/850\n",
      "122/122 [==============================] - 0s 599us/step - loss: 145.9580 - mae: 7.8757 - val_loss: 38.2066 - val_mae: 4.7690\n",
      "Epoch 317/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 139.6775 - mae: 7.7007 - val_loss: 26.1881 - val_mae: 3.7452\n",
      "Epoch 318/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 145.4297 - mae: 7.9744 - val_loss: 24.9973 - val_mae: 3.5042\n",
      "Epoch 319/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 134.5203 - mae: 7.6840 - val_loss: 27.1927 - val_mae: 3.6691\n",
      "Epoch 320/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 137.3983 - mae: 7.8033 - val_loss: 29.6473 - val_mae: 3.8429\n",
      "Epoch 321/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 148.3827 - mae: 8.0364 - val_loss: 33.4825 - val_mae: 4.3161\n",
      "Epoch 322/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 133.8583 - mae: 7.6984 - val_loss: 23.7872 - val_mae: 3.4999\n",
      "Epoch 323/850\n",
      "122/122 [==============================] - 0s 655us/step - loss: 146.3218 - mae: 7.9546 - val_loss: 30.2515 - val_mae: 4.1050\n",
      "Epoch 324/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 138.3600 - mae: 7.7897 - val_loss: 26.0653 - val_mae: 3.6915\n",
      "Epoch 325/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 130.4051 - mae: 7.7099 - val_loss: 26.8778 - val_mae: 3.7964\n",
      "Epoch 326/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 139.5330 - mae: 7.8186 - val_loss: 25.6756 - val_mae: 3.6594\n",
      "Epoch 327/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 138.5509 - mae: 7.7589 - val_loss: 26.6242 - val_mae: 3.6592\n",
      "Epoch 328/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 147.3980 - mae: 7.9190 - val_loss: 29.7541 - val_mae: 4.0263\n",
      "Epoch 329/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 144.0717 - mae: 7.8519 - val_loss: 29.6488 - val_mae: 3.8511\n",
      "Epoch 330/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 132.7591 - mae: 7.6758 - val_loss: 25.9154 - val_mae: 3.6131\n",
      "Epoch 331/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 133.0485 - mae: 7.7290 - val_loss: 30.9564 - val_mae: 4.1509\n",
      "Epoch 332/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 135.7240 - mae: 7.6828 - val_loss: 37.1823 - val_mae: 4.6880\n",
      "Epoch 333/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 129.6984 - mae: 7.5653 - val_loss: 27.8822 - val_mae: 3.7185\n",
      "Epoch 334/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 144.2377 - mae: 7.8781 - val_loss: 29.2430 - val_mae: 3.9363\n",
      "Epoch 335/850\n",
      "122/122 [==============================] - 0s 592us/step - loss: 139.0651 - mae: 7.8256 - val_loss: 33.1403 - val_mae: 4.3444\n",
      "Epoch 336/850\n",
      "122/122 [==============================] - 0s 704us/step - loss: 130.9796 - mae: 7.6352 - val_loss: 26.3099 - val_mae: 3.7244\n",
      "Epoch 337/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 136.8208 - mae: 7.8803 - val_loss: 33.1520 - val_mae: 4.2620\n",
      "Epoch 338/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 137.3042 - mae: 7.7982 - val_loss: 24.3280 - val_mae: 3.5102\n",
      "Epoch 339/850\n",
      "122/122 [==============================] - 0s 593us/step - loss: 134.5769 - mae: 7.8107 - val_loss: 28.3608 - val_mae: 3.8439\n",
      "Epoch 340/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 131.2472 - mae: 7.6711 - val_loss: 33.8271 - val_mae: 4.3027\n",
      "Epoch 341/850\n",
      "122/122 [==============================] - 0s 696us/step - loss: 127.0379 - mae: 7.5375 - val_loss: 30.0974 - val_mae: 4.0659\n",
      "Epoch 342/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 119.0909 - mae: 7.3720 - val_loss: 28.3072 - val_mae: 3.9673\n",
      "Epoch 343/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 126.5892 - mae: 7.5193 - val_loss: 35.4286 - val_mae: 4.5520\n",
      "Epoch 344/850\n",
      "122/122 [==============================] - 0s 583us/step - loss: 129.3039 - mae: 7.6460 - val_loss: 28.4963 - val_mae: 3.9156\n",
      "Epoch 345/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 127.9956 - mae: 7.6325 - val_loss: 27.9251 - val_mae: 3.8423\n",
      "Epoch 346/850\n",
      "122/122 [==============================] - 0s 585us/step - loss: 140.4945 - mae: 7.8552 - val_loss: 25.3810 - val_mae: 3.5457\n",
      "Epoch 347/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 126.3052 - mae: 7.4619 - val_loss: 26.6233 - val_mae: 3.6830\n",
      "Epoch 348/850\n",
      "122/122 [==============================] - 0s 676us/step - loss: 120.3082 - mae: 7.4487 - val_loss: 35.9544 - val_mae: 4.6310\n",
      "Epoch 349/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 127.9506 - mae: 7.6714 - val_loss: 38.3935 - val_mae: 4.6854\n",
      "Epoch 350/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 123.6363 - mae: 7.5208 - val_loss: 34.6277 - val_mae: 4.4390\n",
      "Epoch 351/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 124.0146 - mae: 7.5131 - val_loss: 26.1647 - val_mae: 3.7199\n",
      "Epoch 352/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 119.8390 - mae: 7.4273 - val_loss: 28.3352 - val_mae: 3.8210\n",
      "Epoch 353/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 125.9783 - mae: 7.5713 - val_loss: 28.8027 - val_mae: 3.9367\n",
      "Epoch 354/850\n",
      "122/122 [==============================] - 0s 589us/step - loss: 120.2313 - mae: 7.3819 - val_loss: 29.9285 - val_mae: 3.7849\n",
      "Epoch 355/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 121.7985 - mae: 7.3350 - val_loss: 25.8464 - val_mae: 3.6128\n",
      "Epoch 356/850\n",
      "122/122 [==============================] - 0s 704us/step - loss: 123.0942 - mae: 7.4923 - val_loss: 40.7791 - val_mae: 4.9051\n",
      "Epoch 357/850\n",
      "122/122 [==============================] - 0s 606us/step - loss: 119.9790 - mae: 7.3562 - val_loss: 33.1006 - val_mae: 4.3305\n",
      "Epoch 358/850\n",
      "122/122 [==============================] - 0s 576us/step - loss: 127.6924 - mae: 7.6575 - val_loss: 31.8733 - val_mae: 4.1671\n",
      "Epoch 359/850\n",
      "122/122 [==============================] - 0s 616us/step - loss: 126.9084 - mae: 7.4981 - val_loss: 26.3051 - val_mae: 3.6427\n",
      "Epoch 360/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 116.7862 - mae: 7.3002 - val_loss: 25.2987 - val_mae: 3.5920\n",
      "Epoch 361/850\n",
      "122/122 [==============================] - 0s 594us/step - loss: 123.0684 - mae: 7.4700 - val_loss: 32.1423 - val_mae: 4.3490\n",
      "Epoch 362/850\n",
      "122/122 [==============================] - 0s 713us/step - loss: 124.2164 - mae: 7.4860 - val_loss: 32.5232 - val_mae: 4.1988\n",
      "Epoch 363/850\n",
      "122/122 [==============================] - 0s 575us/step - loss: 124.2378 - mae: 7.5416 - val_loss: 30.1844 - val_mae: 4.0560\n",
      "Epoch 364/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 122.0638 - mae: 7.4128 - val_loss: 23.3226 - val_mae: 3.4080\n",
      "Epoch 365/850\n",
      "122/122 [==============================] - 0s 574us/step - loss: 114.6605 - mae: 7.2648 - val_loss: 35.6538 - val_mae: 4.6041\n",
      "Epoch 366/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 119.9047 - mae: 7.4052 - val_loss: 26.8494 - val_mae: 3.6769\n",
      "Epoch 367/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 122.0741 - mae: 7.4039 - val_loss: 26.1818 - val_mae: 3.5916\n",
      "Epoch 368/850\n",
      "122/122 [==============================] - 0s 574us/step - loss: 122.3478 - mae: 7.5422 - val_loss: 26.9502 - val_mae: 3.7646\n",
      "Epoch 369/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 117.8183 - mae: 7.4194 - val_loss: 28.9106 - val_mae: 3.9335\n",
      "Epoch 370/850\n",
      "122/122 [==============================] - 0s 660us/step - loss: 110.7858 - mae: 7.1461 - val_loss: 24.9576 - val_mae: 3.5512\n",
      "Epoch 371/850\n",
      "122/122 [==============================] - 0s 598us/step - loss: 114.9326 - mae: 7.3227 - val_loss: 27.9785 - val_mae: 3.8978\n",
      "Epoch 372/850\n",
      "122/122 [==============================] - 0s 598us/step - loss: 118.6096 - mae: 7.2538 - val_loss: 30.0489 - val_mae: 4.0894\n",
      "Epoch 373/850\n",
      "122/122 [==============================] - 0s 591us/step - loss: 112.4841 - mae: 7.3227 - val_loss: 31.0962 - val_mae: 4.1833\n",
      "Epoch 374/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 115.4431 - mae: 7.2681 - val_loss: 32.7382 - val_mae: 4.0523\n",
      "Epoch 375/850\n",
      "122/122 [==============================] - 0s 586us/step - loss: 113.8996 - mae: 7.2236 - val_loss: 30.9951 - val_mae: 4.0415\n",
      "Epoch 376/850\n",
      "122/122 [==============================] - 0s 649us/step - loss: 114.6584 - mae: 7.2975 - val_loss: 29.1763 - val_mae: 3.8618\n",
      "Epoch 377/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 110.7385 - mae: 7.1647 - val_loss: 36.3506 - val_mae: 4.5510\n",
      "Epoch 378/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 121.2070 - mae: 7.3831 - val_loss: 31.6595 - val_mae: 4.1939\n",
      "Epoch 379/850\n",
      "122/122 [==============================] - 0s 575us/step - loss: 122.8566 - mae: 7.4979 - val_loss: 26.9397 - val_mae: 3.7216\n",
      "Epoch 380/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 106.6664 - mae: 7.0715 - val_loss: 30.7917 - val_mae: 4.1057\n",
      "Epoch 381/850\n",
      "122/122 [==============================] - 0s 681us/step - loss: 117.6743 - mae: 7.3312 - val_loss: 36.0847 - val_mae: 4.4622\n",
      "Epoch 382/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 114.4027 - mae: 7.3294 - val_loss: 29.3978 - val_mae: 4.0252\n",
      "Epoch 383/850\n",
      "122/122 [==============================] - 0s 665us/step - loss: 121.2862 - mae: 7.3709 - val_loss: 35.8145 - val_mae: 4.5188\n",
      "Epoch 384/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 109.5075 - mae: 7.1798 - val_loss: 29.5984 - val_mae: 3.9138\n",
      "Epoch 385/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 107.0165 - mae: 7.1053 - val_loss: 25.7983 - val_mae: 3.6396\n",
      "Epoch 386/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 115.2309 - mae: 7.2780 - val_loss: 28.6129 - val_mae: 3.8784\n",
      "Epoch 387/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 110.6168 - mae: 7.1544 - val_loss: 33.9012 - val_mae: 4.2696\n",
      "Epoch 388/850\n",
      "122/122 [==============================] - 0s 594us/step - loss: 110.3726 - mae: 7.2420 - val_loss: 41.8678 - val_mae: 4.7832\n",
      "Epoch 389/850\n",
      "122/122 [==============================] - 0s 718us/step - loss: 112.9368 - mae: 7.2579 - val_loss: 27.9927 - val_mae: 3.7981\n",
      "Epoch 390/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 111.9212 - mae: 7.2374 - val_loss: 28.9389 - val_mae: 3.8429\n",
      "Epoch 391/850\n",
      "122/122 [==============================] - 0s 629us/step - loss: 104.9400 - mae: 7.1012 - val_loss: 30.9552 - val_mae: 3.9407\n",
      "Epoch 392/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 111.6395 - mae: 7.2499 - val_loss: 37.6015 - val_mae: 4.6064\n",
      "Epoch 393/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 111.7089 - mae: 7.2900 - val_loss: 29.9058 - val_mae: 4.0760\n",
      "Epoch 394/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 108.2088 - mae: 7.1551 - val_loss: 29.1389 - val_mae: 3.8203\n",
      "Epoch 395/850\n",
      "122/122 [==============================] - 0s 632us/step - loss: 95.6108 - mae: 6.7849 - val_loss: 24.1590 - val_mae: 3.4806\n",
      "Epoch 396/850\n",
      "122/122 [==============================] - 0s 593us/step - loss: 105.6163 - mae: 7.0720 - val_loss: 28.2122 - val_mae: 3.7325\n",
      "Epoch 397/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 103.8365 - mae: 7.0677 - val_loss: 29.1429 - val_mae: 3.8164\n",
      "Epoch 398/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 106.5480 - mae: 7.0916 - val_loss: 31.7574 - val_mae: 4.1375\n",
      "Epoch 399/850\n",
      "122/122 [==============================] - 0s 696us/step - loss: 100.6466 - mae: 6.9422 - val_loss: 33.5640 - val_mae: 4.2953\n",
      "Epoch 400/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 101.1122 - mae: 6.9368 - val_loss: 31.0030 - val_mae: 4.1379\n",
      "Epoch 401/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 104.7409 - mae: 7.0785 - val_loss: 29.0273 - val_mae: 3.7636\n",
      "Epoch 402/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 101.3326 - mae: 6.9716 - val_loss: 36.1183 - val_mae: 4.5215\n",
      "Epoch 403/850\n",
      "122/122 [==============================] - 0s 591us/step - loss: 98.7998 - mae: 6.8961 - val_loss: 26.3595 - val_mae: 3.6482\n",
      "Epoch 404/850\n",
      "122/122 [==============================] - 0s 590us/step - loss: 103.6204 - mae: 7.0279 - val_loss: 33.0513 - val_mae: 4.1643\n",
      "Epoch 405/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 101.1915 - mae: 6.9699 - val_loss: 27.6043 - val_mae: 3.7396\n",
      "Epoch 406/850\n",
      "122/122 [==============================] - 0s 674us/step - loss: 102.1504 - mae: 6.9455 - val_loss: 28.9395 - val_mae: 4.0269\n",
      "Epoch 407/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 104.3282 - mae: 7.0850 - val_loss: 29.4182 - val_mae: 3.8063\n",
      "Epoch 408/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 100.6286 - mae: 7.0214 - val_loss: 31.5031 - val_mae: 4.1067\n",
      "Epoch 409/850\n",
      "122/122 [==============================] - 0s 598us/step - loss: 104.1753 - mae: 6.9924 - val_loss: 29.1326 - val_mae: 3.8868\n",
      "Epoch 410/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 104.8399 - mae: 6.9652 - val_loss: 32.1669 - val_mae: 4.1470\n",
      "Epoch 411/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 102.8482 - mae: 7.0592 - val_loss: 39.7263 - val_mae: 4.6397\n",
      "Epoch 412/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 107.6564 - mae: 7.1057 - val_loss: 33.3169 - val_mae: 4.2367\n",
      "Epoch 413/850\n",
      "122/122 [==============================] - 0s 673us/step - loss: 99.6139 - mae: 6.8615 - val_loss: 37.3497 - val_mae: 4.5137\n",
      "Epoch 414/850\n",
      "122/122 [==============================] - 0s 601us/step - loss: 98.5023 - mae: 6.9347 - val_loss: 41.6442 - val_mae: 4.7805\n",
      "Epoch 415/850\n",
      "122/122 [==============================] - 0s 629us/step - loss: 104.2052 - mae: 7.0149 - val_loss: 27.9692 - val_mae: 3.8061\n",
      "Epoch 416/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 96.4086 - mae: 6.8760 - val_loss: 32.3106 - val_mae: 4.1379\n",
      "Epoch 417/850\n",
      "122/122 [==============================] - 0s 598us/step - loss: 97.4275 - mae: 6.9344 - val_loss: 29.2595 - val_mae: 3.8587\n",
      "Epoch 418/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 95.7659 - mae: 6.8245 - val_loss: 30.1922 - val_mae: 3.8292\n",
      "Epoch 419/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 103.1976 - mae: 7.0542 - val_loss: 44.4872 - val_mae: 5.1438\n",
      "Epoch 420/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 99.2209 - mae: 6.9796 - val_loss: 32.4294 - val_mae: 4.1972\n",
      "Epoch 421/850\n",
      "122/122 [==============================] - 0s 601us/step - loss: 101.0721 - mae: 7.0178 - val_loss: 43.8627 - val_mae: 5.0198\n",
      "Epoch 422/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 98.8637 - mae: 6.9755 - val_loss: 27.7987 - val_mae: 3.7851\n",
      "Epoch 423/850\n",
      "122/122 [==============================] - 0s 593us/step - loss: 93.5164 - mae: 6.7244 - val_loss: 32.3785 - val_mae: 4.2280\n",
      "Epoch 424/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 99.7136 - mae: 6.9225 - val_loss: 36.3630 - val_mae: 4.4578\n",
      "Epoch 425/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 93.7279 - mae: 6.9097 - val_loss: 29.8384 - val_mae: 3.8614\n",
      "Epoch 426/850\n",
      "122/122 [==============================] - 0s 661us/step - loss: 95.9659 - mae: 6.8195 - val_loss: 27.8845 - val_mae: 3.7705\n",
      "Epoch 427/850\n",
      "122/122 [==============================] - 0s 629us/step - loss: 92.7894 - mae: 6.6771 - val_loss: 31.9302 - val_mae: 4.0768\n",
      "Epoch 428/850\n",
      "122/122 [==============================] - 0s 601us/step - loss: 96.0582 - mae: 6.8360 - val_loss: 27.8610 - val_mae: 3.7426\n",
      "Epoch 429/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 90.7504 - mae: 6.7499 - val_loss: 33.6853 - val_mae: 4.2353\n",
      "Epoch 430/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 90.0251 - mae: 6.5367 - val_loss: 31.6011 - val_mae: 4.1023\n",
      "Epoch 431/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 98.3367 - mae: 6.9875 - val_loss: 32.3011 - val_mae: 4.1413\n",
      "Epoch 432/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 93.7117 - mae: 6.7635 - val_loss: 29.2710 - val_mae: 3.9030\n",
      "Epoch 433/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 91.8639 - mae: 6.7698 - val_loss: 43.6173 - val_mae: 5.0801\n",
      "Epoch 434/850\n",
      "122/122 [==============================] - 0s 706us/step - loss: 98.6350 - mae: 6.9156 - val_loss: 29.1860 - val_mae: 3.9700\n",
      "Epoch 435/850\n",
      "122/122 [==============================] - 0s 584us/step - loss: 89.3175 - mae: 6.6669 - val_loss: 41.4270 - val_mae: 4.7657\n",
      "Epoch 436/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 92.7706 - mae: 6.8031 - val_loss: 30.1683 - val_mae: 4.0077\n",
      "Epoch 437/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 94.2399 - mae: 6.7823 - val_loss: 39.1156 - val_mae: 4.8940\n",
      "Epoch 438/850\n",
      "122/122 [==============================] - 0s 580us/step - loss: 87.8998 - mae: 6.5481 - val_loss: 40.9058 - val_mae: 4.8215\n",
      "Epoch 439/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 94.2947 - mae: 6.7796 - val_loss: 33.2261 - val_mae: 4.2243\n",
      "Epoch 440/850\n",
      "122/122 [==============================] - 0s 575us/step - loss: 86.5303 - mae: 6.5457 - val_loss: 27.5927 - val_mae: 3.8265\n",
      "Epoch 441/850\n",
      "122/122 [==============================] - 0s 687us/step - loss: 93.6872 - mae: 6.7492 - val_loss: 28.3708 - val_mae: 3.7130\n",
      "Epoch 442/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 89.4118 - mae: 6.6960 - val_loss: 26.3150 - val_mae: 3.6049\n",
      "Epoch 443/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 86.3357 - mae: 6.5787 - val_loss: 30.7029 - val_mae: 3.9558\n",
      "Epoch 444/850\n",
      "122/122 [==============================] - 0s 623us/step - loss: 83.2115 - mae: 6.4217 - val_loss: 31.3162 - val_mae: 4.1462\n",
      "Epoch 445/850\n",
      "122/122 [==============================] - 0s 601us/step - loss: 85.2878 - mae: 6.5284 - val_loss: 28.1118 - val_mae: 3.7688\n",
      "Epoch 446/850\n",
      "122/122 [==============================] - 0s 647us/step - loss: 85.5135 - mae: 6.4944 - val_loss: 37.7997 - val_mae: 4.6311\n",
      "Epoch 447/850\n",
      "122/122 [==============================] - 0s 623us/step - loss: 97.2965 - mae: 6.8389 - val_loss: 27.6769 - val_mae: 3.8105\n",
      "Epoch 448/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 88.9706 - mae: 6.5680 - val_loss: 28.9118 - val_mae: 3.9285\n",
      "Epoch 449/850\n",
      "122/122 [==============================] - 0s 592us/step - loss: 89.6127 - mae: 6.6597 - val_loss: 30.0977 - val_mae: 3.9940\n",
      "Epoch 450/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 96.6174 - mae: 6.8339 - val_loss: 28.5812 - val_mae: 3.7862\n",
      "Epoch 451/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 91.9166 - mae: 6.7586 - val_loss: 36.3629 - val_mae: 4.4094\n",
      "Epoch 452/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 85.3042 - mae: 6.4820 - val_loss: 30.0793 - val_mae: 4.0924\n",
      "Epoch 453/850\n",
      "122/122 [==============================] - 0s 676us/step - loss: 86.8534 - mae: 6.5751 - val_loss: 29.8733 - val_mae: 3.9380\n",
      "Epoch 454/850\n",
      "122/122 [==============================] - 0s 586us/step - loss: 92.1707 - mae: 6.7836 - val_loss: 30.9748 - val_mae: 4.0564\n",
      "Epoch 455/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 85.2715 - mae: 6.4961 - val_loss: 45.9765 - val_mae: 5.2606\n",
      "Epoch 456/850\n",
      "122/122 [==============================] - 0s 568us/step - loss: 82.4583 - mae: 6.4731 - val_loss: 34.9346 - val_mae: 4.4110\n",
      "Epoch 457/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 86.9895 - mae: 6.5357 - val_loss: 27.5713 - val_mae: 3.7116\n",
      "Epoch 458/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 82.5168 - mae: 6.4428 - val_loss: 31.9604 - val_mae: 4.1806\n",
      "Epoch 459/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 81.0984 - mae: 6.4289 - val_loss: 28.4071 - val_mae: 3.9068\n",
      "Epoch 460/850\n",
      "122/122 [==============================] - 0s 633us/step - loss: 88.6978 - mae: 6.6805 - val_loss: 27.9748 - val_mae: 3.8391\n",
      "Epoch 461/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 82.7508 - mae: 6.5392 - val_loss: 28.8850 - val_mae: 3.9360\n",
      "Epoch 462/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 83.3330 - mae: 6.5034 - val_loss: 38.8360 - val_mae: 4.7890\n",
      "Epoch 463/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 82.5320 - mae: 6.4447 - val_loss: 33.9734 - val_mae: 4.3845\n",
      "Epoch 464/850\n",
      "122/122 [==============================] - 0s 574us/step - loss: 86.8244 - mae: 6.5812 - val_loss: 30.3928 - val_mae: 3.9598\n",
      "Epoch 465/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 79.8428 - mae: 6.3585 - val_loss: 28.4956 - val_mae: 3.8554\n",
      "Epoch 466/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 88.5737 - mae: 6.5363 - val_loss: 28.0608 - val_mae: 3.8139\n",
      "Epoch 467/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 82.9018 - mae: 6.5185 - val_loss: 30.6799 - val_mae: 3.9872\n",
      "Epoch 468/850\n",
      "122/122 [==============================] - 0s 570us/step - loss: 78.5064 - mae: 6.3315 - val_loss: 31.1686 - val_mae: 4.1180\n",
      "Epoch 469/850\n",
      "122/122 [==============================] - 0s 620us/step - loss: 84.5267 - mae: 6.5977 - val_loss: 27.0909 - val_mae: 3.7404\n",
      "Epoch 470/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 80.4984 - mae: 6.3897 - val_loss: 36.7664 - val_mae: 4.5973\n",
      "Epoch 471/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 79.7227 - mae: 6.4188 - val_loss: 26.7597 - val_mae: 3.7237\n",
      "Epoch 472/850\n",
      "122/122 [==============================] - 0s 682us/step - loss: 84.0725 - mae: 6.5069 - val_loss: 31.1272 - val_mae: 3.9821\n",
      "Epoch 473/850\n",
      "122/122 [==============================] - 0s 590us/step - loss: 88.4258 - mae: 6.5651 - val_loss: 27.0656 - val_mae: 3.7954\n",
      "Epoch 474/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 82.5705 - mae: 6.5048 - val_loss: 27.4819 - val_mae: 3.8047\n",
      "Epoch 475/850\n",
      "122/122 [==============================] - 0s 579us/step - loss: 80.5636 - mae: 6.3994 - val_loss: 27.8278 - val_mae: 3.7273\n",
      "Epoch 476/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 78.5460 - mae: 6.2987 - val_loss: 28.9867 - val_mae: 3.9530\n",
      "Epoch 477/850\n",
      "122/122 [==============================] - 0s 578us/step - loss: 80.6453 - mae: 6.3845 - val_loss: 27.0352 - val_mae: 3.7343\n",
      "Epoch 478/850\n",
      "122/122 [==============================] - 0s 664us/step - loss: 78.6957 - mae: 6.3227 - val_loss: 29.3076 - val_mae: 3.9356\n",
      "Epoch 479/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 86.8442 - mae: 6.6278 - val_loss: 33.0872 - val_mae: 4.1612\n",
      "Epoch 480/850\n",
      "122/122 [==============================] - 0s 636us/step - loss: 80.3709 - mae: 6.3811 - val_loss: 31.5630 - val_mae: 4.1997\n",
      "Epoch 481/850\n",
      "122/122 [==============================] - 0s 606us/step - loss: 79.9546 - mae: 6.3489 - val_loss: 29.7768 - val_mae: 3.9412\n",
      "Epoch 482/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 80.7105 - mae: 6.3171 - val_loss: 26.6026 - val_mae: 3.6989\n",
      "Epoch 483/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 76.2459 - mae: 6.2043 - val_loss: 27.3539 - val_mae: 3.7599\n",
      "Epoch 484/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 77.3360 - mae: 6.2696 - val_loss: 39.1751 - val_mae: 4.8745\n",
      "Epoch 485/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 77.8272 - mae: 6.2784 - val_loss: 33.0440 - val_mae: 4.2726\n",
      "Epoch 486/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 78.0021 - mae: 6.2915 - val_loss: 26.6170 - val_mae: 3.6827\n",
      "Epoch 487/850\n",
      "122/122 [==============================] - 0s 567us/step - loss: 74.4299 - mae: 6.2221 - val_loss: 28.3041 - val_mae: 3.7696\n",
      "Epoch 488/850\n",
      "122/122 [==============================] - 0s 705us/step - loss: 81.6088 - mae: 6.4630 - val_loss: 29.3035 - val_mae: 3.8770\n",
      "Epoch 489/850\n",
      "122/122 [==============================] - 0s 572us/step - loss: 82.7295 - mae: 6.3006 - val_loss: 48.0237 - val_mae: 5.2757\n",
      "Epoch 490/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 72.9855 - mae: 6.1801 - val_loss: 31.1967 - val_mae: 3.8833\n",
      "Epoch 491/850\n",
      "122/122 [==============================] - 0s 577us/step - loss: 79.9660 - mae: 6.3891 - val_loss: 28.8632 - val_mae: 3.8528\n",
      "Epoch 492/850\n",
      "122/122 [==============================] - 0s 600us/step - loss: 77.2152 - mae: 6.3162 - val_loss: 30.9844 - val_mae: 4.1022\n",
      "Epoch 493/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 76.3971 - mae: 6.2431 - val_loss: 27.9813 - val_mae: 3.7650\n",
      "Epoch 494/850\n",
      "122/122 [==============================] - 0s 684us/step - loss: 79.5606 - mae: 6.3523 - val_loss: 29.7256 - val_mae: 3.8955\n",
      "Epoch 495/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 74.8987 - mae: 6.2767 - val_loss: 31.7220 - val_mae: 4.1816\n",
      "Epoch 496/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 77.3516 - mae: 6.2232 - val_loss: 28.2265 - val_mae: 3.8493\n",
      "Epoch 497/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 75.2090 - mae: 6.2651 - val_loss: 26.2643 - val_mae: 3.6856\n",
      "Epoch 498/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 75.7846 - mae: 6.3004 - val_loss: 26.6646 - val_mae: 3.6976\n",
      "Epoch 499/850\n",
      "122/122 [==============================] - 0s 671us/step - loss: 73.4304 - mae: 6.1388 - val_loss: 33.1734 - val_mae: 4.3112\n",
      "Epoch 500/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 75.8071 - mae: 6.2673 - val_loss: 30.5128 - val_mae: 3.9556\n",
      "Epoch 501/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 74.8926 - mae: 6.2917 - val_loss: 40.0923 - val_mae: 4.7874\n",
      "Epoch 502/850\n",
      "122/122 [==============================] - 0s 631us/step - loss: 79.1035 - mae: 6.2896 - val_loss: 36.6878 - val_mae: 4.4945\n",
      "Epoch 503/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 77.2973 - mae: 6.3105 - val_loss: 38.4769 - val_mae: 4.7319\n",
      "Epoch 504/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 76.3695 - mae: 6.2947 - val_loss: 27.6085 - val_mae: 3.7585\n",
      "Epoch 505/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 70.1644 - mae: 6.0865 - val_loss: 31.9079 - val_mae: 4.3293\n",
      "Epoch 506/850\n",
      "122/122 [==============================] - 0s 565us/step - loss: 70.9149 - mae: 6.0917 - val_loss: 28.5206 - val_mae: 3.8945\n",
      "Epoch 507/850\n",
      "122/122 [==============================] - 0s 674us/step - loss: 76.8787 - mae: 6.2640 - val_loss: 29.0207 - val_mae: 3.9147\n",
      "Epoch 508/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 73.3428 - mae: 6.1809 - val_loss: 35.4787 - val_mae: 4.4968\n",
      "Epoch 509/850\n",
      "122/122 [==============================] - 0s 600us/step - loss: 71.8670 - mae: 6.1297 - val_loss: 35.9951 - val_mae: 4.3598\n",
      "Epoch 510/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 74.5697 - mae: 6.1716 - val_loss: 35.7700 - val_mae: 4.1797\n",
      "Epoch 511/850\n",
      "122/122 [==============================] - 0s 583us/step - loss: 76.7639 - mae: 6.2882 - val_loss: 29.4754 - val_mae: 3.8238\n",
      "Epoch 512/850\n",
      "122/122 [==============================] - 0s 764us/step - loss: 71.8046 - mae: 6.0921 - val_loss: 37.1264 - val_mae: 4.6609\n",
      "Epoch 513/850\n",
      "122/122 [==============================] - 0s 638us/step - loss: 73.3798 - mae: 6.0478 - val_loss: 40.4669 - val_mae: 4.8526\n",
      "Epoch 514/850\n",
      "122/122 [==============================] - 0s 690us/step - loss: 74.1561 - mae: 6.2158 - val_loss: 29.6383 - val_mae: 3.9852\n",
      "Epoch 515/850\n",
      "122/122 [==============================] - 0s 631us/step - loss: 71.6092 - mae: 6.0634 - val_loss: 27.7605 - val_mae: 3.9246\n",
      "Epoch 516/850\n",
      "122/122 [==============================] - 0s 654us/step - loss: 74.8227 - mae: 6.2287 - val_loss: 28.4938 - val_mae: 3.8828\n",
      "Epoch 517/850\n",
      "122/122 [==============================] - 0s 647us/step - loss: 74.2414 - mae: 6.0988 - val_loss: 34.5715 - val_mae: 4.3103\n",
      "Epoch 518/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 76.2444 - mae: 6.2303 - val_loss: 38.4581 - val_mae: 4.5985\n",
      "Epoch 519/850\n",
      "122/122 [==============================] - 0s 638us/step - loss: 72.6278 - mae: 6.1897 - val_loss: 33.8598 - val_mae: 4.3458\n",
      "Epoch 520/850\n",
      "122/122 [==============================] - 0s 621us/step - loss: 73.5551 - mae: 6.2238 - val_loss: 30.3083 - val_mae: 3.9185\n",
      "Epoch 521/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 80.9527 - mae: 6.3806 - val_loss: 31.4625 - val_mae: 4.0866\n",
      "Epoch 522/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 72.0822 - mae: 6.1038 - val_loss: 39.9994 - val_mae: 4.6343\n",
      "Epoch 523/850\n",
      "122/122 [==============================] - 0s 672us/step - loss: 76.8336 - mae: 6.2418 - val_loss: 33.0559 - val_mae: 4.1560\n",
      "Epoch 524/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 75.2391 - mae: 6.1802 - val_loss: 33.2018 - val_mae: 4.2880\n",
      "Epoch 525/850\n",
      "122/122 [==============================] - 0s 655us/step - loss: 71.6141 - mae: 6.1141 - val_loss: 33.2102 - val_mae: 4.2073\n",
      "Epoch 526/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 81.9294 - mae: 6.3041 - val_loss: 31.9347 - val_mae: 4.0091\n",
      "Epoch 527/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 76.9235 - mae: 6.3171 - val_loss: 35.6453 - val_mae: 4.4346\n",
      "Epoch 528/850\n",
      "122/122 [==============================] - 0s 606us/step - loss: 73.9015 - mae: 6.1458 - val_loss: 28.1755 - val_mae: 3.7904\n",
      "Epoch 529/850\n",
      "122/122 [==============================] - 0s 643us/step - loss: 66.9136 - mae: 5.9652 - val_loss: 33.4450 - val_mae: 4.2451\n",
      "Epoch 530/850\n",
      "122/122 [==============================] - 0s 705us/step - loss: 74.3077 - mae: 6.1245 - val_loss: 29.8190 - val_mae: 4.0428\n",
      "Epoch 531/850\n",
      "122/122 [==============================] - 0s 611us/step - loss: 75.9624 - mae: 6.1919 - val_loss: 28.7400 - val_mae: 3.8348\n",
      "Epoch 532/850\n",
      "122/122 [==============================] - 0s 663us/step - loss: 71.1582 - mae: 6.0397 - val_loss: 29.9015 - val_mae: 3.8916\n",
      "Epoch 533/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 71.7184 - mae: 6.1018 - val_loss: 34.4589 - val_mae: 4.3791\n",
      "Epoch 534/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 66.0791 - mae: 5.9483 - val_loss: 37.1853 - val_mae: 4.5863\n",
      "Epoch 535/850\n",
      "122/122 [==============================] - 0s 646us/step - loss: 78.8878 - mae: 6.2544 - val_loss: 33.8285 - val_mae: 4.1907\n",
      "Epoch 536/850\n",
      "122/122 [==============================] - 0s 669us/step - loss: 73.7987 - mae: 6.0748 - val_loss: 31.1028 - val_mae: 3.9437\n",
      "Epoch 537/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 67.4925 - mae: 5.9149 - val_loss: 32.8289 - val_mae: 4.1980\n",
      "Epoch 538/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 68.9159 - mae: 6.0018 - val_loss: 31.5987 - val_mae: 4.0910\n",
      "Epoch 539/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 68.7420 - mae: 5.9873 - val_loss: 34.7858 - val_mae: 4.3404\n",
      "Epoch 540/850\n",
      "122/122 [==============================] - 0s 640us/step - loss: 74.3477 - mae: 6.1232 - val_loss: 37.2397 - val_mae: 4.5246\n",
      "Epoch 541/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 67.4215 - mae: 5.9143 - val_loss: 32.2634 - val_mae: 3.9882\n",
      "Epoch 542/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 73.6938 - mae: 6.1730 - val_loss: 27.9053 - val_mae: 3.6419\n",
      "Epoch 543/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 67.3005 - mae: 5.8848 - val_loss: 29.4232 - val_mae: 3.7608\n",
      "Epoch 544/850\n",
      "122/122 [==============================] - 0s 805us/step - loss: 63.2719 - mae: 5.9241 - val_loss: 34.6407 - val_mae: 4.4106\n",
      "Epoch 545/850\n",
      "122/122 [==============================] - 0s 743us/step - loss: 75.1253 - mae: 6.1459 - val_loss: 29.0608 - val_mae: 3.7526\n",
      "Epoch 546/850\n",
      "122/122 [==============================] - 0s 704us/step - loss: 71.7281 - mae: 6.1059 - val_loss: 30.0491 - val_mae: 3.8744\n",
      "Epoch 547/850\n",
      "122/122 [==============================] - 0s 698us/step - loss: 72.0108 - mae: 6.0407 - val_loss: 25.9778 - val_mae: 3.6015\n",
      "Epoch 548/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 67.9012 - mae: 6.0135 - val_loss: 30.3539 - val_mae: 3.9967\n",
      "Epoch 549/850\n",
      "122/122 [==============================] - 0s 652us/step - loss: 67.2044 - mae: 5.9942 - val_loss: 29.6888 - val_mae: 3.9709\n",
      "Epoch 550/850\n",
      "122/122 [==============================] - 0s 688us/step - loss: 65.5450 - mae: 5.8876 - val_loss: 42.3553 - val_mae: 4.9705\n",
      "Epoch 551/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 66.2594 - mae: 5.8864 - val_loss: 32.9848 - val_mae: 4.2172\n",
      "Epoch 552/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 70.9013 - mae: 6.0124 - val_loss: 31.4309 - val_mae: 4.1772\n",
      "Epoch 553/850\n",
      "122/122 [==============================] - 0s 662us/step - loss: 69.8718 - mae: 6.0538 - val_loss: 33.7274 - val_mae: 4.2725\n",
      "Epoch 554/850\n",
      "122/122 [==============================] - 0s 795us/step - loss: 66.3497 - mae: 5.9153 - val_loss: 34.6958 - val_mae: 4.2801\n",
      "Epoch 555/850\n",
      "122/122 [==============================] - 0s 584us/step - loss: 72.7584 - mae: 6.0847 - val_loss: 30.0152 - val_mae: 3.9763\n",
      "Epoch 556/850\n",
      "122/122 [==============================] - 0s 651us/step - loss: 68.8291 - mae: 5.9891 - val_loss: 28.6506 - val_mae: 3.8561\n",
      "Epoch 557/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 66.1828 - mae: 5.8785 - val_loss: 27.1138 - val_mae: 3.7171\n",
      "Epoch 558/850\n",
      "122/122 [==============================] - 0s 596us/step - loss: 70.1037 - mae: 5.9850 - val_loss: 32.3917 - val_mae: 4.2089\n",
      "Epoch 559/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 71.4523 - mae: 6.0178 - val_loss: 36.9629 - val_mae: 4.5054\n",
      "Epoch 560/850\n",
      "122/122 [==============================] - 0s 763us/step - loss: 67.4203 - mae: 5.9834 - val_loss: 33.8986 - val_mae: 4.3090\n",
      "Epoch 561/850\n",
      "122/122 [==============================] - 0s 649us/step - loss: 76.3215 - mae: 6.0704 - val_loss: 33.7641 - val_mae: 4.3958\n",
      "Epoch 562/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 66.8830 - mae: 5.9055 - val_loss: 32.3151 - val_mae: 4.0260\n",
      "Epoch 563/850\n",
      "122/122 [==============================] - 0s 684us/step - loss: 68.0945 - mae: 5.9533 - val_loss: 30.7797 - val_mae: 4.0802\n",
      "Epoch 564/850\n",
      "122/122 [==============================] - 0s 701us/step - loss: 75.0882 - mae: 6.1368 - val_loss: 32.0586 - val_mae: 3.9950\n",
      "Epoch 565/850\n",
      "122/122 [==============================] - 0s 727us/step - loss: 64.6609 - mae: 5.7796 - val_loss: 29.7682 - val_mae: 3.8677\n",
      "Epoch 566/850\n",
      "122/122 [==============================] - 0s 723us/step - loss: 70.0029 - mae: 5.9496 - val_loss: 42.4259 - val_mae: 5.0291\n",
      "Epoch 567/850\n",
      "122/122 [==============================] - 0s 775us/step - loss: 71.1349 - mae: 5.9899 - val_loss: 28.8782 - val_mae: 3.7564\n",
      "Epoch 568/850\n",
      "122/122 [==============================] - 0s 789us/step - loss: 67.1950 - mae: 5.9575 - val_loss: 29.9618 - val_mae: 3.8817\n",
      "Epoch 569/850\n",
      "122/122 [==============================] - 0s 745us/step - loss: 67.5900 - mae: 5.9174 - val_loss: 29.8431 - val_mae: 3.9506\n",
      "Epoch 570/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 63.8549 - mae: 5.8368 - val_loss: 45.1433 - val_mae: 5.2171\n",
      "Epoch 571/850\n",
      "122/122 [==============================] - 0s 705us/step - loss: 65.0602 - mae: 5.7962 - val_loss: 30.1854 - val_mae: 3.8988\n",
      "Epoch 572/850\n",
      "122/122 [==============================] - 0s 715us/step - loss: 64.7117 - mae: 5.7886 - val_loss: 36.7864 - val_mae: 4.5670\n",
      "Epoch 573/850\n",
      "122/122 [==============================] - 0s 897us/step - loss: 65.6251 - mae: 5.8869 - val_loss: 32.1066 - val_mae: 4.1889\n",
      "Epoch 574/850\n",
      "122/122 [==============================] - 0s 832us/step - loss: 68.6796 - mae: 5.9205 - val_loss: 35.2659 - val_mae: 4.2298\n",
      "Epoch 575/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 65.2258 - mae: 5.7924 - val_loss: 28.9534 - val_mae: 3.8988\n",
      "Epoch 576/850\n",
      "122/122 [==============================] - 0s 646us/step - loss: 69.6156 - mae: 5.9526 - val_loss: 28.5063 - val_mae: 3.7994\n",
      "Epoch 577/850\n",
      "122/122 [==============================] - 0s 678us/step - loss: 64.8380 - mae: 5.8369 - val_loss: 33.8921 - val_mae: 4.2947\n",
      "Epoch 578/850\n",
      "122/122 [==============================] - 0s 661us/step - loss: 68.2353 - mae: 5.8723 - val_loss: 32.7245 - val_mae: 4.2901\n",
      "Epoch 579/850\n",
      "122/122 [==============================] - 0s 685us/step - loss: 61.9412 - mae: 5.7170 - val_loss: 33.7052 - val_mae: 4.3072\n",
      "Epoch 580/850\n",
      "122/122 [==============================] - 0s 674us/step - loss: 69.5761 - mae: 5.9924 - val_loss: 29.1060 - val_mae: 3.8908\n",
      "Epoch 581/850\n",
      "122/122 [==============================] - 0s 646us/step - loss: 66.8034 - mae: 5.9135 - val_loss: 38.0332 - val_mae: 4.7074\n",
      "Epoch 582/850\n",
      "122/122 [==============================] - 0s 652us/step - loss: 66.5450 - mae: 5.8877 - val_loss: 36.0099 - val_mae: 4.5100\n",
      "Epoch 583/850\n",
      "122/122 [==============================] - 0s 640us/step - loss: 66.3824 - mae: 5.9067 - val_loss: 36.1357 - val_mae: 4.4786\n",
      "Epoch 584/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 63.6982 - mae: 5.8474 - val_loss: 39.4707 - val_mae: 4.8120\n",
      "Epoch 585/850\n",
      "122/122 [==============================] - 0s 647us/step - loss: 65.7592 - mae: 5.8522 - val_loss: 29.5674 - val_mae: 3.9497\n",
      "Epoch 586/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 68.5087 - mae: 5.8668 - val_loss: 38.3922 - val_mae: 4.6057\n",
      "Epoch 587/850\n",
      "122/122 [==============================] - 0s 703us/step - loss: 62.8430 - mae: 5.7977 - val_loss: 31.9302 - val_mae: 4.0049\n",
      "Epoch 588/850\n",
      "122/122 [==============================] - 0s 671us/step - loss: 64.8645 - mae: 5.8297 - val_loss: 30.4179 - val_mae: 4.0005\n",
      "Epoch 589/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 66.6604 - mae: 5.9089 - val_loss: 40.1717 - val_mae: 4.7620\n",
      "Epoch 590/850\n",
      "122/122 [==============================] - 0s 671us/step - loss: 67.1738 - mae: 5.9302 - val_loss: 32.4597 - val_mae: 4.1989\n",
      "Epoch 591/850\n",
      "122/122 [==============================] - 0s 655us/step - loss: 65.3444 - mae: 5.9515 - val_loss: 38.4975 - val_mae: 4.7739\n",
      "Epoch 592/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 64.8996 - mae: 5.8299 - val_loss: 31.0820 - val_mae: 4.0440\n",
      "Epoch 593/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 62.0989 - mae: 5.6996 - val_loss: 32.0526 - val_mae: 4.3106\n",
      "Epoch 594/850\n",
      "122/122 [==============================] - 0s 725us/step - loss: 64.1849 - mae: 5.8663 - val_loss: 29.8797 - val_mae: 3.9053\n",
      "Epoch 595/850\n",
      "122/122 [==============================] - 0s 698us/step - loss: 67.1380 - mae: 5.9165 - val_loss: 27.8751 - val_mae: 3.7796\n",
      "Epoch 596/850\n",
      "122/122 [==============================] - 0s 630us/step - loss: 67.6716 - mae: 5.9043 - val_loss: 30.3184 - val_mae: 4.1143\n",
      "Epoch 597/850\n",
      "122/122 [==============================] - 0s 631us/step - loss: 64.4988 - mae: 5.7545 - val_loss: 31.7433 - val_mae: 4.1513\n",
      "Epoch 598/850\n",
      "122/122 [==============================] - 0s 672us/step - loss: 62.5111 - mae: 5.7291 - val_loss: 34.6274 - val_mae: 4.4161\n",
      "Epoch 599/850\n",
      "122/122 [==============================] - 0s 619us/step - loss: 64.2947 - mae: 5.8500 - val_loss: 46.5339 - val_mae: 5.2839\n",
      "Epoch 600/850\n",
      "122/122 [==============================] - 0s 633us/step - loss: 69.2252 - mae: 5.8822 - val_loss: 36.7921 - val_mae: 4.3941\n",
      "Epoch 601/850\n",
      "122/122 [==============================] - 0s 659us/step - loss: 65.5954 - mae: 5.8185 - val_loss: 38.3978 - val_mae: 4.7659\n",
      "Epoch 602/850\n",
      "122/122 [==============================] - 0s 760us/step - loss: 66.0156 - mae: 5.8722 - val_loss: 33.4425 - val_mae: 4.3058\n",
      "Epoch 603/850\n",
      "122/122 [==============================] - 0s 666us/step - loss: 63.8092 - mae: 5.8085 - val_loss: 30.7387 - val_mae: 4.0542\n",
      "Epoch 604/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 66.5844 - mae: 5.8540 - val_loss: 35.5983 - val_mae: 4.4458\n",
      "Epoch 605/850\n",
      "122/122 [==============================] - 0s 660us/step - loss: 65.6925 - mae: 5.7852 - val_loss: 40.1124 - val_mae: 4.7818\n",
      "Epoch 606/850\n",
      "122/122 [==============================] - 0s 766us/step - loss: 67.8775 - mae: 5.9340 - val_loss: 30.2709 - val_mae: 3.9427\n",
      "Epoch 607/850\n",
      "122/122 [==============================] - 0s 749us/step - loss: 66.8797 - mae: 5.8355 - val_loss: 38.1767 - val_mae: 4.6198\n",
      "Epoch 608/850\n",
      "122/122 [==============================] - 0s 785us/step - loss: 62.4679 - mae: 5.7565 - val_loss: 33.7465 - val_mae: 4.2823\n",
      "Epoch 609/850\n",
      "122/122 [==============================] - 0s 698us/step - loss: 64.4535 - mae: 5.8213 - val_loss: 36.1585 - val_mae: 4.5161\n",
      "Epoch 610/850\n",
      "122/122 [==============================] - 0s 736us/step - loss: 61.6235 - mae: 5.7293 - val_loss: 29.8370 - val_mae: 3.8258\n",
      "Epoch 611/850\n",
      "122/122 [==============================] - 0s 726us/step - loss: 65.3559 - mae: 5.8284 - val_loss: 29.7559 - val_mae: 3.9784\n",
      "Epoch 612/850\n",
      "122/122 [==============================] - 0s 805us/step - loss: 60.4233 - mae: 5.7504 - val_loss: 36.4115 - val_mae: 4.4642\n",
      "Epoch 613/850\n",
      "122/122 [==============================] - 0s 752us/step - loss: 62.7248 - mae: 5.7382 - val_loss: 37.1349 - val_mae: 4.5128\n",
      "Epoch 614/850\n",
      "122/122 [==============================] - 0s 787us/step - loss: 64.3591 - mae: 5.8218 - val_loss: 40.7003 - val_mae: 4.8850\n",
      "Epoch 615/850\n",
      "122/122 [==============================] - 0s 815us/step - loss: 63.8095 - mae: 5.7583 - val_loss: 43.5037 - val_mae: 5.0101\n",
      "Epoch 616/850\n",
      "122/122 [==============================] - 0s 813us/step - loss: 63.2195 - mae: 5.7750 - val_loss: 34.4138 - val_mae: 4.1999\n",
      "Epoch 617/850\n",
      "122/122 [==============================] - 0s 774us/step - loss: 60.8850 - mae: 5.7105 - val_loss: 29.3971 - val_mae: 3.8980\n",
      "Epoch 618/850\n",
      "122/122 [==============================] - 0s 655us/step - loss: 62.8517 - mae: 5.8299 - val_loss: 38.0816 - val_mae: 4.5753\n",
      "Epoch 619/850\n",
      "122/122 [==============================] - 0s 720us/step - loss: 61.2306 - mae: 5.6928 - val_loss: 31.0252 - val_mae: 3.9670\n",
      "Epoch 620/850\n",
      "122/122 [==============================] - 0s 797us/step - loss: 62.0633 - mae: 5.7992 - val_loss: 35.8291 - val_mae: 4.3079\n",
      "Epoch 621/850\n",
      "122/122 [==============================] - 0s 723us/step - loss: 60.0363 - mae: 5.6609 - val_loss: 32.9238 - val_mae: 4.1041\n",
      "Epoch 622/850\n",
      "122/122 [==============================] - 0s 710us/step - loss: 63.9354 - mae: 5.7795 - val_loss: 33.3183 - val_mae: 4.1692\n",
      "Epoch 623/850\n",
      "122/122 [==============================] - 0s 790us/step - loss: 62.5087 - mae: 5.7779 - val_loss: 30.5989 - val_mae: 3.8745\n",
      "Epoch 624/850\n",
      "122/122 [==============================] - 0s 782us/step - loss: 60.9730 - mae: 5.6446 - val_loss: 44.2154 - val_mae: 5.0823\n",
      "Epoch 625/850\n",
      "122/122 [==============================] - 0s 681us/step - loss: 62.4868 - mae: 5.7642 - val_loss: 38.3937 - val_mae: 4.6256\n",
      "Epoch 626/850\n",
      "122/122 [==============================] - 0s 689us/step - loss: 62.1655 - mae: 5.6965 - val_loss: 43.5623 - val_mae: 4.9294\n",
      "Epoch 627/850\n",
      "122/122 [==============================] - 0s 757us/step - loss: 62.4962 - mae: 5.7132 - val_loss: 37.1390 - val_mae: 4.4570\n",
      "Epoch 628/850\n",
      "122/122 [==============================] - 0s 734us/step - loss: 58.7806 - mae: 5.6251 - val_loss: 31.7953 - val_mae: 4.1290\n",
      "Epoch 629/850\n",
      "122/122 [==============================] - 0s 732us/step - loss: 61.4620 - mae: 5.7269 - val_loss: 30.9020 - val_mae: 3.9788\n",
      "Epoch 630/850\n",
      "122/122 [==============================] - 0s 795us/step - loss: 59.6505 - mae: 5.6401 - val_loss: 30.6692 - val_mae: 4.0829\n",
      "Epoch 631/850\n",
      "122/122 [==============================] - 0s 688us/step - loss: 59.3948 - mae: 5.6113 - val_loss: 30.4338 - val_mae: 4.0893\n",
      "Epoch 632/850\n",
      "122/122 [==============================] - 0s 689us/step - loss: 62.5251 - mae: 5.6787 - val_loss: 32.2360 - val_mae: 4.1991\n",
      "Epoch 633/850\n",
      "122/122 [==============================] - 0s 607us/step - loss: 64.9352 - mae: 5.8130 - val_loss: 41.0015 - val_mae: 4.9027\n",
      "Epoch 634/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 62.7484 - mae: 5.8033 - val_loss: 35.9543 - val_mae: 4.4390\n",
      "Epoch 635/850\n",
      "122/122 [==============================] - 0s 666us/step - loss: 60.7436 - mae: 5.6733 - val_loss: 33.0360 - val_mae: 4.1645\n",
      "Epoch 636/850\n",
      "122/122 [==============================] - 0s 670us/step - loss: 65.3028 - mae: 5.7397 - val_loss: 32.7731 - val_mae: 4.2115\n",
      "Epoch 637/850\n",
      "122/122 [==============================] - 0s 653us/step - loss: 59.8884 - mae: 5.6732 - val_loss: 34.1552 - val_mae: 4.2066\n",
      "Epoch 638/850\n",
      "122/122 [==============================] - 0s 638us/step - loss: 65.9181 - mae: 5.8605 - val_loss: 38.6341 - val_mae: 4.6473\n",
      "Epoch 639/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 62.5117 - mae: 5.7631 - val_loss: 28.7480 - val_mae: 3.8727\n",
      "Epoch 640/850\n",
      "122/122 [==============================] - 0s 652us/step - loss: 58.3863 - mae: 5.5823 - val_loss: 33.8408 - val_mae: 4.2911\n",
      "Epoch 641/850\n",
      "122/122 [==============================] - 0s 636us/step - loss: 64.9590 - mae: 5.6654 - val_loss: 45.5626 - val_mae: 5.2068\n",
      "Epoch 642/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 61.7973 - mae: 5.7200 - val_loss: 35.2047 - val_mae: 4.2981\n",
      "Epoch 643/850\n",
      "122/122 [==============================] - 0s 697us/step - loss: 61.0588 - mae: 5.6285 - val_loss: 29.5994 - val_mae: 3.8603\n",
      "Epoch 644/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 63.1504 - mae: 5.7643 - val_loss: 34.0907 - val_mae: 4.2509\n",
      "Epoch 645/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 62.0443 - mae: 5.7120 - val_loss: 31.3765 - val_mae: 4.0073\n",
      "Epoch 646/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 61.2259 - mae: 5.7033 - val_loss: 39.7047 - val_mae: 4.6987\n",
      "Epoch 647/850\n",
      "122/122 [==============================] - 0s 762us/step - loss: 59.6573 - mae: 5.6000 - val_loss: 38.1767 - val_mae: 4.6486\n",
      "Epoch 648/850\n",
      "122/122 [==============================] - 0s 729us/step - loss: 63.0406 - mae: 5.7673 - val_loss: 34.2273 - val_mae: 4.2322\n",
      "Epoch 649/850\n",
      "122/122 [==============================] - 0s 599us/step - loss: 60.6047 - mae: 5.6925 - val_loss: 32.4563 - val_mae: 4.1531\n",
      "Epoch 650/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 61.2755 - mae: 5.6567 - val_loss: 34.2981 - val_mae: 4.2648\n",
      "Epoch 651/850\n",
      "122/122 [==============================] - 0s 725us/step - loss: 65.6465 - mae: 5.7870 - val_loss: 39.8890 - val_mae: 4.7627\n",
      "Epoch 652/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 63.4841 - mae: 5.7751 - val_loss: 35.0333 - val_mae: 4.2833\n",
      "Epoch 653/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 62.0496 - mae: 5.7571 - val_loss: 34.1252 - val_mae: 4.2727\n",
      "Epoch 654/850\n",
      "122/122 [==============================] - 0s 714us/step - loss: 61.6610 - mae: 5.6483 - val_loss: 36.9380 - val_mae: 4.2839\n",
      "Epoch 655/850\n",
      "122/122 [==============================] - 0s 704us/step - loss: 57.3376 - mae: 5.6188 - val_loss: 32.4372 - val_mae: 3.9836\n",
      "Epoch 656/850\n",
      "122/122 [==============================] - 0s 681us/step - loss: 57.7637 - mae: 5.5962 - val_loss: 40.0342 - val_mae: 4.8278\n",
      "Epoch 657/850\n",
      "122/122 [==============================] - 0s 659us/step - loss: 62.0393 - mae: 5.7980 - val_loss: 35.3488 - val_mae: 4.3024\n",
      "Epoch 658/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 62.4675 - mae: 5.7490 - val_loss: 34.0259 - val_mae: 4.2551\n",
      "Epoch 659/850\n",
      "122/122 [==============================] - 0s 694us/step - loss: 58.0375 - mae: 5.5172 - val_loss: 35.9845 - val_mae: 4.4290\n",
      "Epoch 660/850\n",
      "122/122 [==============================] - 0s 739us/step - loss: 58.5216 - mae: 5.6014 - val_loss: 31.7390 - val_mae: 4.0521\n",
      "Epoch 661/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 57.2651 - mae: 5.5702 - val_loss: 32.4130 - val_mae: 4.1648\n",
      "Epoch 662/850\n",
      "122/122 [==============================] - 0s 685us/step - loss: 58.6429 - mae: 5.6316 - val_loss: 29.1702 - val_mae: 3.9200\n",
      "Epoch 663/850\n",
      "122/122 [==============================] - 0s 617us/step - loss: 64.4872 - mae: 5.7763 - val_loss: 32.4767 - val_mae: 4.0101\n",
      "Epoch 664/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 56.3028 - mae: 5.5213 - val_loss: 33.7931 - val_mae: 4.2858\n",
      "Epoch 665/850\n",
      "122/122 [==============================] - 0s 717us/step - loss: 58.3125 - mae: 5.6109 - val_loss: 29.8599 - val_mae: 3.8523\n",
      "Epoch 666/850\n",
      "122/122 [==============================] - 0s 801us/step - loss: 60.5661 - mae: 5.6791 - val_loss: 32.8393 - val_mae: 4.0701\n",
      "Epoch 667/850\n",
      "122/122 [==============================] - 0s 672us/step - loss: 60.2369 - mae: 5.6484 - val_loss: 36.1677 - val_mae: 4.3836\n",
      "Epoch 668/850\n",
      "122/122 [==============================] - 0s 666us/step - loss: 56.7990 - mae: 5.5433 - val_loss: 33.1686 - val_mae: 4.1503\n",
      "Epoch 669/850\n",
      "122/122 [==============================] - 0s 777us/step - loss: 58.1312 - mae: 5.4978 - val_loss: 35.1144 - val_mae: 4.3981\n",
      "Epoch 670/850\n",
      "122/122 [==============================] - 0s 698us/step - loss: 59.2484 - mae: 5.5642 - val_loss: 36.9075 - val_mae: 4.6005\n",
      "Epoch 671/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 60.0228 - mae: 5.6790 - val_loss: 29.6386 - val_mae: 3.8934\n",
      "Epoch 672/850\n",
      "122/122 [==============================] - 0s 705us/step - loss: 60.2155 - mae: 5.6268 - val_loss: 38.6405 - val_mae: 4.7091\n",
      "Epoch 673/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 59.4326 - mae: 5.5738 - val_loss: 34.3404 - val_mae: 4.2599\n",
      "Epoch 674/850\n",
      "122/122 [==============================] - 0s 672us/step - loss: 60.1561 - mae: 5.6753 - val_loss: 35.5406 - val_mae: 4.3342\n",
      "Epoch 675/850\n",
      "122/122 [==============================] - 0s 732us/step - loss: 58.1520 - mae: 5.6105 - val_loss: 33.4667 - val_mae: 4.1075\n",
      "Epoch 676/850\n",
      "122/122 [==============================] - 0s 749us/step - loss: 59.0929 - mae: 5.5962 - val_loss: 30.4855 - val_mae: 3.8856\n",
      "Epoch 677/850\n",
      "122/122 [==============================] - 0s 662us/step - loss: 61.1337 - mae: 5.7003 - val_loss: 35.4785 - val_mae: 4.3725\n",
      "Epoch 678/850\n",
      "122/122 [==============================] - 0s 692us/step - loss: 63.1790 - mae: 5.7734 - val_loss: 33.9367 - val_mae: 4.1150\n",
      "Epoch 679/850\n",
      "122/122 [==============================] - 0s 859us/step - loss: 60.8005 - mae: 5.7147 - val_loss: 33.5644 - val_mae: 4.2133\n",
      "Epoch 680/850\n",
      "122/122 [==============================] - 0s 749us/step - loss: 57.1968 - mae: 5.5479 - val_loss: 35.7704 - val_mae: 4.3645\n",
      "Epoch 681/850\n",
      "122/122 [==============================] - 0s 729us/step - loss: 59.3642 - mae: 5.6567 - val_loss: 32.9178 - val_mae: 4.1022\n",
      "Epoch 682/850\n",
      "122/122 [==============================] - 0s 750us/step - loss: 56.0059 - mae: 5.5599 - val_loss: 35.6317 - val_mae: 4.4293\n",
      "Epoch 683/850\n",
      "122/122 [==============================] - 0s 757us/step - loss: 59.0361 - mae: 5.6190 - val_loss: 47.0497 - val_mae: 5.1956\n",
      "Epoch 684/850\n",
      "122/122 [==============================] - 0s 813us/step - loss: 56.9022 - mae: 5.5294 - val_loss: 38.4070 - val_mae: 4.6772\n",
      "Epoch 685/850\n",
      "122/122 [==============================] - 0s 712us/step - loss: 63.1156 - mae: 5.7190 - val_loss: 56.3904 - val_mae: 5.8573\n",
      "Epoch 686/850\n",
      "122/122 [==============================] - 0s 723us/step - loss: 60.3722 - mae: 5.6724 - val_loss: 31.3490 - val_mae: 4.0567\n",
      "Epoch 687/850\n",
      "122/122 [==============================] - 0s 777us/step - loss: 60.3694 - mae: 5.5529 - val_loss: 34.4048 - val_mae: 4.3534\n",
      "Epoch 688/850\n",
      "122/122 [==============================] - 0s 729us/step - loss: 62.4897 - mae: 5.7726 - val_loss: 32.0327 - val_mae: 4.0765\n",
      "Epoch 689/850\n",
      "122/122 [==============================] - 0s 757us/step - loss: 58.2999 - mae: 5.5841 - val_loss: 32.4008 - val_mae: 4.1211\n",
      "Epoch 690/850\n",
      "122/122 [==============================] - 0s 794us/step - loss: 58.8571 - mae: 5.6110 - val_loss: 35.6400 - val_mae: 4.2602\n",
      "Epoch 691/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 59.8324 - mae: 5.6455 - val_loss: 34.6053 - val_mae: 4.2210\n",
      "Epoch 692/850\n",
      "122/122 [==============================] - 0s 651us/step - loss: 62.5291 - mae: 5.7521 - val_loss: 39.6495 - val_mae: 4.6870\n",
      "Epoch 693/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 59.7184 - mae: 5.6615 - val_loss: 37.3551 - val_mae: 4.5889\n",
      "Epoch 694/850\n",
      "122/122 [==============================] - 0s 703us/step - loss: 58.9725 - mae: 5.6767 - val_loss: 31.8283 - val_mae: 4.0734\n",
      "Epoch 695/850\n",
      "122/122 [==============================] - 0s 686us/step - loss: 57.7410 - mae: 5.5676 - val_loss: 33.4540 - val_mae: 4.2122\n",
      "Epoch 696/850\n",
      "122/122 [==============================] - 0s 732us/step - loss: 60.5304 - mae: 5.6268 - val_loss: 35.1566 - val_mae: 4.3375\n",
      "Epoch 697/850\n",
      "122/122 [==============================] - 0s 665us/step - loss: 55.0101 - mae: 5.4574 - val_loss: 32.7018 - val_mae: 4.0343\n",
      "Epoch 698/850\n",
      "122/122 [==============================] - 0s 658us/step - loss: 55.1091 - mae: 5.5408 - val_loss: 43.7850 - val_mae: 4.9231\n",
      "Epoch 699/850\n",
      "122/122 [==============================] - 0s 659us/step - loss: 60.0267 - mae: 5.6563 - val_loss: 32.8315 - val_mae: 4.0871\n",
      "Epoch 700/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 59.2635 - mae: 5.6141 - val_loss: 35.7004 - val_mae: 4.3732\n",
      "Epoch 701/850\n",
      "122/122 [==============================] - 0s 743us/step - loss: 56.4756 - mae: 5.5037 - val_loss: 31.4078 - val_mae: 4.0012\n",
      "Epoch 702/850\n",
      "122/122 [==============================] - 0s 729us/step - loss: 57.9025 - mae: 5.5508 - val_loss: 34.1249 - val_mae: 3.9709\n",
      "Epoch 703/850\n",
      "122/122 [==============================] - 0s 643us/step - loss: 58.1203 - mae: 5.5499 - val_loss: 38.1415 - val_mae: 4.3232\n",
      "Epoch 704/850\n",
      "122/122 [==============================] - 0s 700us/step - loss: 57.0775 - mae: 5.5663 - val_loss: 32.0647 - val_mae: 4.0535\n",
      "Epoch 705/850\n",
      "122/122 [==============================] - 0s 683us/step - loss: 56.0143 - mae: 5.4948 - val_loss: 36.9779 - val_mae: 4.5865\n",
      "Epoch 706/850\n",
      "122/122 [==============================] - 0s 652us/step - loss: 53.9009 - mae: 5.4602 - val_loss: 37.0361 - val_mae: 4.5296\n",
      "Epoch 707/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 58.4050 - mae: 5.5828 - val_loss: 32.7325 - val_mae: 4.0909\n",
      "Epoch 708/850\n",
      "122/122 [==============================] - 0s 638us/step - loss: 54.3096 - mae: 5.3520 - val_loss: 32.5790 - val_mae: 4.1230\n",
      "Epoch 709/850\n",
      "122/122 [==============================] - 0s 853us/step - loss: 55.8853 - mae: 5.4231 - val_loss: 38.7030 - val_mae: 4.5866\n",
      "Epoch 710/850\n",
      "122/122 [==============================] - 0s 670us/step - loss: 57.7710 - mae: 5.5175 - val_loss: 32.9466 - val_mae: 4.1104\n",
      "Epoch 711/850\n",
      "122/122 [==============================] - 0s 590us/step - loss: 53.7359 - mae: 5.3831 - val_loss: 28.7445 - val_mae: 3.8665\n",
      "Epoch 712/850\n",
      "122/122 [==============================] - 0s 953us/step - loss: 56.8050 - mae: 5.5406 - val_loss: 32.7944 - val_mae: 4.2716\n",
      "Epoch 713/850\n",
      "122/122 [==============================] - 0s 723us/step - loss: 56.1883 - mae: 5.4306 - val_loss: 29.4188 - val_mae: 3.8798\n",
      "Epoch 714/850\n",
      "122/122 [==============================] - 0s 699us/step - loss: 54.9124 - mae: 5.4438 - val_loss: 34.3431 - val_mae: 4.2822\n",
      "Epoch 715/850\n",
      "122/122 [==============================] - 0s 813us/step - loss: 56.8060 - mae: 5.5187 - val_loss: 29.2562 - val_mae: 3.8847\n",
      "Epoch 716/850\n",
      "122/122 [==============================] - 0s 700us/step - loss: 58.9653 - mae: 5.5955 - val_loss: 33.7422 - val_mae: 4.1942\n",
      "Epoch 717/850\n",
      "122/122 [==============================] - 0s 683us/step - loss: 57.0849 - mae: 5.5457 - val_loss: 36.6527 - val_mae: 4.5172\n",
      "Epoch 718/850\n",
      "122/122 [==============================] - 0s 739us/step - loss: 55.0344 - mae: 5.4145 - val_loss: 36.3931 - val_mae: 4.5096\n",
      "Epoch 719/850\n",
      "122/122 [==============================] - 0s 720us/step - loss: 58.1775 - mae: 5.6043 - val_loss: 40.8583 - val_mae: 4.6802\n",
      "Epoch 720/850\n",
      "122/122 [==============================] - 0s 662us/step - loss: 56.6930 - mae: 5.4715 - val_loss: 36.5127 - val_mae: 4.4558\n",
      "Epoch 721/850\n",
      "122/122 [==============================] - 0s 795us/step - loss: 54.8561 - mae: 5.3868 - val_loss: 45.9807 - val_mae: 5.1577\n",
      "Epoch 722/850\n",
      "122/122 [==============================] - 0s 807us/step - loss: 56.3445 - mae: 5.5215 - val_loss: 29.5922 - val_mae: 3.8186\n",
      "Epoch 723/850\n",
      "122/122 [==============================] - 0s 707us/step - loss: 54.8073 - mae: 5.4372 - val_loss: 44.6345 - val_mae: 5.1650\n",
      "Epoch 724/850\n",
      "122/122 [==============================] - 0s 610us/step - loss: 54.7403 - mae: 5.4829 - val_loss: 32.1975 - val_mae: 4.1334\n",
      "Epoch 725/850\n",
      "122/122 [==============================] - 0s 582us/step - loss: 53.4154 - mae: 5.3673 - val_loss: 31.6327 - val_mae: 4.0296\n",
      "Epoch 726/850\n",
      "122/122 [==============================] - 0s 672us/step - loss: 58.7685 - mae: 5.5582 - val_loss: 32.0836 - val_mae: 4.1047\n",
      "Epoch 727/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 57.1485 - mae: 5.5341 - val_loss: 42.7816 - val_mae: 5.0920\n",
      "Epoch 728/850\n",
      "122/122 [==============================] - 0s 655us/step - loss: 55.9872 - mae: 5.4921 - val_loss: 34.6508 - val_mae: 4.3383\n",
      "Epoch 729/850\n",
      "122/122 [==============================] - 0s 663us/step - loss: 53.0989 - mae: 5.3940 - val_loss: 38.0299 - val_mae: 4.4480\n",
      "Epoch 730/850\n",
      "122/122 [==============================] - 0s 626us/step - loss: 53.2460 - mae: 5.3074 - val_loss: 30.9273 - val_mae: 3.9218\n",
      "Epoch 731/850\n",
      "122/122 [==============================] - 0s 573us/step - loss: 56.5464 - mae: 5.5187 - val_loss: 37.6047 - val_mae: 4.5478\n",
      "Epoch 732/850\n",
      "122/122 [==============================] - 0s 623us/step - loss: 56.3651 - mae: 5.5028 - val_loss: 35.1377 - val_mae: 4.2249\n",
      "Epoch 733/850\n",
      "122/122 [==============================] - 0s 669us/step - loss: 57.7831 - mae: 5.5925 - val_loss: 37.4495 - val_mae: 4.4609\n",
      "Epoch 734/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 56.7705 - mae: 5.5155 - val_loss: 31.3983 - val_mae: 3.9315\n",
      "Epoch 735/850\n",
      "122/122 [==============================] - 0s 662us/step - loss: 54.5866 - mae: 5.3783 - val_loss: 36.8527 - val_mae: 4.4013\n",
      "Epoch 736/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 57.5738 - mae: 5.5135 - val_loss: 37.4352 - val_mae: 4.4598\n",
      "Epoch 737/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 56.7159 - mae: 5.5674 - val_loss: 41.1635 - val_mae: 4.6353\n",
      "Epoch 738/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 53.9749 - mae: 5.3062 - val_loss: 35.5416 - val_mae: 4.3705\n",
      "Epoch 739/850\n",
      "122/122 [==============================] - 0s 645us/step - loss: 58.0373 - mae: 5.5398 - val_loss: 35.6881 - val_mae: 4.3016\n",
      "Epoch 740/850\n",
      "122/122 [==============================] - 0s 677us/step - loss: 55.6463 - mae: 5.5096 - val_loss: 31.6011 - val_mae: 3.9532\n",
      "Epoch 741/850\n",
      "122/122 [==============================] - 0s 621us/step - loss: 54.6996 - mae: 5.4032 - val_loss: 28.6963 - val_mae: 3.7920\n",
      "Epoch 742/850\n",
      "122/122 [==============================] - 0s 640us/step - loss: 56.6212 - mae: 5.5405 - val_loss: 31.1946 - val_mae: 3.9054\n",
      "Epoch 743/850\n",
      "122/122 [==============================] - 0s 662us/step - loss: 57.4433 - mae: 5.4852 - val_loss: 34.3397 - val_mae: 4.2911\n",
      "Epoch 744/850\n",
      "122/122 [==============================] - 0s 612us/step - loss: 56.9743 - mae: 5.5132 - val_loss: 38.8101 - val_mae: 4.5598\n",
      "Epoch 745/850\n",
      "122/122 [==============================] - 0s 634us/step - loss: 56.4769 - mae: 5.5122 - val_loss: 38.9047 - val_mae: 4.5119\n",
      "Epoch 746/850\n",
      "122/122 [==============================] - 0s 709us/step - loss: 58.1607 - mae: 5.4036 - val_loss: 33.7564 - val_mae: 4.1495\n",
      "Epoch 747/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 59.4923 - mae: 5.5743 - val_loss: 34.9955 - val_mae: 4.1975\n",
      "Epoch 748/850\n",
      "122/122 [==============================] - 0s 704us/step - loss: 60.0782 - mae: 5.6012 - val_loss: 36.7393 - val_mae: 4.3613\n",
      "Epoch 749/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 57.3926 - mae: 5.6032 - val_loss: 33.1666 - val_mae: 4.0644\n",
      "Epoch 750/850\n",
      "122/122 [==============================] - 0s 597us/step - loss: 55.3620 - mae: 5.3357 - val_loss: 34.4220 - val_mae: 4.1607\n",
      "Epoch 751/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 56.5633 - mae: 5.5409 - val_loss: 33.1935 - val_mae: 3.9900\n",
      "Epoch 752/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 58.0182 - mae: 5.4976 - val_loss: 37.5770 - val_mae: 4.2309\n",
      "Epoch 753/850\n",
      "122/122 [==============================] - 0s 675us/step - loss: 55.8988 - mae: 5.4851 - val_loss: 34.9412 - val_mae: 4.1564\n",
      "Epoch 754/850\n",
      "122/122 [==============================] - 0s 628us/step - loss: 53.4547 - mae: 5.3609 - val_loss: 32.3661 - val_mae: 3.9254\n",
      "Epoch 755/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 57.4784 - mae: 5.5004 - val_loss: 36.6639 - val_mae: 4.2940\n",
      "Epoch 756/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 55.8541 - mae: 5.4922 - val_loss: 31.8547 - val_mae: 4.0520\n",
      "Epoch 757/850\n",
      "122/122 [==============================] - 0s 603us/step - loss: 55.2604 - mae: 5.3989 - val_loss: 31.7696 - val_mae: 4.0006\n",
      "Epoch 758/850\n",
      "122/122 [==============================] - 0s 716us/step - loss: 58.8513 - mae: 5.5714 - val_loss: 40.8264 - val_mae: 4.7261\n",
      "Epoch 759/850\n",
      "122/122 [==============================] - 0s 645us/step - loss: 54.9739 - mae: 5.4696 - val_loss: 32.1529 - val_mae: 4.0105\n",
      "Epoch 760/850\n",
      "122/122 [==============================] - 0s 652us/step - loss: 57.8507 - mae: 5.4420 - val_loss: 32.3202 - val_mae: 4.0381\n",
      "Epoch 761/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 55.6968 - mae: 5.5164 - val_loss: 36.6612 - val_mae: 4.3225\n",
      "Epoch 762/850\n",
      "122/122 [==============================] - 0s 606us/step - loss: 53.7793 - mae: 5.3561 - val_loss: 37.5529 - val_mae: 4.3104\n",
      "Epoch 763/850\n",
      "122/122 [==============================] - 0s 622us/step - loss: 62.5872 - mae: 5.7090 - val_loss: 35.8990 - val_mae: 4.1670\n",
      "Epoch 764/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 54.5049 - mae: 5.4637 - val_loss: 33.3963 - val_mae: 3.9150\n",
      "Epoch 765/850\n",
      "122/122 [==============================] - 0s 649us/step - loss: 53.8783 - mae: 5.3643 - val_loss: 43.4552 - val_mae: 4.8953\n",
      "Epoch 766/850\n",
      "122/122 [==============================] - 0s 643us/step - loss: 59.9709 - mae: 5.5772 - val_loss: 40.2253 - val_mae: 4.6757\n",
      "Epoch 767/850\n",
      "122/122 [==============================] - 0s 624us/step - loss: 53.8829 - mae: 5.4225 - val_loss: 42.0255 - val_mae: 4.4019\n",
      "Epoch 768/850\n",
      "122/122 [==============================] - 0s 615us/step - loss: 58.5164 - mae: 5.4200 - val_loss: 30.0646 - val_mae: 3.8864\n",
      "Epoch 769/850\n",
      "122/122 [==============================] - 0s 647us/step - loss: 59.7769 - mae: 5.5324 - val_loss: 34.5793 - val_mae: 4.1537\n",
      "Epoch 770/850\n",
      "122/122 [==============================] - 0s 598us/step - loss: 57.3155 - mae: 5.4677 - val_loss: 40.7820 - val_mae: 4.6149\n",
      "Epoch 771/850\n",
      "122/122 [==============================] - 0s 640us/step - loss: 56.9986 - mae: 5.4737 - val_loss: 37.1772 - val_mae: 4.2897\n",
      "Epoch 772/850\n",
      "122/122 [==============================] - 0s 707us/step - loss: 54.6678 - mae: 5.4241 - val_loss: 33.3228 - val_mae: 4.0935\n",
      "Epoch 773/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 72.0330 - mae: 5.6577 - val_loss: 32.0375 - val_mae: 4.0103\n",
      "Epoch 774/850\n",
      "122/122 [==============================] - 0s 645us/step - loss: 52.8775 - mae: 5.3756 - val_loss: 42.1982 - val_mae: 4.8907\n",
      "Epoch 775/850\n",
      "122/122 [==============================] - 0s 609us/step - loss: 53.1556 - mae: 5.3554 - val_loss: 39.6012 - val_mae: 4.6104\n",
      "Epoch 776/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 59.4878 - mae: 5.5063 - val_loss: 33.6118 - val_mae: 4.0554\n",
      "Epoch 777/850\n",
      "122/122 [==============================] - 0s 670us/step - loss: 55.6157 - mae: 5.4673 - val_loss: 32.1949 - val_mae: 4.0030\n",
      "Epoch 778/850\n",
      "122/122 [==============================] - 0s 760us/step - loss: 58.1007 - mae: 5.5802 - val_loss: 34.7518 - val_mae: 4.1978\n",
      "Epoch 779/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 55.9015 - mae: 5.4013 - val_loss: 33.0684 - val_mae: 4.1285\n",
      "Epoch 780/850\n",
      "122/122 [==============================] - 0s 694us/step - loss: 57.2752 - mae: 5.4648 - val_loss: 39.3241 - val_mae: 4.5227\n",
      "Epoch 781/850\n",
      "122/122 [==============================] - 0s 650us/step - loss: 54.3252 - mae: 5.4110 - val_loss: 34.7795 - val_mae: 4.0290\n",
      "Epoch 782/850\n",
      "122/122 [==============================] - 0s 667us/step - loss: 54.1622 - mae: 5.3485 - val_loss: 32.9485 - val_mae: 4.1444\n",
      "Epoch 783/850\n",
      "122/122 [==============================] - 0s 667us/step - loss: 54.7938 - mae: 5.3958 - val_loss: 38.2156 - val_mae: 4.4037\n",
      "Epoch 784/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 54.4846 - mae: 5.3908 - val_loss: 32.3100 - val_mae: 4.0872\n",
      "Epoch 785/850\n",
      "122/122 [==============================] - 0s 587us/step - loss: 54.5554 - mae: 5.4249 - val_loss: 34.2029 - val_mae: 4.1897\n",
      "Epoch 786/850\n",
      "122/122 [==============================] - 0s 668us/step - loss: 52.3330 - mae: 5.2746 - val_loss: 35.2881 - val_mae: 4.2725\n",
      "Epoch 787/850\n",
      "122/122 [==============================] - 0s 586us/step - loss: 53.2909 - mae: 5.3985 - val_loss: 40.2752 - val_mae: 4.3996\n",
      "Epoch 788/850\n",
      "122/122 [==============================] - 0s 604us/step - loss: 54.9615 - mae: 5.3642 - val_loss: 36.1955 - val_mae: 4.3071\n",
      "Epoch 789/850\n",
      "122/122 [==============================] - 0s 695us/step - loss: 58.2676 - mae: 5.5641 - val_loss: 35.5385 - val_mae: 4.4171\n",
      "Epoch 790/850\n",
      "122/122 [==============================] - 0s 605us/step - loss: 54.5490 - mae: 5.3843 - val_loss: 39.5443 - val_mae: 4.6181\n",
      "Epoch 791/850\n",
      "122/122 [==============================] - 0s 581us/step - loss: 56.3150 - mae: 5.5035 - val_loss: 33.5449 - val_mae: 4.1485\n",
      "Epoch 792/850\n",
      "122/122 [==============================] - 0s 632us/step - loss: 53.5177 - mae: 5.3331 - val_loss: 31.7567 - val_mae: 3.8904\n",
      "Epoch 793/850\n",
      "122/122 [==============================] - 0s 678us/step - loss: 50.2685 - mae: 5.2508 - val_loss: 41.2618 - val_mae: 4.7307\n",
      "Epoch 794/850\n",
      "122/122 [==============================] - 0s 591us/step - loss: 55.2686 - mae: 5.4277 - val_loss: 44.6743 - val_mae: 5.0137\n",
      "Epoch 795/850\n",
      "122/122 [==============================] - 0s 643us/step - loss: 54.6998 - mae: 5.4018 - val_loss: 44.8738 - val_mae: 5.1342\n",
      "Epoch 796/850\n",
      "122/122 [==============================] - 0s 602us/step - loss: 52.3615 - mae: 5.3336 - val_loss: 39.4859 - val_mae: 4.7201\n",
      "Epoch 797/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 53.3398 - mae: 5.4141 - val_loss: 39.6534 - val_mae: 4.6001\n",
      "Epoch 798/850\n",
      "122/122 [==============================] - 0s 627us/step - loss: 54.9289 - mae: 5.4417 - val_loss: 34.8867 - val_mae: 4.3565\n",
      "Epoch 799/850\n",
      "122/122 [==============================] - 0s 608us/step - loss: 55.0993 - mae: 5.3877 - val_loss: 37.4982 - val_mae: 4.4161\n",
      "Epoch 800/850\n",
      "122/122 [==============================] - 0s 736us/step - loss: 54.5061 - mae: 5.4251 - val_loss: 34.6535 - val_mae: 4.2601\n",
      "Epoch 801/850\n",
      "122/122 [==============================] - 0s 601us/step - loss: 53.7836 - mae: 5.3701 - val_loss: 37.0071 - val_mae: 4.3819\n",
      "Epoch 802/850\n",
      "122/122 [==============================] - 0s 642us/step - loss: 55.8873 - mae: 5.4762 - val_loss: 35.0825 - val_mae: 4.4362\n",
      "Epoch 803/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 55.8766 - mae: 5.4501 - val_loss: 32.5002 - val_mae: 4.0537\n",
      "Epoch 804/850\n",
      "122/122 [==============================] - 0s 618us/step - loss: 53.6003 - mae: 5.4118 - val_loss: 31.9282 - val_mae: 4.0258\n",
      "Epoch 805/850\n",
      "122/122 [==============================] - 0s 638us/step - loss: 53.6012 - mae: 5.3586 - val_loss: 40.9582 - val_mae: 4.7796\n",
      "Epoch 806/850\n",
      "122/122 [==============================] - 0s 651us/step - loss: 52.5736 - mae: 5.3170 - val_loss: 36.6839 - val_mae: 4.3338\n",
      "Epoch 807/850\n",
      "122/122 [==============================] - 0s 653us/step - loss: 50.6651 - mae: 5.2491 - val_loss: 38.1871 - val_mae: 4.5668\n",
      "Epoch 808/850\n",
      "122/122 [==============================] - 0s 613us/step - loss: 53.5514 - mae: 5.3415 - val_loss: 38.9787 - val_mae: 4.6850\n",
      "Epoch 809/850\n",
      "122/122 [==============================] - 0s 621us/step - loss: 55.8104 - mae: 5.4680 - val_loss: 32.1431 - val_mae: 3.8779\n",
      "Epoch 810/850\n",
      "122/122 [==============================] - 0s 658us/step - loss: 54.4915 - mae: 5.3139 - val_loss: 41.8833 - val_mae: 4.8103\n",
      "Epoch 811/850\n",
      "122/122 [==============================] - 0s 588us/step - loss: 55.8164 - mae: 5.4624 - val_loss: 42.5119 - val_mae: 4.7350\n",
      "Epoch 812/850\n",
      "122/122 [==============================] - 0s 616us/step - loss: 54.7080 - mae: 5.4391 - val_loss: 37.2716 - val_mae: 4.5576\n",
      "Epoch 813/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 56.2710 - mae: 5.5082 - val_loss: 40.6449 - val_mae: 4.8557\n",
      "Epoch 814/850\n",
      "122/122 [==============================] - 0s 689us/step - loss: 53.7840 - mae: 5.3504 - val_loss: 42.4737 - val_mae: 4.8812\n",
      "Epoch 815/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 54.2012 - mae: 5.4321 - val_loss: 35.5179 - val_mae: 4.3055\n",
      "Epoch 816/850\n",
      "122/122 [==============================] - 0s 614us/step - loss: 58.0146 - mae: 5.3324 - val_loss: 35.3075 - val_mae: 4.4011\n",
      "Epoch 817/850\n",
      "122/122 [==============================] - 0s 637us/step - loss: 53.1608 - mae: 5.3762 - val_loss: 34.7422 - val_mae: 4.1600\n",
      "Epoch 818/850\n",
      "122/122 [==============================] - 0s 575us/step - loss: 55.7642 - mae: 5.4301 - val_loss: 33.0867 - val_mae: 4.1744\n",
      "Epoch 819/850\n",
      "122/122 [==============================] - 0s 657us/step - loss: 49.8672 - mae: 5.2491 - val_loss: 33.3794 - val_mae: 4.1877\n",
      "Epoch 820/850\n",
      "122/122 [==============================] - 0s 641us/step - loss: 53.0539 - mae: 5.2868 - val_loss: 34.8081 - val_mae: 4.1911\n",
      "Epoch 821/850\n",
      "122/122 [==============================] - 0s 623us/step - loss: 54.3751 - mae: 5.4477 - val_loss: 31.7178 - val_mae: 3.9738\n",
      "Epoch 822/850\n",
      "122/122 [==============================] - 0s 644us/step - loss: 52.9552 - mae: 5.2985 - val_loss: 34.6089 - val_mae: 4.2547\n",
      "Epoch 823/850\n",
      "122/122 [==============================] - 0s 792us/step - loss: 52.9046 - mae: 5.3009 - val_loss: 33.1813 - val_mae: 4.0822\n",
      "Epoch 824/850\n",
      "122/122 [==============================] - 0s 751us/step - loss: 51.1545 - mae: 5.2876 - val_loss: 43.5518 - val_mae: 4.7740\n",
      "Epoch 825/850\n",
      "122/122 [==============================] - 0s 679us/step - loss: 53.8568 - mae: 5.3651 - val_loss: 42.6218 - val_mae: 4.8620\n",
      "Epoch 826/850\n",
      "122/122 [==============================] - 0s 724us/step - loss: 55.6036 - mae: 5.4378 - val_loss: 33.3649 - val_mae: 4.0685\n",
      "Epoch 827/850\n",
      "122/122 [==============================] - 0s 741us/step - loss: 54.5887 - mae: 5.3766 - val_loss: 41.8562 - val_mae: 4.8080\n",
      "Epoch 828/850\n",
      "122/122 [==============================] - 0s 690us/step - loss: 52.5316 - mae: 5.2929 - val_loss: 34.8358 - val_mae: 4.2999\n",
      "Epoch 829/850\n",
      "122/122 [==============================] - 0s 739us/step - loss: 55.3508 - mae: 5.4250 - val_loss: 35.9077 - val_mae: 4.3047\n",
      "Epoch 830/850\n",
      "122/122 [==============================] - 0s 740us/step - loss: 55.7023 - mae: 5.3900 - val_loss: 36.4053 - val_mae: 4.3575\n",
      "Epoch 831/850\n",
      "122/122 [==============================] - 0s 769us/step - loss: 54.3911 - mae: 5.4050 - val_loss: 37.8105 - val_mae: 4.3181\n",
      "Epoch 832/850\n",
      "122/122 [==============================] - 0s 753us/step - loss: 56.3089 - mae: 5.4344 - val_loss: 35.0120 - val_mae: 4.3008\n",
      "Epoch 833/850\n",
      "122/122 [==============================] - 0s 673us/step - loss: 57.8637 - mae: 5.4664 - val_loss: 38.4557 - val_mae: 4.4381\n",
      "Epoch 834/850\n",
      "122/122 [==============================] - 0s 823us/step - loss: 50.8941 - mae: 5.2541 - val_loss: 42.7418 - val_mae: 4.9331\n",
      "Epoch 835/850\n",
      "122/122 [==============================] - 0s 695us/step - loss: 52.4487 - mae: 5.2863 - val_loss: 41.5052 - val_mae: 4.6228\n",
      "Epoch 836/850\n",
      "122/122 [==============================] - 0s 720us/step - loss: 51.3393 - mae: 5.2740 - val_loss: 35.6238 - val_mae: 4.2457\n",
      "Epoch 837/850\n",
      "122/122 [==============================] - 0s 679us/step - loss: 54.1980 - mae: 5.3918 - val_loss: 32.3595 - val_mae: 3.9650\n",
      "Epoch 838/850\n",
      "122/122 [==============================] - 0s 754us/step - loss: 53.1921 - mae: 5.3285 - val_loss: 35.9823 - val_mae: 4.3648\n",
      "Epoch 839/850\n",
      "122/122 [==============================] - 0s 709us/step - loss: 57.5662 - mae: 5.4176 - val_loss: 33.8760 - val_mae: 4.1885\n",
      "Epoch 840/850\n",
      "122/122 [==============================] - 0s 693us/step - loss: 53.1089 - mae: 5.3107 - val_loss: 39.2595 - val_mae: 4.5927\n",
      "Epoch 841/850\n",
      "122/122 [==============================] - 0s 877us/step - loss: 53.7989 - mae: 5.3209 - val_loss: 36.4720 - val_mae: 4.4720\n",
      "Epoch 842/850\n",
      "122/122 [==============================] - 0s 702us/step - loss: 53.5953 - mae: 5.3519 - val_loss: 37.4331 - val_mae: 4.5033\n",
      "Epoch 843/850\n",
      "122/122 [==============================] - 0s 683us/step - loss: 53.3296 - mae: 5.4056 - val_loss: 37.2575 - val_mae: 4.2501\n",
      "Epoch 844/850\n",
      "122/122 [==============================] - 0s 645us/step - loss: 56.5306 - mae: 5.4976 - val_loss: 36.0405 - val_mae: 4.4080\n",
      "Epoch 845/850\n",
      "122/122 [==============================] - 0s 676us/step - loss: 52.8941 - mae: 5.3517 - val_loss: 34.4950 - val_mae: 4.2346\n",
      "Epoch 846/850\n",
      "122/122 [==============================] - 0s 665us/step - loss: 54.2610 - mae: 5.3412 - val_loss: 36.7311 - val_mae: 4.4175\n",
      "Epoch 847/850\n",
      "122/122 [==============================] - 0s 639us/step - loss: 55.5120 - mae: 5.4864 - val_loss: 39.3057 - val_mae: 4.4680\n",
      "Epoch 848/850\n",
      "122/122 [==============================] - 0s 625us/step - loss: 50.6705 - mae: 5.2555 - val_loss: 40.4149 - val_mae: 4.5181\n",
      "Epoch 849/850\n",
      "122/122 [==============================] - 0s 635us/step - loss: 54.5128 - mae: 5.3851 - val_loss: 33.2536 - val_mae: 3.9846\n",
      "Epoch 850/850\n",
      "122/122 [==============================] - 0s 656us/step - loss: 52.8023 - mae: 5.2410 - val_loss: 38.0087 - val_mae: 4.4830\n",
      "31/31 [==============================] - 0s 360us/step\n",
      "Epochs: 850 | MAE: 4.4830111594175674\n",
      "Training model with 900 epochs\n",
      "Epoch 1/900\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68657.8516 - mae: 227.3229 - val_loss: 60969.0547 - val_mae: 213.8941\n",
      "Epoch 2/900\n",
      "122/122 [==============================] - 0s 628us/step - loss: 39911.8203 - mae: 169.0844 - val_loss: 14628.2979 - val_mae: 104.3866\n",
      "Epoch 3/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 6580.3906 - mae: 64.2543 - val_loss: 3258.7539 - val_mae: 43.2734\n",
      "Epoch 4/900\n",
      "122/122 [==============================] - 0s 642us/step - loss: 3266.1155 - mae: 43.2640 - val_loss: 2174.9189 - val_mae: 35.4797\n",
      "Epoch 5/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 2515.6724 - mae: 38.4429 - val_loss: 1697.5330 - val_mae: 31.3180\n",
      "Epoch 6/900\n",
      "122/122 [==============================] - 0s 640us/step - loss: 2173.5183 - mae: 36.0201 - val_loss: 1448.2184 - val_mae: 28.8553\n",
      "Epoch 7/900\n",
      "122/122 [==============================] - 0s 675us/step - loss: 1953.3920 - mae: 34.4062 - val_loss: 1265.4955 - val_mae: 26.8716\n",
      "Epoch 8/900\n",
      "122/122 [==============================] - 0s 647us/step - loss: 1802.0090 - mae: 33.2952 - val_loss: 1135.3971 - val_mae: 25.2729\n",
      "Epoch 9/900\n",
      "122/122 [==============================] - 0s 586us/step - loss: 1635.0819 - mae: 31.7716 - val_loss: 1013.9297 - val_mae: 23.7959\n",
      "Epoch 10/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 1615.1935 - mae: 30.9575 - val_loss: 929.4822 - val_mae: 22.7852\n",
      "Epoch 11/900\n",
      "122/122 [==============================] - 0s 619us/step - loss: 1483.5309 - mae: 30.0062 - val_loss: 835.3427 - val_mae: 21.5632\n",
      "Epoch 12/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 1420.5286 - mae: 29.3283 - val_loss: 758.7708 - val_mae: 20.5748\n",
      "Epoch 13/900\n",
      "122/122 [==============================] - 0s 708us/step - loss: 1373.5756 - mae: 29.0285 - val_loss: 688.5009 - val_mae: 19.4661\n",
      "Epoch 14/900\n",
      "122/122 [==============================] - 0s 777us/step - loss: 1269.0129 - mae: 27.9240 - val_loss: 610.1404 - val_mae: 18.3835\n",
      "Epoch 15/900\n",
      "122/122 [==============================] - 0s 621us/step - loss: 1156.2102 - mae: 26.5763 - val_loss: 563.4442 - val_mae: 17.7674\n",
      "Epoch 16/900\n",
      "122/122 [==============================] - 0s 655us/step - loss: 1194.0330 - mae: 27.0048 - val_loss: 501.4403 - val_mae: 16.7085\n",
      "Epoch 17/900\n",
      "122/122 [==============================] - 0s 657us/step - loss: 1086.3445 - mae: 25.7921 - val_loss: 457.1731 - val_mae: 16.0404\n",
      "Epoch 18/900\n",
      "122/122 [==============================] - 0s 621us/step - loss: 1009.8175 - mae: 24.8128 - val_loss: 406.1507 - val_mae: 15.0544\n",
      "Epoch 19/900\n",
      "122/122 [==============================] - 0s 645us/step - loss: 1026.5804 - mae: 24.9567 - val_loss: 399.3958 - val_mae: 14.9706\n",
      "Epoch 20/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 975.1935 - mae: 24.3969 - val_loss: 335.0087 - val_mae: 13.5390\n",
      "Epoch 21/900\n",
      "122/122 [==============================] - 0s 644us/step - loss: 936.9099 - mae: 23.7105 - val_loss: 292.9164 - val_mae: 12.6916\n",
      "Epoch 22/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 925.3425 - mae: 23.2713 - val_loss: 275.2413 - val_mae: 12.3004\n",
      "Epoch 23/900\n",
      "122/122 [==============================] - 0s 649us/step - loss: 862.8444 - mae: 22.5684 - val_loss: 247.9173 - val_mae: 11.9549\n",
      "Epoch 24/900\n",
      "122/122 [==============================] - 0s 709us/step - loss: 825.5131 - mae: 22.1671 - val_loss: 240.2990 - val_mae: 11.8751\n",
      "Epoch 25/900\n",
      "122/122 [==============================] - 0s 709us/step - loss: 777.2577 - mae: 21.5640 - val_loss: 194.2265 - val_mae: 10.2739\n",
      "Epoch 26/900\n",
      "122/122 [==============================] - 0s 697us/step - loss: 772.4377 - mae: 21.2502 - val_loss: 174.3695 - val_mae: 10.0987\n",
      "Epoch 27/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 755.2643 - mae: 20.7304 - val_loss: 151.6980 - val_mae: 9.0952\n",
      "Epoch 28/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 727.4176 - mae: 20.5604 - val_loss: 136.2918 - val_mae: 8.7082\n",
      "Epoch 29/900\n",
      "122/122 [==============================] - 0s 739us/step - loss: 698.3538 - mae: 19.9063 - val_loss: 150.4834 - val_mae: 9.2791\n",
      "Epoch 30/900\n",
      "122/122 [==============================] - 0s 785us/step - loss: 682.1809 - mae: 19.7309 - val_loss: 116.4140 - val_mae: 7.9965\n",
      "Epoch 31/900\n",
      "122/122 [==============================] - 0s 751us/step - loss: 696.1104 - mae: 19.6832 - val_loss: 100.5968 - val_mae: 7.3560\n",
      "Epoch 32/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 651.7386 - mae: 19.2288 - val_loss: 96.3503 - val_mae: 7.2915\n",
      "Epoch 33/900\n",
      "122/122 [==============================] - 0s 734us/step - loss: 640.8841 - mae: 18.8256 - val_loss: 83.2095 - val_mae: 6.6336\n",
      "Epoch 34/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 624.7502 - mae: 18.6152 - val_loss: 75.7981 - val_mae: 6.7434\n",
      "Epoch 35/900\n",
      "122/122 [==============================] - 0s 721us/step - loss: 612.9279 - mae: 18.5892 - val_loss: 77.1672 - val_mae: 6.9259\n",
      "Epoch 36/900\n",
      "122/122 [==============================] - 0s 647us/step - loss: 616.8787 - mae: 18.4149 - val_loss: 66.6000 - val_mae: 5.9567\n",
      "Epoch 37/900\n",
      "122/122 [==============================] - 0s 663us/step - loss: 589.2993 - mae: 17.8487 - val_loss: 61.3623 - val_mae: 6.0250\n",
      "Epoch 38/900\n",
      "122/122 [==============================] - 0s 718us/step - loss: 581.0871 - mae: 17.6487 - val_loss: 62.6979 - val_mae: 6.3492\n",
      "Epoch 39/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 567.4531 - mae: 17.4119 - val_loss: 56.8952 - val_mae: 5.7806\n",
      "Epoch 40/900\n",
      "122/122 [==============================] - 0s 701us/step - loss: 544.7637 - mae: 17.2221 - val_loss: 46.9652 - val_mae: 5.1258\n",
      "Epoch 41/900\n",
      "122/122 [==============================] - 0s 806us/step - loss: 557.6345 - mae: 17.2259 - val_loss: 42.8714 - val_mae: 4.9401\n",
      "Epoch 42/900\n",
      "122/122 [==============================] - 0s 757us/step - loss: 553.3560 - mae: 17.0162 - val_loss: 41.9550 - val_mae: 4.8088\n",
      "Epoch 43/900\n",
      "122/122 [==============================] - 0s 688us/step - loss: 563.3268 - mae: 17.1169 - val_loss: 41.9482 - val_mae: 4.9749\n",
      "Epoch 44/900\n",
      "122/122 [==============================] - 0s 613us/step - loss: 549.7090 - mae: 17.0001 - val_loss: 47.6414 - val_mae: 5.4147\n",
      "Epoch 45/900\n",
      "122/122 [==============================] - 0s 660us/step - loss: 563.8540 - mae: 17.1782 - val_loss: 34.7088 - val_mae: 4.4193\n",
      "Epoch 46/900\n",
      "122/122 [==============================] - 0s 672us/step - loss: 548.3766 - mae: 16.7761 - val_loss: 35.2737 - val_mae: 4.5859\n",
      "Epoch 47/900\n",
      "122/122 [==============================] - 0s 652us/step - loss: 561.9057 - mae: 17.0871 - val_loss: 31.7408 - val_mae: 4.2204\n",
      "Epoch 48/900\n",
      "122/122 [==============================] - 0s 777us/step - loss: 563.6149 - mae: 16.9970 - val_loss: 36.1590 - val_mae: 4.4578\n",
      "Epoch 49/900\n",
      "122/122 [==============================] - 0s 732us/step - loss: 557.1502 - mae: 16.9317 - val_loss: 44.8763 - val_mae: 4.9298\n",
      "Epoch 50/900\n",
      "122/122 [==============================] - 0s 679us/step - loss: 511.7275 - mae: 16.3083 - val_loss: 43.3090 - val_mae: 5.0513\n",
      "Epoch 51/900\n",
      "122/122 [==============================] - 0s 583us/step - loss: 504.7615 - mae: 16.2024 - val_loss: 29.1568 - val_mae: 4.1389\n",
      "Epoch 52/900\n",
      "122/122 [==============================] - 0s 641us/step - loss: 547.0271 - mae: 16.4848 - val_loss: 32.2455 - val_mae: 4.2753\n",
      "Epoch 53/900\n",
      "122/122 [==============================] - 0s 574us/step - loss: 510.6634 - mae: 16.2737 - val_loss: 35.4982 - val_mae: 4.5402\n",
      "Epoch 54/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 532.1808 - mae: 16.5449 - val_loss: 27.4234 - val_mae: 3.8140\n",
      "Epoch 55/900\n",
      "122/122 [==============================] - 0s 790us/step - loss: 504.5411 - mae: 16.1226 - val_loss: 31.1038 - val_mae: 4.3221\n",
      "Epoch 56/900\n",
      "122/122 [==============================] - 0s 650us/step - loss: 504.4285 - mae: 16.0018 - val_loss: 28.6896 - val_mae: 4.0036\n",
      "Epoch 57/900\n",
      "122/122 [==============================] - 0s 599us/step - loss: 524.9051 - mae: 16.1421 - val_loss: 29.6773 - val_mae: 4.1476\n",
      "Epoch 58/900\n",
      "122/122 [==============================] - 0s 649us/step - loss: 492.7476 - mae: 15.8356 - val_loss: 27.7856 - val_mae: 3.9307\n",
      "Epoch 59/900\n",
      "122/122 [==============================] - 0s 616us/step - loss: 495.7346 - mae: 15.7862 - val_loss: 30.1039 - val_mae: 4.1971\n",
      "Epoch 60/900\n",
      "122/122 [==============================] - 0s 695us/step - loss: 490.7145 - mae: 15.5298 - val_loss: 26.6092 - val_mae: 3.7975\n",
      "Epoch 61/900\n",
      "122/122 [==============================] - 0s 687us/step - loss: 499.3987 - mae: 15.6698 - val_loss: 27.5411 - val_mae: 3.9025\n",
      "Epoch 62/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 515.5530 - mae: 15.8537 - val_loss: 29.5676 - val_mae: 4.1206\n",
      "Epoch 63/900\n",
      "122/122 [==============================] - 0s 639us/step - loss: 488.9584 - mae: 15.5709 - val_loss: 29.5349 - val_mae: 3.9126\n",
      "Epoch 64/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 486.3756 - mae: 15.3403 - val_loss: 27.1950 - val_mae: 3.7100\n",
      "Epoch 65/900\n",
      "122/122 [==============================] - 0s 602us/step - loss: 498.1934 - mae: 15.7539 - val_loss: 25.6893 - val_mae: 3.7319\n",
      "Epoch 66/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 464.0248 - mae: 15.0624 - val_loss: 38.0178 - val_mae: 4.8252\n",
      "Epoch 67/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 488.1070 - mae: 15.4694 - val_loss: 30.3203 - val_mae: 3.9834\n",
      "Epoch 68/900\n",
      "122/122 [==============================] - 0s 644us/step - loss: 489.7145 - mae: 15.4528 - val_loss: 24.4297 - val_mae: 3.4220\n",
      "Epoch 69/900\n",
      "122/122 [==============================] - 0s 657us/step - loss: 478.3585 - mae: 15.5431 - val_loss: 22.6115 - val_mae: 3.3595\n",
      "Epoch 70/900\n",
      "122/122 [==============================] - 0s 642us/step - loss: 471.0288 - mae: 15.3359 - val_loss: 29.6161 - val_mae: 3.9823\n",
      "Epoch 71/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 450.6023 - mae: 14.9342 - val_loss: 29.8942 - val_mae: 4.1902\n",
      "Epoch 72/900\n",
      "122/122 [==============================] - 0s 724us/step - loss: 464.7053 - mae: 15.2561 - val_loss: 27.7152 - val_mae: 3.9074\n",
      "Epoch 73/900\n",
      "122/122 [==============================] - 0s 784us/step - loss: 473.2308 - mae: 15.2694 - val_loss: 24.2147 - val_mae: 3.5345\n",
      "Epoch 74/900\n",
      "122/122 [==============================] - 0s 854us/step - loss: 478.3889 - mae: 15.2489 - val_loss: 32.0505 - val_mae: 4.2360\n",
      "Epoch 75/900\n",
      "122/122 [==============================] - 0s 690us/step - loss: 435.5067 - mae: 14.5361 - val_loss: 52.0447 - val_mae: 5.7243\n",
      "Epoch 76/900\n",
      "122/122 [==============================] - 0s 730us/step - loss: 465.8107 - mae: 15.0171 - val_loss: 24.3787 - val_mae: 3.5928\n",
      "Epoch 77/900\n",
      "122/122 [==============================] - 0s 671us/step - loss: 453.8233 - mae: 14.8277 - val_loss: 27.4909 - val_mae: 3.9576\n",
      "Epoch 78/900\n",
      "122/122 [==============================] - 0s 680us/step - loss: 454.8196 - mae: 14.7035 - val_loss: 39.7170 - val_mae: 4.6427\n",
      "Epoch 79/900\n",
      "122/122 [==============================] - 0s 786us/step - loss: 445.0089 - mae: 14.6323 - val_loss: 31.6418 - val_mae: 4.1781\n",
      "Epoch 80/900\n",
      "122/122 [==============================] - 0s 714us/step - loss: 445.0258 - mae: 14.7083 - val_loss: 29.7031 - val_mae: 4.0995\n",
      "Epoch 81/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 431.7882 - mae: 14.4596 - val_loss: 26.8704 - val_mae: 3.5302\n",
      "Epoch 82/900\n",
      "122/122 [==============================] - 0s 669us/step - loss: 444.8189 - mae: 14.5355 - val_loss: 34.0014 - val_mae: 4.2414\n",
      "Epoch 83/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 443.0137 - mae: 14.5077 - val_loss: 30.5358 - val_mae: 4.0703\n",
      "Epoch 84/900\n",
      "122/122 [==============================] - 0s 725us/step - loss: 426.9956 - mae: 14.3203 - val_loss: 29.8078 - val_mae: 4.0663\n",
      "Epoch 85/900\n",
      "122/122 [==============================] - 0s 774us/step - loss: 437.1290 - mae: 14.3524 - val_loss: 25.9690 - val_mae: 3.7561\n",
      "Epoch 86/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 437.5850 - mae: 14.5799 - val_loss: 28.0748 - val_mae: 3.9765\n",
      "Epoch 87/900\n",
      "122/122 [==============================] - 0s 647us/step - loss: 429.1331 - mae: 14.3421 - val_loss: 24.7800 - val_mae: 3.5430\n",
      "Epoch 88/900\n",
      "122/122 [==============================] - 0s 660us/step - loss: 439.3091 - mae: 14.3718 - val_loss: 30.5853 - val_mae: 4.1286\n",
      "Epoch 89/900\n",
      "122/122 [==============================] - 0s 700us/step - loss: 429.5168 - mae: 14.1956 - val_loss: 35.7163 - val_mae: 4.4634\n",
      "Epoch 90/900\n",
      "122/122 [==============================] - 0s 748us/step - loss: 419.8771 - mae: 14.1514 - val_loss: 27.6043 - val_mae: 3.7447\n",
      "Epoch 91/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 429.0778 - mae: 14.3456 - val_loss: 26.1376 - val_mae: 3.6056\n",
      "Epoch 92/900\n",
      "122/122 [==============================] - 0s 884us/step - loss: 400.8625 - mae: 13.8215 - val_loss: 29.6569 - val_mae: 4.0850\n",
      "Epoch 93/900\n",
      "122/122 [==============================] - 0s 763us/step - loss: 412.7448 - mae: 13.9377 - val_loss: 24.6320 - val_mae: 3.6401\n",
      "Epoch 94/900\n",
      "122/122 [==============================] - 0s 675us/step - loss: 427.8391 - mae: 14.0527 - val_loss: 25.3512 - val_mae: 3.6780\n",
      "Epoch 95/900\n",
      "122/122 [==============================] - 0s 707us/step - loss: 411.2376 - mae: 13.9468 - val_loss: 32.7582 - val_mae: 4.1591\n",
      "Epoch 96/900\n",
      "122/122 [==============================] - 0s 686us/step - loss: 401.8102 - mae: 13.7767 - val_loss: 31.3823 - val_mae: 4.2422\n",
      "Epoch 97/900\n",
      "122/122 [==============================] - 0s 932us/step - loss: 391.9999 - mae: 13.6686 - val_loss: 32.0752 - val_mae: 4.1737\n",
      "Epoch 98/900\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 384.3135 - mae: 13.3944 - val_loss: 28.5097 - val_mae: 3.9260\n",
      "Epoch 99/900\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 394.2846 - mae: 13.7226 - val_loss: 24.6785 - val_mae: 3.5925\n",
      "Epoch 100/900\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 353.2008 - mae: 13.0433 - val_loss: 34.0890 - val_mae: 4.3370\n",
      "Epoch 101/900\n",
      "122/122 [==============================] - 0s 911us/step - loss: 415.5181 - mae: 13.7642 - val_loss: 23.0893 - val_mae: 3.3708\n",
      "Epoch 102/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 377.4728 - mae: 13.6707 - val_loss: 30.6806 - val_mae: 4.0150\n",
      "Epoch 103/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 399.7822 - mae: 13.7350 - val_loss: 31.4416 - val_mae: 4.3475\n",
      "Epoch 104/900\n",
      "122/122 [==============================] - 0s 666us/step - loss: 387.9686 - mae: 13.4652 - val_loss: 28.6305 - val_mae: 4.0505\n",
      "Epoch 105/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 382.1826 - mae: 13.4648 - val_loss: 34.4964 - val_mae: 3.9913\n",
      "Epoch 106/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 383.6682 - mae: 13.2766 - val_loss: 28.3065 - val_mae: 3.9448\n",
      "Epoch 107/900\n",
      "122/122 [==============================] - 0s 688us/step - loss: 399.0613 - mae: 13.6712 - val_loss: 25.5393 - val_mae: 3.6695\n",
      "Epoch 108/900\n",
      "122/122 [==============================] - 0s 689us/step - loss: 390.3318 - mae: 13.5859 - val_loss: 26.6589 - val_mae: 3.7769\n",
      "Epoch 109/900\n",
      "122/122 [==============================] - 0s 661us/step - loss: 395.4041 - mae: 13.4985 - val_loss: 31.9173 - val_mae: 4.1389\n",
      "Epoch 110/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 374.1091 - mae: 13.1842 - val_loss: 23.9070 - val_mae: 3.5286\n",
      "Epoch 111/900\n",
      "122/122 [==============================] - 0s 675us/step - loss: 353.1569 - mae: 12.8599 - val_loss: 29.9710 - val_mae: 4.1108\n",
      "Epoch 112/900\n",
      "122/122 [==============================] - 0s 590us/step - loss: 361.5682 - mae: 13.1036 - val_loss: 24.8354 - val_mae: 3.6617\n",
      "Epoch 113/900\n",
      "122/122 [==============================] - 0s 833us/step - loss: 345.5038 - mae: 12.7274 - val_loss: 22.5970 - val_mae: 3.3302\n",
      "Epoch 114/900\n",
      "122/122 [==============================] - 0s 857us/step - loss: 361.5231 - mae: 12.8704 - val_loss: 65.2856 - val_mae: 6.3101\n",
      "Epoch 115/900\n",
      "122/122 [==============================] - 0s 931us/step - loss: 361.8674 - mae: 12.9556 - val_loss: 32.6680 - val_mae: 4.2217\n",
      "Epoch 116/900\n",
      "122/122 [==============================] - 0s 681us/step - loss: 406.0787 - mae: 13.5140 - val_loss: 27.7710 - val_mae: 3.9049\n",
      "Epoch 117/900\n",
      "122/122 [==============================] - 0s 685us/step - loss: 352.7048 - mae: 12.9420 - val_loss: 27.9588 - val_mae: 3.7609\n",
      "Epoch 118/900\n",
      "122/122 [==============================] - 0s 661us/step - loss: 368.0207 - mae: 13.0074 - val_loss: 26.0316 - val_mae: 3.6673\n",
      "Epoch 119/900\n",
      "122/122 [==============================] - 0s 659us/step - loss: 356.3737 - mae: 12.8290 - val_loss: 27.4946 - val_mae: 3.8912\n",
      "Epoch 120/900\n",
      "122/122 [==============================] - 0s 672us/step - loss: 360.5073 - mae: 12.8363 - val_loss: 50.5444 - val_mae: 5.4348\n",
      "Epoch 121/900\n",
      "122/122 [==============================] - 0s 700us/step - loss: 371.8190 - mae: 12.9815 - val_loss: 23.2694 - val_mae: 3.3370\n",
      "Epoch 122/900\n",
      "122/122 [==============================] - 0s 785us/step - loss: 363.2934 - mae: 13.0482 - val_loss: 23.7531 - val_mae: 3.5018\n",
      "Epoch 123/900\n",
      "122/122 [==============================] - 0s 621us/step - loss: 345.8487 - mae: 12.6382 - val_loss: 21.8987 - val_mae: 3.3268\n",
      "Epoch 124/900\n",
      "122/122 [==============================] - 0s 680us/step - loss: 340.7018 - mae: 12.5063 - val_loss: 27.1918 - val_mae: 3.6709\n",
      "Epoch 125/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 351.7591 - mae: 12.7125 - val_loss: 25.9782 - val_mae: 3.7318\n",
      "Epoch 126/900\n",
      "122/122 [==============================] - 0s 609us/step - loss: 346.5652 - mae: 12.5830 - val_loss: 51.4246 - val_mae: 5.6221\n",
      "Epoch 127/900\n",
      "122/122 [==============================] - 0s 609us/step - loss: 346.6205 - mae: 12.6493 - val_loss: 26.7756 - val_mae: 3.6303\n",
      "Epoch 128/900\n",
      "122/122 [==============================] - 0s 711us/step - loss: 325.3097 - mae: 12.2123 - val_loss: 29.8797 - val_mae: 3.6683\n",
      "Epoch 129/900\n",
      "122/122 [==============================] - 0s 701us/step - loss: 323.0664 - mae: 12.2412 - val_loss: 26.4890 - val_mae: 3.7631\n",
      "Epoch 130/900\n",
      "122/122 [==============================] - 0s 827us/step - loss: 333.8539 - mae: 12.3183 - val_loss: 25.1981 - val_mae: 3.6119\n",
      "Epoch 131/900\n",
      "122/122 [==============================] - 0s 659us/step - loss: 337.6127 - mae: 12.4188 - val_loss: 28.0146 - val_mae: 3.9289\n",
      "Epoch 132/900\n",
      "122/122 [==============================] - 0s 729us/step - loss: 346.7268 - mae: 12.6202 - val_loss: 27.6692 - val_mae: 3.7677\n",
      "Epoch 133/900\n",
      "122/122 [==============================] - 0s 868us/step - loss: 323.4181 - mae: 12.2761 - val_loss: 29.1703 - val_mae: 4.0417\n",
      "Epoch 134/900\n",
      "122/122 [==============================] - 0s 900us/step - loss: 347.6252 - mae: 12.6143 - val_loss: 37.5383 - val_mae: 4.7009\n",
      "Epoch 135/900\n",
      "122/122 [==============================] - 0s 769us/step - loss: 339.3387 - mae: 12.3040 - val_loss: 34.6711 - val_mae: 4.5344\n",
      "Epoch 136/900\n",
      "122/122 [==============================] - 0s 747us/step - loss: 324.6049 - mae: 12.0924 - val_loss: 27.2065 - val_mae: 3.8652\n",
      "Epoch 137/900\n",
      "122/122 [==============================] - 0s 797us/step - loss: 324.1641 - mae: 12.0759 - val_loss: 27.1536 - val_mae: 3.5310\n",
      "Epoch 138/900\n",
      "122/122 [==============================] - 0s 665us/step - loss: 323.3203 - mae: 12.0970 - val_loss: 33.5443 - val_mae: 4.3777\n",
      "Epoch 139/900\n",
      "122/122 [==============================] - 0s 791us/step - loss: 333.6200 - mae: 11.9525 - val_loss: 27.8167 - val_mae: 3.8987\n",
      "Epoch 140/900\n",
      "122/122 [==============================] - 0s 808us/step - loss: 321.5428 - mae: 12.0886 - val_loss: 26.3433 - val_mae: 3.7553\n",
      "Epoch 141/900\n",
      "122/122 [==============================] - 0s 821us/step - loss: 332.0817 - mae: 12.0932 - val_loss: 50.5477 - val_mae: 5.3518\n",
      "Epoch 142/900\n",
      "122/122 [==============================] - 0s 825us/step - loss: 324.6633 - mae: 12.0666 - val_loss: 42.8867 - val_mae: 4.9843\n",
      "Epoch 143/900\n",
      "122/122 [==============================] - 0s 737us/step - loss: 321.6302 - mae: 11.8470 - val_loss: 23.3607 - val_mae: 3.4419\n",
      "Epoch 144/900\n",
      "122/122 [==============================] - 0s 788us/step - loss: 313.6510 - mae: 11.8667 - val_loss: 31.5264 - val_mae: 4.2336\n",
      "Epoch 145/900\n",
      "122/122 [==============================] - 0s 842us/step - loss: 316.1488 - mae: 11.8826 - val_loss: 26.4149 - val_mae: 3.6745\n",
      "Epoch 146/900\n",
      "122/122 [==============================] - 0s 783us/step - loss: 296.7250 - mae: 11.6367 - val_loss: 22.6828 - val_mae: 3.3255\n",
      "Epoch 147/900\n",
      "122/122 [==============================] - 0s 787us/step - loss: 297.5326 - mae: 11.6667 - val_loss: 22.7882 - val_mae: 3.3950\n",
      "Epoch 148/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 304.6174 - mae: 11.6696 - val_loss: 41.8014 - val_mae: 4.9014\n",
      "Epoch 149/900\n",
      "122/122 [==============================] - 0s 695us/step - loss: 313.9033 - mae: 11.6014 - val_loss: 25.5168 - val_mae: 3.5833\n",
      "Epoch 150/900\n",
      "122/122 [==============================] - 0s 764us/step - loss: 292.4066 - mae: 11.4130 - val_loss: 25.1531 - val_mae: 3.7163\n",
      "Epoch 151/900\n",
      "122/122 [==============================] - 0s 823us/step - loss: 322.9791 - mae: 11.8127 - val_loss: 27.4142 - val_mae: 3.7409\n",
      "Epoch 152/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 295.2174 - mae: 11.4036 - val_loss: 34.2805 - val_mae: 4.4653\n",
      "Epoch 153/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 298.2719 - mae: 11.5381 - val_loss: 43.1187 - val_mae: 5.0227\n",
      "Epoch 154/900\n",
      "122/122 [==============================] - 0s 700us/step - loss: 286.8383 - mae: 11.3016 - val_loss: 29.9077 - val_mae: 3.7101\n",
      "Epoch 155/900\n",
      "122/122 [==============================] - 0s 679us/step - loss: 291.8846 - mae: 11.3873 - val_loss: 23.5375 - val_mae: 3.3799\n",
      "Epoch 156/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 258.7502 - mae: 11.0069 - val_loss: 24.4228 - val_mae: 3.6006\n",
      "Epoch 157/900\n",
      "122/122 [==============================] - 0s 755us/step - loss: 284.8093 - mae: 11.2592 - val_loss: 24.0431 - val_mae: 3.4926\n",
      "Epoch 158/900\n",
      "122/122 [==============================] - 0s 763us/step - loss: 309.4179 - mae: 11.5612 - val_loss: 22.3979 - val_mae: 3.3733\n",
      "Epoch 159/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 289.5656 - mae: 11.3639 - val_loss: 28.7994 - val_mae: 3.7919\n",
      "Epoch 160/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 304.3553 - mae: 11.5589 - val_loss: 22.6104 - val_mae: 3.3508\n",
      "Epoch 161/900\n",
      "122/122 [==============================] - 0s 743us/step - loss: 279.1902 - mae: 11.1021 - val_loss: 44.1470 - val_mae: 4.9880\n",
      "Epoch 162/900\n",
      "122/122 [==============================] - 0s 743us/step - loss: 281.7127 - mae: 11.2725 - val_loss: 31.7726 - val_mae: 4.0714\n",
      "Epoch 163/900\n",
      "122/122 [==============================] - 0s 791us/step - loss: 270.8086 - mae: 11.1206 - val_loss: 22.8941 - val_mae: 3.3757\n",
      "Epoch 164/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 266.2522 - mae: 10.8713 - val_loss: 21.7293 - val_mae: 3.2740\n",
      "Epoch 165/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 275.8129 - mae: 11.0723 - val_loss: 28.2311 - val_mae: 3.8618\n",
      "Epoch 166/900\n",
      "122/122 [==============================] - 0s 730us/step - loss: 258.6386 - mae: 10.7731 - val_loss: 23.1192 - val_mae: 3.4198\n",
      "Epoch 167/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 266.6432 - mae: 10.7722 - val_loss: 26.8469 - val_mae: 3.7756\n",
      "Epoch 168/900\n",
      "122/122 [==============================] - 0s 750us/step - loss: 290.1438 - mae: 11.0818 - val_loss: 33.9018 - val_mae: 4.4076\n",
      "Epoch 169/900\n",
      "122/122 [==============================] - 0s 881us/step - loss: 272.3691 - mae: 10.8688 - val_loss: 35.5092 - val_mae: 4.4865\n",
      "Epoch 170/900\n",
      "122/122 [==============================] - 0s 689us/step - loss: 270.0153 - mae: 10.8818 - val_loss: 24.6484 - val_mae: 3.5384\n",
      "Epoch 171/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 268.9774 - mae: 10.6880 - val_loss: 22.6684 - val_mae: 3.3951\n",
      "Epoch 172/900\n",
      "122/122 [==============================] - 0s 728us/step - loss: 269.0761 - mae: 10.7100 - val_loss: 22.9065 - val_mae: 3.3991\n",
      "Epoch 173/900\n",
      "122/122 [==============================] - 0s 666us/step - loss: 263.7177 - mae: 10.7078 - val_loss: 22.2871 - val_mae: 3.4175\n",
      "Epoch 174/900\n",
      "122/122 [==============================] - 0s 704us/step - loss: 257.9154 - mae: 10.6680 - val_loss: 23.6365 - val_mae: 3.3994\n",
      "Epoch 175/900\n",
      "122/122 [==============================] - 0s 782us/step - loss: 260.5404 - mae: 10.7482 - val_loss: 24.2435 - val_mae: 3.5242\n",
      "Epoch 176/900\n",
      "122/122 [==============================] - 0s 707us/step - loss: 256.8352 - mae: 10.7047 - val_loss: 21.6637 - val_mae: 3.3392\n",
      "Epoch 177/900\n",
      "122/122 [==============================] - 0s 722us/step - loss: 259.6317 - mae: 10.6470 - val_loss: 21.0850 - val_mae: 3.2354\n",
      "Epoch 178/900\n",
      "122/122 [==============================] - 0s 677us/step - loss: 249.2547 - mae: 10.5543 - val_loss: 26.3753 - val_mae: 3.8004\n",
      "Epoch 179/900\n",
      "122/122 [==============================] - 0s 691us/step - loss: 251.0567 - mae: 10.3929 - val_loss: 24.3763 - val_mae: 3.6759\n",
      "Epoch 180/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 251.3360 - mae: 10.3888 - val_loss: 28.9076 - val_mae: 4.0274\n",
      "Epoch 181/900\n",
      "122/122 [==============================] - 0s 783us/step - loss: 247.0219 - mae: 10.3383 - val_loss: 22.8911 - val_mae: 3.3890\n",
      "Epoch 182/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 251.1525 - mae: 10.4468 - val_loss: 26.8489 - val_mae: 3.8557\n",
      "Epoch 183/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 251.6620 - mae: 10.3605 - val_loss: 21.9442 - val_mae: 3.3150\n",
      "Epoch 184/900\n",
      "122/122 [==============================] - 0s 772us/step - loss: 245.1125 - mae: 10.2844 - val_loss: 29.8111 - val_mae: 4.1035\n",
      "Epoch 185/900\n",
      "122/122 [==============================] - 0s 897us/step - loss: 264.5544 - mae: 10.6264 - val_loss: 26.6394 - val_mae: 3.8489\n",
      "Epoch 186/900\n",
      "122/122 [==============================] - 0s 892us/step - loss: 234.2918 - mae: 10.1726 - val_loss: 24.5030 - val_mae: 3.5819\n",
      "Epoch 187/900\n",
      "122/122 [==============================] - 0s 724us/step - loss: 247.9819 - mae: 10.2502 - val_loss: 28.4127 - val_mae: 3.9570\n",
      "Epoch 188/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 236.1867 - mae: 10.0283 - val_loss: 24.7061 - val_mae: 3.6490\n",
      "Epoch 189/900\n",
      "122/122 [==============================] - 0s 737us/step - loss: 238.0064 - mae: 10.1115 - val_loss: 24.2831 - val_mae: 3.6134\n",
      "Epoch 190/900\n",
      "122/122 [==============================] - 0s 741us/step - loss: 259.8702 - mae: 10.3464 - val_loss: 25.8730 - val_mae: 3.6510\n",
      "Epoch 191/900\n",
      "122/122 [==============================] - 0s 852us/step - loss: 238.8185 - mae: 10.0561 - val_loss: 21.9777 - val_mae: 3.3202\n",
      "Epoch 192/900\n",
      "122/122 [==============================] - 0s 861us/step - loss: 242.2625 - mae: 10.1406 - val_loss: 21.5583 - val_mae: 3.3498\n",
      "Epoch 193/900\n",
      "122/122 [==============================] - 0s 758us/step - loss: 223.0653 - mae: 9.7485 - val_loss: 28.3386 - val_mae: 3.9917\n",
      "Epoch 194/900\n",
      "122/122 [==============================] - 0s 760us/step - loss: 229.7540 - mae: 9.9527 - val_loss: 25.5641 - val_mae: 3.7030\n",
      "Epoch 195/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 234.2749 - mae: 10.0735 - val_loss: 23.1059 - val_mae: 3.4737\n",
      "Epoch 196/900\n",
      "122/122 [==============================] - 0s 773us/step - loss: 231.0668 - mae: 10.0265 - val_loss: 30.0886 - val_mae: 4.2280\n",
      "Epoch 197/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 232.5042 - mae: 10.0022 - val_loss: 31.8048 - val_mae: 4.4049\n",
      "Epoch 198/900\n",
      "122/122 [==============================] - 0s 753us/step - loss: 233.5502 - mae: 10.0189 - val_loss: 38.7597 - val_mae: 4.6758\n",
      "Epoch 199/900\n",
      "122/122 [==============================] - 0s 727us/step - loss: 225.1243 - mae: 9.6611 - val_loss: 22.1393 - val_mae: 3.3413\n",
      "Epoch 200/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 229.6203 - mae: 9.9001 - val_loss: 22.6194 - val_mae: 3.4239\n",
      "Epoch 201/900\n",
      "122/122 [==============================] - 0s 732us/step - loss: 226.4079 - mae: 9.7691 - val_loss: 41.9647 - val_mae: 4.9768\n",
      "Epoch 202/900\n",
      "122/122 [==============================] - 0s 709us/step - loss: 227.4519 - mae: 9.9927 - val_loss: 22.4306 - val_mae: 3.4234\n",
      "Epoch 203/900\n",
      "122/122 [==============================] - 0s 729us/step - loss: 218.9556 - mae: 9.4277 - val_loss: 24.8498 - val_mae: 3.6354\n",
      "Epoch 204/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 226.5249 - mae: 9.7247 - val_loss: 23.5347 - val_mae: 3.4891\n",
      "Epoch 205/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 204.3305 - mae: 9.3740 - val_loss: 28.0716 - val_mae: 3.9834\n",
      "Epoch 206/900\n",
      "122/122 [==============================] - 0s 793us/step - loss: 215.3188 - mae: 9.5830 - val_loss: 29.0481 - val_mae: 3.9630\n",
      "Epoch 207/900\n",
      "122/122 [==============================] - 0s 753us/step - loss: 235.6436 - mae: 9.9284 - val_loss: 24.8204 - val_mae: 3.6331\n",
      "Epoch 208/900\n",
      "122/122 [==============================] - 0s 724us/step - loss: 212.1698 - mae: 9.3709 - val_loss: 22.1217 - val_mae: 3.3345\n",
      "Epoch 209/900\n",
      "122/122 [==============================] - 0s 734us/step - loss: 223.1973 - mae: 9.7307 - val_loss: 28.0826 - val_mae: 3.8277\n",
      "Epoch 210/900\n",
      "122/122 [==============================] - 0s 772us/step - loss: 200.2785 - mae: 9.2306 - val_loss: 24.4840 - val_mae: 3.5778\n",
      "Epoch 211/900\n",
      "122/122 [==============================] - 0s 828us/step - loss: 197.4796 - mae: 9.1691 - val_loss: 25.7340 - val_mae: 3.7967\n",
      "Epoch 212/900\n",
      "122/122 [==============================] - 0s 873us/step - loss: 203.1671 - mae: 9.3112 - val_loss: 25.2548 - val_mae: 3.5945\n",
      "Epoch 213/900\n",
      "122/122 [==============================] - 0s 716us/step - loss: 198.5098 - mae: 9.1626 - val_loss: 23.8943 - val_mae: 3.5100\n",
      "Epoch 214/900\n",
      "122/122 [==============================] - 0s 754us/step - loss: 204.6180 - mae: 9.3886 - val_loss: 31.2636 - val_mae: 4.0888\n",
      "Epoch 215/900\n",
      "122/122 [==============================] - 0s 802us/step - loss: 205.1734 - mae: 9.3806 - val_loss: 24.3868 - val_mae: 3.5146\n",
      "Epoch 216/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 201.0620 - mae: 9.2514 - val_loss: 26.5504 - val_mae: 3.7013\n",
      "Epoch 217/900\n",
      "122/122 [==============================] - 0s 843us/step - loss: 194.1188 - mae: 9.2165 - val_loss: 29.1849 - val_mae: 4.0185\n",
      "Epoch 218/900\n",
      "122/122 [==============================] - 0s 671us/step - loss: 202.4598 - mae: 9.1812 - val_loss: 24.8757 - val_mae: 3.6611\n",
      "Epoch 219/900\n",
      "122/122 [==============================] - 0s 705us/step - loss: 199.1562 - mae: 9.1734 - val_loss: 24.3237 - val_mae: 3.5381\n",
      "Epoch 220/900\n",
      "122/122 [==============================] - 0s 738us/step - loss: 213.0199 - mae: 9.3860 - val_loss: 27.7270 - val_mae: 3.9296\n",
      "Epoch 221/900\n",
      "122/122 [==============================] - 0s 756us/step - loss: 205.3422 - mae: 9.2493 - val_loss: 25.1229 - val_mae: 3.6920\n",
      "Epoch 222/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 192.1492 - mae: 9.1200 - val_loss: 28.3104 - val_mae: 4.0611\n",
      "Epoch 223/900\n",
      "122/122 [==============================] - 0s 729us/step - loss: 195.5390 - mae: 9.0531 - val_loss: 24.0033 - val_mae: 3.5605\n",
      "Epoch 224/900\n",
      "122/122 [==============================] - 0s 683us/step - loss: 196.6805 - mae: 9.1578 - val_loss: 24.0445 - val_mae: 3.5257\n",
      "Epoch 225/900\n",
      "122/122 [==============================] - 0s 652us/step - loss: 210.6879 - mae: 9.3154 - val_loss: 34.4529 - val_mae: 4.5387\n",
      "Epoch 226/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 195.5211 - mae: 9.0971 - val_loss: 25.3510 - val_mae: 3.7301\n",
      "Epoch 227/900\n",
      "122/122 [==============================] - 0s 828us/step - loss: 181.0861 - mae: 8.9097 - val_loss: 35.6176 - val_mae: 4.6523\n",
      "Epoch 228/900\n",
      "122/122 [==============================] - 0s 716us/step - loss: 185.7272 - mae: 8.9213 - val_loss: 36.8713 - val_mae: 4.7270\n",
      "Epoch 229/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 182.7416 - mae: 8.8964 - val_loss: 24.8423 - val_mae: 3.5858\n",
      "Epoch 230/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 184.2539 - mae: 8.7906 - val_loss: 24.6440 - val_mae: 3.5771\n",
      "Epoch 231/900\n",
      "122/122 [==============================] - 0s 710us/step - loss: 194.6900 - mae: 8.9647 - val_loss: 61.0047 - val_mae: 6.0471\n",
      "Epoch 232/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 188.4592 - mae: 8.8990 - val_loss: 26.7598 - val_mae: 3.8518\n",
      "Epoch 233/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 177.9447 - mae: 8.5917 - val_loss: 25.5823 - val_mae: 3.7054\n",
      "Epoch 234/900\n",
      "122/122 [==============================] - 0s 843us/step - loss: 187.7636 - mae: 8.8374 - val_loss: 31.7721 - val_mae: 4.3624\n",
      "Epoch 235/900\n",
      "122/122 [==============================] - 0s 754us/step - loss: 175.6427 - mae: 8.7031 - val_loss: 26.1097 - val_mae: 3.6909\n",
      "Epoch 236/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 181.8948 - mae: 8.8100 - val_loss: 32.7463 - val_mae: 4.3090\n",
      "Epoch 237/900\n",
      "122/122 [==============================] - 0s 722us/step - loss: 169.8665 - mae: 8.6120 - val_loss: 28.0594 - val_mae: 3.8375\n",
      "Epoch 238/900\n",
      "122/122 [==============================] - 0s 757us/step - loss: 175.6566 - mae: 8.6846 - val_loss: 41.5986 - val_mae: 4.9252\n",
      "Epoch 239/900\n",
      "122/122 [==============================] - 0s 765us/step - loss: 167.4204 - mae: 8.5113 - val_loss: 26.7423 - val_mae: 3.8824\n",
      "Epoch 240/900\n",
      "122/122 [==============================] - 0s 979us/step - loss: 167.9328 - mae: 8.6539 - val_loss: 28.4724 - val_mae: 4.0456\n",
      "Epoch 241/900\n",
      "122/122 [==============================] - 0s 710us/step - loss: 186.4492 - mae: 8.8486 - val_loss: 30.9445 - val_mae: 4.1681\n",
      "Epoch 242/900\n",
      "122/122 [==============================] - 0s 716us/step - loss: 170.2919 - mae: 8.5727 - val_loss: 27.9522 - val_mae: 3.8456\n",
      "Epoch 243/900\n",
      "122/122 [==============================] - 0s 731us/step - loss: 177.3173 - mae: 8.6057 - val_loss: 28.3217 - val_mae: 3.9440\n",
      "Epoch 244/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 172.7395 - mae: 8.5065 - val_loss: 27.3384 - val_mae: 3.7529\n",
      "Epoch 245/900\n",
      "122/122 [==============================] - 0s 710us/step - loss: 162.1904 - mae: 8.4436 - val_loss: 34.4027 - val_mae: 4.3395\n",
      "Epoch 246/900\n",
      "122/122 [==============================] - 0s 785us/step - loss: 175.1119 - mae: 8.4924 - val_loss: 31.5419 - val_mae: 4.2690\n",
      "Epoch 247/900\n",
      "122/122 [==============================] - 0s 678us/step - loss: 166.2705 - mae: 8.4569 - val_loss: 27.4642 - val_mae: 3.8395\n",
      "Epoch 248/900\n",
      "122/122 [==============================] - 0s 683us/step - loss: 167.1363 - mae: 8.4095 - val_loss: 29.7917 - val_mae: 4.0612\n",
      "Epoch 249/900\n",
      "122/122 [==============================] - 0s 711us/step - loss: 172.2952 - mae: 8.4093 - val_loss: 26.1260 - val_mae: 3.7083\n",
      "Epoch 250/900\n",
      "122/122 [==============================] - 0s 775us/step - loss: 162.5473 - mae: 8.4506 - val_loss: 27.8562 - val_mae: 3.9558\n",
      "Epoch 251/900\n",
      "122/122 [==============================] - 0s 782us/step - loss: 161.7623 - mae: 8.2805 - val_loss: 38.8596 - val_mae: 4.8510\n",
      "Epoch 252/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 176.1823 - mae: 8.4059 - val_loss: 31.4568 - val_mae: 4.2216\n",
      "Epoch 253/900\n",
      "122/122 [==============================] - 0s 825us/step - loss: 165.5196 - mae: 8.3657 - val_loss: 25.5650 - val_mae: 3.6141\n",
      "Epoch 254/900\n",
      "122/122 [==============================] - 0s 784us/step - loss: 157.6933 - mae: 8.2010 - val_loss: 30.9769 - val_mae: 3.9479\n",
      "Epoch 255/900\n",
      "122/122 [==============================] - 0s 726us/step - loss: 168.5049 - mae: 8.4266 - val_loss: 28.3037 - val_mae: 3.9697\n",
      "Epoch 256/900\n",
      "122/122 [==============================] - 0s 752us/step - loss: 176.8123 - mae: 8.6294 - val_loss: 30.4204 - val_mae: 4.1898\n",
      "Epoch 257/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 165.3781 - mae: 8.3820 - val_loss: 28.3046 - val_mae: 3.8308\n",
      "Epoch 258/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 174.6033 - mae: 8.5877 - val_loss: 31.2080 - val_mae: 4.1845\n",
      "Epoch 259/900\n",
      "122/122 [==============================] - 0s 823us/step - loss: 156.8001 - mae: 8.2418 - val_loss: 29.1521 - val_mae: 3.9018\n",
      "Epoch 260/900\n",
      "122/122 [==============================] - 0s 689us/step - loss: 166.7889 - mae: 8.3549 - val_loss: 36.1446 - val_mae: 4.4702\n",
      "Epoch 261/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 167.1009 - mae: 8.4164 - val_loss: 25.3575 - val_mae: 3.5923\n",
      "Epoch 262/900\n",
      "122/122 [==============================] - 0s 725us/step - loss: 161.2780 - mae: 8.2820 - val_loss: 26.6140 - val_mae: 3.7855\n",
      "Epoch 263/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 161.5424 - mae: 8.3032 - val_loss: 26.0040 - val_mae: 3.7205\n",
      "Epoch 264/900\n",
      "122/122 [==============================] - 0s 694us/step - loss: 159.0467 - mae: 8.2480 - val_loss: 44.1136 - val_mae: 5.2315\n",
      "Epoch 265/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 162.8943 - mae: 8.3436 - val_loss: 31.2443 - val_mae: 4.1500\n",
      "Epoch 266/900\n",
      "122/122 [==============================] - 0s 724us/step - loss: 157.9217 - mae: 8.1174 - val_loss: 27.5414 - val_mae: 3.7751\n",
      "Epoch 267/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 150.7064 - mae: 8.0835 - val_loss: 34.0640 - val_mae: 4.2463\n",
      "Epoch 268/900\n",
      "122/122 [==============================] - 0s 755us/step - loss: 155.1010 - mae: 8.2261 - val_loss: 25.5057 - val_mae: 3.6365\n",
      "Epoch 269/900\n",
      "122/122 [==============================] - 0s 777us/step - loss: 154.1089 - mae: 8.0502 - val_loss: 44.1840 - val_mae: 5.0660\n",
      "Epoch 270/900\n",
      "122/122 [==============================] - 0s 803us/step - loss: 149.4653 - mae: 8.0923 - val_loss: 34.8383 - val_mae: 4.4715\n",
      "Epoch 271/900\n",
      "122/122 [==============================] - 0s 744us/step - loss: 149.7146 - mae: 8.0925 - val_loss: 30.6562 - val_mae: 4.1172\n",
      "Epoch 272/900\n",
      "122/122 [==============================] - 0s 851us/step - loss: 153.7035 - mae: 8.1964 - val_loss: 33.4089 - val_mae: 4.2882\n",
      "Epoch 273/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 151.6189 - mae: 8.1374 - val_loss: 27.1387 - val_mae: 3.6506\n",
      "Epoch 274/900\n",
      "122/122 [==============================] - 0s 759us/step - loss: 141.8231 - mae: 8.0092 - val_loss: 32.4625 - val_mae: 4.3006\n",
      "Epoch 275/900\n",
      "122/122 [==============================] - 0s 748us/step - loss: 144.5789 - mae: 7.9108 - val_loss: 25.8514 - val_mae: 3.6244\n",
      "Epoch 276/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 151.0596 - mae: 8.1384 - val_loss: 35.6073 - val_mae: 4.5247\n",
      "Epoch 277/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 148.7253 - mae: 8.0335 - val_loss: 46.0765 - val_mae: 5.2538\n",
      "Epoch 278/900\n",
      "122/122 [==============================] - 0s 861us/step - loss: 143.1031 - mae: 7.9172 - val_loss: 29.9665 - val_mae: 3.9137\n",
      "Epoch 279/900\n",
      "122/122 [==============================] - 0s 787us/step - loss: 149.8612 - mae: 8.0521 - val_loss: 25.4804 - val_mae: 3.6761\n",
      "Epoch 280/900\n",
      "122/122 [==============================] - 0s 772us/step - loss: 137.6660 - mae: 7.7941 - val_loss: 29.8087 - val_mae: 4.1015\n",
      "Epoch 281/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 148.1755 - mae: 8.0078 - val_loss: 36.1029 - val_mae: 4.3778\n",
      "Epoch 282/900\n",
      "122/122 [==============================] - 0s 742us/step - loss: 149.0816 - mae: 8.0360 - val_loss: 28.3180 - val_mae: 3.8809\n",
      "Epoch 283/900\n",
      "122/122 [==============================] - 0s 773us/step - loss: 143.9611 - mae: 7.9207 - val_loss: 28.9068 - val_mae: 3.9172\n",
      "Epoch 284/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 141.1508 - mae: 7.8689 - val_loss: 32.6958 - val_mae: 4.3520\n",
      "Epoch 285/900\n",
      "122/122 [==============================] - 0s 954us/step - loss: 138.5760 - mae: 7.8192 - val_loss: 27.1538 - val_mae: 3.7592\n",
      "Epoch 286/900\n",
      "122/122 [==============================] - 0s 799us/step - loss: 156.2683 - mae: 8.0482 - val_loss: 31.6725 - val_mae: 4.2248\n",
      "Epoch 287/900\n",
      "122/122 [==============================] - 0s 722us/step - loss: 156.1867 - mae: 8.0695 - val_loss: 31.4333 - val_mae: 4.0643\n",
      "Epoch 288/900\n",
      "122/122 [==============================] - 0s 758us/step - loss: 146.4218 - mae: 8.0006 - val_loss: 27.1318 - val_mae: 3.7137\n",
      "Epoch 289/900\n",
      "122/122 [==============================] - 0s 721us/step - loss: 145.0107 - mae: 7.8580 - val_loss: 25.8526 - val_mae: 3.5870\n",
      "Epoch 290/900\n",
      "122/122 [==============================] - 0s 683us/step - loss: 140.4898 - mae: 7.9227 - val_loss: 30.1004 - val_mae: 3.9280\n",
      "Epoch 291/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 142.7793 - mae: 7.8878 - val_loss: 33.6032 - val_mae: 4.3944\n",
      "Epoch 292/900\n",
      "122/122 [==============================] - 0s 742us/step - loss: 135.9843 - mae: 7.8266 - val_loss: 34.5788 - val_mae: 4.2668\n",
      "Epoch 293/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 142.8631 - mae: 7.9119 - val_loss: 37.6943 - val_mae: 4.6471\n",
      "Epoch 294/900\n",
      "122/122 [==============================] - 0s 806us/step - loss: 131.7506 - mae: 7.6960 - val_loss: 28.3623 - val_mae: 3.8980\n",
      "Epoch 295/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 144.7521 - mae: 7.9515 - val_loss: 28.2304 - val_mae: 3.7895\n",
      "Epoch 296/900\n",
      "122/122 [==============================] - 0s 735us/step - loss: 137.5151 - mae: 7.8620 - val_loss: 28.5541 - val_mae: 3.9304\n",
      "Epoch 297/900\n",
      "122/122 [==============================] - 0s 831us/step - loss: 137.0014 - mae: 7.8688 - val_loss: 28.2440 - val_mae: 3.7141\n",
      "Epoch 298/900\n",
      "122/122 [==============================] - 0s 756us/step - loss: 134.5591 - mae: 7.7373 - val_loss: 27.3935 - val_mae: 3.7203\n",
      "Epoch 299/900\n",
      "122/122 [==============================] - 0s 849us/step - loss: 132.1704 - mae: 7.7323 - val_loss: 27.4581 - val_mae: 3.8094\n",
      "Epoch 300/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 136.9362 - mae: 7.7002 - val_loss: 29.7554 - val_mae: 3.9494\n",
      "Epoch 301/900\n",
      "122/122 [==============================] - 0s 718us/step - loss: 126.9819 - mae: 7.6065 - val_loss: 28.7821 - val_mae: 3.7133\n",
      "Epoch 302/900\n",
      "122/122 [==============================] - 0s 714us/step - loss: 143.1323 - mae: 7.9595 - val_loss: 39.1561 - val_mae: 4.5787\n",
      "Epoch 303/900\n",
      "122/122 [==============================] - 0s 692us/step - loss: 130.7451 - mae: 7.7080 - val_loss: 28.4305 - val_mae: 3.8187\n",
      "Epoch 304/900\n",
      "122/122 [==============================] - 0s 766us/step - loss: 134.3932 - mae: 7.6258 - val_loss: 32.2644 - val_mae: 4.0487\n",
      "Epoch 305/900\n",
      "122/122 [==============================] - 0s 716us/step - loss: 130.5380 - mae: 7.5897 - val_loss: 27.3932 - val_mae: 3.7752\n",
      "Epoch 306/900\n",
      "122/122 [==============================] - 0s 830us/step - loss: 133.8611 - mae: 7.6593 - val_loss: 26.6533 - val_mae: 3.6976\n",
      "Epoch 307/900\n",
      "122/122 [==============================] - 0s 732us/step - loss: 127.6077 - mae: 7.5549 - val_loss: 28.1965 - val_mae: 3.8505\n",
      "Epoch 308/900\n",
      "122/122 [==============================] - 0s 747us/step - loss: 129.8004 - mae: 7.7370 - val_loss: 29.7465 - val_mae: 3.8712\n",
      "Epoch 309/900\n",
      "122/122 [==============================] - 0s 656us/step - loss: 132.4321 - mae: 7.6314 - val_loss: 30.1597 - val_mae: 4.0535\n",
      "Epoch 310/900\n",
      "122/122 [==============================] - 0s 783us/step - loss: 129.9370 - mae: 7.6305 - val_loss: 37.3833 - val_mae: 4.5232\n",
      "Epoch 311/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 134.9322 - mae: 7.7541 - val_loss: 30.5881 - val_mae: 4.0282\n",
      "Epoch 312/900\n",
      "122/122 [==============================] - 0s 908us/step - loss: 131.0694 - mae: 7.6345 - val_loss: 32.6483 - val_mae: 4.1998\n",
      "Epoch 313/900\n",
      "122/122 [==============================] - 0s 765us/step - loss: 130.3503 - mae: 7.6106 - val_loss: 28.3590 - val_mae: 3.7875\n",
      "Epoch 314/900\n",
      "122/122 [==============================] - 0s 681us/step - loss: 128.8152 - mae: 7.6930 - val_loss: 34.6528 - val_mae: 4.3105\n",
      "Epoch 315/900\n",
      "122/122 [==============================] - 0s 739us/step - loss: 120.3342 - mae: 7.4324 - val_loss: 31.1229 - val_mae: 4.0066\n",
      "Epoch 316/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 130.8118 - mae: 7.6992 - val_loss: 31.4148 - val_mae: 4.0530\n",
      "Epoch 317/900\n",
      "122/122 [==============================] - 0s 858us/step - loss: 129.9184 - mae: 7.5359 - val_loss: 34.9765 - val_mae: 4.2668\n",
      "Epoch 318/900\n",
      "122/122 [==============================] - 0s 796us/step - loss: 132.2506 - mae: 7.5835 - val_loss: 27.4306 - val_mae: 3.7328\n",
      "Epoch 319/900\n",
      "122/122 [==============================] - 0s 726us/step - loss: 135.0894 - mae: 7.7083 - val_loss: 40.9546 - val_mae: 4.8776\n",
      "Epoch 320/900\n",
      "122/122 [==============================] - 0s 727us/step - loss: 121.1769 - mae: 7.5509 - val_loss: 30.8487 - val_mae: 3.8095\n",
      "Epoch 321/900\n",
      "122/122 [==============================] - 0s 788us/step - loss: 122.6446 - mae: 7.5614 - val_loss: 29.1900 - val_mae: 3.7431\n",
      "Epoch 322/900\n",
      "122/122 [==============================] - 0s 868us/step - loss: 138.9608 - mae: 7.7071 - val_loss: 40.5069 - val_mae: 4.7553\n",
      "Epoch 323/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 120.1740 - mae: 7.4211 - val_loss: 27.7895 - val_mae: 3.8452\n",
      "Epoch 324/900\n",
      "122/122 [==============================] - 0s 723us/step - loss: 117.0910 - mae: 7.3685 - val_loss: 25.2295 - val_mae: 3.5594\n",
      "Epoch 325/900\n",
      "122/122 [==============================] - 0s 828us/step - loss: 130.0153 - mae: 7.7512 - val_loss: 39.0648 - val_mae: 4.5243\n",
      "Epoch 326/900\n",
      "122/122 [==============================] - 0s 920us/step - loss: 124.9862 - mae: 7.5642 - val_loss: 29.6497 - val_mae: 3.9804\n",
      "Epoch 327/900\n",
      "122/122 [==============================] - 0s 736us/step - loss: 125.3399 - mae: 7.5892 - val_loss: 37.5078 - val_mae: 4.6539\n",
      "Epoch 328/900\n",
      "122/122 [==============================] - 0s 751us/step - loss: 125.8290 - mae: 7.6447 - val_loss: 28.3377 - val_mae: 3.7291\n",
      "Epoch 329/900\n",
      "122/122 [==============================] - 0s 746us/step - loss: 121.2817 - mae: 7.4782 - val_loss: 27.2784 - val_mae: 3.6521\n",
      "Epoch 330/900\n",
      "122/122 [==============================] - 0s 815us/step - loss: 111.1873 - mae: 7.2134 - val_loss: 27.3005 - val_mae: 3.6644\n",
      "Epoch 331/900\n",
      "122/122 [==============================] - 0s 867us/step - loss: 114.2238 - mae: 7.2725 - val_loss: 37.5360 - val_mae: 4.5995\n",
      "Epoch 332/900\n",
      "122/122 [==============================] - 0s 724us/step - loss: 119.2397 - mae: 7.4609 - val_loss: 32.6236 - val_mae: 4.1578\n",
      "Epoch 333/900\n",
      "122/122 [==============================] - 0s 765us/step - loss: 121.2628 - mae: 7.4989 - val_loss: 28.2579 - val_mae: 3.8246\n",
      "Epoch 334/900\n",
      "122/122 [==============================] - 0s 799us/step - loss: 115.8257 - mae: 7.2576 - val_loss: 33.7876 - val_mae: 4.1311\n",
      "Epoch 335/900\n",
      "122/122 [==============================] - 0s 758us/step - loss: 108.0264 - mae: 7.2406 - val_loss: 31.6454 - val_mae: 4.0679\n",
      "Epoch 336/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 116.5110 - mae: 7.2811 - val_loss: 28.8888 - val_mae: 3.8975\n",
      "Epoch 337/900\n",
      "122/122 [==============================] - 0s 683us/step - loss: 119.7134 - mae: 7.4741 - val_loss: 28.8719 - val_mae: 3.8803\n",
      "Epoch 338/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 115.3272 - mae: 7.3342 - val_loss: 32.6701 - val_mae: 4.1390\n",
      "Epoch 339/900\n",
      "122/122 [==============================] - 0s 762us/step - loss: 112.5233 - mae: 7.3920 - val_loss: 28.8330 - val_mae: 3.9190\n",
      "Epoch 340/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 123.8230 - mae: 7.4995 - val_loss: 26.4932 - val_mae: 3.6469\n",
      "Epoch 341/900\n",
      "122/122 [==============================] - 0s 831us/step - loss: 116.8467 - mae: 7.3662 - val_loss: 38.6031 - val_mae: 4.6616\n",
      "Epoch 342/900\n",
      "122/122 [==============================] - 0s 759us/step - loss: 119.5442 - mae: 7.3051 - val_loss: 27.4987 - val_mae: 3.7118\n",
      "Epoch 343/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 115.9381 - mae: 7.2577 - val_loss: 35.5661 - val_mae: 4.2754\n",
      "Epoch 344/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 120.3904 - mae: 7.4599 - val_loss: 42.0035 - val_mae: 5.0145\n",
      "Epoch 345/900\n",
      "122/122 [==============================] - 0s 735us/step - loss: 119.3877 - mae: 7.3664 - val_loss: 34.6026 - val_mae: 4.3416\n",
      "Epoch 346/900\n",
      "122/122 [==============================] - 0s 674us/step - loss: 108.5634 - mae: 7.1781 - val_loss: 33.4468 - val_mae: 4.1820\n",
      "Epoch 347/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 115.8529 - mae: 7.2902 - val_loss: 29.6262 - val_mae: 3.9112\n",
      "Epoch 348/900\n",
      "122/122 [==============================] - 0s 775us/step - loss: 112.7369 - mae: 7.3086 - val_loss: 32.7039 - val_mae: 4.1692\n",
      "Epoch 349/900\n",
      "122/122 [==============================] - 0s 755us/step - loss: 112.3858 - mae: 7.2605 - val_loss: 26.0431 - val_mae: 3.6312\n",
      "Epoch 350/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 107.5745 - mae: 7.1752 - val_loss: 31.4398 - val_mae: 4.0632\n",
      "Epoch 351/900\n",
      "122/122 [==============================] - 0s 722us/step - loss: 107.8657 - mae: 7.1735 - val_loss: 29.3561 - val_mae: 3.8610\n",
      "Epoch 352/900\n",
      "122/122 [==============================] - 0s 768us/step - loss: 109.1171 - mae: 7.2131 - val_loss: 25.3446 - val_mae: 3.5321\n",
      "Epoch 353/900\n",
      "122/122 [==============================] - 0s 692us/step - loss: 109.9010 - mae: 7.1736 - val_loss: 31.3633 - val_mae: 4.0835\n",
      "Epoch 354/900\n",
      "122/122 [==============================] - 0s 657us/step - loss: 113.2213 - mae: 7.2189 - val_loss: 47.1985 - val_mae: 5.1610\n",
      "Epoch 355/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 102.7875 - mae: 7.1851 - val_loss: 31.2786 - val_mae: 3.8345\n",
      "Epoch 356/900\n",
      "122/122 [==============================] - 0s 792us/step - loss: 112.3901 - mae: 7.2694 - val_loss: 30.9898 - val_mae: 3.9432\n",
      "Epoch 357/900\n",
      "122/122 [==============================] - 0s 685us/step - loss: 109.4371 - mae: 7.1794 - val_loss: 30.5079 - val_mae: 3.9996\n",
      "Epoch 358/900\n",
      "122/122 [==============================] - 0s 828us/step - loss: 108.5616 - mae: 7.1121 - val_loss: 28.9228 - val_mae: 3.9193\n",
      "Epoch 359/900\n",
      "122/122 [==============================] - 0s 792us/step - loss: 105.0624 - mae: 7.0634 - val_loss: 37.3545 - val_mae: 4.2709\n",
      "Epoch 360/900\n",
      "122/122 [==============================] - 0s 694us/step - loss: 108.0557 - mae: 7.2110 - val_loss: 32.0478 - val_mae: 3.9923\n",
      "Epoch 361/900\n",
      "122/122 [==============================] - 0s 727us/step - loss: 113.9275 - mae: 7.2594 - val_loss: 52.3061 - val_mae: 5.5254\n",
      "Epoch 362/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 106.3012 - mae: 7.1839 - val_loss: 34.1034 - val_mae: 4.0540\n",
      "Epoch 363/900\n",
      "122/122 [==============================] - 0s 718us/step - loss: 110.1045 - mae: 7.1614 - val_loss: 29.4384 - val_mae: 3.9002\n",
      "Epoch 364/900\n",
      "122/122 [==============================] - 0s 847us/step - loss: 110.2895 - mae: 7.1051 - val_loss: 34.0448 - val_mae: 4.2992\n",
      "Epoch 365/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 101.8813 - mae: 6.9988 - val_loss: 31.7524 - val_mae: 4.0756\n",
      "Epoch 366/900\n",
      "122/122 [==============================] - 0s 751us/step - loss: 104.3898 - mae: 7.0591 - val_loss: 32.1129 - val_mae: 4.0097\n",
      "Epoch 367/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 105.5747 - mae: 7.0165 - val_loss: 28.6964 - val_mae: 3.8349\n",
      "Epoch 368/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 95.6595 - mae: 6.8028 - val_loss: 32.9665 - val_mae: 4.0837\n",
      "Epoch 369/900\n",
      "122/122 [==============================] - 0s 858us/step - loss: 110.8708 - mae: 7.2496 - val_loss: 42.3077 - val_mae: 4.8979\n",
      "Epoch 370/900\n",
      "122/122 [==============================] - 0s 762us/step - loss: 104.5638 - mae: 7.1747 - val_loss: 33.4355 - val_mae: 4.2857\n",
      "Epoch 371/900\n",
      "122/122 [==============================] - 0s 739us/step - loss: 101.5826 - mae: 6.9568 - val_loss: 30.4042 - val_mae: 3.8945\n",
      "Epoch 372/900\n",
      "122/122 [==============================] - 0s 673us/step - loss: 105.8500 - mae: 7.0287 - val_loss: 29.4129 - val_mae: 3.8547\n",
      "Epoch 373/900\n",
      "122/122 [==============================] - 0s 805us/step - loss: 101.2784 - mae: 7.0037 - val_loss: 33.8751 - val_mae: 4.2545\n",
      "Epoch 374/900\n",
      "122/122 [==============================] - 0s 824us/step - loss: 106.6186 - mae: 7.0942 - val_loss: 34.6574 - val_mae: 4.3945\n",
      "Epoch 375/900\n",
      "122/122 [==============================] - 0s 793us/step - loss: 102.9986 - mae: 7.0691 - val_loss: 30.6786 - val_mae: 4.0219\n",
      "Epoch 376/900\n",
      "122/122 [==============================] - 0s 750us/step - loss: 109.7839 - mae: 7.2104 - val_loss: 29.7258 - val_mae: 3.8429\n",
      "Epoch 377/900\n",
      "122/122 [==============================] - 0s 677us/step - loss: 105.2719 - mae: 6.9695 - val_loss: 41.2677 - val_mae: 4.8691\n",
      "Epoch 378/900\n",
      "122/122 [==============================] - 0s 771us/step - loss: 102.5309 - mae: 7.0053 - val_loss: 30.3323 - val_mae: 3.8891\n",
      "Epoch 379/900\n",
      "122/122 [==============================] - 0s 918us/step - loss: 99.6087 - mae: 6.9213 - val_loss: 30.6449 - val_mae: 3.9288\n",
      "Epoch 380/900\n",
      "122/122 [==============================] - 0s 730us/step - loss: 110.4936 - mae: 7.0950 - val_loss: 30.4378 - val_mae: 4.0105\n",
      "Epoch 381/900\n",
      "122/122 [==============================] - 0s 711us/step - loss: 96.7749 - mae: 6.8335 - val_loss: 29.8517 - val_mae: 3.9356\n",
      "Epoch 382/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 105.4014 - mae: 7.0480 - val_loss: 36.0498 - val_mae: 4.4712\n",
      "Epoch 383/900\n",
      "122/122 [==============================] - 0s 806us/step - loss: 94.9103 - mae: 6.8061 - val_loss: 28.2210 - val_mae: 3.7940\n",
      "Epoch 384/900\n",
      "122/122 [==============================] - 0s 753us/step - loss: 108.1793 - mae: 7.1264 - val_loss: 26.4214 - val_mae: 3.6542\n",
      "Epoch 385/900\n",
      "122/122 [==============================] - 0s 855us/step - loss: 91.6861 - mae: 6.7076 - val_loss: 30.9791 - val_mae: 3.9301\n",
      "Epoch 386/900\n",
      "122/122 [==============================] - 0s 708us/step - loss: 103.3270 - mae: 6.9749 - val_loss: 28.6440 - val_mae: 3.8945\n",
      "Epoch 387/900\n",
      "122/122 [==============================] - 0s 678us/step - loss: 94.4614 - mae: 6.7258 - val_loss: 31.5621 - val_mae: 3.9858\n",
      "Epoch 388/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 98.9826 - mae: 6.8022 - val_loss: 28.0475 - val_mae: 3.8202\n",
      "Epoch 389/900\n",
      "122/122 [==============================] - 0s 701us/step - loss: 99.1947 - mae: 6.7922 - val_loss: 28.1817 - val_mae: 3.7180\n",
      "Epoch 390/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 97.4225 - mae: 6.8862 - val_loss: 28.2673 - val_mae: 3.9362\n",
      "Epoch 391/900\n",
      "122/122 [==============================] - 0s 781us/step - loss: 100.2997 - mae: 6.9264 - val_loss: 38.4337 - val_mae: 4.6263\n",
      "Epoch 392/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 103.5129 - mae: 7.0138 - val_loss: 29.7993 - val_mae: 3.9062\n",
      "Epoch 393/900\n",
      "122/122 [==============================] - 0s 726us/step - loss: 99.1805 - mae: 6.8681 - val_loss: 35.4000 - val_mae: 4.2744\n",
      "Epoch 394/900\n",
      "122/122 [==============================] - 0s 751us/step - loss: 103.0121 - mae: 6.9678 - val_loss: 31.7293 - val_mae: 3.9917\n",
      "Epoch 395/900\n",
      "122/122 [==============================] - 0s 892us/step - loss: 98.5962 - mae: 6.8124 - val_loss: 27.0793 - val_mae: 3.6172\n",
      "Epoch 396/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 96.4450 - mae: 6.8535 - val_loss: 30.8331 - val_mae: 4.0432\n",
      "Epoch 397/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 95.4460 - mae: 6.8757 - val_loss: 29.4773 - val_mae: 3.8077\n",
      "Epoch 398/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 96.2016 - mae: 6.8804 - val_loss: 33.6337 - val_mae: 4.2581\n",
      "Epoch 399/900\n",
      "122/122 [==============================] - 0s 687us/step - loss: 101.6913 - mae: 6.9672 - val_loss: 37.5626 - val_mae: 4.5274\n",
      "Epoch 400/900\n",
      "122/122 [==============================] - 0s 697us/step - loss: 95.0478 - mae: 6.7252 - val_loss: 31.9816 - val_mae: 4.1458\n",
      "Epoch 401/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 103.1035 - mae: 6.9235 - val_loss: 28.0713 - val_mae: 3.8048\n",
      "Epoch 402/900\n",
      "122/122 [==============================] - 0s 740us/step - loss: 101.5630 - mae: 6.9511 - val_loss: 30.8935 - val_mae: 4.0788\n",
      "Epoch 403/900\n",
      "122/122 [==============================] - 0s 746us/step - loss: 95.4498 - mae: 6.8527 - val_loss: 31.5003 - val_mae: 3.9160\n",
      "Epoch 404/900\n",
      "122/122 [==============================] - 0s 794us/step - loss: 89.7304 - mae: 6.7537 - val_loss: 30.0928 - val_mae: 3.9766\n",
      "Epoch 405/900\n",
      "122/122 [==============================] - 0s 740us/step - loss: 98.5521 - mae: 6.9773 - val_loss: 28.6620 - val_mae: 3.9088\n",
      "Epoch 406/900\n",
      "122/122 [==============================] - 0s 708us/step - loss: 87.7711 - mae: 6.6404 - val_loss: 32.5398 - val_mae: 4.2159\n",
      "Epoch 407/900\n",
      "122/122 [==============================] - 0s 820us/step - loss: 94.5224 - mae: 6.7741 - val_loss: 31.5517 - val_mae: 3.9292\n",
      "Epoch 408/900\n",
      "122/122 [==============================] - 0s 728us/step - loss: 92.6274 - mae: 6.7054 - val_loss: 33.0165 - val_mae: 4.1486\n",
      "Epoch 409/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 98.0081 - mae: 6.8326 - val_loss: 37.3474 - val_mae: 4.4447\n",
      "Epoch 410/900\n",
      "122/122 [==============================] - 0s 783us/step - loss: 94.3330 - mae: 6.7806 - val_loss: 27.3233 - val_mae: 3.7113\n",
      "Epoch 411/900\n",
      "122/122 [==============================] - 0s 769us/step - loss: 91.6929 - mae: 6.7501 - val_loss: 32.2677 - val_mae: 3.9279\n",
      "Epoch 412/900\n",
      "122/122 [==============================] - 0s 734us/step - loss: 88.9403 - mae: 6.6556 - val_loss: 35.3258 - val_mae: 4.4814\n",
      "Epoch 413/900\n",
      "122/122 [==============================] - 0s 691us/step - loss: 96.7799 - mae: 6.7849 - val_loss: 32.6921 - val_mae: 4.0947\n",
      "Epoch 414/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 99.3030 - mae: 6.9254 - val_loss: 28.6525 - val_mae: 3.7331\n",
      "Epoch 415/900\n",
      "122/122 [==============================] - 0s 784us/step - loss: 90.0676 - mae: 6.6320 - val_loss: 32.3844 - val_mae: 4.0357\n",
      "Epoch 416/900\n",
      "122/122 [==============================] - 0s 704us/step - loss: 91.7111 - mae: 6.6615 - val_loss: 30.7227 - val_mae: 4.0139\n",
      "Epoch 417/900\n",
      "122/122 [==============================] - 0s 881us/step - loss: 88.4141 - mae: 6.6428 - val_loss: 29.6539 - val_mae: 3.7611\n",
      "Epoch 418/900\n",
      "122/122 [==============================] - 0s 765us/step - loss: 93.0820 - mae: 6.7552 - val_loss: 41.3191 - val_mae: 4.4521\n",
      "Epoch 419/900\n",
      "122/122 [==============================] - 0s 741us/step - loss: 90.2655 - mae: 6.7110 - val_loss: 31.3823 - val_mae: 3.8508\n",
      "Epoch 420/900\n",
      "122/122 [==============================] - 0s 793us/step - loss: 92.4258 - mae: 6.7412 - val_loss: 30.0417 - val_mae: 3.9646\n",
      "Epoch 421/900\n",
      "122/122 [==============================] - 0s 789us/step - loss: 91.2548 - mae: 6.6574 - val_loss: 30.4563 - val_mae: 3.8562\n",
      "Epoch 422/900\n",
      "122/122 [==============================] - 0s 731us/step - loss: 89.5861 - mae: 6.5946 - val_loss: 39.9287 - val_mae: 4.7436\n",
      "Epoch 423/900\n",
      "122/122 [==============================] - 0s 764us/step - loss: 88.0174 - mae: 6.5559 - val_loss: 37.0728 - val_mae: 4.5106\n",
      "Epoch 424/900\n",
      "122/122 [==============================] - 0s 727us/step - loss: 88.5155 - mae: 6.6126 - val_loss: 28.7183 - val_mae: 3.7043\n",
      "Epoch 425/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 87.2661 - mae: 6.5957 - val_loss: 31.5668 - val_mae: 3.8968\n",
      "Epoch 426/900\n",
      "122/122 [==============================] - 0s 817us/step - loss: 85.8917 - mae: 6.5129 - val_loss: 46.7740 - val_mae: 5.1994\n",
      "Epoch 427/900\n",
      "122/122 [==============================] - 0s 922us/step - loss: 86.8638 - mae: 6.6099 - val_loss: 32.6047 - val_mae: 4.2191\n",
      "Epoch 428/900\n",
      "122/122 [==============================] - 0s 776us/step - loss: 84.0939 - mae: 6.5404 - val_loss: 28.8013 - val_mae: 3.7807\n",
      "Epoch 429/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 92.1292 - mae: 6.5950 - val_loss: 35.9613 - val_mae: 4.4191\n",
      "Epoch 430/900\n",
      "122/122 [==============================] - 0s 686us/step - loss: 92.5333 - mae: 6.7082 - val_loss: 30.2270 - val_mae: 4.0810\n",
      "Epoch 431/900\n",
      "122/122 [==============================] - 0s 738us/step - loss: 92.3605 - mae: 6.6931 - val_loss: 34.3713 - val_mae: 4.3294\n",
      "Epoch 432/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 84.2513 - mae: 6.5712 - val_loss: 27.2371 - val_mae: 3.6711\n",
      "Epoch 433/900\n",
      "122/122 [==============================] - 0s 994us/step - loss: 91.2136 - mae: 6.7526 - val_loss: 31.3302 - val_mae: 3.9618\n",
      "Epoch 434/900\n",
      "122/122 [==============================] - 0s 723us/step - loss: 87.0157 - mae: 6.6011 - val_loss: 30.6843 - val_mae: 4.0374\n",
      "Epoch 435/900\n",
      "122/122 [==============================] - 0s 684us/step - loss: 89.5201 - mae: 6.5717 - val_loss: 31.5845 - val_mae: 3.9238\n",
      "Epoch 436/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 85.9254 - mae: 6.4666 - val_loss: 31.0495 - val_mae: 3.8938\n",
      "Epoch 437/900\n",
      "122/122 [==============================] - 0s 742us/step - loss: 82.7252 - mae: 6.4489 - val_loss: 37.7766 - val_mae: 4.1846\n",
      "Epoch 438/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 87.4291 - mae: 6.5242 - val_loss: 35.3811 - val_mae: 4.3457\n",
      "Epoch 439/900\n",
      "122/122 [==============================] - 0s 858us/step - loss: 80.3561 - mae: 6.3220 - val_loss: 38.1913 - val_mae: 4.6063\n",
      "Epoch 440/900\n",
      "122/122 [==============================] - 0s 753us/step - loss: 86.8129 - mae: 6.5710 - val_loss: 31.5252 - val_mae: 3.9330\n",
      "Epoch 441/900\n",
      "122/122 [==============================] - 0s 849us/step - loss: 85.6973 - mae: 6.5431 - val_loss: 32.6188 - val_mae: 4.0580\n",
      "Epoch 442/900\n",
      "122/122 [==============================] - 0s 734us/step - loss: 82.2573 - mae: 6.3638 - val_loss: 32.2952 - val_mae: 4.0569\n",
      "Epoch 443/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 87.3165 - mae: 6.4690 - val_loss: 29.6575 - val_mae: 3.8324\n",
      "Epoch 444/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 83.5999 - mae: 6.5576 - val_loss: 29.0452 - val_mae: 3.7693\n",
      "Epoch 445/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 78.7076 - mae: 6.2811 - val_loss: 32.0024 - val_mae: 3.9389\n",
      "Epoch 446/900\n",
      "122/122 [==============================] - 0s 825us/step - loss: 83.4877 - mae: 6.4331 - val_loss: 29.8863 - val_mae: 3.8051\n",
      "Epoch 447/900\n",
      "122/122 [==============================] - 0s 797us/step - loss: 86.0149 - mae: 6.6501 - val_loss: 38.6897 - val_mae: 4.4673\n",
      "Epoch 448/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 81.2505 - mae: 6.3971 - val_loss: 42.3395 - val_mae: 4.9980\n",
      "Epoch 449/900\n",
      "122/122 [==============================] - 0s 775us/step - loss: 82.9132 - mae: 6.5078 - val_loss: 43.9430 - val_mae: 4.9657\n",
      "Epoch 450/900\n",
      "122/122 [==============================] - 0s 907us/step - loss: 78.2439 - mae: 6.2505 - val_loss: 30.1677 - val_mae: 3.8763\n",
      "Epoch 451/900\n",
      "122/122 [==============================] - 0s 788us/step - loss: 80.8485 - mae: 6.3619 - val_loss: 36.8100 - val_mae: 4.5768\n",
      "Epoch 452/900\n",
      "122/122 [==============================] - 0s 842us/step - loss: 85.0858 - mae: 6.4610 - val_loss: 32.8977 - val_mae: 3.9571\n",
      "Epoch 453/900\n",
      "122/122 [==============================] - 0s 723us/step - loss: 85.8248 - mae: 6.4291 - val_loss: 28.0840 - val_mae: 3.7908\n",
      "Epoch 454/900\n",
      "122/122 [==============================] - 0s 703us/step - loss: 86.3677 - mae: 6.5013 - val_loss: 31.0587 - val_mae: 3.9746\n",
      "Epoch 455/900\n",
      "122/122 [==============================] - 0s 704us/step - loss: 84.4077 - mae: 6.5081 - val_loss: 31.4321 - val_mae: 4.1001\n",
      "Epoch 456/900\n",
      "122/122 [==============================] - 0s 680us/step - loss: 80.6949 - mae: 6.2923 - val_loss: 31.1004 - val_mae: 3.7911\n",
      "Epoch 457/900\n",
      "122/122 [==============================] - 0s 767us/step - loss: 83.4823 - mae: 6.4412 - val_loss: 36.8897 - val_mae: 4.3820\n",
      "Epoch 458/900\n",
      "122/122 [==============================] - 0s 764us/step - loss: 80.2862 - mae: 6.2941 - val_loss: 31.1425 - val_mae: 3.9002\n",
      "Epoch 459/900\n",
      "122/122 [==============================] - 0s 807us/step - loss: 79.8819 - mae: 6.3157 - val_loss: 30.9507 - val_mae: 3.7871\n",
      "Epoch 460/900\n",
      "122/122 [==============================] - 0s 787us/step - loss: 75.4379 - mae: 6.2475 - val_loss: 30.2315 - val_mae: 3.7931\n",
      "Epoch 461/900\n",
      "122/122 [==============================] - 0s 701us/step - loss: 80.4853 - mae: 6.3677 - val_loss: 42.9132 - val_mae: 4.9014\n",
      "Epoch 462/900\n",
      "122/122 [==============================] - 0s 779us/step - loss: 81.8095 - mae: 6.3824 - val_loss: 39.1928 - val_mae: 4.2082\n",
      "Epoch 463/900\n",
      "122/122 [==============================] - 0s 748us/step - loss: 79.6586 - mae: 6.3268 - val_loss: 35.9719 - val_mae: 4.2708\n",
      "Epoch 464/900\n",
      "122/122 [==============================] - 0s 813us/step - loss: 80.5037 - mae: 6.3630 - val_loss: 37.5517 - val_mae: 4.1473\n",
      "Epoch 465/900\n",
      "122/122 [==============================] - 0s 818us/step - loss: 77.8210 - mae: 6.3268 - val_loss: 33.6385 - val_mae: 4.0115\n",
      "Epoch 466/900\n",
      "122/122 [==============================] - 0s 819us/step - loss: 77.7715 - mae: 6.3542 - val_loss: 29.2000 - val_mae: 3.6980\n",
      "Epoch 467/900\n",
      "122/122 [==============================] - 0s 959us/step - loss: 76.9770 - mae: 6.2631 - val_loss: 32.5396 - val_mae: 4.0316\n",
      "Epoch 468/900\n",
      "122/122 [==============================] - 0s 725us/step - loss: 79.3649 - mae: 6.3142 - val_loss: 32.8731 - val_mae: 4.0292\n",
      "Epoch 469/900\n",
      "122/122 [==============================] - 0s 728us/step - loss: 77.7057 - mae: 6.3746 - val_loss: 65.4697 - val_mae: 6.2639\n",
      "Epoch 470/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 85.8829 - mae: 6.4442 - val_loss: 31.7412 - val_mae: 3.8100\n",
      "Epoch 471/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 77.8526 - mae: 6.3449 - val_loss: 37.5970 - val_mae: 4.3649\n",
      "Epoch 472/900\n",
      "122/122 [==============================] - 0s 705us/step - loss: 78.8697 - mae: 6.2503 - val_loss: 33.7594 - val_mae: 4.1022\n",
      "Epoch 473/900\n",
      "122/122 [==============================] - 0s 844us/step - loss: 74.3443 - mae: 6.1944 - val_loss: 32.0041 - val_mae: 4.0250\n",
      "Epoch 474/900\n",
      "122/122 [==============================] - 0s 705us/step - loss: 78.5480 - mae: 6.3261 - val_loss: 31.6463 - val_mae: 3.9962\n",
      "Epoch 475/900\n",
      "122/122 [==============================] - 0s 697us/step - loss: 76.6020 - mae: 6.2133 - val_loss: 33.2833 - val_mae: 4.0979\n",
      "Epoch 476/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 76.6990 - mae: 6.3422 - val_loss: 40.8050 - val_mae: 4.7216\n",
      "Epoch 477/900\n",
      "122/122 [==============================] - 0s 677us/step - loss: 80.2763 - mae: 6.2547 - val_loss: 35.7784 - val_mae: 4.1740\n",
      "Epoch 478/900\n",
      "122/122 [==============================] - 0s 772us/step - loss: 72.5173 - mae: 6.1418 - val_loss: 31.6403 - val_mae: 3.9920\n",
      "Epoch 479/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 75.1821 - mae: 6.2277 - val_loss: 38.9724 - val_mae: 4.5070\n",
      "Epoch 480/900\n",
      "122/122 [==============================] - 0s 694us/step - loss: 78.2561 - mae: 6.2386 - val_loss: 35.6539 - val_mae: 4.2415\n",
      "Epoch 481/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 74.0537 - mae: 6.2874 - val_loss: 41.1061 - val_mae: 4.8169\n",
      "Epoch 482/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 75.2951 - mae: 6.1771 - val_loss: 35.6164 - val_mae: 4.0645\n",
      "Epoch 483/900\n",
      "122/122 [==============================] - 0s 725us/step - loss: 74.8292 - mae: 6.1826 - val_loss: 32.7324 - val_mae: 3.8961\n",
      "Epoch 484/900\n",
      "122/122 [==============================] - 0s 663us/step - loss: 75.7479 - mae: 6.1772 - val_loss: 33.7295 - val_mae: 4.1811\n",
      "Epoch 485/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 80.6658 - mae: 6.2295 - val_loss: 34.5483 - val_mae: 4.0749\n",
      "Epoch 486/900\n",
      "122/122 [==============================] - 0s 785us/step - loss: 80.3916 - mae: 6.3429 - val_loss: 31.4058 - val_mae: 4.0472\n",
      "Epoch 487/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 73.2596 - mae: 6.1742 - val_loss: 33.0138 - val_mae: 3.9849\n",
      "Epoch 488/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 78.4775 - mae: 6.3019 - val_loss: 34.6373 - val_mae: 4.0540\n",
      "Epoch 489/900\n",
      "122/122 [==============================] - 0s 707us/step - loss: 69.0042 - mae: 6.0132 - val_loss: 35.3514 - val_mae: 4.2195\n",
      "Epoch 490/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 69.6012 - mae: 6.0058 - val_loss: 36.1071 - val_mae: 4.2998\n",
      "Epoch 491/900\n",
      "122/122 [==============================] - 0s 652us/step - loss: 77.0163 - mae: 6.2246 - val_loss: 41.4164 - val_mae: 4.7039\n",
      "Epoch 492/900\n",
      "122/122 [==============================] - 0s 649us/step - loss: 76.4136 - mae: 6.2580 - val_loss: 34.5403 - val_mae: 4.0531\n",
      "Epoch 493/900\n",
      "122/122 [==============================] - 0s 828us/step - loss: 74.0411 - mae: 6.2153 - val_loss: 33.3713 - val_mae: 3.9647\n",
      "Epoch 494/900\n",
      "122/122 [==============================] - 0s 774us/step - loss: 77.0088 - mae: 6.2568 - val_loss: 34.4924 - val_mae: 4.0719\n",
      "Epoch 495/900\n",
      "122/122 [==============================] - 0s 687us/step - loss: 72.1461 - mae: 6.1940 - val_loss: 33.2296 - val_mae: 3.8727\n",
      "Epoch 496/900\n",
      "122/122 [==============================] - 0s 691us/step - loss: 73.7327 - mae: 6.1166 - val_loss: 47.7459 - val_mae: 5.0305\n",
      "Epoch 497/900\n",
      "122/122 [==============================] - 0s 684us/step - loss: 73.2778 - mae: 6.1559 - val_loss: 32.7756 - val_mae: 3.9449\n",
      "Epoch 498/900\n",
      "122/122 [==============================] - 0s 765us/step - loss: 71.0190 - mae: 6.0864 - val_loss: 31.9525 - val_mae: 3.8558\n",
      "Epoch 499/900\n",
      "122/122 [==============================] - 0s 668us/step - loss: 77.0797 - mae: 6.1894 - val_loss: 37.9948 - val_mae: 4.3601\n",
      "Epoch 500/900\n",
      "122/122 [==============================] - 0s 731us/step - loss: 71.9997 - mae: 6.0484 - val_loss: 31.8230 - val_mae: 3.7931\n",
      "Epoch 501/900\n",
      "122/122 [==============================] - 0s 828us/step - loss: 69.2685 - mae: 6.0395 - val_loss: 34.6422 - val_mae: 4.1075\n",
      "Epoch 502/900\n",
      "122/122 [==============================] - 0s 795us/step - loss: 74.6525 - mae: 6.2104 - val_loss: 41.7828 - val_mae: 4.6138\n",
      "Epoch 503/900\n",
      "122/122 [==============================] - 0s 731us/step - loss: 73.7771 - mae: 6.1314 - val_loss: 39.0841 - val_mae: 4.5407\n",
      "Epoch 504/900\n",
      "122/122 [==============================] - 0s 651us/step - loss: 75.1685 - mae: 6.2023 - val_loss: 35.9412 - val_mae: 4.1904\n",
      "Epoch 505/900\n",
      "122/122 [==============================] - 0s 658us/step - loss: 68.7760 - mae: 6.0739 - val_loss: 29.6366 - val_mae: 3.7658\n",
      "Epoch 506/900\n",
      "122/122 [==============================] - 0s 651us/step - loss: 76.6784 - mae: 6.1552 - val_loss: 35.8885 - val_mae: 4.1546\n",
      "Epoch 507/900\n",
      "122/122 [==============================] - 0s 759us/step - loss: 67.8692 - mae: 5.9772 - val_loss: 34.2098 - val_mae: 4.0355\n",
      "Epoch 508/900\n",
      "122/122 [==============================] - 0s 678us/step - loss: 66.1172 - mae: 5.9050 - val_loss: 32.3714 - val_mae: 3.9087\n",
      "Epoch 509/900\n",
      "122/122 [==============================] - 0s 671us/step - loss: 70.7386 - mae: 6.0848 - val_loss: 39.8756 - val_mae: 4.4182\n",
      "Epoch 510/900\n",
      "122/122 [==============================] - 0s 633us/step - loss: 72.0351 - mae: 6.0400 - val_loss: 30.6459 - val_mae: 3.8305\n",
      "Epoch 511/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 72.0826 - mae: 6.0930 - val_loss: 31.8240 - val_mae: 3.8876\n",
      "Epoch 512/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 71.8401 - mae: 6.0097 - val_loss: 32.6287 - val_mae: 4.0148\n",
      "Epoch 513/900\n",
      "122/122 [==============================] - 0s 690us/step - loss: 69.9784 - mae: 5.9627 - val_loss: 42.2786 - val_mae: 4.8183\n",
      "Epoch 514/900\n",
      "122/122 [==============================] - 0s 690us/step - loss: 68.5829 - mae: 6.0063 - val_loss: 39.0217 - val_mae: 4.4586\n",
      "Epoch 515/900\n",
      "122/122 [==============================] - 0s 655us/step - loss: 66.9047 - mae: 5.9526 - val_loss: 45.6850 - val_mae: 5.0510\n",
      "Epoch 516/900\n",
      "122/122 [==============================] - 0s 688us/step - loss: 73.0696 - mae: 6.0678 - val_loss: 31.1445 - val_mae: 3.9754\n",
      "Epoch 517/900\n",
      "122/122 [==============================] - 0s 623us/step - loss: 71.0058 - mae: 6.0137 - val_loss: 31.1845 - val_mae: 3.9647\n",
      "Epoch 518/900\n",
      "122/122 [==============================] - 0s 636us/step - loss: 71.8254 - mae: 6.0015 - val_loss: 32.8254 - val_mae: 3.9276\n",
      "Epoch 519/900\n",
      "122/122 [==============================] - 0s 707us/step - loss: 72.4837 - mae: 6.1485 - val_loss: 40.3718 - val_mae: 4.4537\n",
      "Epoch 520/900\n",
      "122/122 [==============================] - 0s 761us/step - loss: 73.4242 - mae: 6.0155 - val_loss: 42.8503 - val_mae: 4.7323\n",
      "Epoch 521/900\n",
      "122/122 [==============================] - 0s 719us/step - loss: 69.5237 - mae: 5.9925 - val_loss: 33.9381 - val_mae: 3.8962\n",
      "Epoch 522/900\n",
      "122/122 [==============================] - 0s 705us/step - loss: 67.8810 - mae: 5.9690 - val_loss: 33.7042 - val_mae: 3.9153\n",
      "Epoch 523/900\n",
      "122/122 [==============================] - 0s 692us/step - loss: 71.2422 - mae: 6.0963 - val_loss: 34.6524 - val_mae: 3.9473\n",
      "Epoch 524/900\n",
      "122/122 [==============================] - 0s 642us/step - loss: 73.8994 - mae: 6.0483 - val_loss: 41.2841 - val_mae: 4.7348\n",
      "Epoch 525/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 67.7132 - mae: 5.9518 - val_loss: 31.8013 - val_mae: 3.8915\n",
      "Epoch 526/900\n",
      "122/122 [==============================] - 0s 731us/step - loss: 69.7673 - mae: 6.0312 - val_loss: 31.5190 - val_mae: 3.8523\n",
      "Epoch 527/900\n",
      "122/122 [==============================] - 0s 728us/step - loss: 66.5772 - mae: 5.8590 - val_loss: 32.9370 - val_mae: 3.8681\n",
      "Epoch 528/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 68.3981 - mae: 6.0318 - val_loss: 29.3193 - val_mae: 3.7174\n",
      "Epoch 529/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 66.8170 - mae: 5.9175 - val_loss: 44.0533 - val_mae: 4.8095\n",
      "Epoch 530/900\n",
      "122/122 [==============================] - 0s 741us/step - loss: 64.1716 - mae: 5.9185 - val_loss: 36.2707 - val_mae: 4.2062\n",
      "Epoch 531/900\n",
      "122/122 [==============================] - 0s 787us/step - loss: 65.0325 - mae: 5.8265 - val_loss: 34.1898 - val_mae: 4.0959\n",
      "Epoch 532/900\n",
      "122/122 [==============================] - 0s 732us/step - loss: 66.4799 - mae: 5.8826 - val_loss: 38.0892 - val_mae: 4.4567\n",
      "Epoch 533/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 71.3076 - mae: 6.0230 - val_loss: 34.2261 - val_mae: 4.0082\n",
      "Epoch 534/900\n",
      "122/122 [==============================] - 0s 695us/step - loss: 70.2006 - mae: 6.0271 - val_loss: 39.2818 - val_mae: 4.4225\n",
      "Epoch 535/900\n",
      "122/122 [==============================] - 0s 725us/step - loss: 68.1574 - mae: 6.0019 - val_loss: 42.0026 - val_mae: 4.3681\n",
      "Epoch 536/900\n",
      "122/122 [==============================] - 0s 700us/step - loss: 66.0646 - mae: 5.8645 - val_loss: 39.8313 - val_mae: 4.2988\n",
      "Epoch 537/900\n",
      "122/122 [==============================] - 0s 804us/step - loss: 68.7881 - mae: 5.9596 - val_loss: 46.2097 - val_mae: 5.0230\n",
      "Epoch 538/900\n",
      "122/122 [==============================] - 0s 649us/step - loss: 66.1946 - mae: 5.9296 - val_loss: 33.0938 - val_mae: 3.9063\n",
      "Epoch 539/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 70.5936 - mae: 5.9851 - val_loss: 37.5373 - val_mae: 4.3853\n",
      "Epoch 540/900\n",
      "122/122 [==============================] - 0s 664us/step - loss: 72.2289 - mae: 6.0739 - val_loss: 39.2620 - val_mae: 4.4997\n",
      "Epoch 541/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 69.3242 - mae: 5.9174 - val_loss: 38.1942 - val_mae: 4.4277\n",
      "Epoch 542/900\n",
      "122/122 [==============================] - 0s 690us/step - loss: 66.7315 - mae: 5.8768 - val_loss: 35.1137 - val_mae: 4.1820\n",
      "Epoch 543/900\n",
      "122/122 [==============================] - 0s 620us/step - loss: 66.3394 - mae: 5.8788 - val_loss: 30.1341 - val_mae: 3.7042\n",
      "Epoch 544/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 68.8239 - mae: 5.9753 - val_loss: 34.3949 - val_mae: 4.0330\n",
      "Epoch 545/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 63.0619 - mae: 5.8346 - val_loss: 32.5204 - val_mae: 3.9000\n",
      "Epoch 546/900\n",
      "122/122 [==============================] - 0s 652us/step - loss: 61.2257 - mae: 5.7666 - val_loss: 37.0262 - val_mae: 4.2420\n",
      "Epoch 547/900\n",
      "122/122 [==============================] - 0s 717us/step - loss: 68.4403 - mae: 5.9473 - val_loss: 35.4024 - val_mae: 4.2703\n",
      "Epoch 548/900\n",
      "122/122 [==============================] - 0s 721us/step - loss: 65.1639 - mae: 5.8740 - val_loss: 33.6677 - val_mae: 4.1030\n",
      "Epoch 549/900\n",
      "122/122 [==============================] - 0s 691us/step - loss: 58.9626 - mae: 5.6138 - val_loss: 43.1990 - val_mae: 4.5777\n",
      "Epoch 550/900\n",
      "122/122 [==============================] - 0s 952us/step - loss: 62.2564 - mae: 5.8115 - val_loss: 33.4974 - val_mae: 4.0615\n",
      "Epoch 551/900\n",
      "122/122 [==============================] - 0s 753us/step - loss: 70.4081 - mae: 5.8773 - val_loss: 35.8536 - val_mae: 4.2268\n",
      "Epoch 552/900\n",
      "122/122 [==============================] - 0s 725us/step - loss: 68.0105 - mae: 5.8972 - val_loss: 36.7440 - val_mae: 4.2605\n",
      "Epoch 553/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 68.7180 - mae: 5.9931 - val_loss: 43.7435 - val_mae: 4.7534\n",
      "Epoch 554/900\n",
      "122/122 [==============================] - 0s 715us/step - loss: 68.2788 - mae: 5.9619 - val_loss: 40.9887 - val_mae: 4.4793\n",
      "Epoch 555/900\n",
      "122/122 [==============================] - 0s 661us/step - loss: 65.3083 - mae: 5.8157 - val_loss: 39.4374 - val_mae: 4.4653\n",
      "Epoch 556/900\n",
      "122/122 [==============================] - 0s 637us/step - loss: 65.5142 - mae: 5.8349 - val_loss: 35.2448 - val_mae: 4.0097\n",
      "Epoch 557/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 67.4715 - mae: 5.8744 - val_loss: 35.8434 - val_mae: 4.1221\n",
      "Epoch 558/900\n",
      "122/122 [==============================] - 0s 640us/step - loss: 65.4040 - mae: 5.8805 - val_loss: 37.0243 - val_mae: 4.3042\n",
      "Epoch 559/900\n",
      "122/122 [==============================] - 0s 790us/step - loss: 67.1848 - mae: 5.9100 - val_loss: 36.3265 - val_mae: 4.0641\n",
      "Epoch 560/900\n",
      "122/122 [==============================] - 0s 749us/step - loss: 67.6918 - mae: 5.8445 - val_loss: 34.2011 - val_mae: 4.0250\n",
      "Epoch 561/900\n",
      "122/122 [==============================] - 0s 658us/step - loss: 65.0614 - mae: 5.8035 - val_loss: 36.8956 - val_mae: 4.2793\n",
      "Epoch 562/900\n",
      "122/122 [==============================] - 0s 635us/step - loss: 63.2310 - mae: 5.7258 - val_loss: 45.7882 - val_mae: 4.8807\n",
      "Epoch 563/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 66.0782 - mae: 5.8273 - val_loss: 34.9733 - val_mae: 3.9738\n",
      "Epoch 564/900\n",
      "122/122 [==============================] - 0s 748us/step - loss: 64.1992 - mae: 5.7964 - val_loss: 39.1248 - val_mae: 4.3844\n",
      "Epoch 565/900\n",
      "122/122 [==============================] - 0s 640us/step - loss: 64.3146 - mae: 5.8352 - val_loss: 32.9299 - val_mae: 3.9821\n",
      "Epoch 566/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 64.7027 - mae: 5.8742 - val_loss: 32.8626 - val_mae: 4.0328\n",
      "Epoch 567/900\n",
      "122/122 [==============================] - 0s 663us/step - loss: 65.8692 - mae: 5.8375 - val_loss: 34.0793 - val_mae: 4.0136\n",
      "Epoch 568/900\n",
      "122/122 [==============================] - 0s 652us/step - loss: 67.7268 - mae: 5.9325 - val_loss: 32.3910 - val_mae: 3.8489\n",
      "Epoch 569/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 60.9252 - mae: 5.7459 - val_loss: 39.5398 - val_mae: 4.4021\n",
      "Epoch 570/900\n",
      "122/122 [==============================] - 0s 671us/step - loss: 64.6184 - mae: 5.8300 - val_loss: 37.4269 - val_mae: 4.2124\n",
      "Epoch 571/900\n",
      "122/122 [==============================] - 0s 689us/step - loss: 62.2839 - mae: 5.7876 - val_loss: 31.9129 - val_mae: 3.8146\n",
      "Epoch 572/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 62.3828 - mae: 5.7629 - val_loss: 40.9634 - val_mae: 4.6333\n",
      "Epoch 573/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 62.5778 - mae: 5.7012 - val_loss: 34.1376 - val_mae: 4.0759\n",
      "Epoch 574/900\n",
      "122/122 [==============================] - 0s 770us/step - loss: 62.8070 - mae: 5.7240 - val_loss: 53.0253 - val_mae: 5.3765\n",
      "Epoch 575/900\n",
      "122/122 [==============================] - 0s 667us/step - loss: 62.7708 - mae: 5.7756 - val_loss: 43.5482 - val_mae: 4.5990\n",
      "Epoch 576/900\n",
      "122/122 [==============================] - 0s 608us/step - loss: 62.6386 - mae: 5.7794 - val_loss: 37.9726 - val_mae: 4.4200\n",
      "Epoch 577/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 63.4653 - mae: 5.7466 - val_loss: 33.2680 - val_mae: 3.8851\n",
      "Epoch 578/900\n",
      "122/122 [==============================] - 0s 667us/step - loss: 65.1334 - mae: 5.8070 - val_loss: 36.1195 - val_mae: 4.0328\n",
      "Epoch 579/900\n",
      "122/122 [==============================] - 0s 652us/step - loss: 61.7900 - mae: 5.7154 - val_loss: 39.9455 - val_mae: 4.2164\n",
      "Epoch 580/900\n",
      "122/122 [==============================] - 0s 723us/step - loss: 63.8136 - mae: 5.8028 - val_loss: 43.5694 - val_mae: 4.7469\n",
      "Epoch 581/900\n",
      "122/122 [==============================] - 0s 678us/step - loss: 60.8670 - mae: 5.7071 - val_loss: 33.0278 - val_mae: 3.8975\n",
      "Epoch 582/900\n",
      "122/122 [==============================] - 0s 809us/step - loss: 62.2980 - mae: 5.7025 - val_loss: 32.2279 - val_mae: 3.7958\n",
      "Epoch 583/900\n",
      "122/122 [==============================] - 0s 659us/step - loss: 61.8777 - mae: 5.7591 - val_loss: 42.2545 - val_mae: 4.4947\n",
      "Epoch 584/900\n",
      "122/122 [==============================] - 0s 633us/step - loss: 60.7404 - mae: 5.6903 - val_loss: 34.8888 - val_mae: 4.0832\n",
      "Epoch 585/900\n",
      "122/122 [==============================] - 0s 709us/step - loss: 60.5203 - mae: 5.6918 - val_loss: 44.6155 - val_mae: 4.2225\n",
      "Epoch 586/900\n",
      "122/122 [==============================] - 0s 838us/step - loss: 63.6238 - mae: 5.7739 - val_loss: 38.8863 - val_mae: 4.2207\n",
      "Epoch 587/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 63.0368 - mae: 5.7320 - val_loss: 40.0174 - val_mae: 4.4150\n",
      "Epoch 588/900\n",
      "122/122 [==============================] - 0s 742us/step - loss: 64.1566 - mae: 5.7914 - val_loss: 35.6406 - val_mae: 4.0759\n",
      "Epoch 589/900\n",
      "122/122 [==============================] - 0s 710us/step - loss: 60.7145 - mae: 5.6772 - val_loss: 38.3905 - val_mae: 4.3183\n",
      "Epoch 590/900\n",
      "122/122 [==============================] - 0s 686us/step - loss: 65.1070 - mae: 5.8831 - val_loss: 32.9658 - val_mae: 3.9068\n",
      "Epoch 591/900\n",
      "122/122 [==============================] - 0s 647us/step - loss: 58.9601 - mae: 5.6807 - val_loss: 41.2516 - val_mae: 4.4339\n",
      "Epoch 592/900\n",
      "122/122 [==============================] - 0s 763us/step - loss: 58.6936 - mae: 5.6098 - val_loss: 45.9314 - val_mae: 4.8647\n",
      "Epoch 593/900\n",
      "122/122 [==============================] - 0s 742us/step - loss: 58.5509 - mae: 5.5627 - val_loss: 39.9920 - val_mae: 4.2993\n",
      "Epoch 594/900\n",
      "122/122 [==============================] - 0s 756us/step - loss: 61.1886 - mae: 5.7074 - val_loss: 53.1070 - val_mae: 5.3672\n",
      "Epoch 595/900\n",
      "122/122 [==============================] - 0s 667us/step - loss: 62.7728 - mae: 5.7376 - val_loss: 44.4728 - val_mae: 4.7728\n",
      "Epoch 596/900\n",
      "122/122 [==============================] - 0s 625us/step - loss: 67.1792 - mae: 5.8589 - val_loss: 46.0584 - val_mae: 4.7978\n",
      "Epoch 597/900\n",
      "122/122 [==============================] - 0s 669us/step - loss: 62.3613 - mae: 5.7990 - val_loss: 33.7608 - val_mae: 4.0772\n",
      "Epoch 598/900\n",
      "122/122 [==============================] - 0s 611us/step - loss: 60.4726 - mae: 5.6777 - val_loss: 33.0623 - val_mae: 4.0093\n",
      "Epoch 599/900\n",
      "122/122 [==============================] - 0s 756us/step - loss: 62.6974 - mae: 5.8462 - val_loss: 37.9843 - val_mae: 4.3833\n",
      "Epoch 600/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 60.7760 - mae: 5.6229 - val_loss: 39.3956 - val_mae: 4.6320\n",
      "Epoch 601/900\n",
      "122/122 [==============================] - 0s 618us/step - loss: 61.3859 - mae: 5.7222 - val_loss: 46.0120 - val_mae: 4.8351\n",
      "Epoch 602/900\n",
      "122/122 [==============================] - 0s 689us/step - loss: 63.2321 - mae: 5.7193 - val_loss: 34.9000 - val_mae: 4.1110\n",
      "Epoch 603/900\n",
      "122/122 [==============================] - 0s 698us/step - loss: 63.0436 - mae: 5.7241 - val_loss: 35.2833 - val_mae: 4.2941\n",
      "Epoch 604/900\n",
      "122/122 [==============================] - 0s 622us/step - loss: 62.5635 - mae: 5.6729 - val_loss: 37.2755 - val_mae: 4.1611\n",
      "Epoch 605/900\n",
      "122/122 [==============================] - 0s 734us/step - loss: 60.9330 - mae: 5.6743 - val_loss: 41.3204 - val_mae: 4.6694\n",
      "Epoch 606/900\n",
      "122/122 [==============================] - 0s 701us/step - loss: 60.2445 - mae: 5.5938 - val_loss: 44.7686 - val_mae: 4.9905\n",
      "Epoch 607/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 63.3901 - mae: 5.6530 - val_loss: 38.8375 - val_mae: 4.3715\n",
      "Epoch 608/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 59.1447 - mae: 5.6389 - val_loss: 34.9167 - val_mae: 3.9921\n",
      "Epoch 609/900\n",
      "122/122 [==============================] - 0s 744us/step - loss: 59.0555 - mae: 5.6207 - val_loss: 47.2139 - val_mae: 5.0773\n",
      "Epoch 610/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 61.5575 - mae: 5.7334 - val_loss: 50.4512 - val_mae: 5.2466\n",
      "Epoch 611/900\n",
      "122/122 [==============================] - 0s 800us/step - loss: 58.3280 - mae: 5.5371 - val_loss: 44.9893 - val_mae: 4.7043\n",
      "Epoch 612/900\n",
      "122/122 [==============================] - 0s 688us/step - loss: 62.1528 - mae: 5.6795 - val_loss: 44.2208 - val_mae: 4.6301\n",
      "Epoch 613/900\n",
      "122/122 [==============================] - 0s 733us/step - loss: 60.3244 - mae: 5.6220 - val_loss: 37.7601 - val_mae: 4.3795\n",
      "Epoch 614/900\n",
      "122/122 [==============================] - 0s 728us/step - loss: 63.2739 - mae: 5.6998 - val_loss: 50.5596 - val_mae: 5.2230\n",
      "Epoch 615/900\n",
      "122/122 [==============================] - 0s 729us/step - loss: 64.0006 - mae: 5.8032 - val_loss: 37.8673 - val_mae: 4.1228\n",
      "Epoch 616/900\n",
      "122/122 [==============================] - 0s 653us/step - loss: 57.6568 - mae: 5.5701 - val_loss: 35.2484 - val_mae: 3.9007\n",
      "Epoch 617/900\n",
      "122/122 [==============================] - 0s 768us/step - loss: 60.9160 - mae: 5.6524 - val_loss: 38.7948 - val_mae: 4.2681\n",
      "Epoch 618/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 59.9647 - mae: 5.6332 - val_loss: 41.6304 - val_mae: 4.5163\n",
      "Epoch 619/900\n",
      "122/122 [==============================] - 0s 606us/step - loss: 63.2079 - mae: 5.6276 - val_loss: 41.2880 - val_mae: 4.3661\n",
      "Epoch 620/900\n",
      "122/122 [==============================] - 0s 663us/step - loss: 57.7574 - mae: 5.4996 - val_loss: 51.1297 - val_mae: 5.0927\n",
      "Epoch 621/900\n",
      "122/122 [==============================] - 0s 861us/step - loss: 58.3079 - mae: 5.5638 - val_loss: 36.1759 - val_mae: 3.9593\n",
      "Epoch 622/900\n",
      "122/122 [==============================] - 0s 743us/step - loss: 59.9213 - mae: 5.5669 - val_loss: 42.8904 - val_mae: 4.4766\n",
      "Epoch 623/900\n",
      "122/122 [==============================] - 0s 754us/step - loss: 59.1174 - mae: 5.5949 - val_loss: 38.9089 - val_mae: 4.2739\n",
      "Epoch 624/900\n",
      "122/122 [==============================] - 0s 773us/step - loss: 60.3906 - mae: 5.6641 - val_loss: 38.2488 - val_mae: 4.1451\n",
      "Epoch 625/900\n",
      "122/122 [==============================] - 0s 816us/step - loss: 62.0248 - mae: 5.7263 - val_loss: 43.4805 - val_mae: 4.6353\n",
      "Epoch 626/900\n",
      "122/122 [==============================] - 0s 752us/step - loss: 65.2272 - mae: 5.6850 - val_loss: 39.9476 - val_mae: 4.1073\n",
      "Epoch 627/900\n",
      "122/122 [==============================] - 0s 793us/step - loss: 61.7634 - mae: 5.6872 - val_loss: 40.2821 - val_mae: 4.1636\n",
      "Epoch 628/900\n",
      "122/122 [==============================] - 0s 796us/step - loss: 59.7803 - mae: 5.7004 - val_loss: 48.5346 - val_mae: 4.8513\n",
      "Epoch 629/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 58.8898 - mae: 5.5852 - val_loss: 38.7925 - val_mae: 4.0856\n",
      "Epoch 630/900\n",
      "122/122 [==============================] - 0s 739us/step - loss: 60.2195 - mae: 5.6616 - val_loss: 37.8309 - val_mae: 4.0036\n",
      "Epoch 631/900\n",
      "122/122 [==============================] - 0s 718us/step - loss: 64.2304 - mae: 5.6709 - val_loss: 42.7079 - val_mae: 4.4790\n",
      "Epoch 632/900\n",
      "122/122 [==============================] - 0s 671us/step - loss: 57.5301 - mae: 5.5839 - val_loss: 36.3403 - val_mae: 4.1578\n",
      "Epoch 633/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 59.0345 - mae: 5.5708 - val_loss: 32.6874 - val_mae: 3.9675\n",
      "Epoch 634/900\n",
      "122/122 [==============================] - 0s 662us/step - loss: 60.7657 - mae: 5.5956 - val_loss: 34.8372 - val_mae: 4.0375\n",
      "Epoch 635/900\n",
      "122/122 [==============================] - 0s 767us/step - loss: 64.7277 - mae: 5.7472 - val_loss: 38.8443 - val_mae: 4.1104\n",
      "Epoch 636/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 55.2175 - mae: 5.5062 - val_loss: 34.9338 - val_mae: 4.1183\n",
      "Epoch 637/900\n",
      "122/122 [==============================] - 0s 724us/step - loss: 58.4600 - mae: 5.5202 - val_loss: 34.3420 - val_mae: 3.9308\n",
      "Epoch 638/900\n",
      "122/122 [==============================] - 0s 661us/step - loss: 61.3048 - mae: 5.6485 - val_loss: 35.9616 - val_mae: 4.0259\n",
      "Epoch 639/900\n",
      "122/122 [==============================] - 0s 723us/step - loss: 57.9130 - mae: 5.5364 - val_loss: 39.3457 - val_mae: 4.3572\n",
      "Epoch 640/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 61.3244 - mae: 5.6535 - val_loss: 36.3321 - val_mae: 4.0807\n",
      "Epoch 641/900\n",
      "122/122 [==============================] - 0s 759us/step - loss: 57.1187 - mae: 5.5640 - val_loss: 38.2108 - val_mae: 4.2506\n",
      "Epoch 642/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 60.5590 - mae: 5.5906 - val_loss: 35.4643 - val_mae: 4.0275\n",
      "Epoch 643/900\n",
      "122/122 [==============================] - 0s 637us/step - loss: 63.9486 - mae: 5.8000 - val_loss: 35.6740 - val_mae: 4.0765\n",
      "Epoch 644/900\n",
      "122/122 [==============================] - 0s 737us/step - loss: 64.7212 - mae: 5.7901 - val_loss: 35.5480 - val_mae: 4.1692\n",
      "Epoch 645/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 59.0356 - mae: 5.6298 - val_loss: 35.9392 - val_mae: 4.2902\n",
      "Epoch 646/900\n",
      "122/122 [==============================] - 0s 626us/step - loss: 64.0875 - mae: 5.6979 - val_loss: 35.8603 - val_mae: 4.1525\n",
      "Epoch 647/900\n",
      "122/122 [==============================] - 0s 710us/step - loss: 59.5624 - mae: 5.6002 - val_loss: 39.3221 - val_mae: 4.4360\n",
      "Epoch 648/900\n",
      "122/122 [==============================] - 0s 746us/step - loss: 61.0164 - mae: 5.7417 - val_loss: 34.8311 - val_mae: 4.0381\n",
      "Epoch 649/900\n",
      "122/122 [==============================] - 0s 679us/step - loss: 59.1313 - mae: 5.6041 - val_loss: 42.4201 - val_mae: 4.6341\n",
      "Epoch 650/900\n",
      "122/122 [==============================] - 0s 659us/step - loss: 58.6950 - mae: 5.5799 - val_loss: 36.1023 - val_mae: 4.1012\n",
      "Epoch 651/900\n",
      "122/122 [==============================] - 0s 762us/step - loss: 56.6634 - mae: 5.5244 - val_loss: 34.8067 - val_mae: 3.9662\n",
      "Epoch 652/900\n",
      "122/122 [==============================] - 0s 720us/step - loss: 60.7134 - mae: 5.6806 - val_loss: 48.7627 - val_mae: 5.0073\n",
      "Epoch 653/900\n",
      "122/122 [==============================] - 0s 697us/step - loss: 57.9869 - mae: 5.4919 - val_loss: 37.2250 - val_mae: 4.3317\n",
      "Epoch 654/900\n",
      "122/122 [==============================] - 0s 692us/step - loss: 60.5905 - mae: 5.6561 - val_loss: 47.0404 - val_mae: 4.9227\n",
      "Epoch 655/900\n",
      "122/122 [==============================] - 0s 617us/step - loss: 56.8012 - mae: 5.5413 - val_loss: 45.2084 - val_mae: 4.8905\n",
      "Epoch 656/900\n",
      "122/122 [==============================] - 0s 693us/step - loss: 65.5679 - mae: 5.7150 - val_loss: 36.9845 - val_mae: 4.3317\n",
      "Epoch 657/900\n",
      "122/122 [==============================] - 0s 726us/step - loss: 57.0636 - mae: 5.5393 - val_loss: 34.4699 - val_mae: 3.9713\n",
      "Epoch 658/900\n",
      "122/122 [==============================] - 0s 629us/step - loss: 58.2583 - mae: 5.5730 - val_loss: 42.7192 - val_mae: 4.4901\n",
      "Epoch 659/900\n",
      "122/122 [==============================] - 0s 619us/step - loss: 59.3832 - mae: 5.5520 - val_loss: 44.4911 - val_mae: 4.8050\n",
      "Epoch 660/900\n",
      "122/122 [==============================] - 0s 669us/step - loss: 55.4253 - mae: 5.5049 - val_loss: 47.2413 - val_mae: 4.6793\n",
      "Epoch 661/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 58.4635 - mae: 5.5380 - val_loss: 37.5196 - val_mae: 4.1772\n",
      "Epoch 662/900\n",
      "122/122 [==============================] - 0s 614us/step - loss: 60.2131 - mae: 5.5838 - val_loss: 37.2147 - val_mae: 4.2352\n",
      "Epoch 663/900\n",
      "122/122 [==============================] - 0s 667us/step - loss: 60.0272 - mae: 5.6793 - val_loss: 35.4184 - val_mae: 3.9848\n",
      "Epoch 664/900\n",
      "122/122 [==============================] - 0s 670us/step - loss: 56.6742 - mae: 5.5584 - val_loss: 40.8003 - val_mae: 4.0840\n",
      "Epoch 665/900\n",
      "122/122 [==============================] - 0s 687us/step - loss: 62.1411 - mae: 5.6571 - val_loss: 35.3025 - val_mae: 3.9720\n",
      "Epoch 666/900\n",
      "122/122 [==============================] - 0s 919us/step - loss: 58.6987 - mae: 5.5760 - val_loss: 53.1374 - val_mae: 5.0444\n",
      "Epoch 667/900\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 56.3936 - mae: 5.4485 - val_loss: 50.8148 - val_mae: 5.1331\n",
      "Epoch 668/900\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 55.8957 - mae: 5.4000 - val_loss: 41.6513 - val_mae: 4.2514\n",
      "Epoch 669/900\n",
      "122/122 [==============================] - 0s 949us/step - loss: 59.5057 - mae: 5.6304 - val_loss: 33.1030 - val_mae: 3.8238\n",
      "Epoch 670/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 59.8872 - mae: 5.5441 - val_loss: 42.6314 - val_mae: 4.5615\n",
      "Epoch 671/900\n",
      "122/122 [==============================] - 0s 633us/step - loss: 59.7852 - mae: 5.5860 - val_loss: 37.9345 - val_mae: 4.2316\n",
      "Epoch 672/900\n",
      "122/122 [==============================] - 0s 679us/step - loss: 54.9176 - mae: 5.4923 - val_loss: 37.4761 - val_mae: 4.1202\n",
      "Epoch 673/900\n",
      "122/122 [==============================] - 0s 737us/step - loss: 62.3092 - mae: 5.7734 - val_loss: 38.2831 - val_mae: 4.0686\n",
      "Epoch 674/900\n",
      "122/122 [==============================] - 0s 656us/step - loss: 59.3013 - mae: 5.5950 - val_loss: 35.9805 - val_mae: 4.0814\n",
      "Epoch 675/900\n",
      "122/122 [==============================] - 0s 696us/step - loss: 56.4661 - mae: 5.4547 - val_loss: 48.4010 - val_mae: 4.9481\n",
      "Epoch 676/900\n",
      "122/122 [==============================] - 0s 650us/step - loss: 56.3354 - mae: 5.5256 - val_loss: 34.9385 - val_mae: 4.0940\n",
      "Epoch 677/900\n",
      "122/122 [==============================] - 0s 637us/step - loss: 62.6252 - mae: 5.6482 - val_loss: 36.3037 - val_mae: 4.1597\n",
      "Epoch 678/900\n",
      "122/122 [==============================] - 0s 655us/step - loss: 56.2089 - mae: 5.5729 - val_loss: 41.6647 - val_mae: 4.5138\n",
      "Epoch 679/900\n",
      "122/122 [==============================] - 0s 807us/step - loss: 58.6859 - mae: 5.6332 - val_loss: 37.8097 - val_mae: 4.3125\n",
      "Epoch 680/900\n",
      "122/122 [==============================] - 0s 632us/step - loss: 59.7875 - mae: 5.5888 - val_loss: 39.3322 - val_mae: 4.5466\n",
      "Epoch 681/900\n",
      "122/122 [==============================] - 0s 621us/step - loss: 54.1723 - mae: 5.3736 - val_loss: 36.9284 - val_mae: 4.1861\n",
      "Epoch 682/900\n",
      "122/122 [==============================] - 0s 773us/step - loss: 54.7255 - mae: 5.4973 - val_loss: 37.0619 - val_mae: 4.3286\n",
      "Epoch 683/900\n",
      "122/122 [==============================] - 0s 808us/step - loss: 55.8865 - mae: 5.5547 - val_loss: 36.5835 - val_mae: 4.1629\n",
      "Epoch 684/900\n",
      "122/122 [==============================] - 0s 685us/step - loss: 56.2129 - mae: 5.4525 - val_loss: 38.7626 - val_mae: 4.1698\n",
      "Epoch 685/900\n",
      "122/122 [==============================] - 0s 690us/step - loss: 58.4540 - mae: 5.5838 - val_loss: 47.8386 - val_mae: 4.7960\n",
      "Epoch 686/900\n",
      "122/122 [==============================] - 0s 675us/step - loss: 59.7420 - mae: 5.6124 - val_loss: 37.7354 - val_mae: 4.2185\n",
      "Epoch 687/900\n",
      "122/122 [==============================] - 0s 645us/step - loss: 54.3116 - mae: 5.3741 - val_loss: 43.6194 - val_mae: 4.6730\n",
      "Epoch 688/900\n",
      "122/122 [==============================] - 0s 619us/step - loss: 56.6961 - mae: 5.4819 - val_loss: 32.6217 - val_mae: 3.9874\n",
      "Epoch 689/900\n",
      "122/122 [==============================] - 0s 601us/step - loss: 57.3116 - mae: 5.5495 - val_loss: 33.1766 - val_mae: 3.9772\n",
      "Epoch 690/900\n",
      "122/122 [==============================] - 0s 666us/step - loss: 55.6251 - mae: 5.5192 - val_loss: 42.7055 - val_mae: 4.4045\n",
      "Epoch 691/900\n",
      "122/122 [==============================] - 0s 669us/step - loss: 54.4111 - mae: 5.4256 - val_loss: 41.4984 - val_mae: 4.4197\n",
      "Epoch 692/900\n",
      "122/122 [==============================] - 0s 641us/step - loss: 57.3442 - mae: 5.5196 - val_loss: 39.2065 - val_mae: 4.2853\n",
      "Epoch 693/900\n",
      "122/122 [==============================] - 0s 638us/step - loss: 58.2967 - mae: 5.6153 - val_loss: 37.5796 - val_mae: 4.1685\n",
      "Epoch 694/900\n",
      "122/122 [==============================] - 0s 624us/step - loss: 57.7438 - mae: 5.5381 - val_loss: 37.1296 - val_mae: 4.2041\n",
      "Epoch 695/900\n",
      "122/122 [==============================] - 0s 584us/step - loss: 57.5416 - mae: 5.5000 - val_loss: 43.2061 - val_mae: 4.7562\n",
      "Epoch 696/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 58.6402 - mae: 5.5692 - val_loss: 34.5306 - val_mae: 3.9886\n",
      "Epoch 697/900\n",
      "122/122 [==============================] - 0s 606us/step - loss: 59.9389 - mae: 5.5371 - val_loss: 33.4807 - val_mae: 4.0681\n",
      "Epoch 698/900\n",
      "122/122 [==============================] - 0s 711us/step - loss: 56.8967 - mae: 5.4590 - val_loss: 35.4288 - val_mae: 4.1846\n",
      "Epoch 699/900\n",
      "122/122 [==============================] - 0s 636us/step - loss: 58.2204 - mae: 5.5317 - val_loss: 43.6481 - val_mae: 4.7499\n",
      "Epoch 700/900\n",
      "122/122 [==============================] - 0s 602us/step - loss: 55.9903 - mae: 5.4540 - val_loss: 37.9673 - val_mae: 4.3883\n",
      "Epoch 701/900\n",
      "122/122 [==============================] - 0s 634us/step - loss: 54.6072 - mae: 5.4797 - val_loss: 41.5806 - val_mae: 4.7728\n",
      "Epoch 702/900\n",
      "122/122 [==============================] - 0s 628us/step - loss: 57.2668 - mae: 5.4793 - val_loss: 38.3236 - val_mae: 4.5226\n",
      "Epoch 703/900\n",
      "122/122 [==============================] - 0s 601us/step - loss: 51.9362 - mae: 5.2942 - val_loss: 35.7189 - val_mae: 4.0232\n",
      "Epoch 704/900\n",
      "122/122 [==============================] - 0s 633us/step - loss: 58.6672 - mae: 5.6159 - val_loss: 38.9154 - val_mae: 4.4289\n",
      "Epoch 705/900\n",
      "122/122 [==============================] - 0s 762us/step - loss: 59.4651 - mae: 5.6077 - val_loss: 55.1664 - val_mae: 5.6221\n",
      "Epoch 706/900\n",
      "122/122 [==============================] - 0s 621us/step - loss: 57.5769 - mae: 5.5557 - val_loss: 33.3102 - val_mae: 3.9786\n",
      "Epoch 707/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 55.4511 - mae: 5.4256 - val_loss: 36.5627 - val_mae: 4.1534\n",
      "Epoch 708/900\n",
      "122/122 [==============================] - 0s 614us/step - loss: 56.3675 - mae: 5.4126 - val_loss: 37.7527 - val_mae: 4.4043\n",
      "Epoch 709/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 57.2886 - mae: 5.5103 - val_loss: 34.7316 - val_mae: 4.0346\n",
      "Epoch 710/900\n",
      "122/122 [==============================] - 0s 645us/step - loss: 58.8554 - mae: 5.5442 - val_loss: 53.1032 - val_mae: 5.3175\n",
      "Epoch 711/900\n",
      "122/122 [==============================] - 0s 732us/step - loss: 57.8152 - mae: 5.5357 - val_loss: 51.7393 - val_mae: 5.2797\n",
      "Epoch 712/900\n",
      "122/122 [==============================] - 0s 699us/step - loss: 55.4908 - mae: 5.4681 - val_loss: 34.9734 - val_mae: 4.2039\n",
      "Epoch 713/900\n",
      "122/122 [==============================] - 0s 621us/step - loss: 56.6422 - mae: 5.4798 - val_loss: 42.5946 - val_mae: 4.7122\n",
      "Epoch 714/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 57.1981 - mae: 5.5449 - val_loss: 36.3444 - val_mae: 4.1798\n",
      "Epoch 715/900\n",
      "122/122 [==============================] - 0s 599us/step - loss: 57.0643 - mae: 5.4192 - val_loss: 35.7061 - val_mae: 4.0959\n",
      "Epoch 716/900\n",
      "122/122 [==============================] - 0s 684us/step - loss: 58.1467 - mae: 5.5259 - val_loss: 55.2516 - val_mae: 5.4528\n",
      "Epoch 717/900\n",
      "122/122 [==============================] - 0s 642us/step - loss: 53.7915 - mae: 5.3802 - val_loss: 44.1908 - val_mae: 4.6857\n",
      "Epoch 718/900\n",
      "122/122 [==============================] - 0s 609us/step - loss: 59.2962 - mae: 5.5877 - val_loss: 34.9414 - val_mae: 4.0658\n",
      "Epoch 719/900\n",
      "122/122 [==============================] - 0s 645us/step - loss: 63.0214 - mae: 5.6200 - val_loss: 38.0629 - val_mae: 4.3136\n",
      "Epoch 720/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 57.2694 - mae: 5.5400 - val_loss: 33.7757 - val_mae: 3.9713\n",
      "Epoch 721/900\n",
      "122/122 [==============================] - 0s 612us/step - loss: 56.5533 - mae: 5.5153 - val_loss: 32.9089 - val_mae: 3.9298\n",
      "Epoch 722/900\n",
      "122/122 [==============================] - 0s 692us/step - loss: 54.5652 - mae: 5.3758 - val_loss: 39.0514 - val_mae: 4.2695\n",
      "Epoch 723/900\n",
      "122/122 [==============================] - 0s 635us/step - loss: 61.5901 - mae: 5.6427 - val_loss: 44.1501 - val_mae: 4.9013\n",
      "Epoch 724/900\n",
      "122/122 [==============================] - 0s 593us/step - loss: 58.9578 - mae: 5.5583 - val_loss: 34.7170 - val_mae: 4.0140\n",
      "Epoch 725/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 59.0630 - mae: 5.5811 - val_loss: 37.5532 - val_mae: 4.2102\n",
      "Epoch 726/900\n",
      "122/122 [==============================] - 0s 685us/step - loss: 56.9409 - mae: 5.5039 - val_loss: 59.7728 - val_mae: 5.6088\n",
      "Epoch 727/900\n",
      "122/122 [==============================] - 0s 626us/step - loss: 57.3532 - mae: 5.4595 - val_loss: 35.5298 - val_mae: 4.0153\n",
      "Epoch 728/900\n",
      "122/122 [==============================] - 0s 643us/step - loss: 57.8755 - mae: 5.5339 - val_loss: 42.7976 - val_mae: 4.7538\n",
      "Epoch 729/900\n",
      "122/122 [==============================] - 0s 617us/step - loss: 54.8682 - mae: 5.4032 - val_loss: 42.9349 - val_mae: 4.5152\n",
      "Epoch 730/900\n",
      "122/122 [==============================] - 0s 585us/step - loss: 56.2713 - mae: 5.4638 - val_loss: 46.0635 - val_mae: 4.9433\n",
      "Epoch 731/900\n",
      "122/122 [==============================] - 0s 647us/step - loss: 55.8114 - mae: 5.4814 - val_loss: 41.7368 - val_mae: 4.5276\n",
      "Epoch 732/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 56.8730 - mae: 5.5189 - val_loss: 35.1815 - val_mae: 3.9775\n",
      "Epoch 733/900\n",
      "122/122 [==============================] - 0s 596us/step - loss: 54.9580 - mae: 5.4193 - val_loss: 39.0302 - val_mae: 4.3183\n",
      "Epoch 734/900\n",
      "122/122 [==============================] - 0s 634us/step - loss: 55.9993 - mae: 5.4990 - val_loss: 52.4037 - val_mae: 5.2450\n",
      "Epoch 735/900\n",
      "122/122 [==============================] - 0s 581us/step - loss: 54.0915 - mae: 5.4276 - val_loss: 34.6045 - val_mae: 3.9932\n",
      "Epoch 736/900\n",
      "122/122 [==============================] - 0s 601us/step - loss: 57.4037 - mae: 5.5051 - val_loss: 44.5136 - val_mae: 4.7491\n",
      "Epoch 737/900\n",
      "122/122 [==============================] - 0s 625us/step - loss: 57.6565 - mae: 5.5188 - val_loss: 37.2867 - val_mae: 4.2042\n",
      "Epoch 738/900\n",
      "122/122 [==============================] - 0s 660us/step - loss: 55.4820 - mae: 5.4778 - val_loss: 35.1010 - val_mae: 4.0844\n",
      "Epoch 739/900\n",
      "122/122 [==============================] - 0s 597us/step - loss: 55.9695 - mae: 5.4781 - val_loss: 41.2400 - val_mae: 4.5593\n",
      "Epoch 740/900\n",
      "122/122 [==============================] - 0s 605us/step - loss: 56.1023 - mae: 5.4539 - val_loss: 39.8462 - val_mae: 4.3536\n",
      "Epoch 741/900\n",
      "122/122 [==============================] - 0s 691us/step - loss: 54.8247 - mae: 5.4266 - val_loss: 38.9327 - val_mae: 4.2180\n",
      "Epoch 742/900\n",
      "122/122 [==============================] - 0s 607us/step - loss: 56.7287 - mae: 5.4708 - val_loss: 45.6776 - val_mae: 4.8996\n",
      "Epoch 743/900\n",
      "122/122 [==============================] - 0s 609us/step - loss: 54.6017 - mae: 5.4229 - val_loss: 37.3803 - val_mae: 4.2695\n",
      "Epoch 744/900\n",
      "122/122 [==============================] - 0s 602us/step - loss: 54.4947 - mae: 5.4418 - val_loss: 34.5194 - val_mae: 4.2038\n",
      "Epoch 745/900\n",
      "122/122 [==============================] - 0s 644us/step - loss: 56.1806 - mae: 5.3924 - val_loss: 49.9165 - val_mae: 4.9162\n",
      "Epoch 746/900\n",
      "122/122 [==============================] - 0s 597us/step - loss: 56.4950 - mae: 5.4643 - val_loss: 52.5245 - val_mae: 5.3955\n",
      "Epoch 747/900\n",
      "122/122 [==============================] - 0s 628us/step - loss: 56.8318 - mae: 5.4160 - val_loss: 39.2798 - val_mae: 4.4215\n",
      "Epoch 748/900\n",
      "122/122 [==============================] - 0s 632us/step - loss: 55.2636 - mae: 5.4295 - val_loss: 47.9950 - val_mae: 5.0947\n",
      "Epoch 749/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 54.3813 - mae: 5.4025 - val_loss: 39.8029 - val_mae: 4.3990\n",
      "Epoch 750/900\n",
      "122/122 [==============================] - 0s 576us/step - loss: 52.8434 - mae: 5.2791 - val_loss: 37.5059 - val_mae: 4.3180\n",
      "Epoch 751/900\n",
      "122/122 [==============================] - 0s 618us/step - loss: 53.8332 - mae: 5.2844 - val_loss: 46.0889 - val_mae: 4.8533\n",
      "Epoch 752/900\n",
      "122/122 [==============================] - 0s 608us/step - loss: 56.5342 - mae: 5.4768 - val_loss: 32.7674 - val_mae: 3.8889\n",
      "Epoch 753/900\n",
      "122/122 [==============================] - 0s 587us/step - loss: 57.6385 - mae: 5.4580 - val_loss: 43.6633 - val_mae: 4.6543\n",
      "Epoch 754/900\n",
      "122/122 [==============================] - 0s 651us/step - loss: 63.4608 - mae: 5.4773 - val_loss: 42.2168 - val_mae: 4.6186\n",
      "Epoch 755/900\n",
      "122/122 [==============================] - 0s 597us/step - loss: 53.9144 - mae: 5.3859 - val_loss: 46.0137 - val_mae: 4.7688\n",
      "Epoch 756/900\n",
      "122/122 [==============================] - 0s 628us/step - loss: 54.0250 - mae: 5.3847 - val_loss: 43.6447 - val_mae: 4.6047\n",
      "Epoch 757/900\n",
      "122/122 [==============================] - 0s 707us/step - loss: 51.8237 - mae: 5.2907 - val_loss: 41.7891 - val_mae: 4.6271\n",
      "Epoch 758/900\n",
      "122/122 [==============================] - 0s 625us/step - loss: 59.8535 - mae: 5.6639 - val_loss: 52.4278 - val_mae: 5.3265\n",
      "Epoch 759/900\n",
      "122/122 [==============================] - 0s 602us/step - loss: 53.2888 - mae: 5.3902 - val_loss: 37.8523 - val_mae: 4.2510\n",
      "Epoch 760/900\n",
      "122/122 [==============================] - 0s 628us/step - loss: 56.1339 - mae: 5.4649 - val_loss: 38.3546 - val_mae: 4.1327\n",
      "Epoch 761/900\n",
      "122/122 [==============================] - 0s 590us/step - loss: 54.7532 - mae: 5.3554 - val_loss: 42.0103 - val_mae: 4.5373\n",
      "Epoch 762/900\n",
      "122/122 [==============================] - 0s 682us/step - loss: 55.2434 - mae: 5.4040 - val_loss: 35.5848 - val_mae: 3.9489\n",
      "Epoch 763/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 55.9050 - mae: 5.3842 - val_loss: 54.4063 - val_mae: 5.1873\n",
      "Epoch 764/900\n",
      "122/122 [==============================] - 0s 713us/step - loss: 54.5535 - mae: 5.3893 - val_loss: 38.5146 - val_mae: 4.0630\n",
      "Epoch 765/900\n",
      "122/122 [==============================] - 0s 700us/step - loss: 52.7138 - mae: 5.3729 - val_loss: 41.7460 - val_mae: 4.2957\n",
      "Epoch 766/900\n",
      "122/122 [==============================] - 0s 673us/step - loss: 54.9402 - mae: 5.4040 - val_loss: 35.2732 - val_mae: 4.0400\n",
      "Epoch 767/900\n",
      "122/122 [==============================] - 0s 632us/step - loss: 53.6870 - mae: 5.3490 - val_loss: 41.5617 - val_mae: 4.5902\n",
      "Epoch 768/900\n",
      "122/122 [==============================] - 0s 659us/step - loss: 56.1439 - mae: 5.4407 - val_loss: 34.5580 - val_mae: 3.9183\n",
      "Epoch 769/900\n",
      "122/122 [==============================] - 0s 644us/step - loss: 57.4379 - mae: 5.4280 - val_loss: 43.0230 - val_mae: 4.6824\n",
      "Epoch 770/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 53.2122 - mae: 5.3439 - val_loss: 41.0162 - val_mae: 4.5643\n",
      "Epoch 771/900\n",
      "122/122 [==============================] - 0s 655us/step - loss: 55.5798 - mae: 5.4785 - val_loss: 34.6554 - val_mae: 4.0028\n",
      "Epoch 772/900\n",
      "122/122 [==============================] - 0s 643us/step - loss: 56.6269 - mae: 5.4653 - val_loss: 34.8606 - val_mae: 4.1573\n",
      "Epoch 773/900\n",
      "122/122 [==============================] - 0s 595us/step - loss: 61.1017 - mae: 5.6872 - val_loss: 32.8343 - val_mae: 3.9460\n",
      "Epoch 774/900\n",
      "122/122 [==============================] - 0s 579us/step - loss: 57.2109 - mae: 5.4660 - val_loss: 32.6501 - val_mae: 3.9963\n",
      "Epoch 775/900\n",
      "122/122 [==============================] - 0s 641us/step - loss: 64.9293 - mae: 5.6174 - val_loss: 34.6727 - val_mae: 4.1402\n",
      "Epoch 776/900\n",
      "122/122 [==============================] - 0s 632us/step - loss: 51.8956 - mae: 5.3324 - val_loss: 37.3648 - val_mae: 4.2342\n",
      "Epoch 777/900\n",
      "122/122 [==============================] - 0s 663us/step - loss: 52.0947 - mae: 5.3553 - val_loss: 48.3371 - val_mae: 5.0984\n",
      "Epoch 778/900\n",
      "122/122 [==============================] - 0s 673us/step - loss: 53.3622 - mae: 5.4136 - val_loss: 55.8651 - val_mae: 5.5578\n",
      "Epoch 779/900\n",
      "122/122 [==============================] - 0s 590us/step - loss: 51.8263 - mae: 5.2826 - val_loss: 37.4380 - val_mae: 4.2445\n",
      "Epoch 780/900\n",
      "122/122 [==============================] - 0s 624us/step - loss: 50.4365 - mae: 5.2956 - val_loss: 37.7204 - val_mae: 4.2842\n",
      "Epoch 781/900\n",
      "122/122 [==============================] - 0s 610us/step - loss: 54.8566 - mae: 5.3793 - val_loss: 34.6501 - val_mae: 4.0029\n",
      "Epoch 782/900\n",
      "122/122 [==============================] - 0s 574us/step - loss: 53.1640 - mae: 5.3420 - val_loss: 36.2289 - val_mae: 4.1467\n",
      "Epoch 783/900\n",
      "122/122 [==============================] - 0s 642us/step - loss: 54.1479 - mae: 5.3536 - val_loss: 40.5102 - val_mae: 4.4342\n",
      "Epoch 784/900\n",
      "122/122 [==============================] - 0s 606us/step - loss: 52.8311 - mae: 5.3382 - val_loss: 40.0078 - val_mae: 4.2860\n",
      "Epoch 785/900\n",
      "122/122 [==============================] - 0s 681us/step - loss: 54.8271 - mae: 5.3690 - val_loss: 37.1212 - val_mae: 4.1823\n",
      "Epoch 786/900\n",
      "122/122 [==============================] - 0s 745us/step - loss: 57.2172 - mae: 5.4519 - val_loss: 36.8943 - val_mae: 4.1901\n",
      "Epoch 787/900\n",
      "122/122 [==============================] - 0s 668us/step - loss: 55.6768 - mae: 5.3822 - val_loss: 48.4259 - val_mae: 5.1211\n",
      "Epoch 788/900\n",
      "122/122 [==============================] - 0s 604us/step - loss: 52.5425 - mae: 5.3458 - val_loss: 44.1620 - val_mae: 4.4765\n",
      "Epoch 789/900\n",
      "122/122 [==============================] - 0s 650us/step - loss: 52.6380 - mae: 5.2634 - val_loss: 39.9115 - val_mae: 4.3801\n",
      "Epoch 790/900\n",
      "122/122 [==============================] - 0s 671us/step - loss: 52.4411 - mae: 5.2598 - val_loss: 37.8913 - val_mae: 4.3504\n",
      "Epoch 791/900\n",
      "122/122 [==============================] - 0s 729us/step - loss: 54.7495 - mae: 5.4011 - val_loss: 41.4740 - val_mae: 4.7915\n",
      "Epoch 792/900\n",
      "122/122 [==============================] - 0s 679us/step - loss: 52.2956 - mae: 5.2634 - val_loss: 49.8692 - val_mae: 5.3678\n",
      "Epoch 793/900\n",
      "122/122 [==============================] - 0s 616us/step - loss: 51.6496 - mae: 5.2549 - val_loss: 35.5146 - val_mae: 4.2014\n",
      "Epoch 794/900\n",
      "122/122 [==============================] - 0s 578us/step - loss: 51.7597 - mae: 5.3319 - val_loss: 40.9550 - val_mae: 4.4715\n",
      "Epoch 795/900\n",
      "122/122 [==============================] - 0s 660us/step - loss: 54.5584 - mae: 5.3824 - val_loss: 37.5589 - val_mae: 4.3367\n",
      "Epoch 796/900\n",
      "122/122 [==============================] - 0s 608us/step - loss: 54.9830 - mae: 5.4176 - val_loss: 41.1041 - val_mae: 4.6445\n",
      "Epoch 797/900\n",
      "122/122 [==============================] - 0s 595us/step - loss: 55.4943 - mae: 5.4027 - val_loss: 37.0063 - val_mae: 4.2248\n",
      "Epoch 798/900\n",
      "122/122 [==============================] - 0s 708us/step - loss: 54.1182 - mae: 5.3631 - val_loss: 34.1320 - val_mae: 4.0545\n",
      "Epoch 799/900\n",
      "122/122 [==============================] - 0s 620us/step - loss: 54.1473 - mae: 5.4323 - val_loss: 36.7749 - val_mae: 4.1330\n",
      "Epoch 800/900\n",
      "122/122 [==============================] - 0s 640us/step - loss: 54.9639 - mae: 5.4056 - val_loss: 36.6151 - val_mae: 4.1787\n",
      "Epoch 801/900\n",
      "122/122 [==============================] - 0s 640us/step - loss: 51.6230 - mae: 5.2576 - val_loss: 36.2843 - val_mae: 4.2759\n",
      "Epoch 802/900\n",
      "122/122 [==============================] - 0s 578us/step - loss: 53.0572 - mae: 5.3383 - val_loss: 36.6156 - val_mae: 4.2191\n",
      "Epoch 803/900\n",
      "122/122 [==============================] - 0s 606us/step - loss: 53.3244 - mae: 5.3308 - val_loss: 34.7494 - val_mae: 4.0601\n",
      "Epoch 804/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 52.6411 - mae: 5.3257 - val_loss: 33.6680 - val_mae: 4.0169\n",
      "Epoch 805/900\n",
      "122/122 [==============================] - 0s 685us/step - loss: 55.8917 - mae: 5.3534 - val_loss: 33.6405 - val_mae: 4.0185\n",
      "Epoch 806/900\n",
      "122/122 [==============================] - 0s 656us/step - loss: 52.4926 - mae: 5.2755 - val_loss: 34.1331 - val_mae: 4.0731\n",
      "Epoch 807/900\n",
      "122/122 [==============================] - 0s 602us/step - loss: 56.4287 - mae: 5.4160 - val_loss: 35.4344 - val_mae: 4.2140\n",
      "Epoch 808/900\n",
      "122/122 [==============================] - 0s 651us/step - loss: 57.5254 - mae: 5.3237 - val_loss: 38.9276 - val_mae: 4.3856\n",
      "Epoch 809/900\n",
      "122/122 [==============================] - 0s 634us/step - loss: 58.6471 - mae: 5.5924 - val_loss: 38.0829 - val_mae: 4.3593\n",
      "Epoch 810/900\n",
      "122/122 [==============================] - 0s 600us/step - loss: 53.5894 - mae: 5.3129 - val_loss: 41.0462 - val_mae: 4.5720\n",
      "Epoch 811/900\n",
      "122/122 [==============================] - 0s 658us/step - loss: 54.3357 - mae: 5.3375 - val_loss: 36.8392 - val_mae: 4.3033\n",
      "Epoch 812/900\n",
      "122/122 [==============================] - 0s 702us/step - loss: 53.2961 - mae: 5.3988 - val_loss: 34.0453 - val_mae: 4.0615\n",
      "Epoch 813/900\n",
      "122/122 [==============================] - 0s 596us/step - loss: 48.9403 - mae: 5.2392 - val_loss: 40.7458 - val_mae: 4.4469\n",
      "Epoch 814/900\n",
      "122/122 [==============================] - 0s 697us/step - loss: 51.7512 - mae: 5.2027 - val_loss: 43.3486 - val_mae: 4.6126\n",
      "Epoch 815/900\n",
      "122/122 [==============================] - 0s 629us/step - loss: 52.4823 - mae: 5.3031 - val_loss: 41.9218 - val_mae: 4.5729\n",
      "Epoch 816/900\n",
      "122/122 [==============================] - 0s 591us/step - loss: 52.8972 - mae: 5.3567 - val_loss: 39.2132 - val_mae: 4.4713\n",
      "Epoch 817/900\n",
      "122/122 [==============================] - 0s 617us/step - loss: 55.1762 - mae: 5.4527 - val_loss: 40.1552 - val_mae: 4.4825\n",
      "Epoch 818/900\n",
      "122/122 [==============================] - 0s 622us/step - loss: 50.1163 - mae: 5.2296 - val_loss: 40.7103 - val_mae: 4.6203\n",
      "Epoch 819/900\n",
      "122/122 [==============================] - 0s 612us/step - loss: 53.6495 - mae: 5.3531 - val_loss: 32.7729 - val_mae: 3.9699\n",
      "Epoch 820/900\n",
      "122/122 [==============================] - 0s 752us/step - loss: 53.1118 - mae: 5.3538 - val_loss: 37.5210 - val_mae: 4.2978\n",
      "Epoch 821/900\n",
      "122/122 [==============================] - 0s 629us/step - loss: 53.6842 - mae: 5.3950 - val_loss: 39.6834 - val_mae: 4.5067\n",
      "Epoch 822/900\n",
      "122/122 [==============================] - 0s 605us/step - loss: 53.3870 - mae: 5.4248 - val_loss: 36.1761 - val_mae: 4.0768\n",
      "Epoch 823/900\n",
      "122/122 [==============================] - 0s 643us/step - loss: 58.8534 - mae: 5.5372 - val_loss: 36.0366 - val_mae: 4.2095\n",
      "Epoch 824/900\n",
      "122/122 [==============================] - 0s 619us/step - loss: 52.7277 - mae: 5.3399 - val_loss: 36.7658 - val_mae: 4.2389\n",
      "Epoch 825/900\n",
      "122/122 [==============================] - 0s 640us/step - loss: 55.1320 - mae: 5.3103 - val_loss: 37.1041 - val_mae: 4.2192\n",
      "Epoch 826/900\n",
      "122/122 [==============================] - 0s 659us/step - loss: 52.0664 - mae: 5.2875 - val_loss: 35.8656 - val_mae: 4.1947\n",
      "Epoch 827/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 54.4043 - mae: 5.3944 - val_loss: 56.4035 - val_mae: 5.7706\n",
      "Epoch 828/900\n",
      "122/122 [==============================] - 0s 642us/step - loss: 57.9065 - mae: 5.4492 - val_loss: 46.5186 - val_mae: 4.9321\n",
      "Epoch 829/900\n",
      "122/122 [==============================] - 0s 672us/step - loss: 51.7836 - mae: 5.3029 - val_loss: 37.7157 - val_mae: 4.2623\n",
      "Epoch 830/900\n",
      "122/122 [==============================] - 0s 625us/step - loss: 53.5327 - mae: 5.3449 - val_loss: 39.5695 - val_mae: 4.5029\n",
      "Epoch 831/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 50.7230 - mae: 5.2656 - val_loss: 42.4445 - val_mae: 4.7264\n",
      "Epoch 832/900\n",
      "122/122 [==============================] - 0s 598us/step - loss: 53.9539 - mae: 5.2941 - val_loss: 39.4695 - val_mae: 4.4449\n",
      "Epoch 833/900\n",
      "122/122 [==============================] - 0s 626us/step - loss: 52.4979 - mae: 5.2591 - val_loss: 48.7645 - val_mae: 5.2413\n",
      "Epoch 834/900\n",
      "122/122 [==============================] - 0s 614us/step - loss: 53.3242 - mae: 5.2950 - val_loss: 46.1672 - val_mae: 4.9526\n",
      "Epoch 835/900\n",
      "122/122 [==============================] - 0s 608us/step - loss: 54.1595 - mae: 5.3405 - val_loss: 39.6210 - val_mae: 4.4372\n",
      "Epoch 836/900\n",
      "122/122 [==============================] - 0s 598us/step - loss: 51.8036 - mae: 5.3572 - val_loss: 36.5906 - val_mae: 4.1181\n",
      "Epoch 837/900\n",
      "122/122 [==============================] - 0s 656us/step - loss: 53.4120 - mae: 5.3627 - val_loss: 35.8659 - val_mae: 4.2239\n",
      "Epoch 838/900\n",
      "122/122 [==============================] - 0s 634us/step - loss: 50.0993 - mae: 5.2302 - val_loss: 35.1506 - val_mae: 4.1120\n",
      "Epoch 839/900\n",
      "122/122 [==============================] - 0s 706us/step - loss: 53.1179 - mae: 5.2896 - val_loss: 41.6304 - val_mae: 4.5564\n",
      "Epoch 840/900\n",
      "122/122 [==============================] - 0s 613us/step - loss: 52.7313 - mae: 5.3443 - val_loss: 40.1381 - val_mae: 4.3893\n",
      "Epoch 841/900\n",
      "122/122 [==============================] - 0s 638us/step - loss: 51.5689 - mae: 5.2579 - val_loss: 38.1827 - val_mae: 4.2126\n",
      "Epoch 842/900\n",
      "122/122 [==============================] - 0s 661us/step - loss: 56.9542 - mae: 5.3993 - val_loss: 41.8104 - val_mae: 4.6200\n",
      "Epoch 843/900\n",
      "122/122 [==============================] - 0s 737us/step - loss: 53.6214 - mae: 5.3129 - val_loss: 30.4217 - val_mae: 3.7641\n",
      "Epoch 844/900\n",
      "122/122 [==============================] - 0s 653us/step - loss: 56.8432 - mae: 5.4077 - val_loss: 36.0436 - val_mae: 4.1115\n",
      "Epoch 845/900\n",
      "122/122 [==============================] - 0s 683us/step - loss: 54.4303 - mae: 5.3390 - val_loss: 36.4181 - val_mae: 4.1330\n",
      "Epoch 846/900\n",
      "122/122 [==============================] - 0s 669us/step - loss: 55.2514 - mae: 5.4282 - val_loss: 34.0218 - val_mae: 4.0249\n",
      "Epoch 847/900\n",
      "122/122 [==============================] - 0s 608us/step - loss: 50.4657 - mae: 5.2523 - val_loss: 48.6752 - val_mae: 5.0918\n",
      "Epoch 848/900\n",
      "122/122 [==============================] - 0s 592us/step - loss: 51.7124 - mae: 5.2256 - val_loss: 49.0113 - val_mae: 5.1972\n",
      "Epoch 849/900\n",
      "122/122 [==============================] - 0s 676us/step - loss: 52.2800 - mae: 5.2643 - val_loss: 32.6296 - val_mae: 3.9548\n",
      "Epoch 850/900\n",
      "122/122 [==============================] - 0s 675us/step - loss: 53.8137 - mae: 5.3640 - val_loss: 33.3647 - val_mae: 4.0226\n",
      "Epoch 851/900\n",
      "122/122 [==============================] - 0s 643us/step - loss: 53.1625 - mae: 5.3825 - val_loss: 34.2529 - val_mae: 4.1318\n",
      "Epoch 852/900\n",
      "122/122 [==============================] - 0s 686us/step - loss: 52.2197 - mae: 5.3138 - val_loss: 49.2324 - val_mae: 5.1852\n",
      "Epoch 853/900\n",
      "122/122 [==============================] - 0s 623us/step - loss: 53.7392 - mae: 5.3413 - val_loss: 35.2228 - val_mae: 4.1079\n",
      "Epoch 854/900\n",
      "122/122 [==============================] - 0s 608us/step - loss: 49.9915 - mae: 5.2048 - val_loss: 39.8291 - val_mae: 4.5384\n",
      "Epoch 855/900\n",
      "122/122 [==============================] - 0s 655us/step - loss: 52.8567 - mae: 5.3271 - val_loss: 51.0686 - val_mae: 5.3660\n",
      "Epoch 856/900\n",
      "122/122 [==============================] - 0s 589us/step - loss: 52.6641 - mae: 5.3626 - val_loss: 52.0414 - val_mae: 5.3130\n",
      "Epoch 857/900\n",
      "122/122 [==============================] - 0s 615us/step - loss: 54.8350 - mae: 5.3911 - val_loss: 42.4058 - val_mae: 4.8010\n",
      "Epoch 858/900\n",
      "122/122 [==============================] - 0s 639us/step - loss: 51.3539 - mae: 5.2783 - val_loss: 43.2881 - val_mae: 4.7293\n",
      "Epoch 859/900\n",
      "122/122 [==============================] - 0s 591us/step - loss: 52.9951 - mae: 5.2911 - val_loss: 38.5579 - val_mae: 4.3089\n",
      "Epoch 860/900\n",
      "122/122 [==============================] - 0s 685us/step - loss: 53.4286 - mae: 5.3157 - val_loss: 42.6395 - val_mae: 4.7122\n",
      "Epoch 861/900\n",
      "122/122 [==============================] - 0s 650us/step - loss: 53.3975 - mae: 5.3454 - val_loss: 45.3273 - val_mae: 4.9334\n",
      "Epoch 862/900\n",
      "122/122 [==============================] - 0s 605us/step - loss: 52.1447 - mae: 5.2915 - val_loss: 32.8071 - val_mae: 4.0377\n",
      "Epoch 863/900\n",
      "122/122 [==============================] - 0s 619us/step - loss: 53.3907 - mae: 5.2749 - val_loss: 36.9728 - val_mae: 4.3564\n",
      "Epoch 864/900\n",
      "122/122 [==============================] - 0s 613us/step - loss: 51.0064 - mae: 5.2162 - val_loss: 36.6088 - val_mae: 4.2217\n",
      "Epoch 865/900\n",
      "122/122 [==============================] - 0s 646us/step - loss: 50.0553 - mae: 5.2203 - val_loss: 34.7133 - val_mae: 4.1673\n",
      "Epoch 866/900\n",
      "122/122 [==============================] - 0s 630us/step - loss: 51.6466 - mae: 5.3164 - val_loss: 33.5209 - val_mae: 4.0706\n",
      "Epoch 867/900\n",
      "122/122 [==============================] - 0s 600us/step - loss: 53.5320 - mae: 5.2849 - val_loss: 37.8892 - val_mae: 4.3656\n",
      "Epoch 868/900\n",
      "122/122 [==============================] - 0s 709us/step - loss: 51.7326 - mae: 5.3028 - val_loss: 35.2794 - val_mae: 4.1594\n",
      "Epoch 869/900\n",
      "122/122 [==============================] - 0s 619us/step - loss: 52.1316 - mae: 5.2378 - val_loss: 50.7102 - val_mae: 5.3880\n",
      "Epoch 870/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 52.4149 - mae: 5.3492 - val_loss: 32.8412 - val_mae: 4.0094\n",
      "Epoch 871/900\n",
      "122/122 [==============================] - 0s 632us/step - loss: 50.8330 - mae: 5.1962 - val_loss: 36.8592 - val_mae: 4.2487\n",
      "Epoch 872/900\n",
      "122/122 [==============================] - 0s 672us/step - loss: 50.3651 - mae: 5.1770 - val_loss: 35.9114 - val_mae: 4.2364\n",
      "Epoch 873/900\n",
      "122/122 [==============================] - 0s 628us/step - loss: 49.8396 - mae: 5.2909 - val_loss: 34.5697 - val_mae: 4.2276\n",
      "Epoch 874/900\n",
      "122/122 [==============================] - 0s 678us/step - loss: 53.4519 - mae: 5.3582 - val_loss: 37.3302 - val_mae: 4.3793\n",
      "Epoch 875/900\n",
      "122/122 [==============================] - 0s 624us/step - loss: 53.4251 - mae: 5.3449 - val_loss: 37.5681 - val_mae: 4.4580\n",
      "Epoch 876/900\n",
      "122/122 [==============================] - 0s 635us/step - loss: 53.0860 - mae: 5.3240 - val_loss: 34.3905 - val_mae: 4.0426\n",
      "Epoch 877/900\n",
      "122/122 [==============================] - 0s 614us/step - loss: 52.2172 - mae: 5.3247 - val_loss: 34.4650 - val_mae: 4.0937\n",
      "Epoch 878/900\n",
      "122/122 [==============================] - 0s 688us/step - loss: 52.0931 - mae: 5.2869 - val_loss: 35.7129 - val_mae: 4.0612\n",
      "Epoch 879/900\n",
      "122/122 [==============================] - 0s 665us/step - loss: 52.7959 - mae: 5.3533 - val_loss: 42.8307 - val_mae: 4.6191\n",
      "Epoch 880/900\n",
      "122/122 [==============================] - 0s 601us/step - loss: 51.3030 - mae: 5.2012 - val_loss: 39.2650 - val_mae: 4.3716\n",
      "Epoch 881/900\n",
      "122/122 [==============================] - 0s 669us/step - loss: 54.7761 - mae: 5.3533 - val_loss: 41.1811 - val_mae: 4.3932\n",
      "Epoch 882/900\n",
      "122/122 [==============================] - 0s 654us/step - loss: 55.3543 - mae: 5.3693 - val_loss: 40.1086 - val_mae: 4.3459\n",
      "Epoch 883/900\n",
      "122/122 [==============================] - 0s 653us/step - loss: 53.9978 - mae: 5.3480 - val_loss: 38.8821 - val_mae: 4.2185\n",
      "Epoch 884/900\n",
      "122/122 [==============================] - 0s 648us/step - loss: 51.8187 - mae: 5.3035 - val_loss: 37.8220 - val_mae: 4.2700\n",
      "Epoch 885/900\n",
      "122/122 [==============================] - 0s 614us/step - loss: 52.2396 - mae: 5.2516 - val_loss: 48.3643 - val_mae: 4.9637\n",
      "Epoch 886/900\n",
      "122/122 [==============================] - 0s 615us/step - loss: 51.7990 - mae: 5.2917 - val_loss: 44.3767 - val_mae: 4.7336\n",
      "Epoch 887/900\n",
      "122/122 [==============================] - 0s 644us/step - loss: 53.2481 - mae: 5.3641 - val_loss: 35.7129 - val_mae: 4.1093\n",
      "Epoch 888/900\n",
      "122/122 [==============================] - 0s 596us/step - loss: 54.9361 - mae: 5.3392 - val_loss: 38.0663 - val_mae: 4.2622\n",
      "Epoch 889/900\n",
      "122/122 [==============================] - 0s 712us/step - loss: 50.1287 - mae: 5.1279 - val_loss: 36.6603 - val_mae: 4.2123\n",
      "Epoch 890/900\n",
      "122/122 [==============================] - 0s 670us/step - loss: 53.1132 - mae: 5.2467 - val_loss: 41.1869 - val_mae: 4.5052\n",
      "Epoch 891/900\n",
      "122/122 [==============================] - 0s 589us/step - loss: 52.6211 - mae: 5.2652 - val_loss: 43.2598 - val_mae: 4.7782\n",
      "Epoch 892/900\n",
      "122/122 [==============================] - 0s 670us/step - loss: 52.9357 - mae: 5.3178 - val_loss: 59.6381 - val_mae: 5.5262\n",
      "Epoch 893/900\n",
      "122/122 [==============================] - 0s 636us/step - loss: 48.6319 - mae: 5.1510 - val_loss: 39.5387 - val_mae: 4.3533\n",
      "Epoch 894/900\n",
      "122/122 [==============================] - 0s 595us/step - loss: 48.7314 - mae: 5.2254 - val_loss: 42.7741 - val_mae: 4.7705\n",
      "Epoch 895/900\n",
      "122/122 [==============================] - 0s 650us/step - loss: 47.6909 - mae: 5.0979 - val_loss: 39.9662 - val_mae: 4.3593\n",
      "Epoch 896/900\n",
      "122/122 [==============================] - 0s 601us/step - loss: 52.7681 - mae: 5.2934 - val_loss: 39.3329 - val_mae: 4.1804\n",
      "Epoch 897/900\n",
      "122/122 [==============================] - 0s 633us/step - loss: 52.9158 - mae: 5.2545 - val_loss: 41.1852 - val_mae: 4.3486\n",
      "Epoch 898/900\n",
      "122/122 [==============================] - 0s 701us/step - loss: 52.9854 - mae: 5.3212 - val_loss: 47.2381 - val_mae: 4.8967\n",
      "Epoch 899/900\n",
      "122/122 [==============================] - 0s 603us/step - loss: 52.3149 - mae: 5.2775 - val_loss: 50.8470 - val_mae: 5.0954\n",
      "Epoch 900/900\n",
      "122/122 [==============================] - 0s 597us/step - loss: 54.8495 - mae: 5.4008 - val_loss: 38.1808 - val_mae: 4.2847\n",
      "31/31 [==============================] - 0s 333us/step\n",
      "Epochs: 900 | MAE: 4.284747272354518\n",
      "Training model with 950 epochs\n",
      "Epoch 1/950\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 66497.9375 - mae: 223.5991 - val_loss: 54746.0078 - val_mae: 203.1020\n",
      "Epoch 2/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 30733.3047 - mae: 146.6119 - val_loss: 8400.9434 - val_mae: 75.2531\n",
      "Epoch 3/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 5117.0527 - mae: 54.6056 - val_loss: 3302.8896 - val_mae: 41.8313\n",
      "Epoch 4/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 3167.3696 - mae: 42.5201 - val_loss: 2134.2954 - val_mae: 33.6859\n",
      "Epoch 5/950\n",
      "122/122 [==============================] - 0s 694us/step - loss: 2416.6594 - mae: 37.5090 - val_loss: 1637.4873 - val_mae: 29.5959\n",
      "Epoch 6/950\n",
      "122/122 [==============================] - 0s 700us/step - loss: 2046.7515 - mae: 34.4486 - val_loss: 1387.2001 - val_mae: 27.4817\n",
      "Epoch 7/950\n",
      "122/122 [==============================] - 0s 871us/step - loss: 1917.3107 - mae: 33.7509 - val_loss: 1176.1910 - val_mae: 25.3171\n",
      "Epoch 8/950\n",
      "122/122 [==============================] - 0s 712us/step - loss: 1724.8092 - mae: 31.9882 - val_loss: 1043.8285 - val_mae: 24.0005\n",
      "Epoch 9/950\n",
      "122/122 [==============================] - 0s 747us/step - loss: 1652.0437 - mae: 31.2465 - val_loss: 928.6694 - val_mae: 22.6658\n",
      "Epoch 10/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 1501.4999 - mae: 29.9124 - val_loss: 856.7944 - val_mae: 21.9105\n",
      "Epoch 11/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 1392.7305 - mae: 29.4044 - val_loss: 776.9172 - val_mae: 20.7998\n",
      "Epoch 12/950\n",
      "122/122 [==============================] - 0s 590us/step - loss: 1338.3503 - mae: 28.5880 - val_loss: 699.8327 - val_mae: 19.7892\n",
      "Epoch 13/950\n",
      "122/122 [==============================] - 0s 733us/step - loss: 1269.7473 - mae: 27.9366 - val_loss: 630.2533 - val_mae: 18.8176\n",
      "Epoch 14/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 1223.3081 - mae: 27.1369 - val_loss: 575.2801 - val_mae: 18.0787\n",
      "Epoch 15/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 1178.9291 - mae: 26.5977 - val_loss: 529.4540 - val_mae: 17.4994\n",
      "Epoch 16/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 1083.7551 - mae: 25.7572 - val_loss: 481.0378 - val_mae: 16.6227\n",
      "Epoch 17/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 1035.4056 - mae: 25.3207 - val_loss: 428.9939 - val_mae: 15.3479\n",
      "Epoch 18/950\n",
      "122/122 [==============================] - 0s 729us/step - loss: 975.3383 - mae: 24.3183 - val_loss: 381.0148 - val_mae: 14.7076\n",
      "Epoch 19/950\n",
      "122/122 [==============================] - 0s 725us/step - loss: 961.1981 - mae: 23.8479 - val_loss: 332.1453 - val_mae: 13.4927\n",
      "Epoch 20/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 901.2794 - mae: 23.4236 - val_loss: 308.4288 - val_mae: 13.7633\n",
      "Epoch 21/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 859.5081 - mae: 22.7180 - val_loss: 267.5670 - val_mae: 12.3446\n",
      "Epoch 22/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 852.2056 - mae: 22.3638 - val_loss: 225.0094 - val_mae: 11.1474\n",
      "Epoch 23/950\n",
      "122/122 [==============================] - 0s 593us/step - loss: 767.4157 - mae: 21.4137 - val_loss: 214.0229 - val_mae: 10.7721\n",
      "Epoch 24/950\n",
      "122/122 [==============================] - 0s 702us/step - loss: 776.5480 - mae: 21.1241 - val_loss: 175.6479 - val_mae: 9.6965\n",
      "Epoch 25/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 806.3931 - mae: 21.5547 - val_loss: 158.6100 - val_mae: 9.2983\n",
      "Epoch 26/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 739.2195 - mae: 20.6356 - val_loss: 156.6608 - val_mae: 9.7120\n",
      "Epoch 27/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 699.0347 - mae: 19.8182 - val_loss: 137.9644 - val_mae: 8.6852\n",
      "Epoch 28/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 671.9971 - mae: 19.3727 - val_loss: 125.8520 - val_mae: 8.4939\n",
      "Epoch 29/950\n",
      "122/122 [==============================] - 0s 581us/step - loss: 662.0546 - mae: 19.3306 - val_loss: 98.8352 - val_mae: 7.3158\n",
      "Epoch 30/950\n",
      "122/122 [==============================] - 0s 725us/step - loss: 666.8447 - mae: 18.9967 - val_loss: 100.4879 - val_mae: 7.6695\n",
      "Epoch 31/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 649.2580 - mae: 18.6722 - val_loss: 105.9680 - val_mae: 8.2558\n",
      "Epoch 32/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 663.0767 - mae: 18.9505 - val_loss: 89.0067 - val_mae: 7.5702\n",
      "Epoch 33/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 639.9786 - mae: 18.4540 - val_loss: 73.4403 - val_mae: 6.4039\n",
      "Epoch 34/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 602.5411 - mae: 17.9859 - val_loss: 71.2644 - val_mae: 6.2288\n",
      "Epoch 35/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 582.2002 - mae: 17.7498 - val_loss: 62.0169 - val_mae: 6.0767\n",
      "Epoch 36/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 583.9217 - mae: 17.5390 - val_loss: 68.4855 - val_mae: 5.9833\n",
      "Epoch 37/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 565.1517 - mae: 17.2425 - val_loss: 49.6642 - val_mae: 5.2897\n",
      "Epoch 38/950\n",
      "122/122 [==============================] - 0s 679us/step - loss: 569.2957 - mae: 17.2975 - val_loss: 44.9991 - val_mae: 5.1040\n",
      "Epoch 39/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 567.3453 - mae: 17.2118 - val_loss: 48.1185 - val_mae: 5.3637\n",
      "Epoch 40/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 539.7624 - mae: 16.8163 - val_loss: 54.1346 - val_mae: 5.3821\n",
      "Epoch 41/950\n",
      "122/122 [==============================] - 0s 656us/step - loss: 547.5205 - mae: 16.9144 - val_loss: 53.4414 - val_mae: 5.7875\n",
      "Epoch 42/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 527.2440 - mae: 16.6197 - val_loss: 34.7418 - val_mae: 4.1965\n",
      "Epoch 43/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 523.3748 - mae: 16.4964 - val_loss: 36.1373 - val_mae: 4.2538\n",
      "Epoch 44/950\n",
      "122/122 [==============================] - 0s 677us/step - loss: 530.5329 - mae: 16.7199 - val_loss: 38.1961 - val_mae: 4.5159\n",
      "Epoch 45/950\n",
      "122/122 [==============================] - 0s 696us/step - loss: 538.5817 - mae: 16.6259 - val_loss: 36.8489 - val_mae: 4.5293\n",
      "Epoch 46/950\n",
      "122/122 [==============================] - 0s 610us/step - loss: 506.0290 - mae: 16.2764 - val_loss: 31.0492 - val_mae: 4.1581\n",
      "Epoch 47/950\n",
      "122/122 [==============================] - 0s 609us/step - loss: 483.8698 - mae: 15.8375 - val_loss: 30.4538 - val_mae: 3.9636\n",
      "Epoch 48/950\n",
      "122/122 [==============================] - 0s 586us/step - loss: 504.7162 - mae: 16.1514 - val_loss: 28.4223 - val_mae: 3.8030\n",
      "Epoch 49/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 505.2715 - mae: 16.0574 - val_loss: 32.9102 - val_mae: 4.2033\n",
      "Epoch 50/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 494.0924 - mae: 15.8636 - val_loss: 30.2245 - val_mae: 4.1572\n",
      "Epoch 51/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 534.4556 - mae: 16.4295 - val_loss: 45.4403 - val_mae: 5.2392\n",
      "Epoch 52/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 477.6255 - mae: 15.6141 - val_loss: 28.7975 - val_mae: 3.9037\n",
      "Epoch 53/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 492.5666 - mae: 15.6513 - val_loss: 31.6284 - val_mae: 4.1585\n",
      "Epoch 54/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 524.7120 - mae: 16.1521 - val_loss: 39.1080 - val_mae: 4.8785\n",
      "Epoch 55/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 488.4944 - mae: 15.6433 - val_loss: 29.7885 - val_mae: 4.1177\n",
      "Epoch 56/950\n",
      "122/122 [==============================] - 0s 594us/step - loss: 461.1255 - mae: 15.1940 - val_loss: 31.5791 - val_mae: 4.1485\n",
      "Epoch 57/950\n",
      "122/122 [==============================] - 0s 726us/step - loss: 477.1417 - mae: 15.4360 - val_loss: 39.4821 - val_mae: 4.7866\n",
      "Epoch 58/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 505.8040 - mae: 15.9772 - val_loss: 25.9710 - val_mae: 3.6556\n",
      "Epoch 59/950\n",
      "122/122 [==============================] - 0s 597us/step - loss: 505.0702 - mae: 15.7220 - val_loss: 33.2684 - val_mae: 4.4841\n",
      "Epoch 60/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 486.2085 - mae: 15.2808 - val_loss: 46.1556 - val_mae: 5.4101\n",
      "Epoch 61/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 491.2162 - mae: 15.6950 - val_loss: 54.5974 - val_mae: 5.7317\n",
      "Epoch 62/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 473.4716 - mae: 15.3866 - val_loss: 28.0077 - val_mae: 3.8973\n",
      "Epoch 63/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 474.8911 - mae: 15.3688 - val_loss: 33.6747 - val_mae: 4.1473\n",
      "Epoch 64/950\n",
      "122/122 [==============================] - 0s 580us/step - loss: 480.2618 - mae: 15.3080 - val_loss: 34.5631 - val_mae: 4.1114\n",
      "Epoch 65/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 480.2939 - mae: 15.2742 - val_loss: 29.2849 - val_mae: 4.0533\n",
      "Epoch 66/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 481.7193 - mae: 15.3950 - val_loss: 33.8148 - val_mae: 4.4252\n",
      "Epoch 67/950\n",
      "122/122 [==============================] - 0s 582us/step - loss: 452.0958 - mae: 14.8500 - val_loss: 21.5728 - val_mae: 3.2605\n",
      "Epoch 68/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 464.7332 - mae: 15.1495 - val_loss: 33.9917 - val_mae: 4.4881\n",
      "Epoch 69/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 444.8058 - mae: 14.6054 - val_loss: 42.8884 - val_mae: 4.9068\n",
      "Epoch 70/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 444.7323 - mae: 14.5851 - val_loss: 32.6853 - val_mae: 4.3968\n",
      "Epoch 71/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 451.8101 - mae: 14.5797 - val_loss: 27.3374 - val_mae: 3.8643\n",
      "Epoch 72/950\n",
      "122/122 [==============================] - 0s 623us/step - loss: 439.5731 - mae: 14.5432 - val_loss: 39.9205 - val_mae: 4.6144\n",
      "Epoch 73/950\n",
      "122/122 [==============================] - 0s 685us/step - loss: 441.7144 - mae: 14.4726 - val_loss: 25.9557 - val_mae: 3.6849\n",
      "Epoch 74/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 446.3004 - mae: 14.6480 - val_loss: 28.7733 - val_mae: 4.0390\n",
      "Epoch 75/950\n",
      "122/122 [==============================] - 0s 578us/step - loss: 424.4755 - mae: 14.2629 - val_loss: 28.9562 - val_mae: 3.9255\n",
      "Epoch 76/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 440.7973 - mae: 14.6915 - val_loss: 34.2237 - val_mae: 4.2687\n",
      "Epoch 77/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 441.7785 - mae: 14.5664 - val_loss: 36.5749 - val_mae: 4.5981\n",
      "Epoch 78/950\n",
      "122/122 [==============================] - 0s 578us/step - loss: 427.1398 - mae: 14.4773 - val_loss: 26.2317 - val_mae: 3.7099\n",
      "Epoch 79/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 437.8846 - mae: 14.5618 - val_loss: 27.6407 - val_mae: 3.9502\n",
      "Epoch 80/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 436.0409 - mae: 14.3868 - val_loss: 47.1142 - val_mae: 5.1149\n",
      "Epoch 81/950\n",
      "122/122 [==============================] - 0s 718us/step - loss: 440.4428 - mae: 14.4310 - val_loss: 31.2739 - val_mae: 4.1367\n",
      "Epoch 82/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 433.8217 - mae: 14.2759 - val_loss: 29.8430 - val_mae: 4.0749\n",
      "Epoch 83/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 420.7385 - mae: 14.0518 - val_loss: 29.2086 - val_mae: 3.9967\n",
      "Epoch 84/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 419.5455 - mae: 14.0633 - val_loss: 23.7212 - val_mae: 3.5096\n",
      "Epoch 85/950\n",
      "122/122 [==============================] - 0s 591us/step - loss: 416.9829 - mae: 14.0271 - val_loss: 37.8983 - val_mae: 4.7087\n",
      "Epoch 86/950\n",
      "122/122 [==============================] - 0s 591us/step - loss: 402.3348 - mae: 13.7740 - val_loss: 27.8035 - val_mae: 3.9163\n",
      "Epoch 87/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 409.3224 - mae: 13.9248 - val_loss: 30.1835 - val_mae: 3.9291\n",
      "Epoch 88/950\n",
      "122/122 [==============================] - 0s 690us/step - loss: 400.2296 - mae: 13.7569 - val_loss: 29.0107 - val_mae: 3.9311\n",
      "Epoch 89/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 431.8524 - mae: 14.2520 - val_loss: 33.4399 - val_mae: 4.1085\n",
      "Epoch 90/950\n",
      "122/122 [==============================] - 0s 713us/step - loss: 418.4600 - mae: 14.0263 - val_loss: 28.7097 - val_mae: 3.6849\n",
      "Epoch 91/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 391.2843 - mae: 13.6223 - val_loss: 25.3305 - val_mae: 3.6261\n",
      "Epoch 92/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 388.2314 - mae: 13.5298 - val_loss: 29.7349 - val_mae: 4.0086\n",
      "Epoch 93/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 384.0089 - mae: 13.4960 - val_loss: 25.9616 - val_mae: 3.7284\n",
      "Epoch 94/950\n",
      "122/122 [==============================] - 0s 578us/step - loss: 406.8163 - mae: 13.7081 - val_loss: 25.6026 - val_mae: 3.7104\n",
      "Epoch 95/950\n",
      "122/122 [==============================] - 0s 726us/step - loss: 436.1513 - mae: 14.1709 - val_loss: 33.9605 - val_mae: 4.3728\n",
      "Epoch 96/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 393.9345 - mae: 13.5633 - val_loss: 49.3288 - val_mae: 5.3135\n",
      "Epoch 97/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 393.4641 - mae: 13.6565 - val_loss: 26.7795 - val_mae: 3.6270\n",
      "Epoch 98/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 400.7829 - mae: 13.6036 - val_loss: 35.9410 - val_mae: 4.6703\n",
      "Epoch 99/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 394.4956 - mae: 13.5245 - val_loss: 28.1377 - val_mae: 3.9164\n",
      "Epoch 100/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 364.1524 - mae: 13.0037 - val_loss: 22.2179 - val_mae: 3.3443\n",
      "Epoch 101/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 389.8939 - mae: 13.4751 - val_loss: 28.2160 - val_mae: 3.7837\n",
      "Epoch 102/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 387.2190 - mae: 13.4243 - val_loss: 40.3457 - val_mae: 4.8912\n",
      "Epoch 103/950\n",
      "122/122 [==============================] - 0s 680us/step - loss: 400.6443 - mae: 13.4730 - val_loss: 38.6873 - val_mae: 4.6813\n",
      "Epoch 104/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 398.0842 - mae: 13.5380 - val_loss: 24.3252 - val_mae: 3.5310\n",
      "Epoch 105/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 380.1110 - mae: 13.1953 - val_loss: 30.7766 - val_mae: 4.1589\n",
      "Epoch 106/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 394.5719 - mae: 13.3650 - val_loss: 35.4908 - val_mae: 4.5998\n",
      "Epoch 107/950\n",
      "122/122 [==============================] - 0s 598us/step - loss: 393.0719 - mae: 13.4293 - val_loss: 30.1965 - val_mae: 4.0545\n",
      "Epoch 108/950\n",
      "122/122 [==============================] - 0s 656us/step - loss: 385.2638 - mae: 13.3469 - val_loss: 34.9136 - val_mae: 4.4972\n",
      "Epoch 109/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 379.1886 - mae: 13.1914 - val_loss: 25.5940 - val_mae: 3.5981\n",
      "Epoch 110/950\n",
      "122/122 [==============================] - 0s 672us/step - loss: 382.2466 - mae: 13.0528 - val_loss: 26.6456 - val_mae: 3.7363\n",
      "Epoch 111/950\n",
      "122/122 [==============================] - 0s 622us/step - loss: 377.9498 - mae: 13.0297 - val_loss: 27.0452 - val_mae: 3.8123\n",
      "Epoch 112/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 360.7185 - mae: 12.8455 - val_loss: 23.1847 - val_mae: 3.4175\n",
      "Epoch 113/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 346.9456 - mae: 12.6754 - val_loss: 30.1463 - val_mae: 4.0486\n",
      "Epoch 114/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 360.9063 - mae: 12.8434 - val_loss: 34.8610 - val_mae: 4.6989\n",
      "Epoch 115/950\n",
      "122/122 [==============================] - 0s 676us/step - loss: 378.4560 - mae: 13.0119 - val_loss: 29.9987 - val_mae: 3.9284\n",
      "Epoch 116/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 349.2279 - mae: 12.6790 - val_loss: 25.5843 - val_mae: 3.8213\n",
      "Epoch 117/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 348.8932 - mae: 12.5368 - val_loss: 29.2227 - val_mae: 4.1575\n",
      "Epoch 118/950\n",
      "122/122 [==============================] - 0s 698us/step - loss: 362.4309 - mae: 12.6248 - val_loss: 23.8092 - val_mae: 3.5255\n",
      "Epoch 119/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 358.0683 - mae: 12.7176 - val_loss: 30.9417 - val_mae: 4.1801\n",
      "Epoch 120/950\n",
      "122/122 [==============================] - 0s 606us/step - loss: 361.8411 - mae: 12.5823 - val_loss: 30.4077 - val_mae: 4.1319\n",
      "Epoch 121/950\n",
      "122/122 [==============================] - 0s 610us/step - loss: 334.5922 - mae: 12.4548 - val_loss: 24.5453 - val_mae: 3.4322\n",
      "Epoch 122/950\n",
      "122/122 [==============================] - 0s 657us/step - loss: 334.9901 - mae: 12.3902 - val_loss: 31.0036 - val_mae: 4.1744\n",
      "Epoch 123/950\n",
      "122/122 [==============================] - 0s 724us/step - loss: 351.9616 - mae: 12.5731 - val_loss: 22.8468 - val_mae: 3.3226\n",
      "Epoch 124/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 363.5004 - mae: 12.6468 - val_loss: 25.0391 - val_mae: 3.5786\n",
      "Epoch 125/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 332.5583 - mae: 12.4138 - val_loss: 31.2214 - val_mae: 3.8130\n",
      "Epoch 126/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 335.3762 - mae: 12.2069 - val_loss: 26.1205 - val_mae: 3.7236\n",
      "Epoch 127/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 373.3203 - mae: 12.7314 - val_loss: 28.8121 - val_mae: 3.9017\n",
      "Epoch 128/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 336.3305 - mae: 12.2932 - val_loss: 25.3130 - val_mae: 3.6726\n",
      "Epoch 129/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 349.4272 - mae: 12.3043 - val_loss: 30.9118 - val_mae: 4.0209\n",
      "Epoch 130/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 329.0901 - mae: 12.1265 - val_loss: 23.5627 - val_mae: 3.3617\n",
      "Epoch 131/950\n",
      "122/122 [==============================] - 0s 832us/step - loss: 327.4197 - mae: 12.0552 - val_loss: 33.5294 - val_mae: 4.3433\n",
      "Epoch 132/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 336.8802 - mae: 12.3304 - val_loss: 24.7438 - val_mae: 3.4937\n",
      "Epoch 133/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 339.0898 - mae: 12.1816 - val_loss: 37.7254 - val_mae: 4.6398\n",
      "Epoch 134/950\n",
      "122/122 [==============================] - 0s 662us/step - loss: 333.9757 - mae: 12.2622 - val_loss: 49.5241 - val_mae: 5.7363\n",
      "Epoch 135/950\n",
      "122/122 [==============================] - 0s 687us/step - loss: 312.6248 - mae: 11.8367 - val_loss: 23.8317 - val_mae: 3.3958\n",
      "Epoch 136/950\n",
      "122/122 [==============================] - 0s 671us/step - loss: 323.4310 - mae: 11.8829 - val_loss: 28.4890 - val_mae: 3.7355\n",
      "Epoch 137/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 332.6667 - mae: 12.1165 - val_loss: 29.6693 - val_mae: 4.1506\n",
      "Epoch 138/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 324.3504 - mae: 12.0097 - val_loss: 28.7252 - val_mae: 3.9712\n",
      "Epoch 139/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 331.3760 - mae: 12.0309 - val_loss: 23.0619 - val_mae: 3.4242\n",
      "Epoch 140/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 318.6078 - mae: 11.9112 - val_loss: 28.8717 - val_mae: 3.9318\n",
      "Epoch 141/950\n",
      "122/122 [==============================] - 0s 597us/step - loss: 324.2085 - mae: 11.8981 - val_loss: 24.1164 - val_mae: 3.4072\n",
      "Epoch 142/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 306.5589 - mae: 11.6885 - val_loss: 31.5919 - val_mae: 4.1761\n",
      "Epoch 143/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 308.3718 - mae: 11.6004 - val_loss: 24.2279 - val_mae: 3.4587\n",
      "Epoch 144/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 320.3701 - mae: 11.7437 - val_loss: 22.8696 - val_mae: 3.3524\n",
      "Epoch 145/950\n",
      "122/122 [==============================] - 0s 594us/step - loss: 315.7407 - mae: 11.6351 - val_loss: 36.5536 - val_mae: 4.4916\n",
      "Epoch 146/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 317.9732 - mae: 11.7894 - val_loss: 37.3325 - val_mae: 4.6474\n",
      "Epoch 147/950\n",
      "122/122 [==============================] - 0s 680us/step - loss: 309.2010 - mae: 11.6635 - val_loss: 23.5982 - val_mae: 3.4264\n",
      "Epoch 148/950\n",
      "122/122 [==============================] - 0s 608us/step - loss: 270.6728 - mae: 11.0504 - val_loss: 32.2764 - val_mae: 3.8813\n",
      "Epoch 149/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 322.4326 - mae: 11.8318 - val_loss: 33.0978 - val_mae: 4.3989\n",
      "Epoch 150/950\n",
      "122/122 [==============================] - 0s 583us/step - loss: 302.1638 - mae: 11.5729 - val_loss: 25.6008 - val_mae: 3.7155\n",
      "Epoch 151/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 304.3564 - mae: 11.4657 - val_loss: 23.2551 - val_mae: 3.4400\n",
      "Epoch 152/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 307.1275 - mae: 11.4596 - val_loss: 26.1982 - val_mae: 3.6177\n",
      "Epoch 153/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 290.6277 - mae: 11.2453 - val_loss: 21.9887 - val_mae: 3.2347\n",
      "Epoch 154/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 284.7947 - mae: 11.1733 - val_loss: 27.2970 - val_mae: 3.6666\n",
      "Epoch 155/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 286.6386 - mae: 11.2296 - val_loss: 28.1840 - val_mae: 3.9801\n",
      "Epoch 156/950\n",
      "122/122 [==============================] - 0s 590us/step - loss: 308.9310 - mae: 11.4933 - val_loss: 25.5399 - val_mae: 3.6391\n",
      "Epoch 157/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 309.7797 - mae: 11.5108 - val_loss: 35.6418 - val_mae: 4.5751\n",
      "Epoch 158/950\n",
      "122/122 [==============================] - 0s 684us/step - loss: 289.0679 - mae: 11.0838 - val_loss: 24.3217 - val_mae: 3.5848\n",
      "Epoch 159/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 293.3001 - mae: 11.1708 - val_loss: 39.3753 - val_mae: 4.8523\n",
      "Epoch 160/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 286.7751 - mae: 11.2294 - val_loss: 25.6745 - val_mae: 3.5546\n",
      "Epoch 161/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 296.7968 - mae: 11.3298 - val_loss: 30.2587 - val_mae: 4.1950\n",
      "Epoch 162/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 285.9934 - mae: 11.1323 - val_loss: 25.8550 - val_mae: 3.6572\n",
      "Epoch 163/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 294.7841 - mae: 11.1157 - val_loss: 25.3393 - val_mae: 3.5749\n",
      "Epoch 164/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 267.0202 - mae: 10.6148 - val_loss: 24.0912 - val_mae: 3.5053\n",
      "Epoch 165/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 276.3756 - mae: 10.9024 - val_loss: 28.1759 - val_mae: 3.9851\n",
      "Epoch 166/950\n",
      "122/122 [==============================] - 0s 585us/step - loss: 276.3858 - mae: 10.8218 - val_loss: 35.4740 - val_mae: 4.5084\n",
      "Epoch 167/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 274.7081 - mae: 10.8480 - val_loss: 22.8594 - val_mae: 3.3946\n",
      "Epoch 168/950\n",
      "122/122 [==============================] - 0s 706us/step - loss: 267.3224 - mae: 10.7056 - val_loss: 30.0912 - val_mae: 4.1622\n",
      "Epoch 169/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 288.3868 - mae: 11.0644 - val_loss: 21.4110 - val_mae: 3.3191\n",
      "Epoch 170/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 262.6005 - mae: 10.5883 - val_loss: 21.0482 - val_mae: 3.2529\n",
      "Epoch 171/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 267.0703 - mae: 10.5935 - val_loss: 24.6169 - val_mae: 3.5887\n",
      "Epoch 172/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 253.6978 - mae: 10.5050 - val_loss: 26.7425 - val_mae: 3.7729\n",
      "Epoch 173/950\n",
      "122/122 [==============================] - 0s 738us/step - loss: 259.6122 - mae: 10.5836 - val_loss: 21.5218 - val_mae: 3.2710\n",
      "Epoch 174/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 285.6088 - mae: 10.7717 - val_loss: 22.3409 - val_mae: 3.4367\n",
      "Epoch 175/950\n",
      "122/122 [==============================] - 0s 622us/step - loss: 278.3826 - mae: 10.7020 - val_loss: 24.6773 - val_mae: 3.5472\n",
      "Epoch 176/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 269.3907 - mae: 10.6942 - val_loss: 22.6085 - val_mae: 3.3756\n",
      "Epoch 177/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 245.9884 - mae: 10.3639 - val_loss: 25.0948 - val_mae: 3.6637\n",
      "Epoch 178/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 271.5891 - mae: 10.8212 - val_loss: 26.2175 - val_mae: 3.7273\n",
      "Epoch 179/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 246.8674 - mae: 10.2774 - val_loss: 26.3956 - val_mae: 3.7881\n",
      "Epoch 180/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 253.4312 - mae: 10.3395 - val_loss: 26.4566 - val_mae: 3.7607\n",
      "Epoch 181/950\n",
      "122/122 [==============================] - 0s 710us/step - loss: 260.4861 - mae: 10.6138 - val_loss: 36.2997 - val_mae: 4.6287\n",
      "Epoch 182/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 248.1295 - mae: 10.2786 - val_loss: 23.3881 - val_mae: 3.5551\n",
      "Epoch 183/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 253.9592 - mae: 10.2328 - val_loss: 48.9021 - val_mae: 5.3251\n",
      "Epoch 184/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 235.8561 - mae: 10.1344 - val_loss: 27.7226 - val_mae: 3.9377\n",
      "Epoch 185/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 238.7902 - mae: 10.0464 - val_loss: 28.8332 - val_mae: 4.0332\n",
      "Epoch 186/950\n",
      "122/122 [==============================] - 0s 581us/step - loss: 243.0985 - mae: 10.3888 - val_loss: 34.4352 - val_mae: 4.4879\n",
      "Epoch 187/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 237.8330 - mae: 10.1432 - val_loss: 25.7476 - val_mae: 3.7859\n",
      "Epoch 188/950\n",
      "122/122 [==============================] - 0s 677us/step - loss: 237.7707 - mae: 10.0872 - val_loss: 44.7318 - val_mae: 5.1300\n",
      "Epoch 189/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 252.6026 - mae: 10.3080 - val_loss: 26.4385 - val_mae: 3.7876\n",
      "Epoch 190/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 233.8995 - mae: 9.9698 - val_loss: 26.8484 - val_mae: 3.9211\n",
      "Epoch 191/950\n",
      "122/122 [==============================] - 0s 682us/step - loss: 255.1458 - mae: 10.2732 - val_loss: 29.2783 - val_mae: 4.0744\n",
      "Epoch 192/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 220.3817 - mae: 9.7892 - val_loss: 23.7659 - val_mae: 3.5224\n",
      "Epoch 193/950\n",
      "122/122 [==============================] - 0s 669us/step - loss: 235.5958 - mae: 9.9638 - val_loss: 29.6245 - val_mae: 4.0693\n",
      "Epoch 194/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 246.1827 - mae: 9.9609 - val_loss: 22.7639 - val_mae: 3.4031\n",
      "Epoch 195/950\n",
      "122/122 [==============================] - 0s 760us/step - loss: 214.6401 - mae: 9.5286 - val_loss: 26.7581 - val_mae: 3.8433\n",
      "Epoch 196/950\n",
      "122/122 [==============================] - 0s 622us/step - loss: 232.0946 - mae: 9.9250 - val_loss: 21.8042 - val_mae: 3.3188\n",
      "Epoch 197/950\n",
      "122/122 [==============================] - 0s 657us/step - loss: 235.7434 - mae: 9.9859 - val_loss: 27.2403 - val_mae: 3.6809\n",
      "Epoch 198/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 233.9807 - mae: 9.7429 - val_loss: 23.7139 - val_mae: 3.5221\n",
      "Epoch 199/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 237.1246 - mae: 9.7940 - val_loss: 24.9375 - val_mae: 3.6854\n",
      "Epoch 200/950\n",
      "122/122 [==============================] - 0s 647us/step - loss: 217.3296 - mae: 9.5349 - val_loss: 32.6657 - val_mae: 4.3074\n",
      "Epoch 201/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 219.5401 - mae: 9.5481 - val_loss: 21.9515 - val_mae: 3.3271\n",
      "Epoch 202/950\n",
      "122/122 [==============================] - 0s 682us/step - loss: 220.2638 - mae: 9.6960 - val_loss: 23.4343 - val_mae: 3.4464\n",
      "Epoch 203/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 202.1104 - mae: 9.3249 - val_loss: 25.8873 - val_mae: 3.7064\n",
      "Epoch 204/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 226.8363 - mae: 9.6254 - val_loss: 30.1950 - val_mae: 4.1260\n",
      "Epoch 205/950\n",
      "122/122 [==============================] - 0s 678us/step - loss: 220.6614 - mae: 9.5753 - val_loss: 27.4158 - val_mae: 3.9746\n",
      "Epoch 206/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 210.5562 - mae: 9.4089 - val_loss: 26.3137 - val_mae: 3.7735\n",
      "Epoch 207/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 207.0624 - mae: 9.4245 - val_loss: 24.6399 - val_mae: 3.5531\n",
      "Epoch 208/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 214.9517 - mae: 9.5376 - val_loss: 24.1958 - val_mae: 3.5711\n",
      "Epoch 209/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 204.9470 - mae: 9.3384 - val_loss: 23.1865 - val_mae: 3.4851\n",
      "Epoch 210/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 203.0028 - mae: 9.2486 - val_loss: 24.0370 - val_mae: 3.5415\n",
      "Epoch 211/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 208.5419 - mae: 9.4157 - val_loss: 22.1896 - val_mae: 3.3417\n",
      "Epoch 212/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 213.0529 - mae: 9.4736 - val_loss: 24.6145 - val_mae: 3.5652\n",
      "Epoch 213/950\n",
      "122/122 [==============================] - 0s 736us/step - loss: 203.8082 - mae: 9.3376 - val_loss: 28.9438 - val_mae: 4.0597\n",
      "Epoch 214/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 207.3660 - mae: 9.3232 - val_loss: 23.8240 - val_mae: 3.5438\n",
      "Epoch 215/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 200.5183 - mae: 9.2225 - val_loss: 24.5342 - val_mae: 3.5694\n",
      "Epoch 216/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 203.7406 - mae: 9.0908 - val_loss: 23.6139 - val_mae: 3.4731\n",
      "Epoch 217/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 182.6411 - mae: 8.9396 - val_loss: 24.7593 - val_mae: 3.6534\n",
      "Epoch 218/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 192.6315 - mae: 9.0866 - val_loss: 36.8179 - val_mae: 4.6346\n",
      "Epoch 219/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 192.5054 - mae: 9.0615 - val_loss: 29.1529 - val_mae: 3.9506\n",
      "Epoch 220/950\n",
      "122/122 [==============================] - 0s 680us/step - loss: 179.6993 - mae: 8.7890 - val_loss: 23.2840 - val_mae: 3.4845\n",
      "Epoch 221/950\n",
      "122/122 [==============================] - 0s 622us/step - loss: 193.1771 - mae: 9.0191 - val_loss: 30.1532 - val_mae: 4.0789\n",
      "Epoch 222/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 199.7280 - mae: 9.0376 - val_loss: 31.2959 - val_mae: 4.2944\n",
      "Epoch 223/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 193.8794 - mae: 8.9327 - val_loss: 24.1156 - val_mae: 3.5400\n",
      "Epoch 224/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 184.7168 - mae: 8.7738 - val_loss: 34.9197 - val_mae: 4.4978\n",
      "Epoch 225/950\n",
      "122/122 [==============================] - 0s 716us/step - loss: 193.6587 - mae: 8.8620 - val_loss: 25.4276 - val_mae: 3.6300\n",
      "Epoch 226/950\n",
      "122/122 [==============================] - 0s 594us/step - loss: 188.2683 - mae: 8.8859 - val_loss: 23.7527 - val_mae: 3.4621\n",
      "Epoch 227/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 190.0374 - mae: 8.8659 - val_loss: 24.3118 - val_mae: 3.5003\n",
      "Epoch 228/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 184.1260 - mae: 8.8925 - val_loss: 22.5735 - val_mae: 3.3536\n",
      "Epoch 229/950\n",
      "122/122 [==============================] - 0s 603us/step - loss: 171.3842 - mae: 8.4840 - val_loss: 23.9853 - val_mae: 3.5725\n",
      "Epoch 230/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 179.9847 - mae: 8.6664 - val_loss: 28.1815 - val_mae: 3.9898\n",
      "Epoch 231/950\n",
      "122/122 [==============================] - 0s 603us/step - loss: 176.2395 - mae: 8.5258 - val_loss: 25.6789 - val_mae: 3.6699\n",
      "Epoch 232/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 177.5564 - mae: 8.5079 - val_loss: 25.4490 - val_mae: 3.6690\n",
      "Epoch 233/950\n",
      "122/122 [==============================] - 0s 700us/step - loss: 181.6459 - mae: 8.7103 - val_loss: 28.4858 - val_mae: 3.9729\n",
      "Epoch 234/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 187.9382 - mae: 8.7805 - val_loss: 25.3411 - val_mae: 3.6658\n",
      "Epoch 235/950\n",
      "122/122 [==============================] - 0s 657us/step - loss: 171.9301 - mae: 8.4968 - val_loss: 27.5739 - val_mae: 3.7410\n",
      "Epoch 236/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 178.0637 - mae: 8.6987 - val_loss: 25.6513 - val_mae: 3.6034\n",
      "Epoch 237/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 179.4094 - mae: 8.6829 - val_loss: 23.4850 - val_mae: 3.4276\n",
      "Epoch 238/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 166.2926 - mae: 8.4776 - val_loss: 49.5868 - val_mae: 5.3738\n",
      "Epoch 239/950\n",
      "122/122 [==============================] - 0s 751us/step - loss: 181.4731 - mae: 8.6099 - val_loss: 26.6679 - val_mae: 3.7858\n",
      "Epoch 240/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 160.7437 - mae: 8.3296 - val_loss: 27.3102 - val_mae: 3.7856\n",
      "Epoch 241/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 174.0883 - mae: 8.6194 - val_loss: 23.9283 - val_mae: 3.4969\n",
      "Epoch 242/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 161.5235 - mae: 8.2511 - val_loss: 25.6880 - val_mae: 3.6841\n",
      "Epoch 243/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 177.9447 - mae: 8.4915 - val_loss: 30.8341 - val_mae: 4.0818\n",
      "Epoch 244/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 168.8811 - mae: 8.4723 - val_loss: 26.3395 - val_mae: 3.6976\n",
      "Epoch 245/950\n",
      "122/122 [==============================] - 0s 704us/step - loss: 164.7065 - mae: 8.3933 - val_loss: 25.1451 - val_mae: 3.6520\n",
      "Epoch 246/950\n",
      "122/122 [==============================] - 0s 726us/step - loss: 162.4891 - mae: 8.3175 - val_loss: 27.3568 - val_mae: 3.9010\n",
      "Epoch 247/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 165.7304 - mae: 8.4348 - val_loss: 35.0833 - val_mae: 4.5266\n",
      "Epoch 248/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 165.3052 - mae: 8.3387 - val_loss: 43.1997 - val_mae: 5.1151\n",
      "Epoch 249/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 164.0374 - mae: 8.4202 - val_loss: 30.5405 - val_mae: 4.1845\n",
      "Epoch 250/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 161.0190 - mae: 8.2995 - val_loss: 26.3683 - val_mae: 3.7658\n",
      "Epoch 251/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 177.0213 - mae: 8.5964 - val_loss: 27.2707 - val_mae: 3.8830\n",
      "Epoch 252/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 159.2562 - mae: 8.3181 - val_loss: 32.9060 - val_mae: 4.2984\n",
      "Epoch 253/950\n",
      "122/122 [==============================] - 0s 582us/step - loss: 163.9646 - mae: 8.4013 - val_loss: 28.1850 - val_mae: 3.8137\n",
      "Epoch 254/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 159.6657 - mae: 8.3327 - val_loss: 28.8806 - val_mae: 4.0091\n",
      "Epoch 255/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 159.6395 - mae: 8.3217 - val_loss: 26.4976 - val_mae: 3.6171\n",
      "Epoch 256/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 148.2029 - mae: 8.0255 - val_loss: 28.7713 - val_mae: 3.8099\n",
      "Epoch 257/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 148.1693 - mae: 8.0102 - val_loss: 25.8611 - val_mae: 3.6748\n",
      "Epoch 258/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 147.3030 - mae: 7.9749 - val_loss: 27.7377 - val_mae: 3.8055\n",
      "Epoch 259/950\n",
      "122/122 [==============================] - 0s 686us/step - loss: 149.5685 - mae: 8.1096 - val_loss: 30.6034 - val_mae: 4.1501\n",
      "Epoch 260/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 150.7541 - mae: 8.1823 - val_loss: 28.7481 - val_mae: 4.0675\n",
      "Epoch 261/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 156.8588 - mae: 8.1565 - val_loss: 27.2536 - val_mae: 3.8968\n",
      "Epoch 262/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 143.1464 - mae: 7.9286 - val_loss: 28.1815 - val_mae: 3.9458\n",
      "Epoch 263/950\n",
      "122/122 [==============================] - 0s 606us/step - loss: 160.5901 - mae: 8.2186 - val_loss: 28.5634 - val_mae: 3.8939\n",
      "Epoch 264/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 150.1799 - mae: 8.0989 - val_loss: 29.7047 - val_mae: 4.1163\n",
      "Epoch 265/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 152.4199 - mae: 8.0370 - val_loss: 25.3860 - val_mae: 3.6575\n",
      "Epoch 266/950\n",
      "122/122 [==============================] - 0s 707us/step - loss: 156.4454 - mae: 8.2152 - val_loss: 32.7598 - val_mae: 4.2119\n",
      "Epoch 267/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 137.8423 - mae: 7.8213 - val_loss: 30.0048 - val_mae: 4.0151\n",
      "Epoch 268/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 140.5599 - mae: 7.9295 - val_loss: 26.2939 - val_mae: 3.5935\n",
      "Epoch 269/950\n",
      "122/122 [==============================] - 0s 606us/step - loss: 143.7646 - mae: 7.9329 - val_loss: 25.5399 - val_mae: 3.6700\n",
      "Epoch 270/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 140.9205 - mae: 7.9102 - val_loss: 26.8910 - val_mae: 3.8160\n",
      "Epoch 271/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 152.0025 - mae: 8.0736 - val_loss: 27.6060 - val_mae: 3.8854\n",
      "Epoch 272/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 137.9011 - mae: 7.8484 - val_loss: 26.8704 - val_mae: 3.7281\n",
      "Epoch 273/950\n",
      "122/122 [==============================] - 0s 709us/step - loss: 139.5350 - mae: 7.9468 - val_loss: 25.9732 - val_mae: 3.7069\n",
      "Epoch 274/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 142.2057 - mae: 7.9087 - val_loss: 25.3207 - val_mae: 3.6181\n",
      "Epoch 275/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 130.4673 - mae: 7.6058 - val_loss: 23.7893 - val_mae: 3.4228\n",
      "Epoch 276/950\n",
      "122/122 [==============================] - 0s 611us/step - loss: 141.8011 - mae: 7.8558 - val_loss: 26.1162 - val_mae: 3.6320\n",
      "Epoch 277/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 149.4159 - mae: 7.9966 - val_loss: 45.9278 - val_mae: 5.1462\n",
      "Epoch 278/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 143.5939 - mae: 7.9435 - val_loss: 28.0361 - val_mae: 3.9191\n",
      "Epoch 279/950\n",
      "122/122 [==============================] - 0s 692us/step - loss: 136.1295 - mae: 7.8713 - val_loss: 23.2913 - val_mae: 3.4505\n",
      "Epoch 280/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 139.6883 - mae: 7.8529 - val_loss: 29.8468 - val_mae: 3.9900\n",
      "Epoch 281/950\n",
      "122/122 [==============================] - 0s 623us/step - loss: 134.3647 - mae: 7.7697 - val_loss: 27.0280 - val_mae: 3.6870\n",
      "Epoch 282/950\n",
      "122/122 [==============================] - 0s 592us/step - loss: 136.7181 - mae: 7.6961 - val_loss: 26.6895 - val_mae: 3.5894\n",
      "Epoch 283/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 131.2563 - mae: 7.6580 - val_loss: 25.0305 - val_mae: 3.5529\n",
      "Epoch 284/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 135.1230 - mae: 7.7005 - val_loss: 26.2530 - val_mae: 3.7136\n",
      "Epoch 285/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 128.5675 - mae: 7.6222 - val_loss: 28.6444 - val_mae: 3.9103\n",
      "Epoch 286/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 126.1373 - mae: 7.5756 - val_loss: 30.6752 - val_mae: 4.1481\n",
      "Epoch 287/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 134.2827 - mae: 7.6218 - val_loss: 30.1365 - val_mae: 4.1045\n",
      "Epoch 288/950\n",
      "122/122 [==============================] - 0s 585us/step - loss: 128.5474 - mae: 7.6866 - val_loss: 24.3638 - val_mae: 3.5236\n",
      "Epoch 289/950\n",
      "122/122 [==============================] - 0s 582us/step - loss: 136.9735 - mae: 7.8641 - val_loss: 23.6779 - val_mae: 3.3791\n",
      "Epoch 290/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 133.1752 - mae: 7.6685 - val_loss: 27.0155 - val_mae: 3.7180\n",
      "Epoch 291/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 133.9040 - mae: 7.5820 - val_loss: 26.3087 - val_mae: 3.7864\n",
      "Epoch 292/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 133.7953 - mae: 7.7542 - val_loss: 30.5841 - val_mae: 4.1223\n",
      "Epoch 293/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 126.1171 - mae: 7.6075 - val_loss: 30.0955 - val_mae: 4.0113\n",
      "Epoch 294/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 128.4084 - mae: 7.5208 - val_loss: 36.2525 - val_mae: 4.5398\n",
      "Epoch 295/950\n",
      "122/122 [==============================] - 0s 588us/step - loss: 133.1706 - mae: 7.7568 - val_loss: 25.6435 - val_mae: 3.6183\n",
      "Epoch 296/950\n",
      "122/122 [==============================] - 0s 690us/step - loss: 134.4570 - mae: 7.6772 - val_loss: 26.5167 - val_mae: 3.6095\n",
      "Epoch 297/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 128.7209 - mae: 7.6017 - val_loss: 24.2283 - val_mae: 3.4483\n",
      "Epoch 298/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 130.9766 - mae: 7.6960 - val_loss: 40.0451 - val_mae: 4.8242\n",
      "Epoch 299/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 125.3504 - mae: 7.4169 - val_loss: 29.5176 - val_mae: 4.0157\n",
      "Epoch 300/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 128.5730 - mae: 7.5401 - val_loss: 27.6352 - val_mae: 3.8038\n",
      "Epoch 301/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 128.5890 - mae: 7.6535 - val_loss: 27.5061 - val_mae: 3.8381\n",
      "Epoch 302/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 122.3271 - mae: 7.4860 - val_loss: 29.7155 - val_mae: 4.1153\n",
      "Epoch 303/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 117.4875 - mae: 7.3813 - val_loss: 29.2745 - val_mae: 3.9817\n",
      "Epoch 304/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 134.5924 - mae: 7.6771 - val_loss: 29.8748 - val_mae: 4.1193\n",
      "Epoch 305/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 125.7493 - mae: 7.5478 - val_loss: 26.4340 - val_mae: 3.8623\n",
      "Epoch 306/950\n",
      "122/122 [==============================] - 0s 717us/step - loss: 114.4298 - mae: 7.3604 - val_loss: 26.2297 - val_mae: 3.6961\n",
      "Epoch 307/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 124.2848 - mae: 7.6540 - val_loss: 26.5285 - val_mae: 3.7316\n",
      "Epoch 308/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 115.4192 - mae: 7.2890 - val_loss: 31.7713 - val_mae: 4.2646\n",
      "Epoch 309/950\n",
      "122/122 [==============================] - 0s 608us/step - loss: 127.3486 - mae: 7.4774 - val_loss: 28.3117 - val_mae: 3.7667\n",
      "Epoch 310/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 118.7514 - mae: 7.3763 - val_loss: 26.7934 - val_mae: 3.9295\n",
      "Epoch 311/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 123.8780 - mae: 7.4301 - val_loss: 24.6699 - val_mae: 3.6453\n",
      "Epoch 312/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 116.2123 - mae: 7.3852 - val_loss: 26.6250 - val_mae: 3.7968\n",
      "Epoch 313/950\n",
      "122/122 [==============================] - 0s 609us/step - loss: 128.2121 - mae: 7.5471 - val_loss: 43.8897 - val_mae: 5.0722\n",
      "Epoch 314/950\n",
      "122/122 [==============================] - 0s 590us/step - loss: 125.7968 - mae: 7.5200 - val_loss: 38.4226 - val_mae: 4.8625\n",
      "Epoch 315/950\n",
      "122/122 [==============================] - 0s 715us/step - loss: 117.4900 - mae: 7.4289 - val_loss: 23.7616 - val_mae: 3.5440\n",
      "Epoch 316/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 122.7414 - mae: 7.4330 - val_loss: 36.6853 - val_mae: 4.7253\n",
      "Epoch 317/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 118.9055 - mae: 7.3135 - val_loss: 27.4406 - val_mae: 3.8795\n",
      "Epoch 318/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 113.4556 - mae: 7.2603 - val_loss: 25.9082 - val_mae: 3.7518\n",
      "Epoch 319/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 121.8083 - mae: 7.3178 - val_loss: 33.4883 - val_mae: 4.4156\n",
      "Epoch 320/950\n",
      "122/122 [==============================] - 0s 592us/step - loss: 114.1362 - mae: 7.2363 - val_loss: 25.2683 - val_mae: 3.6474\n",
      "Epoch 321/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 119.1808 - mae: 7.3877 - val_loss: 27.5236 - val_mae: 3.8011\n",
      "Epoch 322/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 112.7159 - mae: 7.2178 - val_loss: 24.0372 - val_mae: 3.6156\n",
      "Epoch 323/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 113.8271 - mae: 7.2610 - val_loss: 27.4157 - val_mae: 3.8435\n",
      "Epoch 324/950\n",
      "122/122 [==============================] - 0s 577us/step - loss: 108.9486 - mae: 7.2088 - val_loss: 38.0828 - val_mae: 4.7641\n",
      "Epoch 325/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 110.5044 - mae: 7.2272 - val_loss: 24.8965 - val_mae: 3.6714\n",
      "Epoch 326/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 115.2741 - mae: 7.3373 - val_loss: 31.4086 - val_mae: 4.2487\n",
      "Epoch 327/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 120.3270 - mae: 7.4790 - val_loss: 26.0882 - val_mae: 3.6326\n",
      "Epoch 328/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 115.7363 - mae: 7.2934 - val_loss: 26.1443 - val_mae: 3.7859\n",
      "Epoch 329/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 111.0960 - mae: 7.2445 - val_loss: 27.2703 - val_mae: 3.8514\n",
      "Epoch 330/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 120.4688 - mae: 7.5021 - val_loss: 28.3820 - val_mae: 3.9978\n",
      "Epoch 331/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 113.6345 - mae: 7.2690 - val_loss: 28.4489 - val_mae: 3.7890\n",
      "Epoch 332/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 111.6955 - mae: 7.2280 - val_loss: 27.1016 - val_mae: 3.8326\n",
      "Epoch 333/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 116.5565 - mae: 7.3798 - val_loss: 31.3717 - val_mae: 4.2260\n",
      "Epoch 334/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 106.5322 - mae: 7.1239 - val_loss: 32.1127 - val_mae: 4.3041\n",
      "Epoch 335/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 106.8827 - mae: 7.0781 - val_loss: 28.5301 - val_mae: 3.9647\n",
      "Epoch 336/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 109.9432 - mae: 7.1763 - val_loss: 25.8915 - val_mae: 3.7765\n",
      "Epoch 337/950\n",
      "122/122 [==============================] - 0s 846us/step - loss: 106.5721 - mae: 7.2324 - val_loss: 26.4953 - val_mae: 3.7109\n",
      "Epoch 338/950\n",
      "122/122 [==============================] - 0s 664us/step - loss: 105.7673 - mae: 7.1461 - val_loss: 26.8375 - val_mae: 3.7454\n",
      "Epoch 339/950\n",
      "122/122 [==============================] - 0s 579us/step - loss: 107.4380 - mae: 7.0129 - val_loss: 27.4442 - val_mae: 3.9105\n",
      "Epoch 340/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 109.4029 - mae: 7.2517 - val_loss: 45.1676 - val_mae: 5.2456\n",
      "Epoch 341/950\n",
      "122/122 [==============================] - 0s 606us/step - loss: 102.5766 - mae: 7.0455 - val_loss: 28.4701 - val_mae: 3.8946\n",
      "Epoch 342/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 102.6971 - mae: 7.0227 - val_loss: 26.9253 - val_mae: 3.7530\n",
      "Epoch 343/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 107.6320 - mae: 7.1448 - val_loss: 26.4888 - val_mae: 3.7426\n",
      "Epoch 344/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 108.7207 - mae: 7.1065 - val_loss: 29.8186 - val_mae: 4.1042\n",
      "Epoch 345/950\n",
      "122/122 [==============================] - 0s 662us/step - loss: 102.3139 - mae: 7.0485 - val_loss: 29.4084 - val_mae: 4.0064\n",
      "Epoch 346/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 105.4453 - mae: 7.0575 - val_loss: 27.6124 - val_mae: 3.8682\n",
      "Epoch 347/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 106.2218 - mae: 7.1582 - val_loss: 25.6718 - val_mae: 3.7266\n",
      "Epoch 348/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 92.5123 - mae: 6.8633 - val_loss: 27.6714 - val_mae: 3.7792\n",
      "Epoch 349/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 100.2633 - mae: 7.0285 - val_loss: 29.1247 - val_mae: 4.0698\n",
      "Epoch 350/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 98.2833 - mae: 6.9723 - val_loss: 34.7212 - val_mae: 4.5142\n",
      "Epoch 351/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 98.7597 - mae: 6.9212 - val_loss: 32.0274 - val_mae: 4.2895\n",
      "Epoch 352/950\n",
      "122/122 [==============================] - 0s 677us/step - loss: 96.3097 - mae: 6.8014 - val_loss: 25.9195 - val_mae: 3.7641\n",
      "Epoch 353/950\n",
      "122/122 [==============================] - 0s 611us/step - loss: 97.7133 - mae: 6.9757 - val_loss: 32.3805 - val_mae: 4.2831\n",
      "Epoch 354/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 95.2487 - mae: 6.7260 - val_loss: 27.6422 - val_mae: 3.8651\n",
      "Epoch 355/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 105.1191 - mae: 7.1388 - val_loss: 25.3471 - val_mae: 3.6457\n",
      "Epoch 356/950\n",
      "122/122 [==============================] - 0s 597us/step - loss: 98.0778 - mae: 6.9127 - val_loss: 26.3477 - val_mae: 3.7419\n",
      "Epoch 357/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 99.7696 - mae: 6.9300 - val_loss: 27.3251 - val_mae: 3.7642\n",
      "Epoch 358/950\n",
      "122/122 [==============================] - 0s 603us/step - loss: 100.8648 - mae: 6.9557 - val_loss: 26.8832 - val_mae: 3.7993\n",
      "Epoch 359/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 94.6410 - mae: 6.7710 - val_loss: 28.3385 - val_mae: 3.8158\n",
      "Epoch 360/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 102.8862 - mae: 7.0648 - val_loss: 30.5257 - val_mae: 4.1892\n",
      "Epoch 361/950\n",
      "122/122 [==============================] - 0s 717us/step - loss: 100.4471 - mae: 6.9882 - val_loss: 28.0885 - val_mae: 3.8528\n",
      "Epoch 362/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 92.9218 - mae: 6.8473 - val_loss: 32.8441 - val_mae: 4.1894\n",
      "Epoch 363/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 96.7209 - mae: 6.9236 - val_loss: 26.3837 - val_mae: 3.7064\n",
      "Epoch 364/950\n",
      "122/122 [==============================] - 0s 588us/step - loss: 99.7591 - mae: 7.0075 - val_loss: 35.2318 - val_mae: 4.5199\n",
      "Epoch 365/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 103.2995 - mae: 7.0892 - val_loss: 27.7543 - val_mae: 3.6905\n",
      "Epoch 366/950\n",
      "122/122 [==============================] - 0s 598us/step - loss: 92.8654 - mae: 6.7893 - val_loss: 28.1858 - val_mae: 3.8532\n",
      "Epoch 367/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 97.7599 - mae: 6.8770 - val_loss: 31.7805 - val_mae: 4.2226\n",
      "Epoch 368/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 89.0886 - mae: 6.7429 - val_loss: 40.8121 - val_mae: 4.8448\n",
      "Epoch 369/950\n",
      "122/122 [==============================] - 0s 598us/step - loss: 95.5541 - mae: 6.8116 - val_loss: 27.4400 - val_mae: 3.8062\n",
      "Epoch 370/950\n",
      "122/122 [==============================] - 0s 606us/step - loss: 91.4114 - mae: 6.7426 - val_loss: 32.6362 - val_mae: 4.3382\n",
      "Epoch 371/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 89.8133 - mae: 6.7868 - val_loss: 33.0816 - val_mae: 4.3439\n",
      "Epoch 372/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 99.2746 - mae: 6.9272 - val_loss: 32.2030 - val_mae: 4.1801\n",
      "Epoch 373/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 93.1730 - mae: 6.7481 - val_loss: 29.2562 - val_mae: 4.0505\n",
      "Epoch 374/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 91.2462 - mae: 6.7805 - val_loss: 31.7411 - val_mae: 4.1823\n",
      "Epoch 375/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 93.8236 - mae: 6.6931 - val_loss: 27.9448 - val_mae: 3.8776\n",
      "Epoch 376/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 99.7476 - mae: 6.9497 - val_loss: 28.1676 - val_mae: 3.8730\n",
      "Epoch 377/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 87.9884 - mae: 6.6845 - val_loss: 28.1845 - val_mae: 3.7993\n",
      "Epoch 378/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 86.9907 - mae: 6.6420 - val_loss: 30.3221 - val_mae: 4.0909\n",
      "Epoch 379/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 91.1351 - mae: 6.7553 - val_loss: 27.5033 - val_mae: 3.7765\n",
      "Epoch 380/950\n",
      "122/122 [==============================] - 0s 675us/step - loss: 91.3598 - mae: 6.7303 - val_loss: 28.0634 - val_mae: 3.7008\n",
      "Epoch 381/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 92.1609 - mae: 6.6940 - val_loss: 35.6778 - val_mae: 4.5096\n",
      "Epoch 382/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 87.4579 - mae: 6.6551 - val_loss: 29.6025 - val_mae: 4.0199\n",
      "Epoch 383/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 95.0837 - mae: 6.8462 - val_loss: 37.9014 - val_mae: 4.7544\n",
      "Epoch 384/950\n",
      "122/122 [==============================] - 0s 590us/step - loss: 86.9730 - mae: 6.6663 - val_loss: 35.3106 - val_mae: 4.4996\n",
      "Epoch 385/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 91.5015 - mae: 6.7829 - val_loss: 31.4763 - val_mae: 4.0796\n",
      "Epoch 386/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 88.4512 - mae: 6.5848 - val_loss: 26.7786 - val_mae: 3.7365\n",
      "Epoch 387/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 86.8026 - mae: 6.6609 - val_loss: 30.9350 - val_mae: 4.0787\n",
      "Epoch 388/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 88.9455 - mae: 6.6929 - val_loss: 41.9873 - val_mae: 4.9730\n",
      "Epoch 389/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 86.2967 - mae: 6.7261 - val_loss: 27.8915 - val_mae: 3.8135\n",
      "Epoch 390/950\n",
      "122/122 [==============================] - 0s 593us/step - loss: 86.9631 - mae: 6.6072 - val_loss: 27.8297 - val_mae: 3.7779\n",
      "Epoch 391/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 91.4314 - mae: 6.6553 - val_loss: 40.5651 - val_mae: 4.8736\n",
      "Epoch 392/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 89.8297 - mae: 6.6643 - val_loss: 27.3494 - val_mae: 3.7549\n",
      "Epoch 393/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 81.9292 - mae: 6.5024 - val_loss: 28.5758 - val_mae: 3.9071\n",
      "Epoch 394/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 89.8963 - mae: 6.6615 - val_loss: 28.0909 - val_mae: 3.7533\n",
      "Epoch 395/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 92.9259 - mae: 6.6939 - val_loss: 35.1658 - val_mae: 4.4826\n",
      "Epoch 396/950\n",
      "122/122 [==============================] - 0s 707us/step - loss: 83.2628 - mae: 6.5008 - val_loss: 35.7784 - val_mae: 4.5836\n",
      "Epoch 397/950\n",
      "122/122 [==============================] - 0s 603us/step - loss: 85.7771 - mae: 6.5176 - val_loss: 29.1981 - val_mae: 4.0217\n",
      "Epoch 398/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 87.6580 - mae: 6.5748 - val_loss: 30.9803 - val_mae: 4.0882\n",
      "Epoch 399/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 90.5459 - mae: 6.6113 - val_loss: 32.6288 - val_mae: 4.3079\n",
      "Epoch 400/950\n",
      "122/122 [==============================] - 0s 671us/step - loss: 87.7321 - mae: 6.6307 - val_loss: 27.3622 - val_mae: 3.7207\n",
      "Epoch 401/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 89.6949 - mae: 6.7282 - val_loss: 33.4889 - val_mae: 4.3483\n",
      "Epoch 402/950\n",
      "122/122 [==============================] - 0s 675us/step - loss: 84.0314 - mae: 6.5342 - val_loss: 33.9238 - val_mae: 4.3028\n",
      "Epoch 403/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 85.0498 - mae: 6.5262 - val_loss: 26.3679 - val_mae: 3.7474\n",
      "Epoch 404/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 85.7496 - mae: 6.5408 - val_loss: 26.9327 - val_mae: 3.7083\n",
      "Epoch 405/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 88.3293 - mae: 6.6715 - val_loss: 26.9739 - val_mae: 3.7562\n",
      "Epoch 406/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 83.5542 - mae: 6.4742 - val_loss: 35.7405 - val_mae: 4.5247\n",
      "Epoch 407/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 77.8803 - mae: 6.3726 - val_loss: 28.8409 - val_mae: 3.8548\n",
      "Epoch 408/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 89.6756 - mae: 6.6708 - val_loss: 31.3215 - val_mae: 4.1842\n",
      "Epoch 409/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 83.8293 - mae: 6.4945 - val_loss: 25.9846 - val_mae: 3.6803\n",
      "Epoch 410/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 83.3534 - mae: 6.4795 - val_loss: 28.3711 - val_mae: 3.7638\n",
      "Epoch 411/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 81.5572 - mae: 6.3768 - val_loss: 29.7049 - val_mae: 4.1160\n",
      "Epoch 412/950\n",
      "122/122 [==============================] - 0s 598us/step - loss: 81.4587 - mae: 6.4469 - val_loss: 39.4261 - val_mae: 4.8738\n",
      "Epoch 413/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 79.1145 - mae: 6.3406 - val_loss: 29.1278 - val_mae: 3.8235\n",
      "Epoch 414/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 87.6183 - mae: 6.6194 - val_loss: 32.1809 - val_mae: 4.2999\n",
      "Epoch 415/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 85.3401 - mae: 6.5880 - val_loss: 36.6905 - val_mae: 4.5630\n",
      "Epoch 416/950\n",
      "122/122 [==============================] - 0s 671us/step - loss: 86.3985 - mae: 6.6473 - val_loss: 33.1359 - val_mae: 4.3220\n",
      "Epoch 417/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 82.7763 - mae: 6.4499 - val_loss: 30.6939 - val_mae: 4.0941\n",
      "Epoch 418/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 84.2793 - mae: 6.5435 - val_loss: 40.6361 - val_mae: 4.8295\n",
      "Epoch 419/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 82.0668 - mae: 6.5744 - val_loss: 30.3549 - val_mae: 3.8891\n",
      "Epoch 420/950\n",
      "122/122 [==============================] - 0s 592us/step - loss: 81.3571 - mae: 6.4131 - val_loss: 31.8569 - val_mae: 4.1944\n",
      "Epoch 421/950\n",
      "122/122 [==============================] - 0s 684us/step - loss: 82.5357 - mae: 6.4980 - val_loss: 30.5794 - val_mae: 4.0374\n",
      "Epoch 422/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 84.2087 - mae: 6.5717 - val_loss: 32.7288 - val_mae: 4.2715\n",
      "Epoch 423/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 84.3260 - mae: 6.4934 - val_loss: 34.2175 - val_mae: 4.2341\n",
      "Epoch 424/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 81.1328 - mae: 6.4683 - val_loss: 46.2059 - val_mae: 5.2326\n",
      "Epoch 425/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 77.1625 - mae: 6.2747 - val_loss: 33.1680 - val_mae: 4.2322\n",
      "Epoch 426/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 82.6878 - mae: 6.4706 - val_loss: 35.7990 - val_mae: 4.4680\n",
      "Epoch 427/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 82.1414 - mae: 6.5184 - val_loss: 27.1858 - val_mae: 3.7205\n",
      "Epoch 428/950\n",
      "122/122 [==============================] - 0s 707us/step - loss: 78.0578 - mae: 6.2922 - val_loss: 29.2865 - val_mae: 3.9414\n",
      "Epoch 429/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 83.7607 - mae: 6.5139 - val_loss: 31.2155 - val_mae: 4.0370\n",
      "Epoch 430/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 77.4108 - mae: 6.4199 - val_loss: 29.9244 - val_mae: 3.9509\n",
      "Epoch 431/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 76.2558 - mae: 6.2788 - val_loss: 35.4405 - val_mae: 4.5501\n",
      "Epoch 432/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 78.6025 - mae: 6.3845 - val_loss: 35.8596 - val_mae: 4.2837\n",
      "Epoch 433/950\n",
      "122/122 [==============================] - 0s 585us/step - loss: 82.3563 - mae: 6.4452 - val_loss: 51.5850 - val_mae: 5.5394\n",
      "Epoch 434/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 80.4844 - mae: 6.4208 - val_loss: 29.4250 - val_mae: 3.9694\n",
      "Epoch 435/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 80.5751 - mae: 6.4526 - val_loss: 39.6714 - val_mae: 4.7878\n",
      "Epoch 436/950\n",
      "122/122 [==============================] - 0s 684us/step - loss: 79.9457 - mae: 6.4215 - val_loss: 26.5131 - val_mae: 3.7474\n",
      "Epoch 437/950\n",
      "122/122 [==============================] - 0s 690us/step - loss: 75.4031 - mae: 6.2539 - val_loss: 32.0825 - val_mae: 4.1897\n",
      "Epoch 438/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 77.8006 - mae: 6.3078 - val_loss: 32.4332 - val_mae: 4.2221\n",
      "Epoch 439/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 83.0978 - mae: 6.4972 - val_loss: 43.2120 - val_mae: 4.9916\n",
      "Epoch 440/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 75.2896 - mae: 6.2864 - val_loss: 32.8829 - val_mae: 4.2337\n",
      "Epoch 441/950\n",
      "122/122 [==============================] - 0s 719us/step - loss: 85.5818 - mae: 6.5605 - val_loss: 29.6275 - val_mae: 3.9751\n",
      "Epoch 442/950\n",
      "122/122 [==============================] - 0s 606us/step - loss: 78.4818 - mae: 6.4143 - val_loss: 29.4698 - val_mae: 3.9189\n",
      "Epoch 443/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 77.4558 - mae: 6.3571 - val_loss: 28.5696 - val_mae: 3.9062\n",
      "Epoch 444/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 71.4468 - mae: 6.1536 - val_loss: 30.5403 - val_mae: 3.9853\n",
      "Epoch 445/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 77.9077 - mae: 6.3258 - val_loss: 31.4204 - val_mae: 4.0229\n",
      "Epoch 446/950\n",
      "122/122 [==============================] - 0s 670us/step - loss: 78.7825 - mae: 6.3832 - val_loss: 33.6491 - val_mae: 4.2804\n",
      "Epoch 447/950\n",
      "122/122 [==============================] - 0s 580us/step - loss: 74.6811 - mae: 6.2320 - val_loss: 37.1601 - val_mae: 4.5481\n",
      "Epoch 448/950\n",
      "122/122 [==============================] - 0s 686us/step - loss: 81.2660 - mae: 6.4285 - val_loss: 31.5918 - val_mae: 4.0560\n",
      "Epoch 449/950\n",
      "122/122 [==============================] - 0s 623us/step - loss: 85.9824 - mae: 6.4991 - val_loss: 38.3591 - val_mae: 4.6046\n",
      "Epoch 450/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 80.1690 - mae: 6.4955 - val_loss: 27.2675 - val_mae: 3.6745\n",
      "Epoch 451/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 74.8888 - mae: 6.3004 - val_loss: 27.7029 - val_mae: 3.7681\n",
      "Epoch 452/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 77.9784 - mae: 6.3374 - val_loss: 32.3763 - val_mae: 4.3523\n",
      "Epoch 453/950\n",
      "122/122 [==============================] - 0s 603us/step - loss: 77.9816 - mae: 6.3137 - val_loss: 39.7467 - val_mae: 4.9607\n",
      "Epoch 454/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 79.0569 - mae: 6.3151 - val_loss: 29.5680 - val_mae: 3.9313\n",
      "Epoch 455/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 77.6822 - mae: 6.2615 - val_loss: 35.1205 - val_mae: 4.4976\n",
      "Epoch 456/950\n",
      "122/122 [==============================] - 0s 718us/step - loss: 85.0670 - mae: 6.5269 - val_loss: 29.1947 - val_mae: 3.9746\n",
      "Epoch 457/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 74.8057 - mae: 6.2197 - val_loss: 37.1175 - val_mae: 4.7917\n",
      "Epoch 458/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 75.2460 - mae: 6.2783 - val_loss: 45.1343 - val_mae: 5.2397\n",
      "Epoch 459/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 71.7739 - mae: 6.1607 - val_loss: 36.8451 - val_mae: 4.6658\n",
      "Epoch 460/950\n",
      "122/122 [==============================] - 0s 699us/step - loss: 76.1711 - mae: 6.2566 - val_loss: 33.8130 - val_mae: 4.3143\n",
      "Epoch 461/950\n",
      "122/122 [==============================] - 0s 719us/step - loss: 71.3607 - mae: 6.1647 - val_loss: 28.5875 - val_mae: 3.8532\n",
      "Epoch 462/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 78.0164 - mae: 6.4207 - val_loss: 42.2408 - val_mae: 4.9100\n",
      "Epoch 463/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 74.5656 - mae: 6.2740 - val_loss: 32.8252 - val_mae: 4.3177\n",
      "Epoch 464/950\n",
      "122/122 [==============================] - 0s 588us/step - loss: 75.8775 - mae: 6.2013 - val_loss: 37.6007 - val_mae: 4.6626\n",
      "Epoch 465/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 72.3384 - mae: 6.1691 - val_loss: 28.7477 - val_mae: 3.8368\n",
      "Epoch 466/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 73.1650 - mae: 6.0962 - val_loss: 28.0135 - val_mae: 3.8240\n",
      "Epoch 467/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 72.9198 - mae: 6.2382 - val_loss: 43.9470 - val_mae: 5.0202\n",
      "Epoch 468/950\n",
      "122/122 [==============================] - 0s 704us/step - loss: 72.5891 - mae: 6.2092 - val_loss: 30.7088 - val_mae: 4.0719\n",
      "Epoch 469/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 72.9167 - mae: 6.2394 - val_loss: 33.6076 - val_mae: 4.3671\n",
      "Epoch 470/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 68.1696 - mae: 6.0090 - val_loss: 34.5745 - val_mae: 4.4233\n",
      "Epoch 471/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 69.9315 - mae: 6.0681 - val_loss: 28.5495 - val_mae: 3.8451\n",
      "Epoch 472/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 70.9122 - mae: 6.1162 - val_loss: 30.3721 - val_mae: 4.1604\n",
      "Epoch 473/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 72.8277 - mae: 6.1289 - val_loss: 29.6875 - val_mae: 4.0587\n",
      "Epoch 474/950\n",
      "122/122 [==============================] - 0s 597us/step - loss: 75.4268 - mae: 6.3425 - val_loss: 31.9283 - val_mae: 4.2367\n",
      "Epoch 475/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 76.1327 - mae: 6.2842 - val_loss: 39.9058 - val_mae: 4.8993\n",
      "Epoch 476/950\n",
      "122/122 [==============================] - 0s 709us/step - loss: 76.3016 - mae: 6.3341 - val_loss: 29.1005 - val_mae: 4.0017\n",
      "Epoch 477/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 74.9567 - mae: 6.2162 - val_loss: 32.0676 - val_mae: 4.1503\n",
      "Epoch 478/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 77.8317 - mae: 6.3896 - val_loss: 35.2120 - val_mae: 4.4350\n",
      "Epoch 479/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 71.9385 - mae: 6.2085 - val_loss: 41.5412 - val_mae: 4.9856\n",
      "Epoch 480/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 76.9586 - mae: 6.2755 - val_loss: 33.5542 - val_mae: 4.3236\n",
      "Epoch 481/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 72.2015 - mae: 6.1717 - val_loss: 31.3489 - val_mae: 4.0704\n",
      "Epoch 482/950\n",
      "122/122 [==============================] - 0s 678us/step - loss: 72.2466 - mae: 6.2037 - val_loss: 36.5099 - val_mae: 4.5044\n",
      "Epoch 483/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 73.5100 - mae: 6.1512 - val_loss: 29.4553 - val_mae: 4.0236\n",
      "Epoch 484/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 68.5560 - mae: 6.0674 - val_loss: 29.1206 - val_mae: 3.8605\n",
      "Epoch 485/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 70.3548 - mae: 6.0311 - val_loss: 34.4408 - val_mae: 4.4508\n",
      "Epoch 486/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 75.9520 - mae: 6.2762 - val_loss: 36.1513 - val_mae: 4.6318\n",
      "Epoch 487/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 75.2370 - mae: 6.2542 - val_loss: 34.9350 - val_mae: 4.4113\n",
      "Epoch 488/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 76.4522 - mae: 6.3299 - val_loss: 47.4988 - val_mae: 5.4159\n",
      "Epoch 489/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 66.7824 - mae: 5.9760 - val_loss: 37.0539 - val_mae: 4.7176\n",
      "Epoch 490/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 69.0812 - mae: 6.0319 - val_loss: 33.0263 - val_mae: 4.3306\n",
      "Epoch 491/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 71.5109 - mae: 6.1563 - val_loss: 27.2852 - val_mae: 3.7605\n",
      "Epoch 492/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 67.3945 - mae: 5.9720 - val_loss: 41.5649 - val_mae: 5.0162\n",
      "Epoch 493/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 71.1730 - mae: 6.1049 - val_loss: 36.2748 - val_mae: 4.5686\n",
      "Epoch 494/950\n",
      "122/122 [==============================] - 0s 667us/step - loss: 69.7527 - mae: 6.0347 - val_loss: 33.5831 - val_mae: 4.2404\n",
      "Epoch 495/950\n",
      "122/122 [==============================] - 0s 664us/step - loss: 73.3737 - mae: 6.1303 - val_loss: 44.0322 - val_mae: 5.0801\n",
      "Epoch 496/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 66.7807 - mae: 5.9380 - val_loss: 34.5478 - val_mae: 4.4710\n",
      "Epoch 497/950\n",
      "122/122 [==============================] - 0s 610us/step - loss: 73.9483 - mae: 6.1073 - val_loss: 35.7587 - val_mae: 4.4172\n",
      "Epoch 498/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 68.8389 - mae: 6.1092 - val_loss: 40.1216 - val_mae: 4.7889\n",
      "Epoch 499/950\n",
      "122/122 [==============================] - 0s 594us/step - loss: 68.1084 - mae: 5.9906 - val_loss: 40.3522 - val_mae: 4.7315\n",
      "Epoch 500/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 72.8920 - mae: 6.1033 - val_loss: 36.6735 - val_mae: 4.6420\n",
      "Epoch 501/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 71.7163 - mae: 6.1416 - val_loss: 30.1151 - val_mae: 4.0122\n",
      "Epoch 502/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 73.1023 - mae: 6.0793 - val_loss: 28.6537 - val_mae: 3.8317\n",
      "Epoch 503/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 68.9620 - mae: 6.0547 - val_loss: 34.4763 - val_mae: 4.1706\n",
      "Epoch 504/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 66.6184 - mae: 6.0011 - val_loss: 31.6675 - val_mae: 3.9913\n",
      "Epoch 505/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 68.6292 - mae: 5.9999 - val_loss: 39.7970 - val_mae: 4.8007\n",
      "Epoch 506/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 75.3225 - mae: 6.1988 - val_loss: 28.4963 - val_mae: 3.8160\n",
      "Epoch 507/950\n",
      "122/122 [==============================] - 0s 582us/step - loss: 66.5554 - mae: 5.8802 - val_loss: 29.1468 - val_mae: 3.7657\n",
      "Epoch 508/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 71.8378 - mae: 6.1059 - val_loss: 31.2745 - val_mae: 3.9982\n",
      "Epoch 509/950\n",
      "122/122 [==============================] - 0s 581us/step - loss: 69.7923 - mae: 6.0815 - val_loss: 43.9633 - val_mae: 4.9832\n",
      "Epoch 510/950\n",
      "122/122 [==============================] - 0s 683us/step - loss: 69.4629 - mae: 6.0650 - val_loss: 31.7347 - val_mae: 4.1353\n",
      "Epoch 511/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 68.1769 - mae: 6.0327 - val_loss: 38.1340 - val_mae: 4.7184\n",
      "Epoch 512/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 69.3163 - mae: 6.0482 - val_loss: 35.7042 - val_mae: 4.5118\n",
      "Epoch 513/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 71.2023 - mae: 6.1004 - val_loss: 38.4338 - val_mae: 4.6368\n",
      "Epoch 514/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 70.4929 - mae: 6.1517 - val_loss: 34.8849 - val_mae: 4.3231\n",
      "Epoch 515/950\n",
      "122/122 [==============================] - 0s 591us/step - loss: 69.2970 - mae: 5.9249 - val_loss: 34.0034 - val_mae: 4.3098\n",
      "Epoch 516/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 63.5050 - mae: 5.8776 - val_loss: 35.3953 - val_mae: 4.3539\n",
      "Epoch 517/950\n",
      "122/122 [==============================] - 0s 692us/step - loss: 70.9222 - mae: 6.1181 - val_loss: 38.7075 - val_mae: 4.6941\n",
      "Epoch 518/950\n",
      "122/122 [==============================] - 0s 707us/step - loss: 67.5672 - mae: 6.0390 - val_loss: 30.7104 - val_mae: 4.0536\n",
      "Epoch 519/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 69.6744 - mae: 6.0349 - val_loss: 30.5801 - val_mae: 4.0202\n",
      "Epoch 520/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 69.7686 - mae: 5.9839 - val_loss: 40.1121 - val_mae: 4.6992\n",
      "Epoch 521/950\n",
      "122/122 [==============================] - 0s 593us/step - loss: 69.1188 - mae: 5.9857 - val_loss: 43.4973 - val_mae: 4.9469\n",
      "Epoch 522/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 68.1365 - mae: 5.9957 - val_loss: 42.9903 - val_mae: 4.9159\n",
      "Epoch 523/950\n",
      "122/122 [==============================] - 0s 689us/step - loss: 62.9602 - mae: 5.8659 - val_loss: 33.3611 - val_mae: 4.2459\n",
      "Epoch 524/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 65.8152 - mae: 5.9067 - val_loss: 51.5692 - val_mae: 5.5894\n",
      "Epoch 525/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 70.9761 - mae: 6.1048 - val_loss: 30.2407 - val_mae: 4.0444\n",
      "Epoch 526/950\n",
      "122/122 [==============================] - 0s 701us/step - loss: 65.6145 - mae: 5.9585 - val_loss: 34.4002 - val_mae: 4.2552\n",
      "Epoch 527/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 71.5456 - mae: 6.1155 - val_loss: 32.5649 - val_mae: 4.0253\n",
      "Epoch 528/950\n",
      "122/122 [==============================] - 0s 686us/step - loss: 70.0295 - mae: 6.0942 - val_loss: 28.5809 - val_mae: 3.8200\n",
      "Epoch 529/950\n",
      "122/122 [==============================] - 0s 670us/step - loss: 60.3693 - mae: 5.7628 - val_loss: 38.6917 - val_mae: 4.7421\n",
      "Epoch 530/950\n",
      "122/122 [==============================] - 0s 737us/step - loss: 70.2808 - mae: 6.0286 - val_loss: 38.7338 - val_mae: 4.8630\n",
      "Epoch 531/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 68.0704 - mae: 6.0033 - val_loss: 30.2078 - val_mae: 3.9356\n",
      "Epoch 532/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 66.8457 - mae: 5.9514 - val_loss: 37.6453 - val_mae: 4.5903\n",
      "Epoch 533/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 65.3958 - mae: 5.8868 - val_loss: 34.4594 - val_mae: 4.3363\n",
      "Epoch 534/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 68.3429 - mae: 5.9551 - val_loss: 34.6136 - val_mae: 4.2401\n",
      "Epoch 535/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 66.6228 - mae: 5.9431 - val_loss: 33.7235 - val_mae: 4.1522\n",
      "Epoch 536/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 66.5180 - mae: 5.9111 - val_loss: 32.8788 - val_mae: 4.0489\n",
      "Epoch 537/950\n",
      "122/122 [==============================] - 0s 679us/step - loss: 67.4265 - mae: 6.0346 - val_loss: 33.3695 - val_mae: 4.2730\n",
      "Epoch 538/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 65.7834 - mae: 5.9107 - val_loss: 33.5117 - val_mae: 4.2525\n",
      "Epoch 539/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 68.2030 - mae: 6.0454 - val_loss: 41.7345 - val_mae: 4.9868\n",
      "Epoch 540/950\n",
      "122/122 [==============================] - 0s 582us/step - loss: 69.3219 - mae: 6.0662 - val_loss: 38.4346 - val_mae: 4.7192\n",
      "Epoch 541/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 66.8571 - mae: 6.0146 - val_loss: 41.4670 - val_mae: 4.7757\n",
      "Epoch 542/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 64.9753 - mae: 5.8576 - val_loss: 35.1280 - val_mae: 4.3692\n",
      "Epoch 543/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 72.5970 - mae: 6.1435 - val_loss: 32.4984 - val_mae: 4.0737\n",
      "Epoch 544/950\n",
      "122/122 [==============================] - 0s 656us/step - loss: 69.8661 - mae: 6.1123 - val_loss: 31.4155 - val_mae: 3.9227\n",
      "Epoch 545/950\n",
      "122/122 [==============================] - 0s 656us/step - loss: 67.0797 - mae: 6.0475 - val_loss: 35.4025 - val_mae: 4.2515\n",
      "Epoch 546/950\n",
      "122/122 [==============================] - 0s 608us/step - loss: 67.7083 - mae: 5.9433 - val_loss: 33.5565 - val_mae: 4.1573\n",
      "Epoch 547/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 67.5761 - mae: 6.0009 - val_loss: 38.3453 - val_mae: 4.4948\n",
      "Epoch 548/950\n",
      "122/122 [==============================] - 0s 584us/step - loss: 63.8974 - mae: 5.8650 - val_loss: 45.0380 - val_mae: 5.1866\n",
      "Epoch 549/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 64.9012 - mae: 5.8905 - val_loss: 35.1114 - val_mae: 4.3470\n",
      "Epoch 550/950\n",
      "122/122 [==============================] - 0s 591us/step - loss: 67.6500 - mae: 5.9419 - val_loss: 44.3205 - val_mae: 5.0453\n",
      "Epoch 551/950\n",
      "122/122 [==============================] - 0s 676us/step - loss: 66.1862 - mae: 5.9605 - val_loss: 36.5803 - val_mae: 4.0236\n",
      "Epoch 552/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 66.2477 - mae: 5.9196 - val_loss: 39.7998 - val_mae: 4.4872\n",
      "Epoch 553/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 64.9361 - mae: 5.8853 - val_loss: 42.5556 - val_mae: 4.5296\n",
      "Epoch 554/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 72.3719 - mae: 5.9310 - val_loss: 36.5490 - val_mae: 4.1976\n",
      "Epoch 555/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 64.1340 - mae: 5.8653 - val_loss: 41.0955 - val_mae: 4.7896\n",
      "Epoch 556/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 65.7517 - mae: 5.8893 - val_loss: 34.7930 - val_mae: 4.2782\n",
      "Epoch 557/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 63.7473 - mae: 5.8382 - val_loss: 34.3843 - val_mae: 4.1230\n",
      "Epoch 558/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 71.0810 - mae: 6.1172 - val_loss: 45.0672 - val_mae: 5.0814\n",
      "Epoch 559/950\n",
      "122/122 [==============================] - 0s 662us/step - loss: 65.1413 - mae: 5.8494 - val_loss: 38.1826 - val_mae: 4.5802\n",
      "Epoch 560/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 65.7706 - mae: 5.8549 - val_loss: 34.1286 - val_mae: 4.2200\n",
      "Epoch 561/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 65.1455 - mae: 5.9167 - val_loss: 38.7959 - val_mae: 4.4755\n",
      "Epoch 562/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 68.9818 - mae: 5.9705 - val_loss: 33.4101 - val_mae: 4.0561\n",
      "Epoch 563/950\n",
      "122/122 [==============================] - 0s 762us/step - loss: 65.3543 - mae: 5.9592 - val_loss: 37.6989 - val_mae: 4.4985\n",
      "Epoch 564/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 63.8793 - mae: 5.7820 - val_loss: 36.3437 - val_mae: 4.4849\n",
      "Epoch 565/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 65.5799 - mae: 5.8706 - val_loss: 34.9866 - val_mae: 4.4136\n",
      "Epoch 566/950\n",
      "122/122 [==============================] - 0s 593us/step - loss: 63.4240 - mae: 5.7963 - val_loss: 40.4364 - val_mae: 4.8230\n",
      "Epoch 567/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 62.7127 - mae: 5.8218 - val_loss: 40.3603 - val_mae: 4.7247\n",
      "Epoch 568/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 66.1305 - mae: 5.9739 - val_loss: 32.3435 - val_mae: 4.1011\n",
      "Epoch 569/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 64.3933 - mae: 5.8960 - val_loss: 39.4194 - val_mae: 4.7695\n",
      "Epoch 570/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 66.1664 - mae: 5.9470 - val_loss: 33.3239 - val_mae: 4.1188\n",
      "Epoch 571/950\n",
      "122/122 [==============================] - 0s 702us/step - loss: 64.1326 - mae: 5.8706 - val_loss: 29.7185 - val_mae: 3.8552\n",
      "Epoch 572/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 64.8469 - mae: 5.8559 - val_loss: 41.8639 - val_mae: 4.8096\n",
      "Epoch 573/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 64.0337 - mae: 5.8173 - val_loss: 37.1087 - val_mae: 4.3382\n",
      "Epoch 574/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 66.8820 - mae: 5.9624 - val_loss: 34.9265 - val_mae: 4.2565\n",
      "Epoch 575/950\n",
      "122/122 [==============================] - 0s 584us/step - loss: 63.4050 - mae: 5.8347 - val_loss: 35.5526 - val_mae: 4.4288\n",
      "Epoch 576/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 64.4241 - mae: 5.8331 - val_loss: 39.7120 - val_mae: 4.6644\n",
      "Epoch 577/950\n",
      "122/122 [==============================] - 0s 820us/step - loss: 64.8909 - mae: 5.8813 - val_loss: 42.0617 - val_mae: 4.8844\n",
      "Epoch 578/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 69.6786 - mae: 6.1153 - val_loss: 39.7141 - val_mae: 4.5707\n",
      "Epoch 579/950\n",
      "122/122 [==============================] - 0s 777us/step - loss: 67.2895 - mae: 6.0181 - val_loss: 35.0984 - val_mae: 4.3543\n",
      "Epoch 580/950\n",
      "122/122 [==============================] - 0s 851us/step - loss: 65.1535 - mae: 5.8835 - val_loss: 38.3494 - val_mae: 4.6943\n",
      "Epoch 581/950\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68.0705 - mae: 5.9386 - val_loss: 33.3204 - val_mae: 4.0882\n",
      "Epoch 582/950\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 63.5506 - mae: 5.8213 - val_loss: 40.4764 - val_mae: 4.8504\n",
      "Epoch 583/950\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 65.1904 - mae: 5.8881 - val_loss: 39.0915 - val_mae: 4.6943\n",
      "Epoch 584/950\n",
      "122/122 [==============================] - 0s 723us/step - loss: 70.4225 - mae: 6.0957 - val_loss: 38.3700 - val_mae: 4.5714\n",
      "Epoch 585/950\n",
      "122/122 [==============================] - 0s 669us/step - loss: 64.4542 - mae: 5.8748 - val_loss: 43.1175 - val_mae: 4.8432\n",
      "Epoch 586/950\n",
      "122/122 [==============================] - 0s 685us/step - loss: 69.2372 - mae: 5.9786 - val_loss: 36.2413 - val_mae: 4.4533\n",
      "Epoch 587/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 64.2218 - mae: 5.8809 - val_loss: 35.6684 - val_mae: 4.2552\n",
      "Epoch 588/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 65.5294 - mae: 5.8443 - val_loss: 37.2796 - val_mae: 4.2963\n",
      "Epoch 589/950\n",
      "122/122 [==============================] - 0s 698us/step - loss: 67.3912 - mae: 5.9595 - val_loss: 35.4894 - val_mae: 4.1899\n",
      "Epoch 590/950\n",
      "122/122 [==============================] - 0s 608us/step - loss: 62.3164 - mae: 5.8799 - val_loss: 36.2799 - val_mae: 4.3341\n",
      "Epoch 591/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 61.6473 - mae: 5.8125 - val_loss: 32.3601 - val_mae: 3.9495\n",
      "Epoch 592/950\n",
      "122/122 [==============================] - 0s 676us/step - loss: 60.7270 - mae: 5.7643 - val_loss: 35.8046 - val_mae: 4.2738\n",
      "Epoch 593/950\n",
      "122/122 [==============================] - 0s 751us/step - loss: 60.9533 - mae: 5.7435 - val_loss: 33.1852 - val_mae: 4.1178\n",
      "Epoch 594/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 66.4958 - mae: 5.8941 - val_loss: 35.2433 - val_mae: 4.2379\n",
      "Epoch 595/950\n",
      "122/122 [==============================] - 0s 668us/step - loss: 63.5099 - mae: 5.9177 - val_loss: 45.8631 - val_mae: 5.0249\n",
      "Epoch 596/950\n",
      "122/122 [==============================] - 0s 867us/step - loss: 62.7074 - mae: 5.7881 - val_loss: 41.0358 - val_mae: 4.6833\n",
      "Epoch 597/950\n",
      "122/122 [==============================] - 0s 757us/step - loss: 64.4854 - mae: 5.8303 - val_loss: 39.4426 - val_mae: 4.5577\n",
      "Epoch 598/950\n",
      "122/122 [==============================] - 0s 804us/step - loss: 65.9491 - mae: 5.8765 - val_loss: 44.8336 - val_mae: 5.0078\n",
      "Epoch 599/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 65.4680 - mae: 5.9226 - val_loss: 41.1175 - val_mae: 4.5704\n",
      "Epoch 600/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 61.5493 - mae: 5.7166 - val_loss: 41.0138 - val_mae: 4.6659\n",
      "Epoch 601/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 66.5097 - mae: 5.8797 - val_loss: 31.1692 - val_mae: 3.9508\n",
      "Epoch 602/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 62.2521 - mae: 5.8352 - val_loss: 32.3444 - val_mae: 4.0188\n",
      "Epoch 603/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 64.0573 - mae: 5.8296 - val_loss: 37.9613 - val_mae: 4.4775\n",
      "Epoch 604/950\n",
      "122/122 [==============================] - 0s 765us/step - loss: 60.0225 - mae: 5.7080 - val_loss: 32.9787 - val_mae: 4.1158\n",
      "Epoch 605/950\n",
      "122/122 [==============================] - 0s 742us/step - loss: 63.2158 - mae: 5.8236 - val_loss: 45.2976 - val_mae: 5.0898\n",
      "Epoch 606/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 61.7696 - mae: 5.8253 - val_loss: 45.0598 - val_mae: 4.9155\n",
      "Epoch 607/950\n",
      "122/122 [==============================] - 0s 647us/step - loss: 57.9324 - mae: 5.6697 - val_loss: 38.0068 - val_mae: 4.5502\n",
      "Epoch 608/950\n",
      "122/122 [==============================] - 0s 699us/step - loss: 62.1665 - mae: 5.8422 - val_loss: 37.9111 - val_mae: 4.3049\n",
      "Epoch 609/950\n",
      "122/122 [==============================] - 0s 671us/step - loss: 63.3003 - mae: 5.8041 - val_loss: 36.7447 - val_mae: 4.3748\n",
      "Epoch 610/950\n",
      "122/122 [==============================] - 0s 757us/step - loss: 62.0725 - mae: 5.7991 - val_loss: 32.6988 - val_mae: 4.0514\n",
      "Epoch 611/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 63.5032 - mae: 5.8491 - val_loss: 36.4917 - val_mae: 4.4557\n",
      "Epoch 612/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 60.9657 - mae: 5.7545 - val_loss: 40.9835 - val_mae: 4.7989\n",
      "Epoch 613/950\n",
      "122/122 [==============================] - 0s 583us/step - loss: 63.0006 - mae: 5.8551 - val_loss: 46.7025 - val_mae: 5.0975\n",
      "Epoch 614/950\n",
      "122/122 [==============================] - 0s 608us/step - loss: 60.9217 - mae: 5.8046 - val_loss: 36.3518 - val_mae: 4.5546\n",
      "Epoch 615/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 61.6659 - mae: 5.7370 - val_loss: 42.5674 - val_mae: 4.9814\n",
      "Epoch 616/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 63.1515 - mae: 5.8233 - val_loss: 32.6834 - val_mae: 4.1848\n",
      "Epoch 617/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 64.8024 - mae: 5.9072 - val_loss: 45.2912 - val_mae: 5.0347\n",
      "Epoch 618/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 58.5649 - mae: 5.6532 - val_loss: 32.3897 - val_mae: 4.0332\n",
      "Epoch 619/950\n",
      "122/122 [==============================] - 0s 695us/step - loss: 62.5163 - mae: 5.7382 - val_loss: 31.9082 - val_mae: 4.1326\n",
      "Epoch 620/950\n",
      "122/122 [==============================] - 0s 724us/step - loss: 63.8899 - mae: 5.7718 - val_loss: 36.3878 - val_mae: 4.5386\n",
      "Epoch 621/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 59.6547 - mae: 5.6652 - val_loss: 54.5300 - val_mae: 5.6045\n",
      "Epoch 622/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 63.0714 - mae: 5.8224 - val_loss: 33.4809 - val_mae: 4.2088\n",
      "Epoch 623/950\n",
      "122/122 [==============================] - 0s 609us/step - loss: 61.3542 - mae: 5.7282 - val_loss: 52.8130 - val_mae: 5.6411\n",
      "Epoch 624/950\n",
      "122/122 [==============================] - 0s 582us/step - loss: 62.3293 - mae: 5.7823 - val_loss: 33.3293 - val_mae: 4.3060\n",
      "Epoch 625/950\n",
      "122/122 [==============================] - 0s 656us/step - loss: 64.2960 - mae: 5.8610 - val_loss: 30.8422 - val_mae: 4.0040\n",
      "Epoch 626/950\n",
      "122/122 [==============================] - 0s 665us/step - loss: 58.4157 - mae: 5.7304 - val_loss: 42.5118 - val_mae: 5.0390\n",
      "Epoch 627/950\n",
      "122/122 [==============================] - 0s 770us/step - loss: 62.1748 - mae: 5.7313 - val_loss: 34.4153 - val_mae: 4.2975\n",
      "Epoch 628/950\n",
      "122/122 [==============================] - 0s 676us/step - loss: 63.1223 - mae: 5.8257 - val_loss: 40.9083 - val_mae: 4.7958\n",
      "Epoch 629/950\n",
      "122/122 [==============================] - 0s 692us/step - loss: 61.4115 - mae: 5.7492 - val_loss: 31.9702 - val_mae: 4.0366\n",
      "Epoch 630/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 63.7806 - mae: 5.9091 - val_loss: 47.6571 - val_mae: 5.3337\n",
      "Epoch 631/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 63.1462 - mae: 5.8082 - val_loss: 35.2695 - val_mae: 4.4147\n",
      "Epoch 632/950\n",
      "122/122 [==============================] - 0s 735us/step - loss: 60.5738 - mae: 5.7119 - val_loss: 35.4906 - val_mae: 4.3895\n",
      "Epoch 633/950\n",
      "122/122 [==============================] - 0s 676us/step - loss: 60.2117 - mae: 5.6469 - val_loss: 31.8276 - val_mae: 4.0156\n",
      "Epoch 634/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 59.7440 - mae: 5.6866 - val_loss: 34.2566 - val_mae: 4.2066\n",
      "Epoch 635/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 62.0939 - mae: 5.6991 - val_loss: 38.2805 - val_mae: 4.3320\n",
      "Epoch 636/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 61.2682 - mae: 5.6615 - val_loss: 31.6529 - val_mae: 4.1318\n",
      "Epoch 637/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 61.2009 - mae: 5.7517 - val_loss: 35.7506 - val_mae: 4.1740\n",
      "Epoch 638/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 66.6052 - mae: 5.8225 - val_loss: 30.0671 - val_mae: 3.9487\n",
      "Epoch 639/950\n",
      "122/122 [==============================] - 0s 585us/step - loss: 61.4642 - mae: 5.7547 - val_loss: 39.0556 - val_mae: 4.4946\n",
      "Epoch 640/950\n",
      "122/122 [==============================] - 0s 731us/step - loss: 61.0281 - mae: 5.7039 - val_loss: 35.5698 - val_mae: 4.2845\n",
      "Epoch 641/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 64.7477 - mae: 5.8133 - val_loss: 47.8484 - val_mae: 5.4144\n",
      "Epoch 642/950\n",
      "122/122 [==============================] - 0s 675us/step - loss: 59.4710 - mae: 5.7637 - val_loss: 29.4012 - val_mae: 3.8908\n",
      "Epoch 643/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 59.4559 - mae: 5.6530 - val_loss: 41.0991 - val_mae: 4.8514\n",
      "Epoch 644/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 60.6723 - mae: 5.6621 - val_loss: 34.5893 - val_mae: 4.2953\n",
      "Epoch 645/950\n",
      "122/122 [==============================] - 0s 623us/step - loss: 64.1693 - mae: 5.8661 - val_loss: 46.8342 - val_mae: 5.1351\n",
      "Epoch 646/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 59.7548 - mae: 5.6496 - val_loss: 36.7161 - val_mae: 4.3568\n",
      "Epoch 647/950\n",
      "122/122 [==============================] - 0s 703us/step - loss: 62.9016 - mae: 5.7542 - val_loss: 38.0570 - val_mae: 4.6392\n",
      "Epoch 648/950\n",
      "122/122 [==============================] - 0s 720us/step - loss: 65.3299 - mae: 5.8393 - val_loss: 32.7010 - val_mae: 4.0142\n",
      "Epoch 649/950\n",
      "122/122 [==============================] - 0s 682us/step - loss: 63.7782 - mae: 5.7993 - val_loss: 41.2287 - val_mae: 4.7693\n",
      "Epoch 650/950\n",
      "122/122 [==============================] - 0s 691us/step - loss: 62.0523 - mae: 5.7823 - val_loss: 38.9970 - val_mae: 4.5807\n",
      "Epoch 651/950\n",
      "122/122 [==============================] - 0s 710us/step - loss: 57.5930 - mae: 5.6494 - val_loss: 31.0966 - val_mae: 3.9713\n",
      "Epoch 652/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 59.7457 - mae: 5.6798 - val_loss: 41.7869 - val_mae: 4.6876\n",
      "Epoch 653/950\n",
      "122/122 [==============================] - 0s 697us/step - loss: 59.9964 - mae: 5.6905 - val_loss: 35.5278 - val_mae: 4.3393\n",
      "Epoch 654/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 60.3860 - mae: 5.6491 - val_loss: 46.1520 - val_mae: 5.1275\n",
      "Epoch 655/950\n",
      "122/122 [==============================] - 0s 688us/step - loss: 61.8767 - mae: 5.7645 - val_loss: 34.4530 - val_mae: 4.0420\n",
      "Epoch 656/950\n",
      "122/122 [==============================] - 0s 684us/step - loss: 63.4574 - mae: 5.7331 - val_loss: 34.8377 - val_mae: 4.1670\n",
      "Epoch 657/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 62.7324 - mae: 5.8346 - val_loss: 36.5919 - val_mae: 4.1469\n",
      "Epoch 658/950\n",
      "122/122 [==============================] - 0s 677us/step - loss: 61.2213 - mae: 5.6945 - val_loss: 39.0288 - val_mae: 4.5376\n",
      "Epoch 659/950\n",
      "122/122 [==============================] - 0s 580us/step - loss: 63.4785 - mae: 5.7016 - val_loss: 37.8374 - val_mae: 4.3298\n",
      "Epoch 660/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 58.0932 - mae: 5.7042 - val_loss: 36.3548 - val_mae: 3.8948\n",
      "Epoch 661/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 65.2908 - mae: 5.8419 - val_loss: 37.5190 - val_mae: 4.1901\n",
      "Epoch 662/950\n",
      "122/122 [==============================] - 0s 662us/step - loss: 61.8210 - mae: 5.8492 - val_loss: 40.9689 - val_mae: 4.5640\n",
      "Epoch 663/950\n",
      "122/122 [==============================] - 0s 711us/step - loss: 64.5472 - mae: 5.7872 - val_loss: 33.2990 - val_mae: 4.0454\n",
      "Epoch 664/950\n",
      "122/122 [==============================] - 0s 678us/step - loss: 61.5596 - mae: 5.7841 - val_loss: 35.4662 - val_mae: 4.3754\n",
      "Epoch 665/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 64.4260 - mae: 5.8946 - val_loss: 36.6094 - val_mae: 4.4314\n",
      "Epoch 666/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 58.8855 - mae: 5.5774 - val_loss: 32.8787 - val_mae: 4.0963\n",
      "Epoch 667/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 58.9654 - mae: 5.6741 - val_loss: 37.1840 - val_mae: 4.5925\n",
      "Epoch 668/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 61.1155 - mae: 5.7867 - val_loss: 32.7636 - val_mae: 4.1495\n",
      "Epoch 669/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 54.8461 - mae: 5.5422 - val_loss: 32.6395 - val_mae: 4.1168\n",
      "Epoch 670/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 58.0467 - mae: 5.6848 - val_loss: 32.8499 - val_mae: 4.2141\n",
      "Epoch 671/950\n",
      "122/122 [==============================] - 0s 588us/step - loss: 60.8913 - mae: 5.7615 - val_loss: 41.1143 - val_mae: 4.8499\n",
      "Epoch 672/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 61.2195 - mae: 5.7726 - val_loss: 33.7218 - val_mae: 4.3065\n",
      "Epoch 673/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 61.4740 - mae: 5.7296 - val_loss: 34.1571 - val_mae: 4.3321\n",
      "Epoch 674/950\n",
      "122/122 [==============================] - 0s 695us/step - loss: 58.0447 - mae: 5.6690 - val_loss: 31.3466 - val_mae: 3.9359\n",
      "Epoch 675/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 62.7881 - mae: 5.7672 - val_loss: 29.3681 - val_mae: 3.8569\n",
      "Epoch 676/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 60.1583 - mae: 5.7498 - val_loss: 34.5205 - val_mae: 4.3551\n",
      "Epoch 677/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 67.4917 - mae: 5.8383 - val_loss: 33.1254 - val_mae: 4.2419\n",
      "Epoch 678/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 60.6306 - mae: 5.6955 - val_loss: 34.4426 - val_mae: 3.8918\n",
      "Epoch 679/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 60.2136 - mae: 5.7807 - val_loss: 29.7387 - val_mae: 3.8430\n",
      "Epoch 680/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 59.9285 - mae: 5.7393 - val_loss: 37.4344 - val_mae: 4.4598\n",
      "Epoch 681/950\n",
      "122/122 [==============================] - 0s 613us/step - loss: 61.8051 - mae: 5.7393 - val_loss: 40.7445 - val_mae: 4.8556\n",
      "Epoch 682/950\n",
      "122/122 [==============================] - 0s 687us/step - loss: 59.4731 - mae: 5.7305 - val_loss: 37.6754 - val_mae: 4.0728\n",
      "Epoch 683/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 60.4721 - mae: 5.6978 - val_loss: 33.6956 - val_mae: 4.1333\n",
      "Epoch 684/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 63.7802 - mae: 5.8367 - val_loss: 34.9743 - val_mae: 4.2108\n",
      "Epoch 685/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 60.9682 - mae: 5.6683 - val_loss: 39.8540 - val_mae: 4.6681\n",
      "Epoch 686/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 59.6757 - mae: 5.7102 - val_loss: 35.9580 - val_mae: 4.3430\n",
      "Epoch 687/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 62.1874 - mae: 5.7840 - val_loss: 52.0797 - val_mae: 5.5572\n",
      "Epoch 688/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 61.4895 - mae: 5.7570 - val_loss: 39.2245 - val_mae: 4.5237\n",
      "Epoch 689/950\n",
      "122/122 [==============================] - 0s 603us/step - loss: 64.2677 - mae: 5.8940 - val_loss: 34.6709 - val_mae: 4.2564\n",
      "Epoch 690/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 59.9063 - mae: 5.7583 - val_loss: 35.4606 - val_mae: 4.2657\n",
      "Epoch 691/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 58.8007 - mae: 5.6627 - val_loss: 36.3339 - val_mae: 4.3263\n",
      "Epoch 692/950\n",
      "122/122 [==============================] - 0s 596us/step - loss: 59.1125 - mae: 5.7063 - val_loss: 43.3745 - val_mae: 4.9599\n",
      "Epoch 693/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 58.7367 - mae: 5.6452 - val_loss: 37.2479 - val_mae: 4.3353\n",
      "Epoch 694/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 60.1693 - mae: 5.7314 - val_loss: 34.5282 - val_mae: 4.1323\n",
      "Epoch 695/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 58.0426 - mae: 5.6420 - val_loss: 45.1130 - val_mae: 4.8962\n",
      "Epoch 696/950\n",
      "122/122 [==============================] - 0s 586us/step - loss: 61.5987 - mae: 5.7535 - val_loss: 34.9826 - val_mae: 4.2679\n",
      "Epoch 697/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 58.2440 - mae: 5.5945 - val_loss: 58.1340 - val_mae: 5.8997\n",
      "Epoch 698/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 58.1883 - mae: 5.7069 - val_loss: 35.2563 - val_mae: 4.3246\n",
      "Epoch 699/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 62.3047 - mae: 5.7650 - val_loss: 43.2557 - val_mae: 4.8130\n",
      "Epoch 700/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 58.0813 - mae: 5.6708 - val_loss: 36.8827 - val_mae: 4.2555\n",
      "Epoch 701/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 58.5581 - mae: 5.6454 - val_loss: 37.6605 - val_mae: 4.4494\n",
      "Epoch 702/950\n",
      "122/122 [==============================] - 0s 679us/step - loss: 60.3486 - mae: 5.7709 - val_loss: 34.2551 - val_mae: 4.2504\n",
      "Epoch 703/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 59.5617 - mae: 5.5718 - val_loss: 38.3921 - val_mae: 4.4638\n",
      "Epoch 704/950\n",
      "122/122 [==============================] - 0s 580us/step - loss: 59.2897 - mae: 5.7036 - val_loss: 35.2159 - val_mae: 4.2730\n",
      "Epoch 705/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 58.9273 - mae: 5.6219 - val_loss: 33.2142 - val_mae: 4.0269\n",
      "Epoch 706/950\n",
      "122/122 [==============================] - 0s 597us/step - loss: 58.1855 - mae: 5.6628 - val_loss: 37.3589 - val_mae: 4.1669\n",
      "Epoch 707/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 60.0275 - mae: 5.6559 - val_loss: 41.2270 - val_mae: 4.6404\n",
      "Epoch 708/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 58.5726 - mae: 5.6438 - val_loss: 43.3071 - val_mae: 4.8352\n",
      "Epoch 709/950\n",
      "122/122 [==============================] - 0s 729us/step - loss: 58.2960 - mae: 5.5657 - val_loss: 36.3669 - val_mae: 4.3031\n",
      "Epoch 710/950\n",
      "122/122 [==============================] - 0s 704us/step - loss: 60.9899 - mae: 5.7554 - val_loss: 32.5856 - val_mae: 3.9975\n",
      "Epoch 711/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 63.6630 - mae: 5.7248 - val_loss: 30.1468 - val_mae: 3.8631\n",
      "Epoch 712/950\n",
      "122/122 [==============================] - 0s 576us/step - loss: 58.2765 - mae: 5.6644 - val_loss: 37.3861 - val_mae: 4.6771\n",
      "Epoch 713/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 59.2649 - mae: 5.6826 - val_loss: 43.8185 - val_mae: 5.0106\n",
      "Epoch 714/950\n",
      "122/122 [==============================] - 0s 573us/step - loss: 57.1707 - mae: 5.6089 - val_loss: 34.9056 - val_mae: 4.2554\n",
      "Epoch 715/950\n",
      "122/122 [==============================] - 0s 674us/step - loss: 58.7332 - mae: 5.6188 - val_loss: 41.9234 - val_mae: 4.8043\n",
      "Epoch 716/950\n",
      "122/122 [==============================] - 0s 647us/step - loss: 56.0178 - mae: 5.5779 - val_loss: 39.8141 - val_mae: 4.4547\n",
      "Epoch 717/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 59.0011 - mae: 5.6706 - val_loss: 32.2335 - val_mae: 3.9958\n",
      "Epoch 718/950\n",
      "122/122 [==============================] - 0s 609us/step - loss: 59.9388 - mae: 5.6817 - val_loss: 42.4957 - val_mae: 4.7794\n",
      "Epoch 719/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 59.7588 - mae: 5.7317 - val_loss: 33.7834 - val_mae: 3.9924\n",
      "Epoch 720/950\n",
      "122/122 [==============================] - 0s 590us/step - loss: 57.7545 - mae: 5.6304 - val_loss: 38.6301 - val_mae: 4.4875\n",
      "Epoch 721/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 59.9485 - mae: 5.6882 - val_loss: 33.0550 - val_mae: 3.9831\n",
      "Epoch 722/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 58.7542 - mae: 5.7142 - val_loss: 44.6257 - val_mae: 4.9293\n",
      "Epoch 723/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 59.8351 - mae: 5.7527 - val_loss: 39.9001 - val_mae: 4.5766\n",
      "Epoch 724/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 59.3512 - mae: 5.7413 - val_loss: 31.7940 - val_mae: 3.8599\n",
      "Epoch 725/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 56.6527 - mae: 5.5582 - val_loss: 35.3412 - val_mae: 4.3340\n",
      "Epoch 726/950\n",
      "122/122 [==============================] - 0s 679us/step - loss: 58.7675 - mae: 5.6253 - val_loss: 34.4286 - val_mae: 4.2091\n",
      "Epoch 727/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 56.4631 - mae: 5.5696 - val_loss: 33.7232 - val_mae: 4.0446\n",
      "Epoch 728/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 59.5953 - mae: 5.7130 - val_loss: 40.0272 - val_mae: 4.6803\n",
      "Epoch 729/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 63.0263 - mae: 5.8080 - val_loss: 32.3049 - val_mae: 3.9840\n",
      "Epoch 730/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 58.8899 - mae: 5.6177 - val_loss: 47.6252 - val_mae: 5.1715\n",
      "Epoch 731/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 61.3815 - mae: 5.7096 - val_loss: 40.0859 - val_mae: 4.6447\n",
      "Epoch 732/950\n",
      "122/122 [==============================] - 0s 671us/step - loss: 56.8315 - mae: 5.5729 - val_loss: 39.0531 - val_mae: 4.5778\n",
      "Epoch 733/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 57.9830 - mae: 5.6549 - val_loss: 44.9147 - val_mae: 4.9826\n",
      "Epoch 734/950\n",
      "122/122 [==============================] - 0s 622us/step - loss: 59.5717 - mae: 5.7072 - val_loss: 39.0406 - val_mae: 4.5751\n",
      "Epoch 735/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 59.1412 - mae: 5.6997 - val_loss: 38.9073 - val_mae: 4.4198\n",
      "Epoch 736/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 58.6899 - mae: 5.6075 - val_loss: 39.1912 - val_mae: 4.3570\n",
      "Epoch 737/950\n",
      "122/122 [==============================] - 0s 588us/step - loss: 59.6496 - mae: 5.6584 - val_loss: 36.2602 - val_mae: 4.4141\n",
      "Epoch 738/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 56.9152 - mae: 5.5810 - val_loss: 42.9599 - val_mae: 4.8921\n",
      "Epoch 739/950\n",
      "122/122 [==============================] - 0s 696us/step - loss: 57.4763 - mae: 5.6269 - val_loss: 39.3709 - val_mae: 4.6240\n",
      "Epoch 740/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 57.5956 - mae: 5.6113 - val_loss: 35.8036 - val_mae: 4.1496\n",
      "Epoch 741/950\n",
      "122/122 [==============================] - 0s 588us/step - loss: 55.7130 - mae: 5.5613 - val_loss: 43.4245 - val_mae: 4.9142\n",
      "Epoch 742/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 60.2713 - mae: 5.6612 - val_loss: 40.3472 - val_mae: 4.6709\n",
      "Epoch 743/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 59.3665 - mae: 5.6217 - val_loss: 34.7474 - val_mae: 4.1690\n",
      "Epoch 744/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 59.0301 - mae: 5.6454 - val_loss: 34.8410 - val_mae: 4.1412\n",
      "Epoch 745/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 64.0499 - mae: 5.8442 - val_loss: 35.6682 - val_mae: 4.2304\n",
      "Epoch 746/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 60.6618 - mae: 5.7020 - val_loss: 35.7895 - val_mae: 4.3894\n",
      "Epoch 747/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 63.4864 - mae: 5.7887 - val_loss: 45.9087 - val_mae: 5.0764\n",
      "Epoch 748/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 58.2206 - mae: 5.7153 - val_loss: 42.6980 - val_mae: 4.8640\n",
      "Epoch 749/950\n",
      "122/122 [==============================] - 0s 599us/step - loss: 60.8998 - mae: 5.7592 - val_loss: 36.2814 - val_mae: 4.2622\n",
      "Epoch 750/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 58.2693 - mae: 5.6987 - val_loss: 37.7180 - val_mae: 4.3563\n",
      "Epoch 751/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 59.5326 - mae: 5.6706 - val_loss: 36.2939 - val_mae: 4.2104\n",
      "Epoch 752/950\n",
      "122/122 [==============================] - 0s 684us/step - loss: 56.9192 - mae: 5.5692 - val_loss: 31.4876 - val_mae: 3.9941\n",
      "Epoch 753/950\n",
      "122/122 [==============================] - 0s 716us/step - loss: 57.9374 - mae: 5.6097 - val_loss: 51.2793 - val_mae: 5.4370\n",
      "Epoch 754/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 60.2228 - mae: 5.7176 - val_loss: 36.2662 - val_mae: 4.2260\n",
      "Epoch 755/950\n",
      "122/122 [==============================] - 0s 616us/step - loss: 57.2886 - mae: 5.5797 - val_loss: 35.8235 - val_mae: 4.2261\n",
      "Epoch 756/950\n",
      "122/122 [==============================] - 0s 598us/step - loss: 57.1747 - mae: 5.5775 - val_loss: 41.1090 - val_mae: 4.5045\n",
      "Epoch 757/950\n",
      "122/122 [==============================] - 0s 622us/step - loss: 55.0691 - mae: 5.4701 - val_loss: 41.2957 - val_mae: 4.8022\n",
      "Epoch 758/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 55.8144 - mae: 5.5824 - val_loss: 45.6451 - val_mae: 4.9572\n",
      "Epoch 759/950\n",
      "122/122 [==============================] - 0s 595us/step - loss: 59.1351 - mae: 5.6754 - val_loss: 39.4567 - val_mae: 4.5487\n",
      "Epoch 760/950\n",
      "122/122 [==============================] - 0s 675us/step - loss: 57.3611 - mae: 5.5891 - val_loss: 34.4285 - val_mae: 4.2169\n",
      "Epoch 761/950\n",
      "122/122 [==============================] - 0s 609us/step - loss: 59.9691 - mae: 5.6898 - val_loss: 34.6952 - val_mae: 4.1858\n",
      "Epoch 762/950\n",
      "122/122 [==============================] - 0s 597us/step - loss: 55.3449 - mae: 5.5544 - val_loss: 40.4269 - val_mae: 4.5364\n",
      "Epoch 763/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 55.4717 - mae: 5.5488 - val_loss: 36.2823 - val_mae: 4.0965\n",
      "Epoch 764/950\n",
      "122/122 [==============================] - 0s 592us/step - loss: 58.7939 - mae: 5.6879 - val_loss: 43.2308 - val_mae: 4.6870\n",
      "Epoch 765/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 57.3253 - mae: 5.6607 - val_loss: 37.7207 - val_mae: 4.1915\n",
      "Epoch 766/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 55.1616 - mae: 5.4572 - val_loss: 40.9911 - val_mae: 4.7890\n",
      "Epoch 767/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 58.8221 - mae: 5.6386 - val_loss: 36.7534 - val_mae: 4.4082\n",
      "Epoch 768/950\n",
      "122/122 [==============================] - 0s 719us/step - loss: 57.2642 - mae: 5.5651 - val_loss: 44.0418 - val_mae: 5.0306\n",
      "Epoch 769/950\n",
      "122/122 [==============================] - 0s 691us/step - loss: 58.3415 - mae: 5.6300 - val_loss: 34.5003 - val_mae: 4.2100\n",
      "Epoch 770/950\n",
      "122/122 [==============================] - 0s 611us/step - loss: 54.9756 - mae: 5.4667 - val_loss: 41.1424 - val_mae: 4.7942\n",
      "Epoch 771/950\n",
      "122/122 [==============================] - 0s 694us/step - loss: 59.0754 - mae: 5.7315 - val_loss: 35.4910 - val_mae: 4.2345\n",
      "Epoch 772/950\n",
      "122/122 [==============================] - 0s 712us/step - loss: 60.9586 - mae: 5.7736 - val_loss: 39.4153 - val_mae: 4.4294\n",
      "Epoch 773/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 55.9885 - mae: 5.5611 - val_loss: 38.0879 - val_mae: 4.4587\n",
      "Epoch 774/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 56.2737 - mae: 5.5428 - val_loss: 39.0743 - val_mae: 4.5151\n",
      "Epoch 775/950\n",
      "122/122 [==============================] - 0s 687us/step - loss: 57.1040 - mae: 5.5419 - val_loss: 37.6094 - val_mae: 4.4469\n",
      "Epoch 776/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 57.9767 - mae: 5.6189 - val_loss: 41.6435 - val_mae: 4.7628\n",
      "Epoch 777/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 57.3956 - mae: 5.5786 - val_loss: 34.3719 - val_mae: 4.0362\n",
      "Epoch 778/950\n",
      "122/122 [==============================] - 0s 660us/step - loss: 57.9452 - mae: 5.5895 - val_loss: 38.5728 - val_mae: 4.3846\n",
      "Epoch 779/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 60.6494 - mae: 5.7216 - val_loss: 38.5935 - val_mae: 4.5277\n",
      "Epoch 780/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 57.5699 - mae: 5.6296 - val_loss: 32.6053 - val_mae: 3.9878\n",
      "Epoch 781/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 56.7096 - mae: 5.5741 - val_loss: 37.3813 - val_mae: 4.2608\n",
      "Epoch 782/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 58.8172 - mae: 5.6542 - val_loss: 36.0311 - val_mae: 4.2415\n",
      "Epoch 783/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 57.3179 - mae: 5.6141 - val_loss: 33.0329 - val_mae: 4.0139\n",
      "Epoch 784/950\n",
      "122/122 [==============================] - 0s 623us/step - loss: 58.3400 - mae: 5.6239 - val_loss: 48.2329 - val_mae: 5.1808\n",
      "Epoch 785/950\n",
      "122/122 [==============================] - 0s 611us/step - loss: 56.0808 - mae: 5.5179 - val_loss: 47.0874 - val_mae: 5.0863\n",
      "Epoch 786/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 57.7079 - mae: 5.5823 - val_loss: 42.6318 - val_mae: 4.8893\n",
      "Epoch 787/950\n",
      "122/122 [==============================] - 0s 605us/step - loss: 56.1511 - mae: 5.5430 - val_loss: 36.2206 - val_mae: 4.3877\n",
      "Epoch 788/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 62.1613 - mae: 5.6919 - val_loss: 39.8103 - val_mae: 4.5585\n",
      "Epoch 789/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 58.8485 - mae: 5.6270 - val_loss: 39.7084 - val_mae: 4.6487\n",
      "Epoch 790/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 56.4151 - mae: 5.5505 - val_loss: 39.3370 - val_mae: 4.5463\n",
      "Epoch 791/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 52.8729 - mae: 5.4081 - val_loss: 35.3622 - val_mae: 4.2227\n",
      "Epoch 792/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 56.0249 - mae: 5.5365 - val_loss: 38.4274 - val_mae: 4.5035\n",
      "Epoch 793/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 55.8058 - mae: 5.4690 - val_loss: 40.4313 - val_mae: 4.6651\n",
      "Epoch 794/950\n",
      "122/122 [==============================] - 0s 710us/step - loss: 55.9330 - mae: 5.5182 - val_loss: 41.9025 - val_mae: 4.7792\n",
      "Epoch 795/950\n",
      "122/122 [==============================] - 0s 641us/step - loss: 53.9013 - mae: 5.5093 - val_loss: 36.5821 - val_mae: 4.3094\n",
      "Epoch 796/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 60.1029 - mae: 5.6500 - val_loss: 35.7817 - val_mae: 4.2565\n",
      "Epoch 797/950\n",
      "122/122 [==============================] - 0s 625us/step - loss: 55.0689 - mae: 5.4843 - val_loss: 35.3594 - val_mae: 4.1815\n",
      "Epoch 798/950\n",
      "122/122 [==============================] - 0s 617us/step - loss: 56.9377 - mae: 5.5020 - val_loss: 34.1193 - val_mae: 4.2060\n",
      "Epoch 799/950\n",
      "122/122 [==============================] - 0s 611us/step - loss: 58.2155 - mae: 5.6149 - val_loss: 43.6510 - val_mae: 4.9593\n",
      "Epoch 800/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 56.3156 - mae: 5.5696 - val_loss: 35.3857 - val_mae: 4.0752\n",
      "Epoch 801/950\n",
      "122/122 [==============================] - 0s 591us/step - loss: 52.2426 - mae: 5.3434 - val_loss: 34.8277 - val_mae: 4.3322\n",
      "Epoch 802/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 59.9177 - mae: 5.6748 - val_loss: 35.3234 - val_mae: 4.2426\n",
      "Epoch 803/950\n",
      "122/122 [==============================] - 0s 621us/step - loss: 55.2014 - mae: 5.5029 - val_loss: 36.6862 - val_mae: 4.3463\n",
      "Epoch 804/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 56.2240 - mae: 5.5374 - val_loss: 35.9105 - val_mae: 4.3828\n",
      "Epoch 805/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 56.5545 - mae: 5.5614 - val_loss: 34.5463 - val_mae: 4.2457\n",
      "Epoch 806/950\n",
      "122/122 [==============================] - 0s 681us/step - loss: 54.6794 - mae: 5.5024 - val_loss: 39.1559 - val_mae: 4.4460\n",
      "Epoch 807/950\n",
      "122/122 [==============================] - 0s 657us/step - loss: 61.6811 - mae: 5.6939 - val_loss: 42.2796 - val_mae: 4.8029\n",
      "Epoch 808/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 55.6254 - mae: 5.5826 - val_loss: 36.3656 - val_mae: 4.3759\n",
      "Epoch 809/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 56.9516 - mae: 5.5700 - val_loss: 43.7519 - val_mae: 4.9246\n",
      "Epoch 810/950\n",
      "122/122 [==============================] - 0s 678us/step - loss: 53.7536 - mae: 5.4805 - val_loss: 45.5545 - val_mae: 4.9907\n",
      "Epoch 811/950\n",
      "122/122 [==============================] - 0s 746us/step - loss: 59.2335 - mae: 5.6692 - val_loss: 36.9764 - val_mae: 4.2797\n",
      "Epoch 812/950\n",
      "122/122 [==============================] - 0s 785us/step - loss: 59.2849 - mae: 5.6436 - val_loss: 35.7026 - val_mae: 4.2796\n",
      "Epoch 813/950\n",
      "122/122 [==============================] - 0s 746us/step - loss: 60.8323 - mae: 5.7427 - val_loss: 37.9364 - val_mae: 4.5864\n",
      "Epoch 814/950\n",
      "122/122 [==============================] - 0s 693us/step - loss: 56.9971 - mae: 5.6170 - val_loss: 34.3015 - val_mae: 4.2477\n",
      "Epoch 815/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 55.8423 - mae: 5.5068 - val_loss: 39.3605 - val_mae: 4.3828\n",
      "Epoch 816/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 54.6223 - mae: 5.4641 - val_loss: 36.5718 - val_mae: 4.2698\n",
      "Epoch 817/950\n",
      "122/122 [==============================] - 0s 733us/step - loss: 55.8170 - mae: 5.5043 - val_loss: 37.3414 - val_mae: 4.2999\n",
      "Epoch 818/950\n",
      "122/122 [==============================] - 0s 837us/step - loss: 58.6848 - mae: 5.6051 - val_loss: 39.9060 - val_mae: 4.3016\n",
      "Epoch 819/950\n",
      "122/122 [==============================] - 0s 668us/step - loss: 57.8830 - mae: 5.6273 - val_loss: 34.1375 - val_mae: 4.0206\n",
      "Epoch 820/950\n",
      "122/122 [==============================] - 0s 733us/step - loss: 54.3485 - mae: 5.4422 - val_loss: 32.6773 - val_mae: 3.8991\n",
      "Epoch 821/950\n",
      "122/122 [==============================] - 0s 684us/step - loss: 56.2193 - mae: 5.6166 - val_loss: 35.3844 - val_mae: 4.2093\n",
      "Epoch 822/950\n",
      "122/122 [==============================] - 0s 680us/step - loss: 58.0310 - mae: 5.6218 - val_loss: 34.2240 - val_mae: 4.1354\n",
      "Epoch 823/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 53.7875 - mae: 5.4687 - val_loss: 35.8131 - val_mae: 4.1853\n",
      "Epoch 824/950\n",
      "122/122 [==============================] - 0s 667us/step - loss: 56.4739 - mae: 5.5813 - val_loss: 41.2624 - val_mae: 4.6660\n",
      "Epoch 825/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 58.5834 - mae: 5.6030 - val_loss: 32.9027 - val_mae: 4.0646\n",
      "Epoch 826/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 62.1943 - mae: 5.7710 - val_loss: 34.2935 - val_mae: 4.2585\n",
      "Epoch 827/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 57.0839 - mae: 5.5886 - val_loss: 37.2406 - val_mae: 4.3823\n",
      "Epoch 828/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 54.3498 - mae: 5.4186 - val_loss: 35.6676 - val_mae: 4.2910\n",
      "Epoch 829/950\n",
      "122/122 [==============================] - 0s 657us/step - loss: 57.7236 - mae: 5.6640 - val_loss: 37.7898 - val_mae: 4.3362\n",
      "Epoch 830/950\n",
      "122/122 [==============================] - 0s 673us/step - loss: 56.6252 - mae: 5.5820 - val_loss: 33.5958 - val_mae: 4.0702\n",
      "Epoch 831/950\n",
      "122/122 [==============================] - 0s 700us/step - loss: 52.6523 - mae: 5.4140 - val_loss: 31.1299 - val_mae: 3.9844\n",
      "Epoch 832/950\n",
      "122/122 [==============================] - 0s 659us/step - loss: 56.3266 - mae: 5.5378 - val_loss: 36.8820 - val_mae: 4.3060\n",
      "Epoch 833/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 54.6506 - mae: 5.4834 - val_loss: 34.7845 - val_mae: 4.1499\n",
      "Epoch 834/950\n",
      "122/122 [==============================] - 0s 638us/step - loss: 58.8152 - mae: 5.6505 - val_loss: 32.8814 - val_mae: 4.0286\n",
      "Epoch 835/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 54.9842 - mae: 5.4971 - val_loss: 47.8036 - val_mae: 5.1347\n",
      "Epoch 836/950\n",
      "122/122 [==============================] - 0s 585us/step - loss: 57.8410 - mae: 5.6176 - val_loss: 32.5760 - val_mae: 3.9504\n",
      "Epoch 837/950\n",
      "122/122 [==============================] - 0s 758us/step - loss: 60.4261 - mae: 5.7265 - val_loss: 40.5937 - val_mae: 4.5698\n",
      "Epoch 838/950\n",
      "122/122 [==============================] - 0s 664us/step - loss: 57.4512 - mae: 5.5949 - val_loss: 39.4050 - val_mae: 4.5013\n",
      "Epoch 839/950\n",
      "122/122 [==============================] - 0s 655us/step - loss: 55.4899 - mae: 5.4824 - val_loss: 46.9381 - val_mae: 5.0200\n",
      "Epoch 840/950\n",
      "122/122 [==============================] - 0s 693us/step - loss: 56.2091 - mae: 5.4822 - val_loss: 37.0697 - val_mae: 4.2391\n",
      "Epoch 841/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 59.3270 - mae: 5.6045 - val_loss: 45.2335 - val_mae: 4.9455\n",
      "Epoch 842/950\n",
      "122/122 [==============================] - 0s 664us/step - loss: 58.3786 - mae: 5.6301 - val_loss: 40.3407 - val_mae: 4.4886\n",
      "Epoch 843/950\n",
      "122/122 [==============================] - 0s 688us/step - loss: 56.5755 - mae: 5.5068 - val_loss: 36.0150 - val_mae: 4.2695\n",
      "Epoch 844/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 54.5905 - mae: 5.4429 - val_loss: 33.7177 - val_mae: 4.1586\n",
      "Epoch 845/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 56.7810 - mae: 5.6084 - val_loss: 40.9772 - val_mae: 4.5264\n",
      "Epoch 846/950\n",
      "122/122 [==============================] - 0s 768us/step - loss: 55.4328 - mae: 5.4263 - val_loss: 49.2197 - val_mae: 5.2088\n",
      "Epoch 847/950\n",
      "122/122 [==============================] - 0s 721us/step - loss: 55.9898 - mae: 5.4986 - val_loss: 38.0030 - val_mae: 4.4302\n",
      "Epoch 848/950\n",
      "122/122 [==============================] - 0s 678us/step - loss: 55.7563 - mae: 5.4463 - val_loss: 31.1429 - val_mae: 3.8890\n",
      "Epoch 849/950\n",
      "122/122 [==============================] - 0s 670us/step - loss: 52.8302 - mae: 5.4189 - val_loss: 33.9380 - val_mae: 4.0524\n",
      "Epoch 850/950\n",
      "122/122 [==============================] - 0s 708us/step - loss: 56.0799 - mae: 5.4741 - val_loss: 41.6380 - val_mae: 4.5567\n",
      "Epoch 851/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 58.0444 - mae: 5.5750 - val_loss: 34.4694 - val_mae: 4.1547\n",
      "Epoch 852/950\n",
      "122/122 [==============================] - 0s 702us/step - loss: 54.5883 - mae: 5.4841 - val_loss: 35.4651 - val_mae: 4.1326\n",
      "Epoch 853/950\n",
      "122/122 [==============================] - 0s 680us/step - loss: 56.1601 - mae: 5.5318 - val_loss: 42.3767 - val_mae: 4.6182\n",
      "Epoch 854/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 55.9055 - mae: 5.5618 - val_loss: 40.1914 - val_mae: 4.4755\n",
      "Epoch 855/950\n",
      "122/122 [==============================] - 0s 729us/step - loss: 57.1089 - mae: 5.6114 - val_loss: 37.5417 - val_mae: 4.3050\n",
      "Epoch 856/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 57.4198 - mae: 5.5925 - val_loss: 33.3608 - val_mae: 4.0457\n",
      "Epoch 857/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 57.4748 - mae: 5.6283 - val_loss: 38.7206 - val_mae: 4.5205\n",
      "Epoch 858/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 56.9702 - mae: 5.5791 - val_loss: 33.2724 - val_mae: 3.8437\n",
      "Epoch 859/950\n",
      "122/122 [==============================] - 0s 589us/step - loss: 55.4899 - mae: 5.5658 - val_loss: 36.5155 - val_mae: 4.1326\n",
      "Epoch 860/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 56.2116 - mae: 5.5387 - val_loss: 33.5499 - val_mae: 3.9317\n",
      "Epoch 861/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 53.1591 - mae: 5.3876 - val_loss: 34.1816 - val_mae: 4.1546\n",
      "Epoch 862/950\n",
      "122/122 [==============================] - 0s 601us/step - loss: 56.0596 - mae: 5.5602 - val_loss: 38.9352 - val_mae: 4.4819\n",
      "Epoch 863/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 54.4124 - mae: 5.4762 - val_loss: 36.1658 - val_mae: 4.3549\n",
      "Epoch 864/950\n",
      "122/122 [==============================] - 0s 665us/step - loss: 54.4979 - mae: 5.5540 - val_loss: 35.4470 - val_mae: 4.2067\n",
      "Epoch 865/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 54.6568 - mae: 5.5048 - val_loss: 34.2913 - val_mae: 4.0686\n",
      "Epoch 866/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 56.7271 - mae: 5.3917 - val_loss: 32.9666 - val_mae: 4.0459\n",
      "Epoch 867/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 56.9700 - mae: 5.5794 - val_loss: 36.5119 - val_mae: 4.1455\n",
      "Epoch 868/950\n",
      "122/122 [==============================] - 0s 619us/step - loss: 57.1008 - mae: 5.6095 - val_loss: 37.2099 - val_mae: 4.4328\n",
      "Epoch 869/950\n",
      "122/122 [==============================] - 0s 626us/step - loss: 54.0893 - mae: 5.4524 - val_loss: 48.8217 - val_mae: 5.1575\n",
      "Epoch 870/950\n",
      "122/122 [==============================] - 0s 632us/step - loss: 55.1521 - mae: 5.4909 - val_loss: 37.3365 - val_mae: 4.5325\n",
      "Epoch 871/950\n",
      "122/122 [==============================] - 0s 672us/step - loss: 54.4894 - mae: 5.5178 - val_loss: 36.9159 - val_mae: 4.3908\n",
      "Epoch 872/950\n",
      "122/122 [==============================] - 0s 643us/step - loss: 54.4103 - mae: 5.4534 - val_loss: 46.9118 - val_mae: 5.1237\n",
      "Epoch 873/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 55.7376 - mae: 5.5508 - val_loss: 34.4645 - val_mae: 4.2073\n",
      "Epoch 874/950\n",
      "122/122 [==============================] - 0s 634us/step - loss: 57.0607 - mae: 5.5492 - val_loss: 44.4462 - val_mae: 4.7877\n",
      "Epoch 875/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 58.0004 - mae: 5.5850 - val_loss: 39.5030 - val_mae: 4.6331\n",
      "Epoch 876/950\n",
      "122/122 [==============================] - 0s 636us/step - loss: 55.0059 - mae: 5.4810 - val_loss: 34.2027 - val_mae: 4.1828\n",
      "Epoch 877/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 55.2675 - mae: 5.5523 - val_loss: 32.1656 - val_mae: 4.0477\n",
      "Epoch 878/950\n",
      "122/122 [==============================] - 0s 651us/step - loss: 54.1138 - mae: 5.4496 - val_loss: 40.8353 - val_mae: 4.3789\n",
      "Epoch 879/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 57.5263 - mae: 5.5093 - val_loss: 38.3131 - val_mae: 4.2115\n",
      "Epoch 880/950\n",
      "122/122 [==============================] - 0s 585us/step - loss: 53.4955 - mae: 5.4018 - val_loss: 33.5369 - val_mae: 4.1329\n",
      "Epoch 881/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 53.1679 - mae: 5.4280 - val_loss: 33.6391 - val_mae: 4.1291\n",
      "Epoch 882/950\n",
      "122/122 [==============================] - 0s 654us/step - loss: 53.7535 - mae: 5.4835 - val_loss: 42.0070 - val_mae: 4.7760\n",
      "Epoch 883/950\n",
      "122/122 [==============================] - 0s 629us/step - loss: 54.9942 - mae: 5.4617 - val_loss: 37.4689 - val_mae: 4.3661\n",
      "Epoch 884/950\n",
      "122/122 [==============================] - 0s 644us/step - loss: 55.4726 - mae: 5.5224 - val_loss: 47.6679 - val_mae: 5.0204\n",
      "Epoch 885/950\n",
      "122/122 [==============================] - 0s 709us/step - loss: 59.2970 - mae: 5.6635 - val_loss: 47.4303 - val_mae: 4.9256\n",
      "Epoch 886/950\n",
      "122/122 [==============================] - 0s 642us/step - loss: 58.2440 - mae: 5.5340 - val_loss: 35.5494 - val_mae: 4.2209\n",
      "Epoch 887/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 55.5623 - mae: 5.4826 - val_loss: 40.5591 - val_mae: 4.5460\n",
      "Epoch 888/950\n",
      "122/122 [==============================] - 0s 658us/step - loss: 55.4781 - mae: 5.5264 - val_loss: 37.0940 - val_mae: 4.4177\n",
      "Epoch 889/950\n",
      "122/122 [==============================] - 0s 614us/step - loss: 55.8484 - mae: 5.5119 - val_loss: 34.1561 - val_mae: 4.1656\n",
      "Epoch 890/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 59.4798 - mae: 5.5975 - val_loss: 31.1099 - val_mae: 3.8814\n",
      "Epoch 891/950\n",
      "122/122 [==============================] - 0s 704us/step - loss: 58.4207 - mae: 5.5839 - val_loss: 35.3176 - val_mae: 4.1520\n",
      "Epoch 892/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 53.8080 - mae: 5.4840 - val_loss: 43.7396 - val_mae: 4.9857\n",
      "Epoch 893/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 54.9398 - mae: 5.4956 - val_loss: 39.3003 - val_mae: 4.6208\n",
      "Epoch 894/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 55.9782 - mae: 5.5165 - val_loss: 43.6886 - val_mae: 4.7747\n",
      "Epoch 895/950\n",
      "122/122 [==============================] - 0s 657us/step - loss: 55.9230 - mae: 5.6664 - val_loss: 38.8454 - val_mae: 4.4012\n",
      "Epoch 896/950\n",
      "122/122 [==============================] - 0s 731us/step - loss: 52.4705 - mae: 5.3488 - val_loss: 32.5054 - val_mae: 3.9957\n",
      "Epoch 897/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 55.9576 - mae: 5.5333 - val_loss: 41.2405 - val_mae: 4.6729\n",
      "Epoch 898/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 56.3557 - mae: 5.5709 - val_loss: 34.7049 - val_mae: 4.0865\n",
      "Epoch 899/950\n",
      "122/122 [==============================] - 0s 608us/step - loss: 51.3053 - mae: 5.3261 - val_loss: 42.8807 - val_mae: 4.8501\n",
      "Epoch 900/950\n",
      "122/122 [==============================] - 0s 676us/step - loss: 56.1921 - mae: 5.4543 - val_loss: 35.1530 - val_mae: 4.1733\n",
      "Epoch 901/950\n",
      "122/122 [==============================] - 0s 600us/step - loss: 54.2333 - mae: 5.4733 - val_loss: 36.0552 - val_mae: 4.1671\n",
      "Epoch 902/950\n",
      "122/122 [==============================] - 0s 736us/step - loss: 57.3675 - mae: 5.6459 - val_loss: 35.7170 - val_mae: 4.1032\n",
      "Epoch 903/950\n",
      "122/122 [==============================] - 0s 645us/step - loss: 56.3942 - mae: 5.4691 - val_loss: 46.6805 - val_mae: 4.9809\n",
      "Epoch 904/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 52.9936 - mae: 5.4062 - val_loss: 41.9906 - val_mae: 4.6059\n",
      "Epoch 905/950\n",
      "122/122 [==============================] - 0s 615us/step - loss: 52.2510 - mae: 5.4068 - val_loss: 54.0101 - val_mae: 5.2838\n",
      "Epoch 906/950\n",
      "122/122 [==============================] - 0s 630us/step - loss: 53.1810 - mae: 5.4145 - val_loss: 42.4896 - val_mae: 4.5560\n",
      "Epoch 907/950\n",
      "122/122 [==============================] - 0s 612us/step - loss: 58.0205 - mae: 5.5888 - val_loss: 38.1855 - val_mae: 4.0520\n",
      "Epoch 908/950\n",
      "122/122 [==============================] - 0s 637us/step - loss: 55.4980 - mae: 5.5045 - val_loss: 42.2160 - val_mae: 4.5741\n",
      "Epoch 909/950\n",
      "122/122 [==============================] - 0s 666us/step - loss: 54.9218 - mae: 5.4910 - val_loss: 45.4776 - val_mae: 4.5044\n",
      "Epoch 910/950\n",
      "122/122 [==============================] - 0s 610us/step - loss: 54.0074 - mae: 5.4205 - val_loss: 38.5665 - val_mae: 4.3585\n",
      "Epoch 911/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 54.0043 - mae: 5.4427 - val_loss: 39.1495 - val_mae: 4.5237\n",
      "Epoch 912/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 54.5412 - mae: 5.4813 - val_loss: 38.8461 - val_mae: 4.4089\n",
      "Epoch 913/950\n",
      "122/122 [==============================] - 0s 665us/step - loss: 57.7259 - mae: 5.5238 - val_loss: 46.4882 - val_mae: 4.8945\n",
      "Epoch 914/950\n",
      "122/122 [==============================] - 0s 604us/step - loss: 53.0697 - mae: 5.3942 - val_loss: 46.2453 - val_mae: 4.7573\n",
      "Epoch 915/950\n",
      "122/122 [==============================] - 0s 706us/step - loss: 54.0904 - mae: 5.4377 - val_loss: 41.3689 - val_mae: 4.5335\n",
      "Epoch 916/950\n",
      "122/122 [==============================] - 0s 628us/step - loss: 54.9626 - mae: 5.4963 - val_loss: 32.8244 - val_mae: 3.8602\n",
      "Epoch 917/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 54.1449 - mae: 5.5432 - val_loss: 34.6607 - val_mae: 3.9395\n",
      "Epoch 918/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 52.2869 - mae: 5.3556 - val_loss: 39.7549 - val_mae: 4.2834\n",
      "Epoch 919/950\n",
      "122/122 [==============================] - 0s 602us/step - loss: 54.6184 - mae: 5.4281 - val_loss: 36.8483 - val_mae: 4.1227\n",
      "Epoch 920/950\n",
      "122/122 [==============================] - 0s 650us/step - loss: 57.6625 - mae: 5.5990 - val_loss: 36.6738 - val_mae: 4.1745\n",
      "Epoch 921/950\n",
      "122/122 [==============================] - 0s 593us/step - loss: 53.5295 - mae: 5.4293 - val_loss: 47.3896 - val_mae: 4.9614\n",
      "Epoch 922/950\n",
      "122/122 [==============================] - 0s 705us/step - loss: 53.3168 - mae: 5.4249 - val_loss: 44.8610 - val_mae: 4.8078\n",
      "Epoch 923/950\n",
      "122/122 [==============================] - 0s 607us/step - loss: 56.0771 - mae: 5.5685 - val_loss: 35.9976 - val_mae: 4.0725\n",
      "Epoch 924/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 56.8741 - mae: 5.5890 - val_loss: 40.2189 - val_mae: 4.5978\n",
      "Epoch 925/950\n",
      "122/122 [==============================] - 0s 633us/step - loss: 54.8299 - mae: 5.4249 - val_loss: 38.5199 - val_mae: 4.3554\n",
      "Epoch 926/950\n",
      "122/122 [==============================] - 0s 663us/step - loss: 56.1726 - mae: 5.4929 - val_loss: 36.2990 - val_mae: 4.2538\n",
      "Epoch 927/950\n",
      "122/122 [==============================] - 0s 624us/step - loss: 56.4061 - mae: 5.4601 - val_loss: 47.8287 - val_mae: 5.0532\n",
      "Epoch 928/950\n",
      "122/122 [==============================] - 0s 652us/step - loss: 57.0785 - mae: 5.5805 - val_loss: 51.7083 - val_mae: 5.1345\n",
      "Epoch 929/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 57.0633 - mae: 5.5351 - val_loss: 33.9321 - val_mae: 3.9408\n",
      "Epoch 930/950\n",
      "122/122 [==============================] - 0s 639us/step - loss: 53.5276 - mae: 5.4649 - val_loss: 48.0172 - val_mae: 4.6294\n",
      "Epoch 931/950\n",
      "122/122 [==============================] - 0s 674us/step - loss: 58.0530 - mae: 5.4444 - val_loss: 44.7200 - val_mae: 4.8039\n",
      "Epoch 932/950\n",
      "122/122 [==============================] - 0s 661us/step - loss: 53.5133 - mae: 5.3942 - val_loss: 40.1892 - val_mae: 4.3245\n",
      "Epoch 933/950\n",
      "122/122 [==============================] - 0s 631us/step - loss: 54.5986 - mae: 5.4600 - val_loss: 41.6186 - val_mae: 4.5324\n",
      "Epoch 934/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 53.4014 - mae: 5.4172 - val_loss: 42.7666 - val_mae: 4.6252\n",
      "Epoch 935/950\n",
      "122/122 [==============================] - 0s 693us/step - loss: 53.4535 - mae: 5.4771 - val_loss: 39.3425 - val_mae: 4.2072\n",
      "Epoch 936/950\n",
      "122/122 [==============================] - 0s 653us/step - loss: 56.5141 - mae: 5.5181 - val_loss: 45.7821 - val_mae: 4.8472\n",
      "Epoch 937/950\n",
      "122/122 [==============================] - 0s 627us/step - loss: 54.2648 - mae: 5.4388 - val_loss: 38.6223 - val_mae: 4.2824\n",
      "Epoch 938/950\n",
      "122/122 [==============================] - 0s 623us/step - loss: 54.9671 - mae: 5.4820 - val_loss: 42.6087 - val_mae: 4.6577\n",
      "Epoch 939/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 51.9677 - mae: 5.3965 - val_loss: 42.3461 - val_mae: 4.5304\n",
      "Epoch 940/950\n",
      "122/122 [==============================] - 0s 635us/step - loss: 55.5929 - mae: 5.5295 - val_loss: 43.1990 - val_mae: 4.7429\n",
      "Epoch 941/950\n",
      "122/122 [==============================] - 0s 618us/step - loss: 56.1424 - mae: 5.4858 - val_loss: 41.1556 - val_mae: 4.3760\n",
      "Epoch 942/950\n",
      "122/122 [==============================] - 0s 640us/step - loss: 55.3397 - mae: 5.5036 - val_loss: 39.4368 - val_mae: 4.4144\n",
      "Epoch 943/950\n",
      "122/122 [==============================] - 0s 693us/step - loss: 54.4217 - mae: 5.4092 - val_loss: 39.4378 - val_mae: 4.2994\n",
      "Epoch 944/950\n",
      "122/122 [==============================] - 0s 648us/step - loss: 53.9466 - mae: 5.4063 - val_loss: 36.2680 - val_mae: 4.1421\n",
      "Epoch 945/950\n",
      "122/122 [==============================] - 0s 620us/step - loss: 55.7061 - mae: 5.4817 - val_loss: 53.1226 - val_mae: 5.2524\n",
      "Epoch 946/950\n",
      "122/122 [==============================] - 0s 710us/step - loss: 56.2941 - mae: 5.4888 - val_loss: 39.0457 - val_mae: 4.2983\n",
      "Epoch 947/950\n",
      "122/122 [==============================] - 0s 611us/step - loss: 57.3930 - mae: 5.6148 - val_loss: 39.5133 - val_mae: 4.2297\n",
      "Epoch 948/950\n",
      "122/122 [==============================] - 0s 649us/step - loss: 55.3877 - mae: 5.5644 - val_loss: 41.2621 - val_mae: 4.3883\n",
      "Epoch 949/950\n",
      "122/122 [==============================] - 0s 646us/step - loss: 57.5312 - mae: 5.6312 - val_loss: 41.8301 - val_mae: 4.5205\n",
      "Epoch 950/950\n",
      "122/122 [==============================] - 0s 791us/step - loss: 54.2124 - mae: 5.4236 - val_loss: 42.5110 - val_mae: 4.6664\n",
      "31/31 [==============================] - 0s 345us/step\n",
      "Epochs: 950 | MAE: 4.666406692270132\n",
      "Training model with 1000 epochs\n",
      "Epoch 1/1000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 68013.0781 - mae: 226.2023 - val_loss: 58950.4766 - val_mae: 210.4867\n",
      "Epoch 2/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 37261.8555 - mae: 162.8957 - val_loss: 12574.3691 - val_mae: 95.6754\n",
      "Epoch 3/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 6378.1709 - mae: 62.0690 - val_loss: 3364.8452 - val_mae: 43.6076\n",
      "Epoch 4/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 3260.8179 - mae: 43.2599 - val_loss: 2133.5378 - val_mae: 34.6563\n",
      "Epoch 5/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 2543.9568 - mae: 38.0029 - val_loss: 1626.5916 - val_mae: 30.3751\n",
      "Epoch 6/1000\n",
      "122/122 [==============================] - 0s 698us/step - loss: 2192.2778 - mae: 35.2195 - val_loss: 1342.0834 - val_mae: 27.4374\n",
      "Epoch 7/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 1887.3274 - mae: 33.3495 - val_loss: 1193.4067 - val_mae: 25.8463\n",
      "Epoch 8/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 1805.1620 - mae: 32.4752 - val_loss: 1055.9768 - val_mae: 24.2178\n",
      "Epoch 9/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 1693.4434 - mae: 31.7484 - val_loss: 948.1029 - val_mae: 23.1513\n",
      "Epoch 10/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 1548.7904 - mae: 30.2231 - val_loss: 845.2493 - val_mae: 21.8035\n",
      "Epoch 11/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 1467.3696 - mae: 29.5230 - val_loss: 786.1324 - val_mae: 20.9819\n",
      "Epoch 12/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 1404.4003 - mae: 28.9055 - val_loss: 702.6025 - val_mae: 20.0305\n",
      "Epoch 13/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 1310.4744 - mae: 28.2221 - val_loss: 643.6955 - val_mae: 19.2220\n",
      "Epoch 14/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 1266.8351 - mae: 27.7069 - val_loss: 576.2447 - val_mae: 18.0327\n",
      "Epoch 15/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 1166.3290 - mae: 26.4964 - val_loss: 536.2357 - val_mae: 17.5838\n",
      "Epoch 16/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 1099.2831 - mae: 25.6552 - val_loss: 489.7604 - val_mae: 16.6618\n",
      "Epoch 17/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 1011.7026 - mae: 25.0686 - val_loss: 434.0908 - val_mae: 15.6821\n",
      "Epoch 18/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 967.6104 - mae: 24.4229 - val_loss: 398.7498 - val_mae: 15.4404\n",
      "Epoch 19/1000\n",
      "122/122 [==============================] - 0s 648us/step - loss: 980.9059 - mae: 24.2936 - val_loss: 352.6655 - val_mae: 14.0735\n",
      "Epoch 20/1000\n",
      "122/122 [==============================] - 0s 679us/step - loss: 949.7532 - mae: 23.9437 - val_loss: 330.3853 - val_mae: 13.6622\n",
      "Epoch 21/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 892.8581 - mae: 23.1155 - val_loss: 300.3197 - val_mae: 12.8149\n",
      "Epoch 22/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 879.8138 - mae: 22.9863 - val_loss: 283.7286 - val_mae: 13.3311\n",
      "Epoch 23/1000\n",
      "122/122 [==============================] - 0s 584us/step - loss: 816.5952 - mae: 22.0664 - val_loss: 248.3689 - val_mae: 11.7126\n",
      "Epoch 24/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 826.7725 - mae: 21.8929 - val_loss: 205.9642 - val_mae: 10.6950\n",
      "Epoch 25/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 815.3419 - mae: 21.8149 - val_loss: 194.2100 - val_mae: 10.3885\n",
      "Epoch 26/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 762.4400 - mae: 20.9957 - val_loss: 180.6424 - val_mae: 10.0043\n",
      "Epoch 27/1000\n",
      "122/122 [==============================] - 0s 693us/step - loss: 727.8883 - mae: 20.5044 - val_loss: 155.6525 - val_mae: 9.6121\n",
      "Epoch 28/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 716.2111 - mae: 20.2716 - val_loss: 137.8029 - val_mae: 8.5912\n",
      "Epoch 29/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 705.5195 - mae: 19.9866 - val_loss: 133.4953 - val_mae: 8.8984\n",
      "Epoch 30/1000\n",
      "122/122 [==============================] - 0s 681us/step - loss: 669.6998 - mae: 19.4721 - val_loss: 125.4567 - val_mae: 8.4345\n",
      "Epoch 31/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 649.3052 - mae: 19.2312 - val_loss: 116.1897 - val_mae: 8.0616\n",
      "Epoch 32/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 641.6899 - mae: 18.9344 - val_loss: 104.7220 - val_mae: 8.2324\n",
      "Epoch 33/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 627.2759 - mae: 18.6558 - val_loss: 90.4277 - val_mae: 7.0991\n",
      "Epoch 34/1000\n",
      "122/122 [==============================] - 0s 690us/step - loss: 640.1593 - mae: 18.6949 - val_loss: 95.8085 - val_mae: 7.6431\n",
      "Epoch 35/1000\n",
      "122/122 [==============================] - 0s 645us/step - loss: 598.7629 - mae: 18.0572 - val_loss: 70.3061 - val_mae: 6.2766\n",
      "Epoch 36/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 594.9699 - mae: 18.0010 - val_loss: 60.8744 - val_mae: 5.9310\n",
      "Epoch 37/1000\n",
      "122/122 [==============================] - 0s 639us/step - loss: 589.1331 - mae: 17.7648 - val_loss: 71.0294 - val_mae: 6.5285\n",
      "Epoch 38/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 580.6304 - mae: 17.6033 - val_loss: 59.9454 - val_mae: 5.7238\n",
      "Epoch 39/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 585.6546 - mae: 17.4704 - val_loss: 66.4329 - val_mae: 5.9630\n",
      "Epoch 40/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 556.8104 - mae: 17.0034 - val_loss: 52.9078 - val_mae: 5.4463\n",
      "Epoch 41/1000\n",
      "122/122 [==============================] - 0s 708us/step - loss: 538.8095 - mae: 16.7764 - val_loss: 45.8609 - val_mae: 4.9061\n",
      "Epoch 42/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 539.3998 - mae: 17.0349 - val_loss: 46.0944 - val_mae: 5.0146\n",
      "Epoch 43/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 528.5524 - mae: 16.7026 - val_loss: 42.6573 - val_mae: 4.8662\n",
      "Epoch 44/1000\n",
      "122/122 [==============================] - 0s 564us/step - loss: 526.1769 - mae: 16.5289 - val_loss: 36.4852 - val_mae: 4.3031\n",
      "Epoch 45/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 547.2363 - mae: 16.6177 - val_loss: 35.5890 - val_mae: 4.3075\n",
      "Epoch 46/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 532.7750 - mae: 16.6433 - val_loss: 38.9947 - val_mae: 4.5238\n",
      "Epoch 47/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 547.6258 - mae: 16.5456 - val_loss: 45.1384 - val_mae: 5.0046\n",
      "Epoch 48/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 529.7202 - mae: 16.5194 - val_loss: 60.8508 - val_mae: 6.1346\n",
      "Epoch 49/1000\n",
      "122/122 [==============================] - 0s 716us/step - loss: 504.6890 - mae: 16.1506 - val_loss: 29.9601 - val_mae: 4.0923\n",
      "Epoch 50/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 508.7022 - mae: 16.1865 - val_loss: 64.6225 - val_mae: 6.5875\n",
      "Epoch 51/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 498.6280 - mae: 15.8416 - val_loss: 29.1784 - val_mae: 3.9456\n",
      "Epoch 52/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 494.3553 - mae: 15.8651 - val_loss: 40.2923 - val_mae: 4.8115\n",
      "Epoch 53/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 510.3387 - mae: 15.9241 - val_loss: 31.5693 - val_mae: 4.0257\n",
      "Epoch 54/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 472.3798 - mae: 15.5917 - val_loss: 29.8469 - val_mae: 4.0467\n",
      "Epoch 55/1000\n",
      "122/122 [==============================] - 0s 732us/step - loss: 473.2961 - mae: 15.7584 - val_loss: 32.4058 - val_mae: 4.0385\n",
      "Epoch 56/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 482.3467 - mae: 15.4547 - val_loss: 46.4049 - val_mae: 5.3285\n",
      "Epoch 57/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 475.7150 - mae: 15.5852 - val_loss: 35.0824 - val_mae: 4.2673\n",
      "Epoch 58/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 480.1147 - mae: 15.4423 - val_loss: 35.8736 - val_mae: 4.2100\n",
      "Epoch 59/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 483.1416 - mae: 15.4182 - val_loss: 31.9330 - val_mae: 4.1245\n",
      "Epoch 60/1000\n",
      "122/122 [==============================] - 0s 659us/step - loss: 489.3161 - mae: 15.4962 - val_loss: 33.0645 - val_mae: 4.1359\n",
      "Epoch 61/1000\n",
      "122/122 [==============================] - 0s 669us/step - loss: 463.6271 - mae: 15.2470 - val_loss: 39.3767 - val_mae: 4.4977\n",
      "Epoch 62/1000\n",
      "122/122 [==============================] - 0s 734us/step - loss: 451.1393 - mae: 15.0799 - val_loss: 44.4594 - val_mae: 5.0760\n",
      "Epoch 63/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 477.3400 - mae: 15.3488 - val_loss: 43.8382 - val_mae: 4.8945\n",
      "Epoch 64/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 462.0834 - mae: 15.1085 - val_loss: 34.9800 - val_mae: 4.4447\n",
      "Epoch 65/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 444.4355 - mae: 14.7821 - val_loss: 29.5502 - val_mae: 3.9081\n",
      "Epoch 66/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 458.8963 - mae: 14.9654 - val_loss: 31.8167 - val_mae: 4.1795\n",
      "Epoch 67/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 444.5710 - mae: 14.9353 - val_loss: 31.5778 - val_mae: 4.1347\n",
      "Epoch 68/1000\n",
      "122/122 [==============================] - 0s 676us/step - loss: 441.1287 - mae: 14.7548 - val_loss: 31.1652 - val_mae: 4.0437\n",
      "Epoch 69/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 476.7297 - mae: 15.2278 - val_loss: 33.3283 - val_mae: 4.3500\n",
      "Epoch 70/1000\n",
      "122/122 [==============================] - 0s 583us/step - loss: 436.1920 - mae: 14.7759 - val_loss: 27.7798 - val_mae: 3.7304\n",
      "Epoch 71/1000\n",
      "122/122 [==============================] - 0s 693us/step - loss: 428.7791 - mae: 14.6142 - val_loss: 36.5360 - val_mae: 4.5260\n",
      "Epoch 72/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 431.4304 - mae: 14.5366 - val_loss: 26.0409 - val_mae: 3.6812\n",
      "Epoch 73/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 422.0436 - mae: 14.4146 - val_loss: 30.2189 - val_mae: 3.9947\n",
      "Epoch 74/1000\n",
      "122/122 [==============================] - 0s 639us/step - loss: 430.4933 - mae: 14.2719 - val_loss: 33.5916 - val_mae: 4.3772\n",
      "Epoch 75/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 441.3287 - mae: 14.6212 - val_loss: 26.9384 - val_mae: 3.7485\n",
      "Epoch 76/1000\n",
      "122/122 [==============================] - 0s 648us/step - loss: 428.8132 - mae: 14.4824 - val_loss: 29.6800 - val_mae: 4.0730\n",
      "Epoch 77/1000\n",
      "122/122 [==============================] - 0s 671us/step - loss: 421.3167 - mae: 14.2664 - val_loss: 35.5430 - val_mae: 4.2599\n",
      "Epoch 78/1000\n",
      "122/122 [==============================] - 0s 675us/step - loss: 421.0799 - mae: 14.2805 - val_loss: 39.3976 - val_mae: 4.5595\n",
      "Epoch 79/1000\n",
      "122/122 [==============================] - 0s 574us/step - loss: 416.4052 - mae: 14.1807 - val_loss: 27.7507 - val_mae: 3.9182\n",
      "Epoch 80/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 415.0946 - mae: 14.0605 - val_loss: 28.6537 - val_mae: 3.8956\n",
      "Epoch 81/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 440.5953 - mae: 14.5217 - val_loss: 26.2029 - val_mae: 3.7542\n",
      "Epoch 82/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 408.3445 - mae: 14.1631 - val_loss: 45.4965 - val_mae: 5.1818\n",
      "Epoch 83/1000\n",
      "122/122 [==============================] - 0s 657us/step - loss: 377.3761 - mae: 13.5990 - val_loss: 28.3666 - val_mae: 3.8917\n",
      "Epoch 84/1000\n",
      "122/122 [==============================] - 0s 687us/step - loss: 420.9579 - mae: 13.9985 - val_loss: 27.0458 - val_mae: 3.8983\n",
      "Epoch 85/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 412.2466 - mae: 13.9392 - val_loss: 24.8545 - val_mae: 3.5895\n",
      "Epoch 86/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 422.9613 - mae: 13.9977 - val_loss: 51.7474 - val_mae: 5.4763\n",
      "Epoch 87/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 393.6540 - mae: 13.7123 - val_loss: 29.9483 - val_mae: 4.0260\n",
      "Epoch 88/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 391.5162 - mae: 13.5430 - val_loss: 31.8421 - val_mae: 4.0422\n",
      "Epoch 89/1000\n",
      "122/122 [==============================] - 0s 670us/step - loss: 390.8848 - mae: 13.3749 - val_loss: 25.7072 - val_mae: 3.6302\n",
      "Epoch 90/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 370.7161 - mae: 13.3381 - val_loss: 25.9680 - val_mae: 3.7327\n",
      "Epoch 91/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 375.8342 - mae: 13.2251 - val_loss: 23.8356 - val_mae: 3.5643\n",
      "Epoch 92/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 398.8284 - mae: 13.6084 - val_loss: 33.3331 - val_mae: 4.3480\n",
      "Epoch 93/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 385.3664 - mae: 13.4546 - val_loss: 24.0034 - val_mae: 3.4422\n",
      "Epoch 94/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 387.8321 - mae: 13.4046 - val_loss: 25.9169 - val_mae: 3.5670\n",
      "Epoch 95/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 372.2865 - mae: 13.2797 - val_loss: 24.6896 - val_mae: 3.5465\n",
      "Epoch 96/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 358.0667 - mae: 12.8338 - val_loss: 26.0472 - val_mae: 3.5712\n",
      "Epoch 97/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 376.2004 - mae: 13.1003 - val_loss: 27.4660 - val_mae: 3.7754\n",
      "Epoch 98/1000\n",
      "122/122 [==============================] - 0s 649us/step - loss: 377.8941 - mae: 13.0915 - val_loss: 27.9807 - val_mae: 3.8686\n",
      "Epoch 99/1000\n",
      "122/122 [==============================] - 0s 663us/step - loss: 370.9857 - mae: 12.9526 - val_loss: 25.7087 - val_mae: 3.5662\n",
      "Epoch 100/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 377.6040 - mae: 13.1550 - val_loss: 24.3783 - val_mae: 3.4867\n",
      "Epoch 101/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 381.3116 - mae: 13.2279 - val_loss: 28.4472 - val_mae: 3.8111\n",
      "Epoch 102/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 360.5573 - mae: 12.9044 - val_loss: 32.4097 - val_mae: 4.2435\n",
      "Epoch 103/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 374.9810 - mae: 12.8797 - val_loss: 32.4588 - val_mae: 4.1028\n",
      "Epoch 104/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 365.4151 - mae: 12.9596 - val_loss: 25.0330 - val_mae: 3.5930\n",
      "Epoch 105/1000\n",
      "122/122 [==============================] - 0s 652us/step - loss: 332.1071 - mae: 12.5011 - val_loss: 66.7444 - val_mae: 6.4106\n",
      "Epoch 106/1000\n",
      "122/122 [==============================] - 0s 678us/step - loss: 340.2510 - mae: 12.5258 - val_loss: 27.3887 - val_mae: 3.8015\n",
      "Epoch 107/1000\n",
      "122/122 [==============================] - 0s 664us/step - loss: 351.3813 - mae: 12.6610 - val_loss: 32.9525 - val_mae: 4.2470\n",
      "Epoch 108/1000\n",
      "122/122 [==============================] - 0s 575us/step - loss: 359.2116 - mae: 12.7287 - val_loss: 22.5183 - val_mae: 3.3682\n",
      "Epoch 109/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 352.0314 - mae: 12.5885 - val_loss: 23.5314 - val_mae: 3.4898\n",
      "Epoch 110/1000\n",
      "122/122 [==============================] - 0s 657us/step - loss: 331.5822 - mae: 12.2904 - val_loss: 28.7290 - val_mae: 3.8457\n",
      "Epoch 111/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 323.4765 - mae: 12.2143 - val_loss: 22.9576 - val_mae: 3.3447\n",
      "Epoch 112/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 319.4281 - mae: 12.1620 - val_loss: 24.0067 - val_mae: 3.5857\n",
      "Epoch 113/1000\n",
      "122/122 [==============================] - 0s 699us/step - loss: 337.7850 - mae: 12.3427 - val_loss: 33.8539 - val_mae: 4.1778\n",
      "Epoch 114/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 337.3282 - mae: 12.4077 - val_loss: 25.7551 - val_mae: 3.6475\n",
      "Epoch 115/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 325.6248 - mae: 12.0549 - val_loss: 33.3512 - val_mae: 4.2823\n",
      "Epoch 116/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 338.9696 - mae: 12.3440 - val_loss: 33.2896 - val_mae: 4.2886\n",
      "Epoch 117/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 348.6682 - mae: 12.4532 - val_loss: 28.8486 - val_mae: 3.6669\n",
      "Epoch 118/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 320.7390 - mae: 11.9063 - val_loss: 27.0106 - val_mae: 3.6646\n",
      "Epoch 119/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 346.6707 - mae: 12.3911 - val_loss: 28.2353 - val_mae: 3.7799\n",
      "Epoch 120/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 340.6229 - mae: 12.2640 - val_loss: 23.5563 - val_mae: 3.3972\n",
      "Epoch 121/1000\n",
      "122/122 [==============================] - 0s 666us/step - loss: 350.9593 - mae: 12.3948 - val_loss: 34.1637 - val_mae: 4.4438\n",
      "Epoch 122/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 326.4091 - mae: 12.0649 - val_loss: 25.0034 - val_mae: 3.4354\n",
      "Epoch 123/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 329.3138 - mae: 11.9705 - val_loss: 25.6286 - val_mae: 3.4962\n",
      "Epoch 124/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 296.3795 - mae: 11.6263 - val_loss: 32.2808 - val_mae: 4.1700\n",
      "Epoch 125/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 327.4208 - mae: 12.0096 - val_loss: 25.1319 - val_mae: 3.5841\n",
      "Epoch 126/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 320.6340 - mae: 11.8357 - val_loss: 56.1406 - val_mae: 5.7582\n",
      "Epoch 127/1000\n",
      "122/122 [==============================] - 0s 580us/step - loss: 300.1209 - mae: 11.6879 - val_loss: 24.8725 - val_mae: 3.5038\n",
      "Epoch 128/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 297.5468 - mae: 11.4878 - val_loss: 33.6670 - val_mae: 4.3694\n",
      "Epoch 129/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 293.3863 - mae: 11.5177 - val_loss: 23.6751 - val_mae: 3.4017\n",
      "Epoch 130/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 306.7976 - mae: 11.6662 - val_loss: 23.9426 - val_mae: 3.4909\n",
      "Epoch 131/1000\n",
      "122/122 [==============================] - 0s 735us/step - loss: 321.0657 - mae: 11.6562 - val_loss: 29.0736 - val_mae: 3.9845\n",
      "Epoch 132/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 291.3221 - mae: 11.3659 - val_loss: 25.3283 - val_mae: 3.5970\n",
      "Epoch 133/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 285.5938 - mae: 11.2189 - val_loss: 41.3262 - val_mae: 4.7202\n",
      "Epoch 134/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 286.5243 - mae: 11.3041 - val_loss: 33.1318 - val_mae: 4.2743\n",
      "Epoch 135/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 280.4536 - mae: 11.2668 - val_loss: 22.9605 - val_mae: 3.3897\n",
      "Epoch 136/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 276.1059 - mae: 11.1805 - val_loss: 40.9833 - val_mae: 4.7973\n",
      "Epoch 137/1000\n",
      "122/122 [==============================] - 0s 668us/step - loss: 283.9950 - mae: 11.3132 - val_loss: 22.5982 - val_mae: 3.3605\n",
      "Epoch 138/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 303.3016 - mae: 11.4850 - val_loss: 51.4422 - val_mae: 5.4439\n",
      "Epoch 139/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 271.5743 - mae: 11.0163 - val_loss: 22.1321 - val_mae: 3.3234\n",
      "Epoch 140/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 273.5087 - mae: 11.0776 - val_loss: 52.6393 - val_mae: 5.4431\n",
      "Epoch 141/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 264.1566 - mae: 10.8817 - val_loss: 50.2105 - val_mae: 5.3071\n",
      "Epoch 142/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 271.3275 - mae: 10.9397 - val_loss: 34.7391 - val_mae: 4.3921\n",
      "Epoch 143/1000\n",
      "122/122 [==============================] - 0s 692us/step - loss: 257.2697 - mae: 10.7346 - val_loss: 25.9964 - val_mae: 3.7981\n",
      "Epoch 144/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 261.4855 - mae: 10.8991 - val_loss: 26.4941 - val_mae: 3.6940\n",
      "Epoch 145/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 272.9402 - mae: 11.0466 - val_loss: 22.0632 - val_mae: 3.3038\n",
      "Epoch 146/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 271.2014 - mae: 10.9366 - val_loss: 41.9829 - val_mae: 5.0102\n",
      "Epoch 147/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 256.5439 - mae: 10.6832 - val_loss: 23.3558 - val_mae: 3.4385\n",
      "Epoch 148/1000\n",
      "122/122 [==============================] - 0s 735us/step - loss: 256.4339 - mae: 10.6462 - val_loss: 47.3678 - val_mae: 5.1879\n",
      "Epoch 149/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 245.4709 - mae: 10.4231 - val_loss: 37.3558 - val_mae: 4.6078\n",
      "Epoch 150/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 252.1519 - mae: 10.5072 - val_loss: 28.0567 - val_mae: 4.0523\n",
      "Epoch 151/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 273.2577 - mae: 10.9831 - val_loss: 29.5922 - val_mae: 4.1214\n",
      "Epoch 152/1000\n",
      "122/122 [==============================] - 0s 670us/step - loss: 241.7022 - mae: 10.3254 - val_loss: 82.9497 - val_mae: 6.6627\n",
      "Epoch 153/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 247.6870 - mae: 10.3933 - val_loss: 72.4236 - val_mae: 6.2390\n",
      "Epoch 154/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 252.1393 - mae: 10.5229 - val_loss: 42.4546 - val_mae: 4.9248\n",
      "Epoch 155/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 234.6756 - mae: 10.2270 - val_loss: 73.0685 - val_mae: 6.3358\n",
      "Epoch 156/1000\n",
      "122/122 [==============================] - 0s 659us/step - loss: 227.3425 - mae: 10.0467 - val_loss: 29.5206 - val_mae: 3.9475\n",
      "Epoch 157/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 254.9142 - mae: 10.4317 - val_loss: 35.8423 - val_mae: 4.5219\n",
      "Epoch 158/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 229.4079 - mae: 10.1456 - val_loss: 48.8200 - val_mae: 5.2843\n",
      "Epoch 159/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 220.8026 - mae: 9.9623 - val_loss: 23.7693 - val_mae: 3.6159\n",
      "Epoch 160/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 222.9538 - mae: 9.8986 - val_loss: 83.9383 - val_mae: 6.8111\n",
      "Epoch 161/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 225.1525 - mae: 9.9441 - val_loss: 77.3586 - val_mae: 6.5009\n",
      "Epoch 162/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 216.4502 - mae: 9.8688 - val_loss: 61.4839 - val_mae: 6.0282\n",
      "Epoch 163/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 206.1334 - mae: 9.6120 - val_loss: 64.2413 - val_mae: 6.1128\n",
      "Epoch 164/1000\n",
      "122/122 [==============================] - 0s 580us/step - loss: 218.8453 - mae: 9.7422 - val_loss: 79.4689 - val_mae: 6.6172\n",
      "Epoch 165/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 210.5267 - mae: 9.7768 - val_loss: 49.8868 - val_mae: 5.3460\n",
      "Epoch 166/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 213.9060 - mae: 9.6681 - val_loss: 83.3380 - val_mae: 6.8969\n",
      "Epoch 167/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 185.4801 - mae: 9.2404 - val_loss: 73.4927 - val_mae: 6.3923\n",
      "Epoch 168/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 198.4341 - mae: 9.4313 - val_loss: 51.2650 - val_mae: 5.5342\n",
      "Epoch 169/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 196.9172 - mae: 9.3656 - val_loss: 94.1848 - val_mae: 7.1349\n",
      "Epoch 170/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 196.7538 - mae: 9.4255 - val_loss: 103.2405 - val_mae: 7.4459\n",
      "Epoch 171/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 191.4815 - mae: 9.1601 - val_loss: 119.4361 - val_mae: 7.9749\n",
      "Epoch 172/1000\n",
      "122/122 [==============================] - 0s 708us/step - loss: 186.9845 - mae: 9.1841 - val_loss: 89.9729 - val_mae: 7.1638\n",
      "Epoch 173/1000\n",
      "122/122 [==============================] - 0s 694us/step - loss: 195.3409 - mae: 9.2518 - val_loss: 39.7106 - val_mae: 4.8107\n",
      "Epoch 174/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 180.9196 - mae: 9.1878 - val_loss: 138.4679 - val_mae: 8.5537\n",
      "Epoch 175/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 178.0349 - mae: 8.9304 - val_loss: 68.2640 - val_mae: 6.2387\n",
      "Epoch 176/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 192.1623 - mae: 9.3404 - val_loss: 103.9444 - val_mae: 7.6016\n",
      "Epoch 177/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 182.0344 - mae: 9.0530 - val_loss: 99.4359 - val_mae: 7.2744\n",
      "Epoch 178/1000\n",
      "122/122 [==============================] - 0s 777us/step - loss: 181.1517 - mae: 9.0038 - val_loss: 110.0922 - val_mae: 7.8000\n",
      "Epoch 179/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 185.0076 - mae: 9.1934 - val_loss: 112.5101 - val_mae: 7.8527\n",
      "Epoch 180/1000\n",
      "122/122 [==============================] - 0s 657us/step - loss: 168.7291 - mae: 8.8647 - val_loss: 136.3083 - val_mae: 8.4935\n",
      "Epoch 181/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 174.5703 - mae: 8.9213 - val_loss: 150.7760 - val_mae: 8.8958\n",
      "Epoch 182/1000\n",
      "122/122 [==============================] - 0s 699us/step - loss: 179.6733 - mae: 8.9170 - val_loss: 178.0076 - val_mae: 9.6339\n",
      "Epoch 183/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 168.2978 - mae: 8.6518 - val_loss: 68.4454 - val_mae: 6.3241\n",
      "Epoch 184/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 164.8055 - mae: 8.5975 - val_loss: 96.6219 - val_mae: 7.2714\n",
      "Epoch 185/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 157.2289 - mae: 8.4424 - val_loss: 115.7386 - val_mae: 7.9600\n",
      "Epoch 186/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 160.0443 - mae: 8.4537 - val_loss: 131.2983 - val_mae: 8.3715\n",
      "Epoch 187/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 157.4986 - mae: 8.4225 - val_loss: 153.4062 - val_mae: 9.0451\n",
      "Epoch 188/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 161.8611 - mae: 8.5637 - val_loss: 129.1469 - val_mae: 8.4167\n",
      "Epoch 189/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 154.4905 - mae: 8.3414 - val_loss: 146.1145 - val_mae: 8.8456\n",
      "Epoch 190/1000\n",
      "122/122 [==============================] - 0s 687us/step - loss: 145.5581 - mae: 8.1641 - val_loss: 154.4139 - val_mae: 9.0985\n",
      "Epoch 191/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 147.5406 - mae: 8.1387 - val_loss: 176.9500 - val_mae: 9.6843\n",
      "Epoch 192/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 161.2980 - mae: 8.3453 - val_loss: 179.2030 - val_mae: 9.7867\n",
      "Epoch 193/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 145.9089 - mae: 8.1149 - val_loss: 149.2818 - val_mae: 8.8917\n",
      "Epoch 194/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 143.2607 - mae: 8.1981 - val_loss: 98.1868 - val_mae: 7.4814\n",
      "Epoch 195/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 136.8971 - mae: 7.9628 - val_loss: 203.8843 - val_mae: 10.3184\n",
      "Epoch 196/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 137.2531 - mae: 7.8166 - val_loss: 196.1914 - val_mae: 10.2137\n",
      "Epoch 197/1000\n",
      "122/122 [==============================] - 0s 731us/step - loss: 148.2228 - mae: 8.0832 - val_loss: 190.5868 - val_mae: 10.0186\n",
      "Epoch 198/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 134.5402 - mae: 7.8402 - val_loss: 173.1548 - val_mae: 9.4804\n",
      "Epoch 199/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 144.9362 - mae: 8.0383 - val_loss: 155.0626 - val_mae: 9.1025\n",
      "Epoch 200/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 144.9822 - mae: 7.8740 - val_loss: 234.2584 - val_mae: 10.8080\n",
      "Epoch 201/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 135.0917 - mae: 7.8578 - val_loss: 108.3735 - val_mae: 7.8164\n",
      "Epoch 202/1000\n",
      "122/122 [==============================] - 0s 639us/step - loss: 133.3462 - mae: 7.7693 - val_loss: 209.6818 - val_mae: 10.4635\n",
      "Epoch 203/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 129.6182 - mae: 7.7909 - val_loss: 125.5959 - val_mae: 8.2222\n",
      "Epoch 204/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 124.9228 - mae: 7.5854 - val_loss: 188.5886 - val_mae: 9.9094\n",
      "Epoch 205/1000\n",
      "122/122 [==============================] - 0s 674us/step - loss: 135.7421 - mae: 7.7258 - val_loss: 195.3459 - val_mae: 10.0559\n",
      "Epoch 206/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 126.3143 - mae: 7.5765 - val_loss: 246.3768 - val_mae: 11.0333\n",
      "Epoch 207/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 127.3574 - mae: 7.6022 - val_loss: 233.1158 - val_mae: 10.9546\n",
      "Epoch 208/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 117.4264 - mae: 7.4649 - val_loss: 207.1277 - val_mae: 10.3823\n",
      "Epoch 209/1000\n",
      "122/122 [==============================] - 0s 720us/step - loss: 131.6568 - mae: 7.6349 - val_loss: 235.4574 - val_mae: 11.0682\n",
      "Epoch 210/1000\n",
      "122/122 [==============================] - 0s 828us/step - loss: 127.2605 - mae: 7.5693 - val_loss: 222.5655 - val_mae: 10.5284\n",
      "Epoch 211/1000\n",
      "122/122 [==============================] - 0s 714us/step - loss: 126.9648 - mae: 7.5164 - val_loss: 241.8226 - val_mae: 10.9638\n",
      "Epoch 212/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 125.2248 - mae: 7.5094 - val_loss: 205.3844 - val_mae: 10.2596\n",
      "Epoch 213/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 128.9540 - mae: 7.5418 - val_loss: 227.8374 - val_mae: 10.7304\n",
      "Epoch 214/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 114.2308 - mae: 7.1820 - val_loss: 172.3754 - val_mae: 9.6758\n",
      "Epoch 215/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 118.2349 - mae: 7.2758 - val_loss: 212.7045 - val_mae: 10.6023\n",
      "Epoch 216/1000\n",
      "122/122 [==============================] - 0s 735us/step - loss: 121.5258 - mae: 7.3974 - val_loss: 218.1010 - val_mae: 10.5144\n",
      "Epoch 217/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 114.3479 - mae: 7.1606 - val_loss: 230.7702 - val_mae: 10.8814\n",
      "Epoch 218/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 105.3545 - mae: 6.9836 - val_loss: 170.4045 - val_mae: 9.6366\n",
      "Epoch 219/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 109.4240 - mae: 7.0758 - val_loss: 260.6442 - val_mae: 11.4874\n",
      "Epoch 220/1000\n",
      "122/122 [==============================] - 0s 648us/step - loss: 120.9093 - mae: 7.3079 - val_loss: 196.2936 - val_mae: 10.3951\n",
      "Epoch 221/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 101.4954 - mae: 6.9647 - val_loss: 236.0180 - val_mae: 10.9953\n",
      "Epoch 222/1000\n",
      "122/122 [==============================] - 0s 648us/step - loss: 109.3276 - mae: 6.9920 - val_loss: 203.3390 - val_mae: 10.3290\n",
      "Epoch 223/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 107.8252 - mae: 7.0229 - val_loss: 183.4270 - val_mae: 9.9501\n",
      "Epoch 224/1000\n",
      "122/122 [==============================] - 0s 696us/step - loss: 101.3483 - mae: 6.8021 - val_loss: 270.1771 - val_mae: 11.6602\n",
      "Epoch 225/1000\n",
      "122/122 [==============================] - 0s 743us/step - loss: 110.5603 - mae: 6.9582 - val_loss: 350.4131 - val_mae: 12.9867\n",
      "Epoch 226/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 104.6008 - mae: 6.9359 - val_loss: 267.7926 - val_mae: 11.6592\n",
      "Epoch 227/1000\n",
      "122/122 [==============================] - 0s 645us/step - loss: 104.9584 - mae: 6.8480 - val_loss: 189.6526 - val_mae: 9.9534\n",
      "Epoch 228/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 108.3435 - mae: 6.9498 - val_loss: 320.9781 - val_mae: 12.3192\n",
      "Epoch 229/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 110.3399 - mae: 6.9786 - val_loss: 310.0345 - val_mae: 12.3622\n",
      "Epoch 230/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 105.5352 - mae: 6.7753 - val_loss: 219.3692 - val_mae: 10.6026\n",
      "Epoch 231/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 109.1003 - mae: 6.8459 - val_loss: 276.1501 - val_mae: 11.6299\n",
      "Epoch 232/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 91.0369 - mae: 6.6247 - val_loss: 262.2106 - val_mae: 11.4747\n",
      "Epoch 233/1000\n",
      "122/122 [==============================] - 0s 694us/step - loss: 102.5945 - mae: 6.7785 - val_loss: 264.9979 - val_mae: 11.5308\n",
      "Epoch 234/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 100.8367 - mae: 6.7716 - val_loss: 207.8832 - val_mae: 10.3417\n",
      "Epoch 235/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 107.0535 - mae: 6.8804 - val_loss: 236.5423 - val_mae: 10.8911\n",
      "Epoch 236/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 103.8960 - mae: 6.7821 - val_loss: 289.8713 - val_mae: 11.8427\n",
      "Epoch 237/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 93.1920 - mae: 6.5272 - val_loss: 211.0489 - val_mae: 10.3477\n",
      "Epoch 238/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 101.6834 - mae: 6.6036 - val_loss: 216.9074 - val_mae: 10.3604\n",
      "Epoch 239/1000\n",
      "122/122 [==============================] - 0s 652us/step - loss: 97.5607 - mae: 6.5956 - val_loss: 256.7542 - val_mae: 11.4248\n",
      "Epoch 240/1000\n",
      "122/122 [==============================] - 0s 680us/step - loss: 99.6477 - mae: 6.6994 - val_loss: 255.0457 - val_mae: 11.2993\n",
      "Epoch 241/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 95.9076 - mae: 6.6358 - val_loss: 299.0092 - val_mae: 11.9567\n",
      "Epoch 242/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 95.6179 - mae: 6.5400 - val_loss: 284.7363 - val_mae: 11.6830\n",
      "Epoch 243/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 94.4956 - mae: 6.4630 - val_loss: 306.5050 - val_mae: 12.2248\n",
      "Epoch 244/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 89.6678 - mae: 6.4249 - val_loss: 356.3743 - val_mae: 13.0131\n",
      "Epoch 245/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 92.7247 - mae: 6.3915 - val_loss: 324.4191 - val_mae: 12.2323\n",
      "Epoch 246/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 92.1711 - mae: 6.5073 - val_loss: 352.2740 - val_mae: 12.7009\n",
      "Epoch 247/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 93.5149 - mae: 6.5000 - val_loss: 242.8912 - val_mae: 11.1142\n",
      "Epoch 248/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 98.6011 - mae: 6.5602 - val_loss: 321.5734 - val_mae: 12.3555\n",
      "Epoch 249/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 89.7226 - mae: 6.3130 - val_loss: 314.7350 - val_mae: 12.2724\n",
      "Epoch 250/1000\n",
      "122/122 [==============================] - 0s 726us/step - loss: 84.1853 - mae: 6.2950 - val_loss: 256.3996 - val_mae: 11.0735\n",
      "Epoch 251/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 94.3870 - mae: 6.4647 - val_loss: 285.8904 - val_mae: 11.8913\n",
      "Epoch 252/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 88.2164 - mae: 6.3255 - val_loss: 290.7239 - val_mae: 11.7605\n",
      "Epoch 253/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 80.8633 - mae: 6.0778 - val_loss: 351.9158 - val_mae: 12.7343\n",
      "Epoch 254/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 83.6912 - mae: 6.2321 - val_loss: 365.1368 - val_mae: 13.0876\n",
      "Epoch 255/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 86.1392 - mae: 6.1227 - val_loss: 362.5165 - val_mae: 12.6518\n",
      "Epoch 256/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 87.3935 - mae: 6.2510 - val_loss: 272.8809 - val_mae: 11.3020\n",
      "Epoch 257/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 91.7216 - mae: 6.2784 - val_loss: 323.6789 - val_mae: 12.2761\n",
      "Epoch 258/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 81.0403 - mae: 6.0377 - val_loss: 321.2048 - val_mae: 12.2582\n",
      "Epoch 259/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 90.4633 - mae: 6.3334 - val_loss: 330.6407 - val_mae: 12.4342\n",
      "Epoch 260/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 87.3861 - mae: 6.3360 - val_loss: 280.3352 - val_mae: 11.4884\n",
      "Epoch 261/1000\n",
      "122/122 [==============================] - 0s 655us/step - loss: 85.9723 - mae: 6.1621 - val_loss: 294.0726 - val_mae: 11.8560\n",
      "Epoch 262/1000\n",
      "122/122 [==============================] - 0s 664us/step - loss: 84.0894 - mae: 6.1166 - val_loss: 321.5829 - val_mae: 12.1656\n",
      "Epoch 263/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 85.2624 - mae: 6.1951 - val_loss: 362.1151 - val_mae: 12.9937\n",
      "Epoch 264/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 81.4676 - mae: 5.9939 - val_loss: 361.5687 - val_mae: 12.8649\n",
      "Epoch 265/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 92.1570 - mae: 6.2725 - val_loss: 370.5060 - val_mae: 12.9796\n",
      "Epoch 266/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 84.2199 - mae: 6.2368 - val_loss: 338.1345 - val_mae: 12.4034\n",
      "Epoch 267/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 88.6050 - mae: 6.1807 - val_loss: 407.0069 - val_mae: 13.3926\n",
      "Epoch 268/1000\n",
      "122/122 [==============================] - 0s 733us/step - loss: 78.5318 - mae: 5.9228 - val_loss: 315.7750 - val_mae: 12.0924\n",
      "Epoch 269/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 81.9385 - mae: 5.9908 - val_loss: 286.9778 - val_mae: 11.6395\n",
      "Epoch 270/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 74.2349 - mae: 5.8655 - val_loss: 387.7757 - val_mae: 13.3041\n",
      "Epoch 271/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 82.2346 - mae: 5.9446 - val_loss: 384.8100 - val_mae: 13.0826\n",
      "Epoch 272/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 89.3468 - mae: 6.1158 - val_loss: 266.7982 - val_mae: 11.1993\n",
      "Epoch 273/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 75.9237 - mae: 5.8556 - val_loss: 333.5844 - val_mae: 12.4500\n",
      "Epoch 274/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 78.5844 - mae: 5.9034 - val_loss: 325.3725 - val_mae: 12.1065\n",
      "Epoch 275/1000\n",
      "122/122 [==============================] - 0s 687us/step - loss: 81.8505 - mae: 6.0077 - val_loss: 355.8382 - val_mae: 12.5604\n",
      "Epoch 276/1000\n",
      "122/122 [==============================] - 0s 639us/step - loss: 82.7074 - mae: 6.0318 - val_loss: 270.9872 - val_mae: 11.3327\n",
      "Epoch 277/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 82.1262 - mae: 5.9675 - val_loss: 333.8173 - val_mae: 12.5446\n",
      "Epoch 278/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 72.2067 - mae: 5.8399 - val_loss: 420.6523 - val_mae: 13.6511\n",
      "Epoch 279/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 75.5976 - mae: 5.7401 - val_loss: 283.3698 - val_mae: 11.5087\n",
      "Epoch 280/1000\n",
      "122/122 [==============================] - 0s 686us/step - loss: 77.1155 - mae: 5.8568 - val_loss: 408.1101 - val_mae: 13.3942\n",
      "Epoch 281/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 81.5830 - mae: 5.8612 - val_loss: 294.8628 - val_mae: 11.6367\n",
      "Epoch 282/1000\n",
      "122/122 [==============================] - 0s 717us/step - loss: 71.3633 - mae: 5.7582 - val_loss: 340.1357 - val_mae: 12.2106\n",
      "Epoch 283/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 81.0985 - mae: 5.8690 - val_loss: 379.8076 - val_mae: 13.2080\n",
      "Epoch 284/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 83.6589 - mae: 5.9226 - val_loss: 310.0290 - val_mae: 11.6681\n",
      "Epoch 285/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 71.4607 - mae: 5.7435 - val_loss: 271.2674 - val_mae: 11.1555\n",
      "Epoch 286/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 77.5727 - mae: 5.7212 - val_loss: 305.6779 - val_mae: 11.6781\n",
      "Epoch 287/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 75.5946 - mae: 5.6918 - val_loss: 375.2614 - val_mae: 12.8798\n",
      "Epoch 288/1000\n",
      "122/122 [==============================] - 0s 696us/step - loss: 72.3888 - mae: 5.7179 - val_loss: 352.0321 - val_mae: 12.4883\n",
      "Epoch 289/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 75.2973 - mae: 5.7534 - val_loss: 398.1984 - val_mae: 13.0786\n",
      "Epoch 290/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 73.7062 - mae: 5.7308 - val_loss: 351.6242 - val_mae: 12.4184\n",
      "Epoch 291/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 73.1158 - mae: 5.6940 - val_loss: 299.3345 - val_mae: 11.4383\n",
      "Epoch 292/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 71.7901 - mae: 5.6372 - val_loss: 381.6116 - val_mae: 12.8082\n",
      "Epoch 293/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 82.3907 - mae: 5.9218 - val_loss: 351.5550 - val_mae: 12.2675\n",
      "Epoch 294/1000\n",
      "122/122 [==============================] - 0s 712us/step - loss: 79.7586 - mae: 5.7544 - val_loss: 359.4702 - val_mae: 12.3831\n",
      "Epoch 295/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 72.4379 - mae: 5.6540 - val_loss: 367.9639 - val_mae: 12.6585\n",
      "Epoch 296/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 74.2999 - mae: 5.6480 - val_loss: 339.9697 - val_mae: 12.2374\n",
      "Epoch 297/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 86.5532 - mae: 5.8867 - val_loss: 379.8008 - val_mae: 12.6972\n",
      "Epoch 298/1000\n",
      "122/122 [==============================] - 0s 572us/step - loss: 77.4614 - mae: 5.7461 - val_loss: 385.9812 - val_mae: 12.7627\n",
      "Epoch 299/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 77.8974 - mae: 5.6704 - val_loss: 318.3566 - val_mae: 11.7970\n",
      "Epoch 300/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 73.6745 - mae: 5.7832 - val_loss: 294.3710 - val_mae: 11.3919\n",
      "Epoch 301/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 73.4244 - mae: 5.6202 - val_loss: 301.8087 - val_mae: 11.4544\n",
      "Epoch 302/1000\n",
      "122/122 [==============================] - 0s 665us/step - loss: 73.9676 - mae: 5.6949 - val_loss: 268.1917 - val_mae: 10.9993\n",
      "Epoch 303/1000\n",
      "122/122 [==============================] - 0s 678us/step - loss: 73.0311 - mae: 5.6627 - val_loss: 327.5981 - val_mae: 11.9327\n",
      "Epoch 304/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 69.8455 - mae: 5.6158 - val_loss: 300.2399 - val_mae: 11.4670\n",
      "Epoch 305/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 73.5176 - mae: 5.7375 - val_loss: 266.8669 - val_mae: 10.7931\n",
      "Epoch 306/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 72.7883 - mae: 5.7217 - val_loss: 361.1337 - val_mae: 12.5691\n",
      "Epoch 307/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 70.9376 - mae: 5.5377 - val_loss: 348.5125 - val_mae: 12.4418\n",
      "Epoch 308/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 74.5763 - mae: 5.6895 - val_loss: 380.0918 - val_mae: 12.7842\n",
      "Epoch 309/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 72.3672 - mae: 5.6720 - val_loss: 375.6606 - val_mae: 12.7599\n",
      "Epoch 310/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 78.4828 - mae: 5.6922 - val_loss: 385.7830 - val_mae: 12.7993\n",
      "Epoch 311/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 69.6523 - mae: 5.6032 - val_loss: 294.7793 - val_mae: 11.5787\n",
      "Epoch 312/1000\n",
      "122/122 [==============================] - 0s 719us/step - loss: 72.9303 - mae: 5.5968 - val_loss: 376.1726 - val_mae: 12.8009\n",
      "Epoch 313/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 71.4287 - mae: 5.5653 - val_loss: 431.6949 - val_mae: 13.1712\n",
      "Epoch 314/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 74.8311 - mae: 5.5824 - val_loss: 294.6383 - val_mae: 11.3257\n",
      "Epoch 315/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 76.4830 - mae: 5.6353 - val_loss: 318.3173 - val_mae: 11.7313\n",
      "Epoch 316/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 62.1449 - mae: 5.3656 - val_loss: 296.7116 - val_mae: 11.5452\n",
      "Epoch 317/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 76.2457 - mae: 5.6873 - val_loss: 347.5542 - val_mae: 12.1424\n",
      "Epoch 318/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 60.2508 - mae: 5.2485 - val_loss: 364.1095 - val_mae: 12.5185\n",
      "Epoch 319/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 67.3174 - mae: 5.4460 - val_loss: 374.8626 - val_mae: 12.3816\n",
      "Epoch 320/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 66.2101 - mae: 5.5005 - val_loss: 337.0246 - val_mae: 11.8849\n",
      "Epoch 321/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 73.4584 - mae: 5.5785 - val_loss: 270.1518 - val_mae: 10.9944\n",
      "Epoch 322/1000\n",
      "122/122 [==============================] - 0s 664us/step - loss: 69.2930 - mae: 5.5058 - val_loss: 295.1020 - val_mae: 11.3585\n",
      "Epoch 323/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 69.5322 - mae: 5.5205 - val_loss: 345.5391 - val_mae: 11.9670\n",
      "Epoch 324/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 69.5301 - mae: 5.5235 - val_loss: 264.4921 - val_mae: 10.8079\n",
      "Epoch 325/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 60.2883 - mae: 5.2672 - val_loss: 275.5123 - val_mae: 11.2854\n",
      "Epoch 326/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 68.2186 - mae: 5.4879 - val_loss: 392.8912 - val_mae: 12.8249\n",
      "Epoch 327/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 65.2060 - mae: 5.4440 - val_loss: 357.4548 - val_mae: 12.1174\n",
      "Epoch 328/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 72.5140 - mae: 5.5536 - val_loss: 305.6979 - val_mae: 11.5386\n",
      "Epoch 329/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 68.1708 - mae: 5.4255 - val_loss: 286.7119 - val_mae: 11.1569\n",
      "Epoch 330/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 68.8311 - mae: 5.5036 - val_loss: 223.4819 - val_mae: 9.9731\n",
      "Epoch 331/1000\n",
      "122/122 [==============================] - 0s 722us/step - loss: 67.7054 - mae: 5.5113 - val_loss: 304.0831 - val_mae: 11.2444\n",
      "Epoch 332/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 62.5468 - mae: 5.2930 - val_loss: 315.6973 - val_mae: 11.6775\n",
      "Epoch 333/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 69.5510 - mae: 5.5422 - val_loss: 313.5798 - val_mae: 11.6114\n",
      "Epoch 334/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 67.1983 - mae: 5.4237 - val_loss: 360.0684 - val_mae: 12.1881\n",
      "Epoch 335/1000\n",
      "122/122 [==============================] - 0s 649us/step - loss: 67.8525 - mae: 5.4456 - val_loss: 305.7691 - val_mae: 11.4031\n",
      "Epoch 336/1000\n",
      "122/122 [==============================] - 0s 691us/step - loss: 69.0193 - mae: 5.3702 - val_loss: 243.0854 - val_mae: 10.3936\n",
      "Epoch 337/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 65.5686 - mae: 5.3821 - val_loss: 250.3073 - val_mae: 10.4984\n",
      "Epoch 338/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 65.8271 - mae: 5.3658 - val_loss: 365.8132 - val_mae: 12.2552\n",
      "Epoch 339/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 67.2137 - mae: 5.4251 - val_loss: 336.5662 - val_mae: 11.8074\n",
      "Epoch 340/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 64.8020 - mae: 5.3152 - val_loss: 277.0241 - val_mae: 11.0593\n",
      "Epoch 341/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 69.5362 - mae: 5.4245 - val_loss: 373.9029 - val_mae: 12.3777\n",
      "Epoch 342/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 62.7603 - mae: 5.3571 - val_loss: 303.4554 - val_mae: 11.3523\n",
      "Epoch 343/1000\n",
      "122/122 [==============================] - 0s 688us/step - loss: 62.4499 - mae: 5.3530 - val_loss: 339.5275 - val_mae: 11.7035\n",
      "Epoch 344/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 65.9438 - mae: 5.3749 - val_loss: 260.1907 - val_mae: 10.6811\n",
      "Epoch 345/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 65.2020 - mae: 5.4401 - val_loss: 271.6301 - val_mae: 10.8532\n",
      "Epoch 346/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 60.5040 - mae: 5.2672 - val_loss: 362.4457 - val_mae: 12.1358\n",
      "Epoch 347/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 67.5133 - mae: 5.3509 - val_loss: 261.4450 - val_mae: 10.7245\n",
      "Epoch 348/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 67.0301 - mae: 5.3935 - val_loss: 327.0912 - val_mae: 11.5740\n",
      "Epoch 349/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 72.7949 - mae: 5.5201 - val_loss: 358.0953 - val_mae: 12.0078\n",
      "Epoch 350/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 69.0369 - mae: 5.4813 - val_loss: 311.8285 - val_mae: 11.4046\n",
      "Epoch 351/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 63.4103 - mae: 5.2264 - val_loss: 294.3351 - val_mae: 11.1206\n",
      "Epoch 352/1000\n",
      "122/122 [==============================] - 0s 706us/step - loss: 64.5050 - mae: 5.3157 - val_loss: 346.4554 - val_mae: 11.8709\n",
      "Epoch 353/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 62.7289 - mae: 5.2750 - val_loss: 314.5733 - val_mae: 11.3758\n",
      "Epoch 354/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 62.5946 - mae: 5.2858 - val_loss: 224.6455 - val_mae: 9.8345\n",
      "Epoch 355/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 67.1246 - mae: 5.4432 - val_loss: 293.5100 - val_mae: 11.0486\n",
      "Epoch 356/1000\n",
      "122/122 [==============================] - 0s 699us/step - loss: 76.0811 - mae: 5.6117 - val_loss: 328.1563 - val_mae: 11.6038\n",
      "Epoch 357/1000\n",
      "122/122 [==============================] - 0s 782us/step - loss: 65.4026 - mae: 5.3402 - val_loss: 278.4852 - val_mae: 10.7893\n",
      "Epoch 358/1000\n",
      "122/122 [==============================] - 0s 664us/step - loss: 67.4023 - mae: 5.3147 - val_loss: 290.4887 - val_mae: 11.0088\n",
      "Epoch 359/1000\n",
      "122/122 [==============================] - 0s 722us/step - loss: 60.8377 - mae: 5.3173 - val_loss: 254.1083 - val_mae: 10.3401\n",
      "Epoch 360/1000\n",
      "122/122 [==============================] - 0s 676us/step - loss: 69.3999 - mae: 5.4805 - val_loss: 282.2536 - val_mae: 10.8668\n",
      "Epoch 361/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 61.3394 - mae: 5.2974 - val_loss: 337.5734 - val_mae: 11.6289\n",
      "Epoch 362/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 63.1518 - mae: 5.3039 - val_loss: 270.5130 - val_mae: 10.6411\n",
      "Epoch 363/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 64.1715 - mae: 5.3355 - val_loss: 344.8710 - val_mae: 11.8123\n",
      "Epoch 364/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 60.7342 - mae: 5.1897 - val_loss: 247.8589 - val_mae: 10.2384\n",
      "Epoch 365/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 59.8782 - mae: 5.2782 - val_loss: 333.7276 - val_mae: 11.7643\n",
      "Epoch 366/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 62.1365 - mae: 5.1786 - val_loss: 262.5678 - val_mae: 10.4384\n",
      "Epoch 367/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 65.2791 - mae: 5.2782 - val_loss: 216.1420 - val_mae: 9.6764\n",
      "Epoch 368/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 63.6205 - mae: 5.1865 - val_loss: 385.2708 - val_mae: 12.2864\n",
      "Epoch 369/1000\n",
      "122/122 [==============================] - 0s 670us/step - loss: 65.4975 - mae: 5.3823 - val_loss: 304.1307 - val_mae: 11.3288\n",
      "Epoch 370/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 61.6464 - mae: 5.2197 - val_loss: 345.3807 - val_mae: 11.8859\n",
      "Epoch 371/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 66.7545 - mae: 5.4111 - val_loss: 299.3447 - val_mae: 11.0249\n",
      "Epoch 372/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 69.0893 - mae: 5.3740 - val_loss: 273.4128 - val_mae: 10.6919\n",
      "Epoch 373/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 62.5911 - mae: 5.2002 - val_loss: 280.3958 - val_mae: 10.8018\n",
      "Epoch 374/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 65.4437 - mae: 5.4490 - val_loss: 401.0150 - val_mae: 12.7594\n",
      "Epoch 375/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 60.1659 - mae: 5.1485 - val_loss: 371.2991 - val_mae: 12.0331\n",
      "Epoch 376/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 57.5564 - mae: 5.1069 - val_loss: 278.7472 - val_mae: 10.7921\n",
      "Epoch 377/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 65.5076 - mae: 5.2897 - val_loss: 232.3397 - val_mae: 10.0294\n",
      "Epoch 378/1000\n",
      "122/122 [==============================] - 0s 764us/step - loss: 70.2418 - mae: 5.4234 - val_loss: 363.9210 - val_mae: 11.9759\n",
      "Epoch 379/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 66.5019 - mae: 5.4305 - val_loss: 290.0986 - val_mae: 10.9371\n",
      "Epoch 380/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 65.2584 - mae: 5.3451 - val_loss: 357.0965 - val_mae: 12.3220\n",
      "Epoch 381/1000\n",
      "122/122 [==============================] - 0s 573us/step - loss: 64.5138 - mae: 5.3165 - val_loss: 288.9470 - val_mae: 11.0696\n",
      "Epoch 382/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 59.3604 - mae: 5.1730 - val_loss: 238.3212 - val_mae: 9.9952\n",
      "Epoch 383/1000\n",
      "122/122 [==============================] - 0s 673us/step - loss: 60.9559 - mae: 5.1902 - val_loss: 372.6740 - val_mae: 12.0908\n",
      "Epoch 384/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 64.9426 - mae: 5.2409 - val_loss: 366.5479 - val_mae: 11.9330\n",
      "Epoch 385/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 67.3163 - mae: 5.3183 - val_loss: 351.6345 - val_mae: 11.7678\n",
      "Epoch 386/1000\n",
      "122/122 [==============================] - 0s 655us/step - loss: 58.6972 - mae: 5.1375 - val_loss: 275.2907 - val_mae: 10.6087\n",
      "Epoch 387/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 63.8486 - mae: 5.2655 - val_loss: 327.1487 - val_mae: 11.5923\n",
      "Epoch 388/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 68.8318 - mae: 5.2812 - val_loss: 352.7193 - val_mae: 12.0020\n",
      "Epoch 389/1000\n",
      "122/122 [==============================] - 0s 666us/step - loss: 67.9656 - mae: 5.3548 - val_loss: 265.1212 - val_mae: 10.5480\n",
      "Epoch 390/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 59.1968 - mae: 5.1371 - val_loss: 301.1070 - val_mae: 11.1289\n",
      "Epoch 391/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 61.2565 - mae: 5.1440 - val_loss: 270.7433 - val_mae: 10.6236\n",
      "Epoch 392/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 65.6440 - mae: 5.2152 - val_loss: 356.6089 - val_mae: 11.8775\n",
      "Epoch 393/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 62.3521 - mae: 5.2510 - val_loss: 334.0699 - val_mae: 11.5404\n",
      "Epoch 394/1000\n",
      "122/122 [==============================] - 0s 658us/step - loss: 68.1067 - mae: 5.4290 - val_loss: 333.1240 - val_mae: 11.5887\n",
      "Epoch 395/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 59.9350 - mae: 5.1894 - val_loss: 296.8062 - val_mae: 10.9150\n",
      "Epoch 396/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 58.2423 - mae: 5.0506 - val_loss: 299.6539 - val_mae: 10.9999\n",
      "Epoch 397/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 60.0718 - mae: 5.1429 - val_loss: 362.9301 - val_mae: 11.9460\n",
      "Epoch 398/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 67.6745 - mae: 5.3150 - val_loss: 412.5912 - val_mae: 12.5177\n",
      "Epoch 399/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 60.1945 - mae: 5.1584 - val_loss: 284.4406 - val_mae: 10.7229\n",
      "Epoch 400/1000\n",
      "122/122 [==============================] - 0s 669us/step - loss: 60.1280 - mae: 5.2199 - val_loss: 318.6234 - val_mae: 11.3849\n",
      "Epoch 401/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 63.1059 - mae: 5.2016 - val_loss: 275.2199 - val_mae: 10.5327\n",
      "Epoch 402/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 63.9402 - mae: 5.0679 - val_loss: 352.7553 - val_mae: 11.7284\n",
      "Epoch 403/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 67.7645 - mae: 5.4383 - val_loss: 346.3772 - val_mae: 11.7306\n",
      "Epoch 404/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 60.1230 - mae: 5.1130 - val_loss: 325.1276 - val_mae: 11.4235\n",
      "Epoch 405/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 61.3047 - mae: 5.1454 - val_loss: 361.0786 - val_mae: 11.9951\n",
      "Epoch 406/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 65.9203 - mae: 5.3165 - val_loss: 365.3760 - val_mae: 11.8956\n",
      "Epoch 407/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 60.9113 - mae: 5.0891 - val_loss: 387.8924 - val_mae: 12.3404\n",
      "Epoch 408/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 63.8092 - mae: 5.3020 - val_loss: 276.2056 - val_mae: 10.5610\n",
      "Epoch 409/1000\n",
      "122/122 [==============================] - 0s 783us/step - loss: 63.0018 - mae: 5.2470 - val_loss: 312.9389 - val_mae: 11.1813\n",
      "Epoch 410/1000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 57.6565 - mae: 5.0480 - val_loss: 323.1122 - val_mae: 11.3864\n",
      "Epoch 411/1000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 57.7052 - mae: 5.0177 - val_loss: 436.7707 - val_mae: 12.8573\n",
      "Epoch 412/1000\n",
      "122/122 [==============================] - 0s 942us/step - loss: 61.2369 - mae: 5.0677 - val_loss: 265.6868 - val_mae: 10.4001\n",
      "Epoch 413/1000\n",
      "122/122 [==============================] - 0s 835us/step - loss: 58.7442 - mae: 5.0633 - val_loss: 313.2468 - val_mae: 11.1747\n",
      "Epoch 414/1000\n",
      "122/122 [==============================] - 0s 677us/step - loss: 64.3806 - mae: 5.2525 - val_loss: 289.9425 - val_mae: 10.8477\n",
      "Epoch 415/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 65.7185 - mae: 5.3138 - val_loss: 234.1330 - val_mae: 9.9225\n",
      "Epoch 416/1000\n",
      "122/122 [==============================] - 0s 739us/step - loss: 56.0823 - mae: 5.0684 - val_loss: 311.8716 - val_mae: 11.3740\n",
      "Epoch 417/1000\n",
      "122/122 [==============================] - 0s 737us/step - loss: 61.4832 - mae: 5.2168 - val_loss: 311.7735 - val_mae: 11.2560\n",
      "Epoch 418/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 59.5364 - mae: 5.1312 - val_loss: 347.6803 - val_mae: 11.6966\n",
      "Epoch 419/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 64.1986 - mae: 5.2276 - val_loss: 447.1655 - val_mae: 12.9852\n",
      "Epoch 420/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 59.6463 - mae: 5.1368 - val_loss: 298.5033 - val_mae: 11.0086\n",
      "Epoch 421/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 61.6701 - mae: 5.1082 - val_loss: 315.5414 - val_mae: 11.1611\n",
      "Epoch 422/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 64.6042 - mae: 5.1969 - val_loss: 360.4148 - val_mae: 11.8554\n",
      "Epoch 423/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 58.5911 - mae: 5.0736 - val_loss: 366.8779 - val_mae: 12.1141\n",
      "Epoch 424/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 59.9075 - mae: 5.1232 - val_loss: 396.7116 - val_mae: 12.5059\n",
      "Epoch 425/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 58.8787 - mae: 5.0871 - val_loss: 310.6119 - val_mae: 11.1989\n",
      "Epoch 426/1000\n",
      "122/122 [==============================] - 0s 934us/step - loss: 58.7370 - mae: 5.1525 - val_loss: 374.0221 - val_mae: 12.1620\n",
      "Epoch 427/1000\n",
      "122/122 [==============================] - 0s 793us/step - loss: 57.4805 - mae: 5.0533 - val_loss: 433.1156 - val_mae: 12.8893\n",
      "Epoch 428/1000\n",
      "122/122 [==============================] - 0s 745us/step - loss: 53.2144 - mae: 4.9513 - val_loss: 377.8907 - val_mae: 12.1651\n",
      "Epoch 429/1000\n",
      "122/122 [==============================] - 0s 677us/step - loss: 56.5583 - mae: 4.9605 - val_loss: 375.1758 - val_mae: 12.0140\n",
      "Epoch 430/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 63.6065 - mae: 5.2364 - val_loss: 380.8329 - val_mae: 12.2020\n",
      "Epoch 431/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 69.6336 - mae: 5.4037 - val_loss: 382.2948 - val_mae: 12.3613\n",
      "Epoch 432/1000\n",
      "122/122 [==============================] - 0s 679us/step - loss: 55.1651 - mae: 5.0728 - val_loss: 436.2728 - val_mae: 12.9176\n",
      "Epoch 433/1000\n",
      "122/122 [==============================] - 0s 594us/step - loss: 57.2152 - mae: 5.0775 - val_loss: 342.0386 - val_mae: 11.7081\n",
      "Epoch 434/1000\n",
      "122/122 [==============================] - 0s 726us/step - loss: 62.4789 - mae: 5.1700 - val_loss: 260.4296 - val_mae: 10.3723\n",
      "Epoch 435/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 64.7194 - mae: 5.3113 - val_loss: 310.0694 - val_mae: 11.1607\n",
      "Epoch 436/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 64.3717 - mae: 5.1577 - val_loss: 384.7494 - val_mae: 12.2464\n",
      "Epoch 437/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 54.5765 - mae: 4.9903 - val_loss: 436.3340 - val_mae: 12.9678\n",
      "Epoch 438/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 57.4997 - mae: 5.1038 - val_loss: 366.8989 - val_mae: 11.8625\n",
      "Epoch 439/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 66.5660 - mae: 5.3731 - val_loss: 283.9061 - val_mae: 10.8724\n",
      "Epoch 440/1000\n",
      "122/122 [==============================] - 0s 669us/step - loss: 55.0882 - mae: 4.9209 - val_loss: 444.1836 - val_mae: 12.8987\n",
      "Epoch 441/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 57.7715 - mae: 5.0881 - val_loss: 338.9975 - val_mae: 11.5527\n",
      "Epoch 442/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 60.0595 - mae: 5.1089 - val_loss: 460.2376 - val_mae: 13.2289\n",
      "Epoch 443/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 60.9885 - mae: 5.2010 - val_loss: 368.6960 - val_mae: 11.9362\n",
      "Epoch 444/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 65.5690 - mae: 5.2720 - val_loss: 445.6772 - val_mae: 13.0003\n",
      "Epoch 445/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 58.7234 - mae: 5.1761 - val_loss: 387.2964 - val_mae: 12.2284\n",
      "Epoch 446/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 61.5606 - mae: 5.1622 - val_loss: 330.5499 - val_mae: 11.5477\n",
      "Epoch 447/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 56.0572 - mae: 5.0000 - val_loss: 298.6165 - val_mae: 10.9678\n",
      "Epoch 448/1000\n",
      "122/122 [==============================] - 0s 645us/step - loss: 59.2824 - mae: 5.1575 - val_loss: 456.5874 - val_mae: 13.0459\n",
      "Epoch 449/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 60.2300 - mae: 5.1835 - val_loss: 297.5567 - val_mae: 11.0028\n",
      "Epoch 450/1000\n",
      "122/122 [==============================] - 0s 702us/step - loss: 56.1355 - mae: 5.0380 - val_loss: 351.8307 - val_mae: 11.7964\n",
      "Epoch 451/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 62.7748 - mae: 5.1869 - val_loss: 407.4554 - val_mae: 12.3110\n",
      "Epoch 452/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 56.5620 - mae: 4.9764 - val_loss: 495.2376 - val_mae: 13.4504\n",
      "Epoch 453/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 57.7827 - mae: 5.0312 - val_loss: 323.2885 - val_mae: 11.2050\n",
      "Epoch 454/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 61.7806 - mae: 5.2126 - val_loss: 452.9403 - val_mae: 12.9698\n",
      "Epoch 455/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 54.7656 - mae: 5.0071 - val_loss: 364.2383 - val_mae: 11.8141\n",
      "Epoch 456/1000\n",
      "122/122 [==============================] - 0s 583us/step - loss: 56.7530 - mae: 5.0222 - val_loss: 501.4287 - val_mae: 13.6162\n",
      "Epoch 457/1000\n",
      "122/122 [==============================] - 0s 594us/step - loss: 62.9488 - mae: 5.1632 - val_loss: 330.3526 - val_mae: 11.3497\n",
      "Epoch 458/1000\n",
      "122/122 [==============================] - 0s 748us/step - loss: 57.4593 - mae: 5.1072 - val_loss: 519.1555 - val_mae: 13.7548\n",
      "Epoch 459/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 53.8445 - mae: 5.0427 - val_loss: 414.3680 - val_mae: 12.5451\n",
      "Epoch 460/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 64.3246 - mae: 5.2586 - val_loss: 466.3243 - val_mae: 12.9784\n",
      "Epoch 461/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 63.1918 - mae: 5.2074 - val_loss: 345.7063 - val_mae: 11.6705\n",
      "Epoch 462/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 55.0225 - mae: 5.1267 - val_loss: 460.6433 - val_mae: 12.9572\n",
      "Epoch 463/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 55.7815 - mae: 4.9448 - val_loss: 392.4202 - val_mae: 12.1077\n",
      "Epoch 464/1000\n",
      "122/122 [==============================] - 0s 692us/step - loss: 59.3109 - mae: 5.0670 - val_loss: 462.2279 - val_mae: 13.1703\n",
      "Epoch 465/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 53.1095 - mae: 5.0532 - val_loss: 489.6779 - val_mae: 13.4270\n",
      "Epoch 466/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 56.0422 - mae: 5.1021 - val_loss: 443.6063 - val_mae: 12.8250\n",
      "Epoch 467/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 66.8215 - mae: 5.3939 - val_loss: 342.4365 - val_mae: 11.5621\n",
      "Epoch 468/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 57.8440 - mae: 5.0805 - val_loss: 433.1931 - val_mae: 12.5686\n",
      "Epoch 469/1000\n",
      "122/122 [==============================] - 0s 652us/step - loss: 56.4368 - mae: 5.0224 - val_loss: 475.0214 - val_mae: 13.2289\n",
      "Epoch 470/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 53.8571 - mae: 4.9537 - val_loss: 323.3385 - val_mae: 11.1438\n",
      "Epoch 471/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 58.7123 - mae: 5.1306 - val_loss: 427.1985 - val_mae: 12.5904\n",
      "Epoch 472/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 65.5919 - mae: 5.1702 - val_loss: 342.3322 - val_mae: 11.4845\n",
      "Epoch 473/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 61.8672 - mae: 5.2131 - val_loss: 415.7379 - val_mae: 12.5212\n",
      "Epoch 474/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 60.4685 - mae: 5.1721 - val_loss: 344.8200 - val_mae: 11.4565\n",
      "Epoch 475/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 59.3955 - mae: 5.0537 - val_loss: 471.3226 - val_mae: 13.1832\n",
      "Epoch 476/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 55.3101 - mae: 4.9668 - val_loss: 338.7059 - val_mae: 11.5099\n",
      "Epoch 477/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 57.1967 - mae: 4.9805 - val_loss: 426.1429 - val_mae: 12.6126\n",
      "Epoch 478/1000\n",
      "122/122 [==============================] - 0s 679us/step - loss: 55.1418 - mae: 4.9495 - val_loss: 482.3125 - val_mae: 13.2963\n",
      "Epoch 479/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 58.9050 - mae: 5.0623 - val_loss: 354.4145 - val_mae: 11.6470\n",
      "Epoch 480/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 53.9191 - mae: 4.9073 - val_loss: 320.8969 - val_mae: 11.0209\n",
      "Epoch 481/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 60.3940 - mae: 5.0987 - val_loss: 468.3088 - val_mae: 13.1907\n",
      "Epoch 482/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 56.0405 - mae: 5.0668 - val_loss: 418.7245 - val_mae: 12.3868\n",
      "Epoch 483/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 57.6743 - mae: 5.1030 - val_loss: 369.5696 - val_mae: 11.7841\n",
      "Epoch 484/1000\n",
      "122/122 [==============================] - 0s 655us/step - loss: 54.0901 - mae: 5.0694 - val_loss: 526.2800 - val_mae: 13.7349\n",
      "Epoch 485/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 60.6593 - mae: 5.1421 - val_loss: 488.3648 - val_mae: 13.2487\n",
      "Epoch 486/1000\n",
      "122/122 [==============================] - 0s 688us/step - loss: 52.7208 - mae: 4.9895 - val_loss: 431.2072 - val_mae: 12.5774\n",
      "Epoch 487/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 50.9727 - mae: 4.8981 - val_loss: 432.4355 - val_mae: 12.6880\n",
      "Epoch 488/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 54.6129 - mae: 4.9492 - val_loss: 498.6527 - val_mae: 13.4506\n",
      "Epoch 489/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 64.0615 - mae: 5.2213 - val_loss: 401.1020 - val_mae: 12.1441\n",
      "Epoch 490/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 54.5589 - mae: 5.0013 - val_loss: 367.5891 - val_mae: 11.8242\n",
      "Epoch 491/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 61.0335 - mae: 5.1067 - val_loss: 529.8739 - val_mae: 13.7624\n",
      "Epoch 492/1000\n",
      "122/122 [==============================] - 0s 579us/step - loss: 55.8872 - mae: 5.0808 - val_loss: 416.5181 - val_mae: 12.5991\n",
      "Epoch 493/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 59.1876 - mae: 5.0940 - val_loss: 434.1804 - val_mae: 12.6603\n",
      "Epoch 494/1000\n",
      "122/122 [==============================] - 0s 663us/step - loss: 51.4575 - mae: 4.8580 - val_loss: 576.4091 - val_mae: 14.2638\n",
      "Epoch 495/1000\n",
      "122/122 [==============================] - 0s 584us/step - loss: 55.4914 - mae: 5.0097 - val_loss: 481.9397 - val_mae: 13.1191\n",
      "Epoch 496/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 52.4497 - mae: 5.0081 - val_loss: 376.5483 - val_mae: 12.0246\n",
      "Epoch 497/1000\n",
      "122/122 [==============================] - 0s 669us/step - loss: 56.6220 - mae: 5.0111 - val_loss: 461.2379 - val_mae: 13.0773\n",
      "Epoch 498/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 55.2424 - mae: 4.9346 - val_loss: 465.0219 - val_mae: 13.1190\n",
      "Epoch 499/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 58.4334 - mae: 5.0108 - val_loss: 444.3360 - val_mae: 12.9028\n",
      "Epoch 500/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 54.9198 - mae: 4.9864 - val_loss: 357.1428 - val_mae: 11.7573\n",
      "Epoch 501/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 63.6743 - mae: 5.2546 - val_loss: 363.2159 - val_mae: 11.7214\n",
      "Epoch 502/1000\n",
      "122/122 [==============================] - 0s 718us/step - loss: 62.2327 - mae: 5.1187 - val_loss: 398.0363 - val_mae: 12.1384\n",
      "Epoch 503/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 54.0159 - mae: 4.9773 - val_loss: 398.9357 - val_mae: 12.0826\n",
      "Epoch 504/1000\n",
      "122/122 [==============================] - 0s 655us/step - loss: 54.3208 - mae: 4.9743 - val_loss: 429.6131 - val_mae: 12.6185\n",
      "Epoch 505/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 53.0442 - mae: 4.9232 - val_loss: 451.6510 - val_mae: 12.9178\n",
      "Epoch 506/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 54.2333 - mae: 4.9217 - val_loss: 390.1442 - val_mae: 12.2800\n",
      "Epoch 507/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 63.8785 - mae: 5.2073 - val_loss: 484.8977 - val_mae: 13.3229\n",
      "Epoch 508/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 58.1448 - mae: 5.0160 - val_loss: 453.4633 - val_mae: 12.8912\n",
      "Epoch 509/1000\n",
      "122/122 [==============================] - 0s 686us/step - loss: 61.1156 - mae: 5.1643 - val_loss: 493.4077 - val_mae: 13.3315\n",
      "Epoch 510/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 60.2230 - mae: 5.0816 - val_loss: 524.5674 - val_mae: 13.6400\n",
      "Epoch 511/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 50.7525 - mae: 4.8631 - val_loss: 383.3274 - val_mae: 12.0590\n",
      "Epoch 512/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 59.8401 - mae: 5.0282 - val_loss: 540.7472 - val_mae: 13.9901\n",
      "Epoch 513/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 56.5604 - mae: 4.9932 - val_loss: 551.6340 - val_mae: 14.1745\n",
      "Epoch 514/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 54.3129 - mae: 4.9149 - val_loss: 442.2699 - val_mae: 12.7361\n",
      "Epoch 515/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 54.6147 - mae: 4.9494 - val_loss: 426.6444 - val_mae: 12.5202\n",
      "Epoch 516/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 58.1703 - mae: 5.0359 - val_loss: 505.4036 - val_mae: 13.4305\n",
      "Epoch 517/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 61.6389 - mae: 5.1480 - val_loss: 353.4043 - val_mae: 11.4868\n",
      "Epoch 518/1000\n",
      "122/122 [==============================] - 0s 693us/step - loss: 56.5073 - mae: 4.9655 - val_loss: 355.1908 - val_mae: 11.5562\n",
      "Epoch 519/1000\n",
      "122/122 [==============================] - 0s 569us/step - loss: 55.0102 - mae: 4.9929 - val_loss: 471.7202 - val_mae: 13.0864\n",
      "Epoch 520/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 58.3786 - mae: 5.0120 - val_loss: 442.3751 - val_mae: 12.6557\n",
      "Epoch 521/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 51.8456 - mae: 4.9925 - val_loss: 482.5387 - val_mae: 13.2815\n",
      "Epoch 522/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 57.5674 - mae: 5.0868 - val_loss: 398.2543 - val_mae: 12.0665\n",
      "Epoch 523/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 56.4740 - mae: 5.0349 - val_loss: 610.6860 - val_mae: 14.6652\n",
      "Epoch 524/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 58.0978 - mae: 5.0948 - val_loss: 460.2379 - val_mae: 12.9491\n",
      "Epoch 525/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 55.9951 - mae: 5.0193 - val_loss: 532.0093 - val_mae: 13.7584\n",
      "Epoch 526/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 52.1997 - mae: 4.8480 - val_loss: 479.7218 - val_mae: 13.1283\n",
      "Epoch 527/1000\n",
      "122/122 [==============================] - 0s 731us/step - loss: 48.7240 - mae: 4.7770 - val_loss: 485.0363 - val_mae: 13.2996\n",
      "Epoch 528/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 50.6696 - mae: 4.7552 - val_loss: 528.4156 - val_mae: 13.8071\n",
      "Epoch 529/1000\n",
      "122/122 [==============================] - 0s 667us/step - loss: 50.5767 - mae: 4.8927 - val_loss: 440.0895 - val_mae: 12.6741\n",
      "Epoch 530/1000\n",
      "122/122 [==============================] - 0s 581us/step - loss: 58.3142 - mae: 5.1093 - val_loss: 409.5608 - val_mae: 12.4160\n",
      "Epoch 531/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 54.5049 - mae: 4.9705 - val_loss: 485.0126 - val_mae: 13.1747\n",
      "Epoch 532/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 52.7491 - mae: 4.9644 - val_loss: 610.8580 - val_mae: 14.5158\n",
      "Epoch 533/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 53.7009 - mae: 4.9678 - val_loss: 551.0246 - val_mae: 13.9131\n",
      "Epoch 534/1000\n",
      "122/122 [==============================] - 0s 658us/step - loss: 50.0599 - mae: 4.8920 - val_loss: 500.0937 - val_mae: 13.4917\n",
      "Epoch 535/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 62.4571 - mae: 5.1242 - val_loss: 502.5531 - val_mae: 13.3620\n",
      "Epoch 536/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 54.7543 - mae: 5.0314 - val_loss: 433.7204 - val_mae: 12.5702\n",
      "Epoch 537/1000\n",
      "122/122 [==============================] - 0s 683us/step - loss: 50.8471 - mae: 4.8441 - val_loss: 450.9115 - val_mae: 12.9591\n",
      "Epoch 538/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 49.3998 - mae: 4.8336 - val_loss: 468.8008 - val_mae: 13.0249\n",
      "Epoch 539/1000\n",
      "122/122 [==============================] - 0s 594us/step - loss: 57.8708 - mae: 4.9636 - val_loss: 441.0853 - val_mae: 12.6552\n",
      "Epoch 540/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 57.8726 - mae: 5.0085 - val_loss: 554.7344 - val_mae: 13.9212\n",
      "Epoch 541/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 51.6889 - mae: 4.9181 - val_loss: 563.4719 - val_mae: 14.1871\n",
      "Epoch 542/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 49.1876 - mae: 4.8337 - val_loss: 485.3034 - val_mae: 13.1376\n",
      "Epoch 543/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 52.7178 - mae: 4.8971 - val_loss: 596.1730 - val_mae: 14.6072\n",
      "Epoch 544/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 51.8542 - mae: 4.9476 - val_loss: 538.3593 - val_mae: 13.9774\n",
      "Epoch 545/1000\n",
      "122/122 [==============================] - 0s 687us/step - loss: 52.1973 - mae: 4.9184 - val_loss: 453.1923 - val_mae: 12.7212\n",
      "Epoch 546/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 53.8326 - mae: 4.9366 - val_loss: 490.0450 - val_mae: 13.2547\n",
      "Epoch 547/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 51.7040 - mae: 4.9389 - val_loss: 511.3765 - val_mae: 13.6713\n",
      "Epoch 548/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 55.3688 - mae: 4.9957 - val_loss: 518.3300 - val_mae: 13.5117\n",
      "Epoch 549/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 54.4017 - mae: 5.0203 - val_loss: 533.6145 - val_mae: 13.6826\n",
      "Epoch 550/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 57.1997 - mae: 4.9794 - val_loss: 566.5169 - val_mae: 14.1343\n",
      "Epoch 551/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 50.5891 - mae: 4.8773 - val_loss: 534.9064 - val_mae: 13.7233\n",
      "Epoch 552/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 55.2203 - mae: 5.0027 - val_loss: 485.5950 - val_mae: 13.2038\n",
      "Epoch 553/1000\n",
      "122/122 [==============================] - 0s 716us/step - loss: 55.0038 - mae: 4.9982 - val_loss: 480.4105 - val_mae: 13.1434\n",
      "Epoch 554/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 53.6986 - mae: 4.9776 - val_loss: 531.8568 - val_mae: 13.9275\n",
      "Epoch 555/1000\n",
      "122/122 [==============================] - 0s 665us/step - loss: 56.7365 - mae: 5.0823 - val_loss: 482.4343 - val_mae: 13.2337\n",
      "Epoch 556/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 55.0714 - mae: 4.9046 - val_loss: 470.9799 - val_mae: 12.9656\n",
      "Epoch 557/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 55.1735 - mae: 4.9839 - val_loss: 526.4926 - val_mae: 13.6243\n",
      "Epoch 558/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 55.1894 - mae: 4.9243 - val_loss: 398.7155 - val_mae: 12.2912\n",
      "Epoch 559/1000\n",
      "122/122 [==============================] - 0s 716us/step - loss: 53.2760 - mae: 5.0288 - val_loss: 579.9937 - val_mae: 14.3029\n",
      "Epoch 560/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 50.5200 - mae: 4.8006 - val_loss: 542.6868 - val_mae: 13.8821\n",
      "Epoch 561/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 54.4455 - mae: 4.9102 - val_loss: 457.7969 - val_mae: 12.9807\n",
      "Epoch 562/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 55.9619 - mae: 4.9361 - val_loss: 511.0179 - val_mae: 13.4695\n",
      "Epoch 563/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 55.0434 - mae: 4.9480 - val_loss: 476.9295 - val_mae: 13.2408\n",
      "Epoch 564/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 52.7199 - mae: 4.9058 - val_loss: 536.1086 - val_mae: 13.7368\n",
      "Epoch 565/1000\n",
      "122/122 [==============================] - 0s 681us/step - loss: 59.8903 - mae: 5.1159 - val_loss: 569.9662 - val_mae: 14.2059\n",
      "Epoch 566/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 56.4626 - mae: 5.0757 - val_loss: 555.1132 - val_mae: 13.9268\n",
      "Epoch 567/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 56.2095 - mae: 5.0170 - val_loss: 434.2429 - val_mae: 12.7225\n",
      "Epoch 568/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 60.2146 - mae: 5.0009 - val_loss: 588.9145 - val_mae: 14.4834\n",
      "Epoch 569/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 53.0561 - mae: 4.9580 - val_loss: 561.5374 - val_mae: 13.9753\n",
      "Epoch 570/1000\n",
      "122/122 [==============================] - 0s 583us/step - loss: 51.8670 - mae: 4.8927 - val_loss: 528.1257 - val_mae: 13.6905\n",
      "Epoch 571/1000\n",
      "122/122 [==============================] - 0s 648us/step - loss: 56.5520 - mae: 5.0743 - val_loss: 662.8936 - val_mae: 15.2908\n",
      "Epoch 572/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 54.6451 - mae: 5.0346 - val_loss: 609.7883 - val_mae: 14.5515\n",
      "Epoch 573/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 52.1552 - mae: 4.8770 - val_loss: 523.3274 - val_mae: 13.6326\n",
      "Epoch 574/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 57.5039 - mae: 5.0313 - val_loss: 579.7898 - val_mae: 14.2820\n",
      "Epoch 575/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 53.0758 - mae: 4.9664 - val_loss: 604.0598 - val_mae: 14.5632\n",
      "Epoch 576/1000\n",
      "122/122 [==============================] - 0s 583us/step - loss: 52.5055 - mae: 4.8989 - val_loss: 473.3043 - val_mae: 13.0483\n",
      "Epoch 577/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 54.2788 - mae: 5.0437 - val_loss: 561.6684 - val_mae: 14.0388\n",
      "Epoch 578/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 51.7764 - mae: 4.9114 - val_loss: 547.3900 - val_mae: 13.9547\n",
      "Epoch 579/1000\n",
      "122/122 [==============================] - 0s 649us/step - loss: 49.5254 - mae: 4.8531 - val_loss: 553.7122 - val_mae: 13.9031\n",
      "Epoch 580/1000\n",
      "122/122 [==============================] - 0s 659us/step - loss: 50.1244 - mae: 4.7893 - val_loss: 647.1529 - val_mae: 14.9721\n",
      "Epoch 581/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 60.3123 - mae: 5.1260 - val_loss: 513.4572 - val_mae: 13.4247\n",
      "Epoch 582/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 56.2200 - mae: 5.0498 - val_loss: 554.4103 - val_mae: 13.9150\n",
      "Epoch 583/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 52.9011 - mae: 4.8954 - val_loss: 654.1831 - val_mae: 14.9306\n",
      "Epoch 584/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 55.9244 - mae: 5.0014 - val_loss: 647.2359 - val_mae: 14.8494\n",
      "Epoch 585/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 55.0157 - mae: 4.8522 - val_loss: 530.8297 - val_mae: 13.6068\n",
      "Epoch 586/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 57.8491 - mae: 5.0989 - val_loss: 608.0447 - val_mae: 14.4100\n",
      "Epoch 587/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 53.2860 - mae: 5.0139 - val_loss: 667.6520 - val_mae: 15.0500\n",
      "Epoch 588/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 51.9942 - mae: 4.8724 - val_loss: 521.3029 - val_mae: 13.5480\n",
      "Epoch 589/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 53.9106 - mae: 4.8122 - val_loss: 600.4301 - val_mae: 14.4305\n",
      "Epoch 590/1000\n",
      "122/122 [==============================] - 0s 671us/step - loss: 53.6514 - mae: 4.9318 - val_loss: 554.8467 - val_mae: 13.9295\n",
      "Epoch 591/1000\n",
      "122/122 [==============================] - 0s 728us/step - loss: 51.6426 - mae: 4.8545 - val_loss: 588.1544 - val_mae: 14.2707\n",
      "Epoch 592/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 53.7017 - mae: 4.9137 - val_loss: 590.6844 - val_mae: 14.2830\n",
      "Epoch 593/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 55.0034 - mae: 4.8685 - val_loss: 571.4339 - val_mae: 14.0477\n",
      "Epoch 594/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 59.6577 - mae: 5.0451 - val_loss: 349.5298 - val_mae: 11.5343\n",
      "Epoch 595/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 50.7530 - mae: 4.9252 - val_loss: 528.1019 - val_mae: 13.5472\n",
      "Epoch 596/1000\n",
      "122/122 [==============================] - 0s 579us/step - loss: 52.7920 - mae: 4.8899 - val_loss: 502.7451 - val_mae: 13.2719\n",
      "Epoch 597/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 54.1310 - mae: 4.9419 - val_loss: 612.8859 - val_mae: 14.5866\n",
      "Epoch 598/1000\n",
      "122/122 [==============================] - 0s 570us/step - loss: 54.5491 - mae: 4.8850 - val_loss: 524.3549 - val_mae: 13.5505\n",
      "Epoch 599/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 55.7426 - mae: 4.9156 - val_loss: 467.7790 - val_mae: 12.7987\n",
      "Epoch 600/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 52.6440 - mae: 4.9509 - val_loss: 516.6040 - val_mae: 13.4689\n",
      "Epoch 601/1000\n",
      "122/122 [==============================] - 0s 689us/step - loss: 46.2303 - mae: 4.7384 - val_loss: 563.3187 - val_mae: 14.1354\n",
      "Epoch 602/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 56.2708 - mae: 5.0234 - val_loss: 574.3759 - val_mae: 14.1424\n",
      "Epoch 603/1000\n",
      "122/122 [==============================] - 0s 657us/step - loss: 52.9255 - mae: 4.9265 - val_loss: 553.8130 - val_mae: 13.8769\n",
      "Epoch 604/1000\n",
      "122/122 [==============================] - 0s 655us/step - loss: 50.2090 - mae: 4.7623 - val_loss: 463.7925 - val_mae: 12.8415\n",
      "Epoch 605/1000\n",
      "122/122 [==============================] - 0s 666us/step - loss: 49.8339 - mae: 4.8368 - val_loss: 565.6003 - val_mae: 14.1217\n",
      "Epoch 606/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 52.1485 - mae: 4.9274 - val_loss: 474.5183 - val_mae: 12.9703\n",
      "Epoch 607/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 56.0898 - mae: 5.0016 - val_loss: 764.3782 - val_mae: 16.1430\n",
      "Epoch 608/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 60.9223 - mae: 5.0983 - val_loss: 544.7805 - val_mae: 13.7737\n",
      "Epoch 609/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 56.6594 - mae: 5.0567 - val_loss: 546.4100 - val_mae: 13.7855\n",
      "Epoch 610/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 51.2750 - mae: 4.9104 - val_loss: 558.9894 - val_mae: 14.0450\n",
      "Epoch 611/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 53.4129 - mae: 4.9680 - val_loss: 594.9907 - val_mae: 14.3954\n",
      "Epoch 612/1000\n",
      "122/122 [==============================] - 0s 649us/step - loss: 54.0157 - mae: 4.9824 - val_loss: 720.5106 - val_mae: 15.6992\n",
      "Epoch 613/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 52.1648 - mae: 4.8832 - val_loss: 604.0138 - val_mae: 14.4164\n",
      "Epoch 614/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 54.3053 - mae: 4.9516 - val_loss: 559.6044 - val_mae: 13.9195\n",
      "Epoch 615/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 57.6257 - mae: 4.9060 - val_loss: 498.2437 - val_mae: 13.4271\n",
      "Epoch 616/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 53.5117 - mae: 5.0020 - val_loss: 598.3933 - val_mae: 14.2818\n",
      "Epoch 617/1000\n",
      "122/122 [==============================] - 0s 576us/step - loss: 54.4207 - mae: 4.9910 - val_loss: 643.1979 - val_mae: 14.7109\n",
      "Epoch 618/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 49.3968 - mae: 4.7706 - val_loss: 482.2610 - val_mae: 13.1395\n",
      "Epoch 619/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 53.7625 - mae: 5.0229 - val_loss: 634.8895 - val_mae: 14.6724\n",
      "Epoch 620/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 52.9567 - mae: 5.0235 - val_loss: 616.1398 - val_mae: 14.5043\n",
      "Epoch 621/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 52.8511 - mae: 4.9700 - val_loss: 526.8779 - val_mae: 13.6864\n",
      "Epoch 622/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 56.3717 - mae: 4.9583 - val_loss: 556.9741 - val_mae: 13.9057\n",
      "Epoch 623/1000\n",
      "122/122 [==============================] - 0s 687us/step - loss: 52.0789 - mae: 4.9200 - val_loss: 616.3457 - val_mae: 14.5741\n",
      "Epoch 624/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 47.5215 - mae: 4.7653 - val_loss: 569.2759 - val_mae: 14.0692\n",
      "Epoch 625/1000\n",
      "122/122 [==============================] - 0s 659us/step - loss: 48.9764 - mae: 4.8637 - val_loss: 653.7135 - val_mae: 15.0043\n",
      "Epoch 626/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 50.9562 - mae: 4.9577 - val_loss: 646.9342 - val_mae: 14.9812\n",
      "Epoch 627/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 50.3650 - mae: 4.8572 - val_loss: 635.5239 - val_mae: 14.6994\n",
      "Epoch 628/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 51.8597 - mae: 4.9385 - val_loss: 591.3381 - val_mae: 14.4132\n",
      "Epoch 629/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 53.6647 - mae: 4.9895 - val_loss: 621.9197 - val_mae: 14.5249\n",
      "Epoch 630/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 52.7294 - mae: 5.0042 - val_loss: 596.7670 - val_mae: 14.4687\n",
      "Epoch 631/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 50.3361 - mae: 4.8568 - val_loss: 650.4269 - val_mae: 14.7791\n",
      "Epoch 632/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 51.3786 - mae: 4.7989 - val_loss: 600.0388 - val_mae: 14.2716\n",
      "Epoch 633/1000\n",
      "122/122 [==============================] - 0s 654us/step - loss: 45.9139 - mae: 4.7199 - val_loss: 582.4742 - val_mae: 14.1322\n",
      "Epoch 634/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 50.0259 - mae: 4.8651 - val_loss: 630.2162 - val_mae: 14.6169\n",
      "Epoch 635/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 56.2407 - mae: 4.9652 - val_loss: 577.1891 - val_mae: 14.1370\n",
      "Epoch 636/1000\n",
      "122/122 [==============================] - 0s 579us/step - loss: 50.9100 - mae: 4.8383 - val_loss: 630.5884 - val_mae: 14.6946\n",
      "Epoch 637/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 49.0394 - mae: 4.7759 - val_loss: 554.6078 - val_mae: 13.9340\n",
      "Epoch 638/1000\n",
      "122/122 [==============================] - 0s 708us/step - loss: 52.4697 - mae: 4.8046 - val_loss: 581.1175 - val_mae: 14.0435\n",
      "Epoch 639/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 53.1444 - mae: 4.8698 - val_loss: 562.5715 - val_mae: 14.1074\n",
      "Epoch 640/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 54.9265 - mae: 4.9572 - val_loss: 555.0937 - val_mae: 13.8005\n",
      "Epoch 641/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 50.1546 - mae: 4.8654 - val_loss: 600.0505 - val_mae: 14.3712\n",
      "Epoch 642/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 51.0053 - mae: 4.9032 - val_loss: 673.9600 - val_mae: 15.1691\n",
      "Epoch 643/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 53.2559 - mae: 4.9121 - val_loss: 728.8329 - val_mae: 15.4986\n",
      "Epoch 644/1000\n",
      "122/122 [==============================] - 0s 632us/step - loss: 52.0428 - mae: 4.9395 - val_loss: 634.9907 - val_mae: 14.7121\n",
      "Epoch 645/1000\n",
      "122/122 [==============================] - 0s 658us/step - loss: 52.5524 - mae: 4.8860 - val_loss: 676.7830 - val_mae: 15.1305\n",
      "Epoch 646/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 50.6844 - mae: 4.8202 - val_loss: 542.3408 - val_mae: 13.5803\n",
      "Epoch 647/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 54.6373 - mae: 4.9462 - val_loss: 583.0151 - val_mae: 14.2388\n",
      "Epoch 648/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 49.4532 - mae: 4.8536 - val_loss: 691.8455 - val_mae: 15.1685\n",
      "Epoch 649/1000\n",
      "122/122 [==============================] - 0s 594us/step - loss: 47.4068 - mae: 4.7230 - val_loss: 693.7090 - val_mae: 15.2960\n",
      "Epoch 650/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 49.2691 - mae: 4.7847 - val_loss: 640.7293 - val_mae: 14.8773\n",
      "Epoch 651/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 49.6230 - mae: 4.7936 - val_loss: 599.3130 - val_mae: 14.2655\n",
      "Epoch 652/1000\n",
      "122/122 [==============================] - 0s 652us/step - loss: 55.3699 - mae: 5.0231 - val_loss: 619.6686 - val_mae: 14.5501\n",
      "Epoch 653/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 53.0384 - mae: 4.9985 - val_loss: 605.2387 - val_mae: 14.3493\n",
      "Epoch 654/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 54.0457 - mae: 5.0196 - val_loss: 634.2604 - val_mae: 14.7244\n",
      "Epoch 655/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 48.7790 - mae: 4.8206 - val_loss: 632.8166 - val_mae: 14.5851\n",
      "Epoch 656/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 50.2909 - mae: 4.8321 - val_loss: 544.7733 - val_mae: 13.9458\n",
      "Epoch 657/1000\n",
      "122/122 [==============================] - 0s 578us/step - loss: 54.1006 - mae: 5.0131 - val_loss: 609.8320 - val_mae: 14.5082\n",
      "Epoch 658/1000\n",
      "122/122 [==============================] - 0s 718us/step - loss: 49.8499 - mae: 4.7983 - val_loss: 663.8127 - val_mae: 14.8731\n",
      "Epoch 659/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 51.8130 - mae: 4.8942 - val_loss: 681.6379 - val_mae: 15.1340\n",
      "Epoch 660/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 48.5681 - mae: 4.8284 - val_loss: 626.6713 - val_mae: 14.5702\n",
      "Epoch 661/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 51.7257 - mae: 4.8359 - val_loss: 576.8166 - val_mae: 14.1284\n",
      "Epoch 662/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 54.7073 - mae: 4.9524 - val_loss: 655.4339 - val_mae: 14.8650\n",
      "Epoch 663/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 46.3047 - mae: 4.7136 - val_loss: 799.1636 - val_mae: 16.3319\n",
      "Epoch 664/1000\n",
      "122/122 [==============================] - 0s 676us/step - loss: 50.0363 - mae: 4.8678 - val_loss: 699.0253 - val_mae: 15.2298\n",
      "Epoch 665/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 56.7136 - mae: 4.9912 - val_loss: 622.4546 - val_mae: 14.4045\n",
      "Epoch 666/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 45.4438 - mae: 4.6782 - val_loss: 572.9005 - val_mae: 13.9979\n",
      "Epoch 667/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 48.3291 - mae: 4.8060 - val_loss: 558.5297 - val_mae: 13.7224\n",
      "Epoch 668/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 47.1424 - mae: 4.7708 - val_loss: 678.2955 - val_mae: 15.1113\n",
      "Epoch 669/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 48.9073 - mae: 4.7729 - val_loss: 691.3120 - val_mae: 15.1724\n",
      "Epoch 670/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 51.0603 - mae: 4.8524 - val_loss: 672.8698 - val_mae: 14.9628\n",
      "Epoch 671/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 52.2607 - mae: 4.9409 - val_loss: 653.9431 - val_mae: 14.8478\n",
      "Epoch 672/1000\n",
      "122/122 [==============================] - 0s 700us/step - loss: 48.6790 - mae: 4.8361 - val_loss: 722.4998 - val_mae: 15.5006\n",
      "Epoch 673/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 51.3730 - mae: 4.9187 - val_loss: 742.1226 - val_mae: 15.6820\n",
      "Epoch 674/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 45.7996 - mae: 4.6853 - val_loss: 746.8260 - val_mae: 15.7680\n",
      "Epoch 675/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 50.2335 - mae: 4.8585 - val_loss: 592.2629 - val_mae: 14.1254\n",
      "Epoch 676/1000\n",
      "122/122 [==============================] - 0s 639us/step - loss: 51.7788 - mae: 4.8794 - val_loss: 620.7930 - val_mae: 14.5139\n",
      "Epoch 677/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 48.0387 - mae: 4.7794 - val_loss: 583.7297 - val_mae: 14.1242\n",
      "Epoch 678/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 45.1707 - mae: 4.6773 - val_loss: 624.2877 - val_mae: 14.3990\n",
      "Epoch 679/1000\n",
      "122/122 [==============================] - 0s 683us/step - loss: 51.8221 - mae: 4.9170 - val_loss: 683.1703 - val_mae: 14.9945\n",
      "Epoch 680/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 48.0693 - mae: 4.7908 - val_loss: 679.0126 - val_mae: 15.0896\n",
      "Epoch 681/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 48.0553 - mae: 4.8271 - val_loss: 608.9632 - val_mae: 14.3869\n",
      "Epoch 682/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 48.6030 - mae: 4.7709 - val_loss: 727.6773 - val_mae: 15.6020\n",
      "Epoch 683/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 48.1936 - mae: 4.7602 - val_loss: 632.0669 - val_mae: 14.6492\n",
      "Epoch 684/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 48.3305 - mae: 4.7498 - val_loss: 709.8430 - val_mae: 15.2300\n",
      "Epoch 685/1000\n",
      "122/122 [==============================] - 0s 697us/step - loss: 51.4864 - mae: 4.8882 - val_loss: 680.8475 - val_mae: 14.9630\n",
      "Epoch 686/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 48.4776 - mae: 4.7690 - val_loss: 731.3853 - val_mae: 15.3583\n",
      "Epoch 687/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 50.3230 - mae: 4.7534 - val_loss: 709.7990 - val_mae: 15.1928\n",
      "Epoch 688/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 47.3348 - mae: 4.7511 - val_loss: 652.3155 - val_mae: 14.8475\n",
      "Epoch 689/1000\n",
      "122/122 [==============================] - 0s 580us/step - loss: 54.7282 - mae: 4.8302 - val_loss: 549.7245 - val_mae: 13.6838\n",
      "Epoch 690/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 48.5160 - mae: 4.8720 - val_loss: 649.1503 - val_mae: 14.6549\n",
      "Epoch 691/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 50.9294 - mae: 4.8252 - val_loss: 638.4951 - val_mae: 14.6201\n",
      "Epoch 692/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 48.7722 - mae: 4.7892 - val_loss: 602.6655 - val_mae: 14.3035\n",
      "Epoch 693/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 54.8623 - mae: 5.0035 - val_loss: 698.2528 - val_mae: 15.1358\n",
      "Epoch 694/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 49.1870 - mae: 4.8905 - val_loss: 881.0716 - val_mae: 16.7187\n",
      "Epoch 695/1000\n",
      "122/122 [==============================] - 0s 663us/step - loss: 53.0895 - mae: 4.8749 - val_loss: 703.1124 - val_mae: 15.0879\n",
      "Epoch 696/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 47.2578 - mae: 4.7434 - val_loss: 721.1172 - val_mae: 15.3453\n",
      "Epoch 697/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 46.8591 - mae: 4.7646 - val_loss: 752.3239 - val_mae: 15.6528\n",
      "Epoch 698/1000\n",
      "122/122 [==============================] - 0s 580us/step - loss: 53.1137 - mae: 4.9174 - val_loss: 741.5056 - val_mae: 15.5434\n",
      "Epoch 699/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 47.7181 - mae: 4.7825 - val_loss: 725.6104 - val_mae: 15.5421\n",
      "Epoch 700/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 50.6723 - mae: 4.8631 - val_loss: 715.4783 - val_mae: 15.4692\n",
      "Epoch 701/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 51.1835 - mae: 4.8875 - val_loss: 908.8654 - val_mae: 17.1059\n",
      "Epoch 702/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 54.9416 - mae: 5.0400 - val_loss: 538.8632 - val_mae: 13.7237\n",
      "Epoch 703/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 50.4489 - mae: 4.9321 - val_loss: 811.9507 - val_mae: 16.1837\n",
      "Epoch 704/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 52.6793 - mae: 4.8718 - val_loss: 737.6834 - val_mae: 15.7216\n",
      "Epoch 705/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 50.6320 - mae: 4.8152 - val_loss: 762.4598 - val_mae: 15.6585\n",
      "Epoch 706/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 48.9606 - mae: 4.8558 - val_loss: 776.5369 - val_mae: 16.0202\n",
      "Epoch 707/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 48.4034 - mae: 4.8092 - val_loss: 806.7815 - val_mae: 16.2096\n",
      "Epoch 708/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 51.8415 - mae: 4.8228 - val_loss: 793.7949 - val_mae: 16.1563\n",
      "Epoch 709/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 55.9489 - mae: 5.0874 - val_loss: 740.8024 - val_mae: 15.5438\n",
      "Epoch 710/1000\n",
      "122/122 [==============================] - 0s 680us/step - loss: 50.7782 - mae: 4.9292 - val_loss: 662.9363 - val_mae: 14.8587\n",
      "Epoch 711/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 48.8024 - mae: 4.8555 - val_loss: 932.7780 - val_mae: 17.3001\n",
      "Epoch 712/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 47.7560 - mae: 4.7229 - val_loss: 723.5300 - val_mae: 15.4513\n",
      "Epoch 713/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 50.9818 - mae: 4.9645 - val_loss: 839.0096 - val_mae: 16.3558\n",
      "Epoch 714/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 52.0756 - mae: 4.7969 - val_loss: 673.4520 - val_mae: 14.7507\n",
      "Epoch 715/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 50.7113 - mae: 4.7583 - val_loss: 881.3073 - val_mae: 16.7180\n",
      "Epoch 716/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 48.4077 - mae: 4.7991 - val_loss: 649.0656 - val_mae: 14.6410\n",
      "Epoch 717/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 51.0755 - mae: 4.9033 - val_loss: 717.5784 - val_mae: 15.2748\n",
      "Epoch 718/1000\n",
      "122/122 [==============================] - 0s 639us/step - loss: 47.6469 - mae: 4.7724 - val_loss: 746.1720 - val_mae: 15.6559\n",
      "Epoch 719/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 48.2124 - mae: 4.7403 - val_loss: 813.5869 - val_mae: 16.1416\n",
      "Epoch 720/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 46.8160 - mae: 4.6944 - val_loss: 625.2770 - val_mae: 14.5407\n",
      "Epoch 721/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 48.9124 - mae: 4.7539 - val_loss: 677.1005 - val_mae: 15.1140\n",
      "Epoch 722/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 54.4191 - mae: 4.9072 - val_loss: 777.5956 - val_mae: 15.8911\n",
      "Epoch 723/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 50.1389 - mae: 4.8470 - val_loss: 778.8920 - val_mae: 15.8918\n",
      "Epoch 724/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 49.5762 - mae: 4.8547 - val_loss: 751.5621 - val_mae: 15.6263\n",
      "Epoch 725/1000\n",
      "122/122 [==============================] - 0s 579us/step - loss: 51.0417 - mae: 4.8807 - val_loss: 803.6699 - val_mae: 16.0939\n",
      "Epoch 726/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 55.5090 - mae: 5.0314 - val_loss: 929.1554 - val_mae: 17.3115\n",
      "Epoch 727/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 47.5881 - mae: 4.7417 - val_loss: 810.1379 - val_mae: 16.4126\n",
      "Epoch 728/1000\n",
      "122/122 [==============================] - 0s 674us/step - loss: 52.6427 - mae: 4.8760 - val_loss: 925.2712 - val_mae: 17.2135\n",
      "Epoch 729/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 46.9267 - mae: 4.6417 - val_loss: 825.6559 - val_mae: 16.3677\n",
      "Epoch 730/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 50.7869 - mae: 4.8788 - val_loss: 784.8047 - val_mae: 15.8656\n",
      "Epoch 731/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 47.0231 - mae: 4.8185 - val_loss: 916.9464 - val_mae: 17.0296\n",
      "Epoch 732/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 49.7060 - mae: 4.8503 - val_loss: 809.8815 - val_mae: 16.1654\n",
      "Epoch 733/1000\n",
      "122/122 [==============================] - 0s 684us/step - loss: 53.6646 - mae: 4.9596 - val_loss: 847.6008 - val_mae: 16.4643\n",
      "Epoch 734/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 49.1926 - mae: 4.7252 - val_loss: 767.7212 - val_mae: 15.7150\n",
      "Epoch 735/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 54.0384 - mae: 4.9161 - val_loss: 739.6768 - val_mae: 15.5855\n",
      "Epoch 736/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 48.2299 - mae: 4.7337 - val_loss: 823.0469 - val_mae: 16.2936\n",
      "Epoch 737/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 46.7622 - mae: 4.7438 - val_loss: 874.0405 - val_mae: 16.7536\n",
      "Epoch 738/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 48.2813 - mae: 4.8121 - val_loss: 945.2451 - val_mae: 17.3715\n",
      "Epoch 739/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 51.0589 - mae: 4.8042 - val_loss: 800.0464 - val_mae: 16.1226\n",
      "Epoch 740/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 47.5887 - mae: 4.7360 - val_loss: 841.0061 - val_mae: 16.7410\n",
      "Epoch 741/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 48.7571 - mae: 4.8442 - val_loss: 769.7103 - val_mae: 15.9753\n",
      "Epoch 742/1000\n",
      "122/122 [==============================] - 0s 645us/step - loss: 49.5542 - mae: 4.7621 - val_loss: 790.2691 - val_mae: 16.1203\n",
      "Epoch 743/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 46.2207 - mae: 4.7227 - val_loss: 860.8110 - val_mae: 16.6862\n",
      "Epoch 744/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 51.0075 - mae: 4.9076 - val_loss: 754.6362 - val_mae: 15.8472\n",
      "Epoch 745/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 49.2374 - mae: 4.7325 - val_loss: 780.5244 - val_mae: 15.8833\n",
      "Epoch 746/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 49.0732 - mae: 4.8032 - val_loss: 814.5258 - val_mae: 16.2834\n",
      "Epoch 747/1000\n",
      "122/122 [==============================] - 0s 683us/step - loss: 49.2093 - mae: 4.7639 - val_loss: 1034.9305 - val_mae: 18.0369\n",
      "Epoch 748/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 52.6999 - mae: 4.8525 - val_loss: 838.6906 - val_mae: 16.4002\n",
      "Epoch 749/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 51.5171 - mae: 4.8867 - val_loss: 764.1733 - val_mae: 15.8548\n",
      "Epoch 750/1000\n",
      "122/122 [==============================] - 0s 580us/step - loss: 52.7693 - mae: 4.9939 - val_loss: 895.9882 - val_mae: 16.8955\n",
      "Epoch 751/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 46.0375 - mae: 4.6628 - val_loss: 795.4044 - val_mae: 16.0284\n",
      "Epoch 752/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 51.8667 - mae: 4.8697 - val_loss: 830.5386 - val_mae: 16.3129\n",
      "Epoch 753/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 52.3709 - mae: 4.9458 - val_loss: 921.4014 - val_mae: 17.0865\n",
      "Epoch 754/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 47.9571 - mae: 4.7608 - val_loss: 891.3516 - val_mae: 16.7163\n",
      "Epoch 755/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 51.6019 - mae: 4.8749 - val_loss: 713.2446 - val_mae: 15.2197\n",
      "Epoch 756/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 49.0846 - mae: 4.7912 - val_loss: 822.6340 - val_mae: 16.2626\n",
      "Epoch 757/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 44.1649 - mae: 4.5720 - val_loss: 803.9691 - val_mae: 16.1074\n",
      "Epoch 758/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 49.0671 - mae: 4.7928 - val_loss: 792.4442 - val_mae: 15.9441\n",
      "Epoch 759/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 52.1418 - mae: 4.9038 - val_loss: 923.3447 - val_mae: 17.2445\n",
      "Epoch 760/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 53.5732 - mae: 4.8764 - val_loss: 722.3742 - val_mae: 15.3422\n",
      "Epoch 761/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 49.6709 - mae: 4.7317 - val_loss: 818.4332 - val_mae: 16.0914\n",
      "Epoch 762/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 49.5499 - mae: 4.7788 - val_loss: 846.6062 - val_mae: 16.5331\n",
      "Epoch 763/1000\n",
      "122/122 [==============================] - 0s 712us/step - loss: 48.3395 - mae: 4.8194 - val_loss: 781.1506 - val_mae: 16.1160\n",
      "Epoch 764/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 46.7127 - mae: 4.7491 - val_loss: 827.9273 - val_mae: 16.2466\n",
      "Epoch 765/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 47.7810 - mae: 4.7830 - val_loss: 893.8204 - val_mae: 16.7446\n",
      "Epoch 766/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 51.2956 - mae: 4.9247 - val_loss: 771.9501 - val_mae: 15.7400\n",
      "Epoch 767/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 48.9302 - mae: 4.7979 - val_loss: 873.5381 - val_mae: 16.6987\n",
      "Epoch 768/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 42.9314 - mae: 4.6638 - val_loss: 870.7901 - val_mae: 16.5152\n",
      "Epoch 769/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 49.5809 - mae: 4.8998 - val_loss: 789.8475 - val_mae: 15.8976\n",
      "Epoch 770/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 49.9465 - mae: 4.8425 - val_loss: 881.2581 - val_mae: 16.7908\n",
      "Epoch 771/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 50.9111 - mae: 4.8625 - val_loss: 851.9146 - val_mae: 16.4442\n",
      "Epoch 772/1000\n",
      "122/122 [==============================] - 0s 618us/step - loss: 45.7152 - mae: 4.6501 - val_loss: 892.1667 - val_mae: 16.8475\n",
      "Epoch 773/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 46.4917 - mae: 4.7129 - val_loss: 1012.2061 - val_mae: 17.7908\n",
      "Epoch 774/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 53.7401 - mae: 4.8477 - val_loss: 621.9089 - val_mae: 14.4957\n",
      "Epoch 775/1000\n",
      "122/122 [==============================] - 0s 583us/step - loss: 54.5669 - mae: 4.9501 - val_loss: 805.2374 - val_mae: 16.1174\n",
      "Epoch 776/1000\n",
      "122/122 [==============================] - 0s 657us/step - loss: 50.4085 - mae: 4.7765 - val_loss: 847.5189 - val_mae: 16.5146\n",
      "Epoch 777/1000\n",
      "122/122 [==============================] - 0s 576us/step - loss: 44.0606 - mae: 4.6209 - val_loss: 756.4254 - val_mae: 15.7431\n",
      "Epoch 778/1000\n",
      "122/122 [==============================] - 0s 663us/step - loss: 54.3224 - mae: 4.9028 - val_loss: 919.5999 - val_mae: 17.0972\n",
      "Epoch 779/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 50.9636 - mae: 4.9476 - val_loss: 747.3011 - val_mae: 15.5486\n",
      "Epoch 780/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 51.3712 - mae: 4.8827 - val_loss: 878.4498 - val_mae: 16.7116\n",
      "Epoch 781/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 50.5205 - mae: 4.8713 - val_loss: 889.3526 - val_mae: 16.9288\n",
      "Epoch 782/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 48.3915 - mae: 4.8233 - val_loss: 847.5389 - val_mae: 16.5042\n",
      "Epoch 783/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 52.6630 - mae: 4.9550 - val_loss: 865.4897 - val_mae: 16.4871\n",
      "Epoch 784/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 45.4653 - mae: 4.7729 - val_loss: 827.1271 - val_mae: 16.2214\n",
      "Epoch 785/1000\n",
      "122/122 [==============================] - 0s 682us/step - loss: 50.6669 - mae: 4.8332 - val_loss: 980.2980 - val_mae: 17.5380\n",
      "Epoch 786/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 45.6358 - mae: 4.7324 - val_loss: 821.8322 - val_mae: 16.2461\n",
      "Epoch 787/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 48.2994 - mae: 4.7459 - val_loss: 1097.4541 - val_mae: 18.3673\n",
      "Epoch 788/1000\n",
      "122/122 [==============================] - 0s 665us/step - loss: 50.9477 - mae: 4.8610 - val_loss: 822.4459 - val_mae: 16.3600\n",
      "Epoch 789/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 48.9149 - mae: 4.8984 - val_loss: 894.9477 - val_mae: 16.7624\n",
      "Epoch 790/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 45.8399 - mae: 4.7337 - val_loss: 992.7805 - val_mae: 17.6506\n",
      "Epoch 791/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 50.3068 - mae: 4.7819 - val_loss: 901.9079 - val_mae: 16.8430\n",
      "Epoch 792/1000\n",
      "122/122 [==============================] - 0s 722us/step - loss: 51.5126 - mae: 4.8109 - val_loss: 1065.6400 - val_mae: 18.1058\n",
      "Epoch 793/1000\n",
      "122/122 [==============================] - 0s 653us/step - loss: 53.2105 - mae: 4.8522 - val_loss: 1111.5365 - val_mae: 18.5126\n",
      "Epoch 794/1000\n",
      "122/122 [==============================] - 0s 667us/step - loss: 51.3935 - mae: 4.9755 - val_loss: 888.2805 - val_mae: 16.8002\n",
      "Epoch 795/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 45.1725 - mae: 4.6799 - val_loss: 913.0618 - val_mae: 17.0388\n",
      "Epoch 796/1000\n",
      "122/122 [==============================] - 0s 599us/step - loss: 46.2560 - mae: 4.7063 - val_loss: 871.0845 - val_mae: 16.7193\n",
      "Epoch 797/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 49.0787 - mae: 4.8498 - val_loss: 918.3140 - val_mae: 17.0650\n",
      "Epoch 798/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 44.0882 - mae: 4.6927 - val_loss: 1008.3315 - val_mae: 17.7943\n",
      "Epoch 799/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 43.2306 - mae: 4.5567 - val_loss: 947.0002 - val_mae: 17.2634\n",
      "Epoch 800/1000\n",
      "122/122 [==============================] - 0s 870us/step - loss: 43.3955 - mae: 4.6021 - val_loss: 908.7496 - val_mae: 16.9402\n",
      "Epoch 801/1000\n",
      "122/122 [==============================] - 0s 669us/step - loss: 52.2137 - mae: 4.8568 - val_loss: 889.5043 - val_mae: 16.7521\n",
      "Epoch 802/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 50.9841 - mae: 4.8690 - val_loss: 1080.9712 - val_mae: 18.1911\n",
      "Epoch 803/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 49.0194 - mae: 4.8290 - val_loss: 850.6661 - val_mae: 16.3711\n",
      "Epoch 804/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 43.8192 - mae: 4.5992 - val_loss: 1106.2152 - val_mae: 18.4575\n",
      "Epoch 805/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 51.4092 - mae: 4.9010 - val_loss: 912.7107 - val_mae: 17.0785\n",
      "Epoch 806/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 48.9790 - mae: 4.7502 - val_loss: 966.8535 - val_mae: 17.3729\n",
      "Epoch 807/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 45.4308 - mae: 4.6613 - val_loss: 1051.3848 - val_mae: 17.8722\n",
      "Epoch 808/1000\n",
      "122/122 [==============================] - 0s 657us/step - loss: 44.0627 - mae: 4.5656 - val_loss: 977.0728 - val_mae: 17.4018\n",
      "Epoch 809/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 48.5282 - mae: 4.7506 - val_loss: 861.6414 - val_mae: 16.4441\n",
      "Epoch 810/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 48.3956 - mae: 4.7861 - val_loss: 980.3820 - val_mae: 17.5004\n",
      "Epoch 811/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 46.0844 - mae: 4.7515 - val_loss: 969.5144 - val_mae: 17.4398\n",
      "Epoch 812/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 48.7918 - mae: 4.7081 - val_loss: 1062.3306 - val_mae: 18.0235\n",
      "Epoch 813/1000\n",
      "122/122 [==============================] - 0s 659us/step - loss: 50.3417 - mae: 4.8667 - val_loss: 1071.4572 - val_mae: 18.2777\n",
      "Epoch 814/1000\n",
      "122/122 [==============================] - 0s 624us/step - loss: 48.4593 - mae: 4.7787 - val_loss: 981.9590 - val_mae: 17.5216\n",
      "Epoch 815/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 43.3912 - mae: 4.6041 - val_loss: 904.5263 - val_mae: 16.8946\n",
      "Epoch 816/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 47.3980 - mae: 4.7353 - val_loss: 926.3021 - val_mae: 17.0518\n",
      "Epoch 817/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 46.9924 - mae: 4.7732 - val_loss: 960.4233 - val_mae: 17.3783\n",
      "Epoch 818/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 45.9595 - mae: 4.6897 - val_loss: 1098.9684 - val_mae: 18.4665\n",
      "Epoch 819/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 50.5451 - mae: 4.8539 - val_loss: 1045.7401 - val_mae: 18.0983\n",
      "Epoch 820/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 47.9646 - mae: 4.6700 - val_loss: 992.5416 - val_mae: 17.6662\n",
      "Epoch 821/1000\n",
      "122/122 [==============================] - 0s 689us/step - loss: 48.3932 - mae: 4.7960 - val_loss: 944.3734 - val_mae: 17.1549\n",
      "Epoch 822/1000\n",
      "122/122 [==============================] - 0s 656us/step - loss: 45.0128 - mae: 4.6243 - val_loss: 964.8990 - val_mae: 17.2028\n",
      "Epoch 823/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 45.2131 - mae: 4.6285 - val_loss: 921.4328 - val_mae: 17.0458\n",
      "Epoch 824/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 52.1049 - mae: 4.8153 - val_loss: 1046.8467 - val_mae: 17.9623\n",
      "Epoch 825/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 48.8501 - mae: 4.7447 - val_loss: 1116.4667 - val_mae: 18.4619\n",
      "Epoch 826/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 45.8302 - mae: 4.7802 - val_loss: 940.8643 - val_mae: 17.0675\n",
      "Epoch 827/1000\n",
      "122/122 [==============================] - 0s 710us/step - loss: 48.4310 - mae: 4.8003 - val_loss: 1074.6523 - val_mae: 18.2689\n",
      "Epoch 828/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 48.8559 - mae: 4.8716 - val_loss: 1151.1948 - val_mae: 18.8464\n",
      "Epoch 829/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 48.0354 - mae: 4.7875 - val_loss: 975.6079 - val_mae: 17.5051\n",
      "Epoch 830/1000\n",
      "122/122 [==============================] - 0s 574us/step - loss: 48.7889 - mae: 4.7640 - val_loss: 1099.2480 - val_mae: 18.4623\n",
      "Epoch 831/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 51.7826 - mae: 4.8806 - val_loss: 874.7155 - val_mae: 16.6118\n",
      "Epoch 832/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 51.0918 - mae: 4.8444 - val_loss: 1028.2400 - val_mae: 17.7884\n",
      "Epoch 833/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 46.8760 - mae: 4.7753 - val_loss: 968.1888 - val_mae: 17.3958\n",
      "Epoch 834/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 46.1024 - mae: 4.6703 - val_loss: 1060.3232 - val_mae: 18.1545\n",
      "Epoch 835/1000\n",
      "122/122 [==============================] - 0s 726us/step - loss: 50.8135 - mae: 4.8345 - val_loss: 1111.9431 - val_mae: 18.5012\n",
      "Epoch 836/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 44.4193 - mae: 4.5941 - val_loss: 996.8053 - val_mae: 17.5951\n",
      "Epoch 837/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 49.5452 - mae: 4.8134 - val_loss: 930.4511 - val_mae: 16.9955\n",
      "Epoch 838/1000\n",
      "122/122 [==============================] - 0s 666us/step - loss: 44.5191 - mae: 4.5983 - val_loss: 1024.8811 - val_mae: 17.7764\n",
      "Epoch 839/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 45.5951 - mae: 4.6964 - val_loss: 1059.6013 - val_mae: 18.1266\n",
      "Epoch 840/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 49.9124 - mae: 4.9131 - val_loss: 1070.3688 - val_mae: 18.1442\n",
      "Epoch 841/1000\n",
      "122/122 [==============================] - 0s 586us/step - loss: 46.9536 - mae: 4.7774 - val_loss: 1049.3553 - val_mae: 17.9609\n",
      "Epoch 842/1000\n",
      "122/122 [==============================] - 0s 636us/step - loss: 46.4027 - mae: 4.6984 - val_loss: 1000.5907 - val_mae: 17.6615\n",
      "Epoch 843/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 50.6242 - mae: 4.9428 - val_loss: 1105.9139 - val_mae: 18.2896\n",
      "Epoch 844/1000\n",
      "122/122 [==============================] - 0s 664us/step - loss: 51.5976 - mae: 4.8272 - val_loss: 1164.3378 - val_mae: 18.9986\n",
      "Epoch 845/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 46.2408 - mae: 4.6923 - val_loss: 977.7947 - val_mae: 17.3919\n",
      "Epoch 846/1000\n",
      "122/122 [==============================] - 0s 646us/step - loss: 50.0132 - mae: 4.7919 - val_loss: 984.2457 - val_mae: 17.4598\n",
      "Epoch 847/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 46.8282 - mae: 4.7303 - val_loss: 1068.7833 - val_mae: 18.0317\n",
      "Epoch 848/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 42.0719 - mae: 4.5384 - val_loss: 1050.2444 - val_mae: 17.9367\n",
      "Epoch 849/1000\n",
      "122/122 [==============================] - 0s 720us/step - loss: 48.1633 - mae: 4.6482 - val_loss: 1092.7969 - val_mae: 18.2327\n",
      "Epoch 850/1000\n",
      "122/122 [==============================] - 0s 609us/step - loss: 45.8279 - mae: 4.6802 - val_loss: 995.2593 - val_mae: 17.6643\n",
      "Epoch 851/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 43.7461 - mae: 4.6412 - val_loss: 1047.6926 - val_mae: 17.9111\n",
      "Epoch 852/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 49.0265 - mae: 4.7921 - val_loss: 1112.1660 - val_mae: 18.5034\n",
      "Epoch 853/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 44.5804 - mae: 4.6558 - val_loss: 1050.6632 - val_mae: 17.9645\n",
      "Epoch 854/1000\n",
      "122/122 [==============================] - 0s 610us/step - loss: 41.9849 - mae: 4.5725 - val_loss: 1164.4225 - val_mae: 18.7493\n",
      "Epoch 855/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 46.9546 - mae: 4.7374 - val_loss: 1025.3633 - val_mae: 17.7467\n",
      "Epoch 856/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 46.2355 - mae: 4.7095 - val_loss: 1055.3613 - val_mae: 18.0004\n",
      "Epoch 857/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 43.6173 - mae: 4.6683 - val_loss: 1006.8159 - val_mae: 17.5564\n",
      "Epoch 858/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 49.3793 - mae: 4.8412 - val_loss: 1063.8046 - val_mae: 18.0497\n",
      "Epoch 859/1000\n",
      "122/122 [==============================] - 0s 597us/step - loss: 49.5139 - mae: 4.7809 - val_loss: 933.6077 - val_mae: 17.0745\n",
      "Epoch 860/1000\n",
      "122/122 [==============================] - 0s 696us/step - loss: 47.5674 - mae: 4.7434 - val_loss: 1021.4012 - val_mae: 17.8631\n",
      "Epoch 861/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 46.1925 - mae: 4.7012 - val_loss: 1099.0314 - val_mae: 18.3101\n",
      "Epoch 862/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 48.0628 - mae: 4.7658 - val_loss: 1078.1694 - val_mae: 18.2790\n",
      "Epoch 863/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 50.6137 - mae: 4.7965 - val_loss: 986.3737 - val_mae: 17.5339\n",
      "Epoch 864/1000\n",
      "122/122 [==============================] - 0s 572us/step - loss: 46.6579 - mae: 4.7497 - val_loss: 1064.0928 - val_mae: 18.1607\n",
      "Epoch 865/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 47.1238 - mae: 4.7665 - val_loss: 1111.1644 - val_mae: 18.5022\n",
      "Epoch 866/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 45.9450 - mae: 4.7335 - val_loss: 1071.7531 - val_mae: 18.2556\n",
      "Epoch 867/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 41.3785 - mae: 4.5043 - val_loss: 1179.6466 - val_mae: 19.1089\n",
      "Epoch 868/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 44.0880 - mae: 4.6612 - val_loss: 1026.2107 - val_mae: 17.8373\n",
      "Epoch 869/1000\n",
      "122/122 [==============================] - 0s 579us/step - loss: 52.5193 - mae: 4.8011 - val_loss: 1075.3676 - val_mae: 18.3049\n",
      "Epoch 870/1000\n",
      "122/122 [==============================] - 0s 685us/step - loss: 47.1888 - mae: 4.7999 - val_loss: 1264.7899 - val_mae: 19.7060\n",
      "Epoch 871/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 48.3894 - mae: 4.8099 - val_loss: 1075.5084 - val_mae: 18.2568\n",
      "Epoch 872/1000\n",
      "122/122 [==============================] - 0s 633us/step - loss: 50.0026 - mae: 4.9035 - val_loss: 1244.7208 - val_mae: 19.4937\n",
      "Epoch 873/1000\n",
      "122/122 [==============================] - 0s 585us/step - loss: 44.6886 - mae: 4.6779 - val_loss: 918.2178 - val_mae: 17.0455\n",
      "Epoch 874/1000\n",
      "122/122 [==============================] - 0s 733us/step - loss: 45.0970 - mae: 4.6693 - val_loss: 1049.4568 - val_mae: 18.0098\n",
      "Epoch 875/1000\n",
      "122/122 [==============================] - 0s 704us/step - loss: 45.0710 - mae: 4.6454 - val_loss: 1045.2091 - val_mae: 17.9331\n",
      "Epoch 876/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 49.1264 - mae: 4.7862 - val_loss: 1105.8651 - val_mae: 18.4696\n",
      "Epoch 877/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 44.4513 - mae: 4.7310 - val_loss: 1048.8959 - val_mae: 18.0090\n",
      "Epoch 878/1000\n",
      "122/122 [==============================] - 0s 670us/step - loss: 44.8104 - mae: 4.5888 - val_loss: 1013.6300 - val_mae: 17.6666\n",
      "Epoch 879/1000\n",
      "122/122 [==============================] - 0s 665us/step - loss: 50.6054 - mae: 4.8604 - val_loss: 1062.8109 - val_mae: 18.1524\n",
      "Epoch 880/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 45.8490 - mae: 4.6693 - val_loss: 1106.2461 - val_mae: 18.3835\n",
      "Epoch 881/1000\n",
      "122/122 [==============================] - 0s 613us/step - loss: 45.6546 - mae: 4.5901 - val_loss: 1029.4280 - val_mae: 17.8623\n",
      "Epoch 882/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 44.9033 - mae: 4.6719 - val_loss: 1087.2848 - val_mae: 18.2373\n",
      "Epoch 883/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 45.1684 - mae: 4.6420 - val_loss: 999.6024 - val_mae: 17.5861\n",
      "Epoch 884/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 50.2639 - mae: 4.8122 - val_loss: 1031.9620 - val_mae: 18.1017\n",
      "Epoch 885/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 48.8592 - mae: 4.7761 - val_loss: 1102.6047 - val_mae: 18.2945\n",
      "Epoch 886/1000\n",
      "122/122 [==============================] - 0s 575us/step - loss: 45.2215 - mae: 4.6798 - val_loss: 1114.9060 - val_mae: 18.4589\n",
      "Epoch 887/1000\n",
      "122/122 [==============================] - 0s 672us/step - loss: 48.6339 - mae: 4.7640 - val_loss: 926.1300 - val_mae: 17.0187\n",
      "Epoch 888/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 43.6487 - mae: 4.5975 - val_loss: 1166.2885 - val_mae: 18.7413\n",
      "Epoch 889/1000\n",
      "122/122 [==============================] - 0s 647us/step - loss: 45.2279 - mae: 4.7484 - val_loss: 1155.9540 - val_mae: 18.6862\n",
      "Epoch 890/1000\n",
      "122/122 [==============================] - 0s 693us/step - loss: 46.3723 - mae: 4.6934 - val_loss: 1001.3068 - val_mae: 17.5517\n",
      "Epoch 891/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 44.3496 - mae: 4.6046 - val_loss: 1082.2473 - val_mae: 18.3735\n",
      "Epoch 892/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 47.4015 - mae: 4.7871 - val_loss: 945.5791 - val_mae: 17.3013\n",
      "Epoch 893/1000\n",
      "122/122 [==============================] - 0s 566us/step - loss: 47.4211 - mae: 4.6939 - val_loss: 1069.9901 - val_mae: 18.2747\n",
      "Epoch 894/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 49.2464 - mae: 4.7792 - val_loss: 989.4167 - val_mae: 17.5088\n",
      "Epoch 895/1000\n",
      "122/122 [==============================] - 0s 576us/step - loss: 48.2135 - mae: 4.8016 - val_loss: 1069.5193 - val_mae: 18.2475\n",
      "Epoch 896/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 47.8679 - mae: 4.7149 - val_loss: 1088.7506 - val_mae: 18.3340\n",
      "Epoch 897/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 44.9772 - mae: 4.6594 - val_loss: 956.9404 - val_mae: 17.1637\n",
      "Epoch 898/1000\n",
      "122/122 [==============================] - 0s 635us/step - loss: 46.9854 - mae: 4.7071 - val_loss: 1097.1246 - val_mae: 18.2185\n",
      "Epoch 899/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 40.7701 - mae: 4.4521 - val_loss: 1068.7440 - val_mae: 18.0511\n",
      "Epoch 900/1000\n",
      "122/122 [==============================] - 0s 708us/step - loss: 44.3501 - mae: 4.6408 - val_loss: 953.2959 - val_mae: 17.1692\n",
      "Epoch 901/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 46.8857 - mae: 4.7145 - val_loss: 1039.8771 - val_mae: 17.9331\n",
      "Epoch 902/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 47.3782 - mae: 4.6943 - val_loss: 1035.5310 - val_mae: 17.8293\n",
      "Epoch 903/1000\n",
      "122/122 [==============================] - 0s 588us/step - loss: 44.2535 - mae: 4.6588 - val_loss: 1047.6968 - val_mae: 17.9786\n",
      "Epoch 904/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 48.9673 - mae: 4.8104 - val_loss: 1070.6683 - val_mae: 18.1586\n",
      "Epoch 905/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 48.0730 - mae: 4.6316 - val_loss: 944.4777 - val_mae: 17.3328\n",
      "Epoch 906/1000\n",
      "122/122 [==============================] - 0s 596us/step - loss: 45.4152 - mae: 4.6880 - val_loss: 1066.5060 - val_mae: 18.2636\n",
      "Epoch 907/1000\n",
      "122/122 [==============================] - 0s 641us/step - loss: 47.2264 - mae: 4.7021 - val_loss: 1028.5017 - val_mae: 17.7712\n",
      "Epoch 908/1000\n",
      "122/122 [==============================] - 0s 605us/step - loss: 45.7653 - mae: 4.7185 - val_loss: 981.2673 - val_mae: 17.5092\n",
      "Epoch 909/1000\n",
      "122/122 [==============================] - 0s 674us/step - loss: 51.7597 - mae: 4.9243 - val_loss: 1098.0427 - val_mae: 18.2629\n",
      "Epoch 910/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 47.4666 - mae: 4.7397 - val_loss: 1004.9231 - val_mae: 17.7569\n",
      "Epoch 911/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 46.6084 - mae: 4.7002 - val_loss: 1080.2295 - val_mae: 18.2742\n",
      "Epoch 912/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 43.7493 - mae: 4.6244 - val_loss: 1110.6907 - val_mae: 18.5965\n",
      "Epoch 913/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 49.2449 - mae: 4.7674 - val_loss: 1015.4153 - val_mae: 17.9049\n",
      "Epoch 914/1000\n",
      "122/122 [==============================] - 0s 594us/step - loss: 43.0738 - mae: 4.5836 - val_loss: 1047.5082 - val_mae: 17.9652\n",
      "Epoch 915/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 41.4563 - mae: 4.5518 - val_loss: 963.9658 - val_mae: 17.3817\n",
      "Epoch 916/1000\n",
      "122/122 [==============================] - 0s 592us/step - loss: 45.6306 - mae: 4.7235 - val_loss: 1025.6440 - val_mae: 17.7058\n",
      "Epoch 917/1000\n",
      "122/122 [==============================] - 0s 644us/step - loss: 46.6625 - mae: 4.6636 - val_loss: 1031.3914 - val_mae: 17.8290\n",
      "Epoch 918/1000\n",
      "122/122 [==============================] - 0s 572us/step - loss: 41.2695 - mae: 4.5353 - val_loss: 1062.6884 - val_mae: 18.0895\n",
      "Epoch 919/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 44.6143 - mae: 4.6247 - val_loss: 976.8331 - val_mae: 17.4204\n",
      "Epoch 920/1000\n",
      "122/122 [==============================] - 0s 723us/step - loss: 48.2264 - mae: 4.7429 - val_loss: 977.3716 - val_mae: 17.3811\n",
      "Epoch 921/1000\n",
      "122/122 [==============================] - 0s 617us/step - loss: 43.3013 - mae: 4.6101 - val_loss: 1110.8258 - val_mae: 18.4627\n",
      "Epoch 922/1000\n",
      "122/122 [==============================] - 0s 622us/step - loss: 45.9981 - mae: 4.6746 - val_loss: 1091.7537 - val_mae: 18.2495\n",
      "Epoch 923/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 46.8667 - mae: 4.6572 - val_loss: 1062.1544 - val_mae: 18.1032\n",
      "Epoch 924/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 47.2005 - mae: 4.7391 - val_loss: 974.6426 - val_mae: 17.4147\n",
      "Epoch 925/1000\n",
      "122/122 [==============================] - 0s 648us/step - loss: 43.0994 - mae: 4.5606 - val_loss: 998.5508 - val_mae: 17.5665\n",
      "Epoch 926/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 44.2574 - mae: 4.6058 - val_loss: 996.8223 - val_mae: 17.5519\n",
      "Epoch 927/1000\n",
      "122/122 [==============================] - 0s 643us/step - loss: 47.8023 - mae: 4.7372 - val_loss: 1121.4302 - val_mae: 18.4251\n",
      "Epoch 928/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 47.7172 - mae: 4.7547 - val_loss: 1063.3656 - val_mae: 18.0972\n",
      "Epoch 929/1000\n",
      "122/122 [==============================] - 0s 742us/step - loss: 45.6567 - mae: 4.7147 - val_loss: 944.1118 - val_mae: 17.1276\n",
      "Epoch 930/1000\n",
      "122/122 [==============================] - 0s 661us/step - loss: 48.1552 - mae: 4.7325 - val_loss: 1090.2606 - val_mae: 18.3026\n",
      "Epoch 931/1000\n",
      "122/122 [==============================] - 0s 663us/step - loss: 47.5540 - mae: 4.7373 - val_loss: 949.5727 - val_mae: 17.1918\n",
      "Epoch 932/1000\n",
      "122/122 [==============================] - 0s 574us/step - loss: 45.7976 - mae: 4.6296 - val_loss: 1002.4341 - val_mae: 17.6162\n",
      "Epoch 933/1000\n",
      "122/122 [==============================] - 0s 603us/step - loss: 45.5228 - mae: 4.7172 - val_loss: 981.0742 - val_mae: 17.5300\n",
      "Epoch 934/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 49.7416 - mae: 4.7755 - val_loss: 1082.0496 - val_mae: 18.1981\n",
      "Epoch 935/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 49.6526 - mae: 4.8068 - val_loss: 1013.5649 - val_mae: 17.8566\n",
      "Epoch 936/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 45.1890 - mae: 4.6890 - val_loss: 1010.9167 - val_mae: 17.9090\n",
      "Epoch 937/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 46.5616 - mae: 4.7055 - val_loss: 894.3801 - val_mae: 16.7968\n",
      "Epoch 938/1000\n",
      "122/122 [==============================] - 0s 645us/step - loss: 51.5234 - mae: 4.8935 - val_loss: 938.3162 - val_mae: 17.1010\n",
      "Epoch 939/1000\n",
      "122/122 [==============================] - 0s 602us/step - loss: 49.9250 - mae: 4.8127 - val_loss: 1055.5491 - val_mae: 17.8423\n",
      "Epoch 940/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 46.0823 - mae: 4.6929 - val_loss: 1171.1022 - val_mae: 18.8626\n",
      "Epoch 941/1000\n",
      "122/122 [==============================] - 0s 679us/step - loss: 41.7569 - mae: 4.5431 - val_loss: 1094.5409 - val_mae: 18.2940\n",
      "Epoch 942/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 45.7440 - mae: 4.7642 - val_loss: 1049.6519 - val_mae: 18.0042\n",
      "Epoch 943/1000\n",
      "122/122 [==============================] - 0s 697us/step - loss: 40.9901 - mae: 4.4763 - val_loss: 1021.1370 - val_mae: 17.8511\n",
      "Epoch 944/1000\n",
      "122/122 [==============================] - 0s 634us/step - loss: 46.6106 - mae: 4.6965 - val_loss: 1113.5134 - val_mae: 18.5149\n",
      "Epoch 945/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 46.5182 - mae: 4.6744 - val_loss: 938.6814 - val_mae: 17.1229\n",
      "Epoch 946/1000\n",
      "122/122 [==============================] - 0s 582us/step - loss: 45.2737 - mae: 4.5497 - val_loss: 986.9620 - val_mae: 17.7103\n",
      "Epoch 947/1000\n",
      "122/122 [==============================] - 0s 650us/step - loss: 49.1586 - mae: 4.7537 - val_loss: 1140.7217 - val_mae: 18.7790\n",
      "Epoch 948/1000\n",
      "122/122 [==============================] - 0s 675us/step - loss: 48.5684 - mae: 4.8326 - val_loss: 1012.1841 - val_mae: 17.7483\n",
      "Epoch 949/1000\n",
      "122/122 [==============================] - 0s 628us/step - loss: 48.0235 - mae: 4.8234 - val_loss: 991.3442 - val_mae: 17.5313\n",
      "Epoch 950/1000\n",
      "122/122 [==============================] - 0s 640us/step - loss: 47.6221 - mae: 4.8317 - val_loss: 1086.6111 - val_mae: 18.2234\n",
      "Epoch 951/1000\n",
      "122/122 [==============================] - 0s 591us/step - loss: 48.1459 - mae: 4.7696 - val_loss: 1098.8949 - val_mae: 18.3976\n",
      "Epoch 952/1000\n",
      "122/122 [==============================] - 0s 651us/step - loss: 46.6402 - mae: 4.7067 - val_loss: 1062.5481 - val_mae: 18.1726\n",
      "Epoch 953/1000\n",
      "122/122 [==============================] - 0s 598us/step - loss: 43.9431 - mae: 4.6010 - val_loss: 1035.1615 - val_mae: 17.8853\n",
      "Epoch 954/1000\n",
      "122/122 [==============================] - 0s 682us/step - loss: 48.7012 - mae: 4.8274 - val_loss: 1072.9113 - val_mae: 18.2686\n",
      "Epoch 955/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 47.3003 - mae: 4.7888 - val_loss: 1033.2371 - val_mae: 17.9648\n",
      "Epoch 956/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 45.1282 - mae: 4.7058 - val_loss: 1037.3108 - val_mae: 17.9772\n",
      "Epoch 957/1000\n",
      "122/122 [==============================] - 0s 590us/step - loss: 45.7051 - mae: 4.6256 - val_loss: 1045.7340 - val_mae: 17.9444\n",
      "Epoch 958/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 43.8529 - mae: 4.6546 - val_loss: 1090.9646 - val_mae: 18.2994\n",
      "Epoch 959/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 48.5830 - mae: 4.7927 - val_loss: 1081.8361 - val_mae: 18.2737\n",
      "Epoch 960/1000\n",
      "122/122 [==============================] - 0s 608us/step - loss: 45.4520 - mae: 4.6695 - val_loss: 954.0348 - val_mae: 17.3223\n",
      "Epoch 961/1000\n",
      "122/122 [==============================] - 0s 626us/step - loss: 47.3438 - mae: 4.6263 - val_loss: 930.6950 - val_mae: 17.2028\n",
      "Epoch 962/1000\n",
      "122/122 [==============================] - 0s 589us/step - loss: 44.6377 - mae: 4.6632 - val_loss: 928.4349 - val_mae: 17.0667\n",
      "Epoch 963/1000\n",
      "122/122 [==============================] - 0s 674us/step - loss: 48.6463 - mae: 4.7853 - val_loss: 979.2732 - val_mae: 17.4591\n",
      "Epoch 964/1000\n",
      "122/122 [==============================] - 0s 625us/step - loss: 45.3204 - mae: 4.6556 - val_loss: 999.3608 - val_mae: 17.6388\n",
      "Epoch 965/1000\n",
      "122/122 [==============================] - 0s 623us/step - loss: 46.7456 - mae: 4.7259 - val_loss: 922.6224 - val_mae: 17.2848\n",
      "Epoch 966/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 43.9773 - mae: 4.5826 - val_loss: 991.1169 - val_mae: 17.4993\n",
      "Epoch 967/1000\n",
      "122/122 [==============================] - 0s 587us/step - loss: 50.3740 - mae: 4.7969 - val_loss: 975.0731 - val_mae: 17.4126\n",
      "Epoch 968/1000\n",
      "122/122 [==============================] - 0s 630us/step - loss: 45.0241 - mae: 4.6049 - val_loss: 970.0750 - val_mae: 17.2663\n",
      "Epoch 969/1000\n",
      "122/122 [==============================] - 0s 614us/step - loss: 45.6817 - mae: 4.6452 - val_loss: 1047.7400 - val_mae: 18.0142\n",
      "Epoch 970/1000\n",
      "122/122 [==============================] - 0s 616us/step - loss: 44.5237 - mae: 4.5874 - val_loss: 971.2642 - val_mae: 17.4018\n",
      "Epoch 971/1000\n",
      "122/122 [==============================] - 0s 604us/step - loss: 42.6090 - mae: 4.5669 - val_loss: 1038.6588 - val_mae: 17.9107\n",
      "Epoch 972/1000\n",
      "122/122 [==============================] - 0s 619us/step - loss: 41.9267 - mae: 4.5754 - val_loss: 1044.3622 - val_mae: 17.8653\n",
      "Epoch 973/1000\n",
      "122/122 [==============================] - 0s 612us/step - loss: 43.3193 - mae: 4.6260 - val_loss: 965.4600 - val_mae: 17.3345\n",
      "Epoch 974/1000\n",
      "122/122 [==============================] - 0s 719us/step - loss: 46.6035 - mae: 4.7179 - val_loss: 934.6917 - val_mae: 17.0971\n",
      "Epoch 975/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 45.3532 - mae: 4.7200 - val_loss: 1032.0142 - val_mae: 17.8305\n",
      "Epoch 976/1000\n",
      "122/122 [==============================] - 0s 637us/step - loss: 45.9456 - mae: 4.7552 - val_loss: 1013.0127 - val_mae: 17.8219\n",
      "Epoch 977/1000\n",
      "122/122 [==============================] - 0s 600us/step - loss: 42.5719 - mae: 4.5770 - val_loss: 1115.7289 - val_mae: 18.4606\n",
      "Epoch 978/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 46.7164 - mae: 4.6470 - val_loss: 1060.9740 - val_mae: 17.9979\n",
      "Epoch 979/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 46.9344 - mae: 4.6666 - val_loss: 960.4452 - val_mae: 17.1898\n",
      "Epoch 980/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 48.7691 - mae: 4.8056 - val_loss: 1007.4874 - val_mae: 17.5977\n",
      "Epoch 981/1000\n",
      "122/122 [==============================] - 0s 621us/step - loss: 47.7383 - mae: 4.7881 - val_loss: 940.4086 - val_mae: 17.0791\n",
      "Epoch 982/1000\n",
      "122/122 [==============================] - 0s 620us/step - loss: 46.0827 - mae: 4.6835 - val_loss: 1000.1165 - val_mae: 17.5101\n",
      "Epoch 983/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 48.1774 - mae: 4.7370 - val_loss: 985.6924 - val_mae: 17.3762\n",
      "Epoch 984/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 50.2418 - mae: 4.7663 - val_loss: 1076.7346 - val_mae: 18.0691\n",
      "Epoch 985/1000\n",
      "122/122 [==============================] - 0s 593us/step - loss: 50.0737 - mae: 4.8489 - val_loss: 955.9855 - val_mae: 17.2209\n",
      "Epoch 986/1000\n",
      "122/122 [==============================] - 0s 595us/step - loss: 42.9236 - mae: 4.5503 - val_loss: 1021.3218 - val_mae: 17.7092\n",
      "Epoch 987/1000\n",
      "122/122 [==============================] - 0s 631us/step - loss: 44.6441 - mae: 4.6851 - val_loss: 951.8471 - val_mae: 17.2828\n",
      "Epoch 988/1000\n",
      "122/122 [==============================] - 0s 642us/step - loss: 45.2373 - mae: 4.6579 - val_loss: 1034.9298 - val_mae: 17.8529\n",
      "Epoch 989/1000\n",
      "122/122 [==============================] - 0s 660us/step - loss: 47.5258 - mae: 4.7706 - val_loss: 965.4977 - val_mae: 17.3207\n",
      "Epoch 990/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 45.6482 - mae: 4.7011 - val_loss: 958.9232 - val_mae: 17.4534\n",
      "Epoch 991/1000\n",
      "122/122 [==============================] - 0s 627us/step - loss: 45.2875 - mae: 4.6773 - val_loss: 986.8672 - val_mae: 17.5040\n",
      "Epoch 992/1000\n",
      "122/122 [==============================] - 0s 601us/step - loss: 45.1705 - mae: 4.6551 - val_loss: 980.7565 - val_mae: 17.5660\n",
      "Epoch 993/1000\n",
      "122/122 [==============================] - 0s 694us/step - loss: 42.8105 - mae: 4.5537 - val_loss: 964.2109 - val_mae: 17.3612\n",
      "Epoch 994/1000\n",
      "122/122 [==============================] - 0s 662us/step - loss: 42.9340 - mae: 4.5970 - val_loss: 1049.3033 - val_mae: 17.9864\n",
      "Epoch 995/1000\n",
      "122/122 [==============================] - 0s 611us/step - loss: 49.4087 - mae: 4.8455 - val_loss: 1023.1219 - val_mae: 17.6925\n",
      "Epoch 996/1000\n",
      "122/122 [==============================] - 0s 606us/step - loss: 44.5863 - mae: 4.7014 - val_loss: 1019.5920 - val_mae: 17.6903\n",
      "Epoch 997/1000\n",
      "122/122 [==============================] - 0s 615us/step - loss: 44.9816 - mae: 4.7058 - val_loss: 946.4767 - val_mae: 17.2972\n",
      "Epoch 998/1000\n",
      "122/122 [==============================] - 0s 629us/step - loss: 46.2203 - mae: 4.6720 - val_loss: 1076.5461 - val_mae: 18.1326\n",
      "Epoch 999/1000\n",
      "122/122 [==============================] - 0s 607us/step - loss: 45.1095 - mae: 4.7501 - val_loss: 1019.1791 - val_mae: 17.7512\n",
      "Epoch 1000/1000\n",
      "122/122 [==============================] - 0s 638us/step - loss: 41.5149 - mae: 4.5465 - val_loss: 995.5986 - val_mae: 17.5931\n",
      "31/31 [==============================] - 0s 320us/step\n",
      "Epochs: 1000 | MAE: 17.59313472290039\n"
     ]
    }
   ],
   "source": [
    "mae_results = {}\n",
    "for epoch in range(100, 1001, 50):\n",
    "    print(f\"Training model with {epoch} epochs\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=[X_train.shape[1],]),\n",
    "        Dropout(0.3), # randomly set 30% of neurons to 0 during training to prevent overfitting\n",
    "        Dense(32, activation='relu'), # 2nd layer with only 32 neurons to reduce model complexity\n",
    "        Dense(1, activation='linear') # single out representing the Price\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics='mae')\n",
    "    # adam -> adaptive gradient based optimizer (type of gradient descent): AdaGrad & RMSProp       \n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=epoch,\n",
    "                    batch_size=32\n",
    "                    )\n",
    "    \n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mae_results[epoch] = mae\n",
    "    print(f\"Epochs: {epoch} | MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 3.7399122877854563,\n",
       " 150: 3.673137145996094,\n",
       " 200: 3.5558574425330525,\n",
       " 250: 4.3641277916541465,\n",
       " 300: 4.341572904146635,\n",
       " 350: 3.674060824506711,\n",
       " 400: 9.223212161646135,\n",
       " 450: 11.699360898844402,\n",
       " 500: 4.194033297181741,\n",
       " 550: 4.009001143391926,\n",
       " 600: 3.8716499240973046,\n",
       " 650: 4.038987292010968,\n",
       " 700: 8.05191028129382,\n",
       " 750: 4.2953251377203525,\n",
       " 800: 4.485844051889273,\n",
       " 850: 4.4830111594175674,\n",
       " 900: 4.284747272354518,\n",
       " 950: 4.666406692270132,\n",
       " 1000: 17.59313472290039}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15568f610>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = list(mae_results.keys())\n",
    "mae_values = list(mae_results.values())\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae_values, marker='o', linestyle='-', color='blue', label='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 454us/step - loss: 24.4421 - mae: 3.3766\n",
      "Test Loss: 24.442119598388672, Test MAE: 3.376631021499634\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 391us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 53.16598 ],\n",
       "       [165.90982 ],\n",
       "       [234.89175 ],\n",
       "       [186.91599 ],\n",
       "       [200.98889 ],\n",
       "       [327.57306 ],\n",
       "       [124.425934],\n",
       "       [310.511   ],\n",
       "       [232.30785 ],\n",
       "       [311.1548  ],\n",
       "       [219.70325 ],\n",
       "       [326.35556 ],\n",
       "       [360.28595 ],\n",
       "       [454.15796 ],\n",
       "       [149.31961 ],\n",
       "       [394.00458 ],\n",
       "       [232.8606  ],\n",
       "       [246.82654 ],\n",
       "       [468.94507 ],\n",
       "       [443.3084  ],\n",
       "       [222.89096 ],\n",
       "       [504.56244 ],\n",
       "       [469.98267 ],\n",
       "       [248.65472 ],\n",
       "       [ 76.61895 ],\n",
       "       [214.89973 ],\n",
       "       [176.9992  ],\n",
       "       [ 88.414215],\n",
       "       [236.2771  ],\n",
       "       [ 96.0325  ],\n",
       "       [149.27368 ],\n",
       "       [444.11676 ],\n",
       "       [204.35237 ],\n",
       "       [120.032265],\n",
       "       [383.1347  ],\n",
       "       [205.66064 ],\n",
       "       [228.03241 ],\n",
       "       [265.5633  ],\n",
       "       [202.01688 ],\n",
       "       [ 54.16947 ],\n",
       "       [207.01474 ],\n",
       "       [ 93.06757 ],\n",
       "       [555.56354 ],\n",
       "       [ 39.60001 ],\n",
       "       [213.3809  ],\n",
       "       [ 89.845184],\n",
       "       [149.4267  ],\n",
       "       [472.58984 ],\n",
       "       [419.63425 ],\n",
       "       [151.9839  ],\n",
       "       [382.3241  ],\n",
       "       [226.34207 ],\n",
       "       [371.7729  ],\n",
       "       [182.34692 ],\n",
       "       [ 96.41562 ],\n",
       "       [ 46.648884],\n",
       "       [ 80.93788 ],\n",
       "       [561.7192  ],\n",
       "       [ 75.56683 ],\n",
       "       [122.4536  ],\n",
       "       [454.9203  ],\n",
       "       [101.728584],\n",
       "       [184.86731 ],\n",
       "       [406.36    ],\n",
       "       [ 79.96712 ],\n",
       "       [282.35663 ],\n",
       "       [427.32263 ],\n",
       "       [ 94.46002 ],\n",
       "       [ 90.75644 ],\n",
       "       [220.3081  ],\n",
       "       [477.6519  ],\n",
       "       [211.80685 ],\n",
       "       [272.48505 ],\n",
       "       [ 78.698364],\n",
       "       [ 88.37825 ],\n",
       "       [138.8206  ],\n",
       "       [226.95444 ],\n",
       "       [513.02405 ],\n",
       "       [ 54.83838 ],\n",
       "       [214.22258 ],\n",
       "       [251.94989 ],\n",
       "       [387.23584 ],\n",
       "       [ 54.600945],\n",
       "       [219.03885 ],\n",
       "       [200.99411 ],\n",
       "       [174.07301 ],\n",
       "       [391.02826 ],\n",
       "       [ 53.995255],\n",
       "       [176.93834 ],\n",
       "       [535.9425  ],\n",
       "       [219.57921 ],\n",
       "       [234.43549 ],\n",
       "       [215.03159 ],\n",
       "       [ 98.681435],\n",
       "       [104.391174],\n",
       "       [285.17786 ],\n",
       "       [175.92557 ],\n",
       "       [107.903656],\n",
       "       [416.1335  ],\n",
       "       [273.4048  ],\n",
       "       [118.22502 ],\n",
       "       [477.03107 ],\n",
       "       [204.52295 ],\n",
       "       [104.959915],\n",
       "       [188.03827 ],\n",
       "       [ 45.265324],\n",
       "       [232.80934 ],\n",
       "       [393.94232 ],\n",
       "       [269.2106  ],\n",
       "       [199.61118 ],\n",
       "       [208.98996 ],\n",
       "       [217.98486 ],\n",
       "       [201.21475 ],\n",
       "       [198.88916 ],\n",
       "       [107.30747 ],\n",
       "       [163.3222  ],\n",
       "       [265.74097 ],\n",
       "       [227.51413 ],\n",
       "       [210.77588 ],\n",
       "       [ 83.90452 ],\n",
       "       [189.29633 ],\n",
       "       [228.5412  ],\n",
       "       [180.31012 ],\n",
       "       [188.65736 ],\n",
       "       [191.00316 ],\n",
       "       [ 81.000916],\n",
       "       [147.85803 ],\n",
       "       [ 54.409893],\n",
       "       [149.0635  ],\n",
       "       [208.84367 ],\n",
       "       [ 72.06065 ],\n",
       "       [223.51146 ],\n",
       "       [403.60828 ],\n",
       "       [178.42693 ],\n",
       "       [233.79237 ],\n",
       "       [222.50882 ],\n",
       "       [452.0839  ],\n",
       "       [252.27759 ],\n",
       "       [ 52.23481 ],\n",
       "       [393.40735 ],\n",
       "       [346.00555 ],\n",
       "       [181.91243 ],\n",
       "       [149.13687 ],\n",
       "       [168.8343  ],\n",
       "       [ 45.511887],\n",
       "       [230.87233 ],\n",
       "       [442.2616  ],\n",
       "       [442.30994 ],\n",
       "       [246.47705 ],\n",
       "       [533.92523 ],\n",
       "       [ 54.789463],\n",
       "       [147.68878 ],\n",
       "       [190.67065 ],\n",
       "       [151.59225 ],\n",
       "       [330.59247 ],\n",
       "       [209.48962 ],\n",
       "       [431.146   ],\n",
       "       [195.10797 ],\n",
       "       [441.47415 ],\n",
       "       [424.9444  ],\n",
       "       [156.34378 ],\n",
       "       [242.49826 ],\n",
       "       [200.25461 ],\n",
       "       [248.17378 ],\n",
       "       [162.2724  ],\n",
       "       [205.82204 ],\n",
       "       [518.3587  ],\n",
       "       [196.25258 ],\n",
       "       [276.5123  ],\n",
       "       [446.96353 ],\n",
       "       [333.4757  ],\n",
       "       [ 89.25505 ],\n",
       "       [177.24908 ],\n",
       "       [297.851   ],\n",
       "       [ 84.07837 ],\n",
       "       [141.6719  ],\n",
       "       [153.1887  ],\n",
       "       [111.70059 ],\n",
       "       [273.7993  ],\n",
       "       [440.32202 ],\n",
       "       [107.69114 ],\n",
       "       [459.14532 ],\n",
       "       [191.29807 ],\n",
       "       [129.83351 ],\n",
       "       [202.74619 ],\n",
       "       [196.51474 ],\n",
       "       [ 61.655827],\n",
       "       [114.57672 ],\n",
       "       [137.10574 ],\n",
       "       [431.52808 ],\n",
       "       [546.90027 ],\n",
       "       [385.2504  ],\n",
       "       [297.9452  ],\n",
       "       [189.80783 ],\n",
       "       [121.42656 ],\n",
       "       [212.22108 ],\n",
       "       [274.70795 ],\n",
       "       [414.6675  ],\n",
       "       [244.54767 ],\n",
       "       [192.40887 ],\n",
       "       [446.28687 ],\n",
       "       [193.22043 ],\n",
       "       [104.55856 ],\n",
       "       [157.38477 ],\n",
       "       [324.8484  ],\n",
       "       [ 89.96269 ],\n",
       "       [ 54.50828 ],\n",
       "       [176.59464 ],\n",
       "       [ 99.41199 ],\n",
       "       [ 95.070404],\n",
       "       [196.23746 ],\n",
       "       [ 94.1871  ],\n",
       "       [112.14079 ],\n",
       "       [199.68616 ],\n",
       "       [499.68103 ],\n",
       "       [416.93835 ],\n",
       "       [ 74.31957 ],\n",
       "       [327.43146 ],\n",
       "       [ 82.331276],\n",
       "       [420.06866 ],\n",
       "       [191.26335 ],\n",
       "       [428.83005 ],\n",
       "       [205.31973 ],\n",
       "       [296.79462 ],\n",
       "       [ 92.826385],\n",
       "       [110.92507 ],\n",
       "       [137.93015 ],\n",
       "       [393.95422 ],\n",
       "       [133.41345 ],\n",
       "       [171.49176 ],\n",
       "       [292.9835  ],\n",
       "       [171.56781 ],\n",
       "       [255.5546  ],\n",
       "       [ 75.32592 ],\n",
       "       [281.64877 ],\n",
       "       [135.17159 ],\n",
       "       [195.94499 ],\n",
       "       [394.43118 ],\n",
       "       [550.7935  ],\n",
       "       [219.02655 ],\n",
       "       [458.0878  ],\n",
       "       [328.7787  ],\n",
       "       [251.95862 ],\n",
       "       [217.98297 ],\n",
       "       [286.05994 ],\n",
       "       [142.6987  ],\n",
       "       [451.1034  ],\n",
       "       [296.36676 ],\n",
       "       [105.31892 ],\n",
       "       [158.4385  ],\n",
       "       [280.4724  ],\n",
       "       [183.56621 ],\n",
       "       [260.88165 ],\n",
       "       [123.03997 ],\n",
       "       [228.90454 ],\n",
       "       [215.16246 ],\n",
       "       [211.0863  ],\n",
       "       [ 92.3774  ],\n",
       "       [155.42868 ],\n",
       "       [301.37042 ],\n",
       "       [165.77951 ],\n",
       "       [ 85.70642 ],\n",
       "       [143.16034 ],\n",
       "       [187.1231  ],\n",
       "       [437.06805 ],\n",
       "       [344.53748 ],\n",
       "       [ 54.609974],\n",
       "       [398.0123  ],\n",
       "       [177.83228 ],\n",
       "       [446.1078  ],\n",
       "       [100.85843 ],\n",
       "       [153.97348 ],\n",
       "       [361.5453  ],\n",
       "       [234.65054 ],\n",
       "       [273.45795 ],\n",
       "       [149.93211 ],\n",
       "       [151.69868 ],\n",
       "       [276.7955  ],\n",
       "       [396.98682 ],\n",
       "       [132.24925 ],\n",
       "       [247.90033 ],\n",
       "       [253.599   ],\n",
       "       [354.44293 ],\n",
       "       [ 65.7958  ],\n",
       "       [437.82678 ],\n",
       "       [220.56685 ],\n",
       "       [288.75555 ],\n",
       "       [467.5988  ],\n",
       "       [275.12686 ],\n",
       "       [237.18661 ],\n",
       "       [ 54.168472],\n",
       "       [329.22046 ],\n",
       "       [196.90332 ],\n",
       "       [ 78.06577 ],\n",
       "       [405.64752 ],\n",
       "       [465.42834 ],\n",
       "       [ 55.67928 ],\n",
       "       [ 65.50137 ],\n",
       "       [241.19766 ],\n",
       "       [ 90.06091 ],\n",
       "       [498.40753 ],\n",
       "       [168.02005 ],\n",
       "       [256.85754 ],\n",
       "       [403.70068 ],\n",
       "       [ 54.486523],\n",
       "       [203.96545 ],\n",
       "       [211.38362 ],\n",
       "       [448.10596 ],\n",
       "       [ 93.6434  ],\n",
       "       [ 70.73516 ],\n",
       "       [160.6665  ],\n",
       "       [ 49.15272 ],\n",
       "       [440.15668 ],\n",
       "       [391.87527 ],\n",
       "       [198.6276  ],\n",
       "       [ 83.005844],\n",
       "       [295.52087 ],\n",
       "       [ 53.286724],\n",
       "       [ 96.910286],\n",
       "       [431.48068 ],\n",
       "       [360.65967 ],\n",
       "       [165.35611 ],\n",
       "       [219.93079 ],\n",
       "       [151.90182 ],\n",
       "       [165.73386 ],\n",
       "       [ 76.96058 ],\n",
       "       [249.1739  ],\n",
       "       [245.58298 ],\n",
       "       [131.09714 ],\n",
       "       [172.33582 ],\n",
       "       [184.64813 ],\n",
       "       [ 78.488434],\n",
       "       [141.15198 ],\n",
       "       [390.27856 ],\n",
       "       [451.19678 ],\n",
       "       [385.27524 ],\n",
       "       [448.08228 ],\n",
       "       [ 80.97507 ],\n",
       "       [202.2386  ],\n",
       "       [335.2176  ],\n",
       "       [137.49332 ],\n",
       "       [441.73914 ],\n",
       "       [ 55.711407],\n",
       "       [134.27982 ],\n",
       "       [219.04816 ],\n",
       "       [214.83041 ],\n",
       "       [175.59811 ],\n",
       "       [ 76.94468 ],\n",
       "       [104.340744],\n",
       "       [222.81744 ],\n",
       "       [210.81113 ],\n",
       "       [333.0959  ],\n",
       "       [ 79.888336],\n",
       "       [ 75.09836 ],\n",
       "       [258.65332 ],\n",
       "       [104.76988 ],\n",
       "       [336.16553 ],\n",
       "       [365.18994 ],\n",
       "       [169.50462 ],\n",
       "       [436.5202  ],\n",
       "       [174.66422 ],\n",
       "       [155.84686 ],\n",
       "       [108.9616  ],\n",
       "       [ 74.472885],\n",
       "       [470.19858 ],\n",
       "       [154.95    ],\n",
       "       [253.04947 ],\n",
       "       [467.99634 ],\n",
       "       [327.99817 ],\n",
       "       [ 53.51035 ],\n",
       "       [271.296   ],\n",
       "       [211.03632 ],\n",
       "       [439.22882 ],\n",
       "       [141.51883 ],\n",
       "       [205.41019 ],\n",
       "       [122.867905],\n",
       "       [150.1407  ],\n",
       "       [186.1463  ],\n",
       "       [557.7056  ],\n",
       "       [187.60406 ],\n",
       "       [105.73828 ],\n",
       "       [231.41838 ],\n",
       "       [178.29948 ],\n",
       "       [395.14508 ],\n",
       "       [161.92497 ],\n",
       "       [228.25719 ],\n",
       "       [546.7597  ],\n",
       "       [229.42003 ],\n",
       "       [194.84311 ],\n",
       "       [444.62216 ],\n",
       "       [ 87.31375 ],\n",
       "       [ 71.52969 ],\n",
       "       [213.37105 ],\n",
       "       [ 70.55266 ],\n",
       "       [415.67987 ],\n",
       "       [ 73.10517 ],\n",
       "       [191.43083 ],\n",
       "       [242.69897 ],\n",
       "       [ 84.40695 ],\n",
       "       [171.58055 ],\n",
       "       [212.66257 ],\n",
       "       [167.16977 ],\n",
       "       [140.63593 ],\n",
       "       [177.33542 ],\n",
       "       [198.51526 ],\n",
       "       [ 72.57573 ],\n",
       "       [265.8629  ],\n",
       "       [ 76.09474 ],\n",
       "       [340.6093  ],\n",
       "       [382.4914  ],\n",
       "       [355.65027 ],\n",
       "       [247.87772 ],\n",
       "       [146.17323 ],\n",
       "       [217.29448 ],\n",
       "       [262.92355 ],\n",
       "       [157.81616 ],\n",
       "       [167.69623 ],\n",
       "       [532.73505 ],\n",
       "       [ 55.48021 ],\n",
       "       [ 96.21471 ],\n",
       "       [ 94.9068  ],\n",
       "       [226.78708 ],\n",
       "       [255.29524 ],\n",
       "       [188.61719 ],\n",
       "       [190.31104 ],\n",
       "       [432.2074  ],\n",
       "       [136.8785  ],\n",
       "       [ 92.0446  ],\n",
       "       [269.24597 ],\n",
       "       [374.48547 ],\n",
       "       [142.81409 ],\n",
       "       [ 92.521286],\n",
       "       [174.3484  ],\n",
       "       [257.10852 ],\n",
       "       [173.03624 ],\n",
       "       [138.88469 ],\n",
       "       [253.87233 ],\n",
       "       [285.45267 ],\n",
       "       [233.33362 ],\n",
       "       [433.95972 ],\n",
       "       [ 77.683525],\n",
       "       [263.12384 ],\n",
       "       [339.18842 ],\n",
       "       [ 55.402706],\n",
       "       [208.90051 ],\n",
       "       [139.4941  ],\n",
       "       [365.4381  ],\n",
       "       [205.65948 ],\n",
       "       [ 64.78601 ],\n",
       "       [220.44449 ],\n",
       "       [ 83.767746],\n",
       "       [265.82977 ],\n",
       "       [176.13663 ],\n",
       "       [248.09155 ],\n",
       "       [ 78.52801 ],\n",
       "       [ 40.755825],\n",
       "       [120.52573 ],\n",
       "       [262.47495 ],\n",
       "       [176.05405 ],\n",
       "       [ 73.85079 ],\n",
       "       [215.48456 ],\n",
       "       [267.27493 ],\n",
       "       [532.73737 ],\n",
       "       [331.98718 ],\n",
       "       [180.29239 ],\n",
       "       [207.63039 ],\n",
       "       [415.23877 ],\n",
       "       [170.3675  ],\n",
       "       [306.74255 ],\n",
       "       [251.54083 ],\n",
       "       [215.08463 ],\n",
       "       [210.51706 ],\n",
       "       [ 79.0093  ],\n",
       "       [152.49469 ],\n",
       "       [203.08345 ],\n",
       "       [137.9231  ],\n",
       "       [ 61.54017 ],\n",
       "       [180.89702 ],\n",
       "       [187.67218 ],\n",
       "       [ 65.513054],\n",
       "       [144.4398  ],\n",
       "       [ 93.15943 ],\n",
       "       [ 53.625973],\n",
       "       [213.38124 ],\n",
       "       [160.13531 ],\n",
       "       [210.18027 ],\n",
       "       [ 98.415306],\n",
       "       [188.54623 ],\n",
       "       [210.51321 ],\n",
       "       [465.96606 ],\n",
       "       [100.05829 ],\n",
       "       [157.01761 ],\n",
       "       [414.37372 ],\n",
       "       [335.94012 ],\n",
       "       [ 50.114254],\n",
       "       [547.4964  ],\n",
       "       [530.3478  ],\n",
       "       [209.32053 ],\n",
       "       [204.43158 ],\n",
       "       [ 93.64869 ],\n",
       "       [153.00778 ],\n",
       "       [ 82.6789  ],\n",
       "       [ 82.162704],\n",
       "       [509.26672 ],\n",
       "       [ 54.28235 ],\n",
       "       [ 60.034542],\n",
       "       [112.94473 ],\n",
       "       [ 83.96289 ],\n",
       "       [ 42.4265  ],\n",
       "       [356.63058 ],\n",
       "       [318.85098 ],\n",
       "       [139.69496 ],\n",
       "       [201.08945 ],\n",
       "       [437.22388 ],\n",
       "       [379.47235 ],\n",
       "       [444.5249  ],\n",
       "       [217.40865 ],\n",
       "       [234.1676  ],\n",
       "       [198.66672 ],\n",
       "       [222.93738 ],\n",
       "       [ 63.583755],\n",
       "       [108.51978 ],\n",
       "       [351.4131  ],\n",
       "       [440.32684 ],\n",
       "       [159.31622 ],\n",
       "       [103.319626],\n",
       "       [211.63351 ],\n",
       "       [151.81995 ],\n",
       "       [220.65703 ],\n",
       "       [115.16348 ],\n",
       "       [153.1676  ],\n",
       "       [200.74503 ],\n",
       "       [220.22356 ],\n",
       "       [202.15765 ],\n",
       "       [ 75.726906],\n",
       "       [ 74.11703 ],\n",
       "       [433.5261  ],\n",
       "       [271.74023 ],\n",
       "       [427.3383  ],\n",
       "       [290.31952 ],\n",
       "       [331.3852  ],\n",
       "       [368.5559  ],\n",
       "       [142.01854 ],\n",
       "       [340.28833 ],\n",
       "       [194.46753 ],\n",
       "       [341.98315 ],\n",
       "       [415.43552 ],\n",
       "       [369.67474 ],\n",
       "       [ 60.820225],\n",
       "       [413.18448 ],\n",
       "       [387.74216 ],\n",
       "       [155.28857 ],\n",
       "       [195.82382 ],\n",
       "       [204.66086 ],\n",
       "       [177.7648  ],\n",
       "       [240.91536 ],\n",
       "       [ 97.81653 ],\n",
       "       [184.96747 ],\n",
       "       [456.02606 ],\n",
       "       [383.79523 ],\n",
       "       [215.88522 ],\n",
       "       [561.778   ],\n",
       "       [207.52069 ],\n",
       "       [229.99698 ],\n",
       "       [548.0411  ],\n",
       "       [447.79727 ],\n",
       "       [424.4806  ],\n",
       "       [524.3222  ],\n",
       "       [434.47812 ],\n",
       "       [179.68323 ],\n",
       "       [182.4552  ],\n",
       "       [181.6506  ],\n",
       "       [155.91122 ],\n",
       "       [140.74292 ],\n",
       "       [197.05408 ],\n",
       "       [578.86273 ],\n",
       "       [161.52365 ],\n",
       "       [169.79987 ],\n",
       "       [113.30812 ],\n",
       "       [338.0014  ],\n",
       "       [ 90.35028 ],\n",
       "       [433.1021  ],\n",
       "       [330.72726 ],\n",
       "       [102.50719 ],\n",
       "       [194.06361 ],\n",
       "       [201.2749  ],\n",
       "       [418.1432  ],\n",
       "       [424.98358 ],\n",
       "       [266.60806 ],\n",
       "       [335.04993 ],\n",
       "       [394.86743 ],\n",
       "       [528.6772  ],\n",
       "       [175.795   ],\n",
       "       [413.41998 ],\n",
       "       [256.79523 ],\n",
       "       [440.2574  ],\n",
       "       [423.17838 ],\n",
       "       [249.64404 ],\n",
       "       [204.00092 ],\n",
       "       [210.33772 ],\n",
       "       [213.78377 ],\n",
       "       [450.28983 ],\n",
       "       [194.69522 ],\n",
       "       [186.31244 ],\n",
       "       [190.8125  ],\n",
       "       [173.23645 ],\n",
       "       [219.15974 ],\n",
       "       [406.70642 ],\n",
       "       [149.78236 ],\n",
       "       [331.3764  ],\n",
       "       [111.18307 ],\n",
       "       [ 55.071064],\n",
       "       [ 80.64192 ],\n",
       "       [ 83.446045],\n",
       "       [167.57599 ],\n",
       "       [229.49362 ],\n",
       "       [ 93.56363 ],\n",
       "       [365.58054 ],\n",
       "       [549.4071  ],\n",
       "       [157.49713 ],\n",
       "       [210.02704 ],\n",
       "       [456.60464 ],\n",
       "       [ 52.25643 ],\n",
       "       [194.41629 ],\n",
       "       [ 52.863117],\n",
       "       [248.83649 ],\n",
       "       [249.85236 ],\n",
       "       [401.25836 ],\n",
       "       [119.36061 ],\n",
       "       [ 70.66106 ],\n",
       "       [129.46841 ],\n",
       "       [320.6396  ],\n",
       "       [ 86.08963 ],\n",
       "       [175.07004 ],\n",
       "       [197.89336 ],\n",
       "       [257.28046 ],\n",
       "       [194.93231 ],\n",
       "       [ 92.00125 ],\n",
       "       [107.28505 ],\n",
       "       [144.43494 ],\n",
       "       [386.65717 ],\n",
       "       [433.63538 ],\n",
       "       [171.43124 ],\n",
       "       [305.31824 ],\n",
       "       [197.07451 ],\n",
       "       [213.09473 ],\n",
       "       [168.48184 ],\n",
       "       [205.94312 ],\n",
       "       [ 97.56881 ],\n",
       "       [194.25362 ],\n",
       "       [112.75345 ],\n",
       "       [230.4794  ],\n",
       "       [ 94.36884 ],\n",
       "       [174.20999 ],\n",
       "       [207.94826 ],\n",
       "       [447.07327 ],\n",
       "       [ 69.30382 ],\n",
       "       [141.35701 ],\n",
       "       [154.6493  ],\n",
       "       [182.44263 ],\n",
       "       [258.43234 ],\n",
       "       [430.59732 ],\n",
       "       [448.6768  ],\n",
       "       [139.6204  ],\n",
       "       [ 59.087276],\n",
       "       [400.03268 ],\n",
       "       [352.46313 ],\n",
       "       [159.76436 ],\n",
       "       [ 78.37677 ],\n",
       "       [520.368   ],\n",
       "       [234.69485 ],\n",
       "       [211.89282 ],\n",
       "       [239.73462 ],\n",
       "       [ 92.90773 ],\n",
       "       [122.85094 ],\n",
       "       [ 76.236145],\n",
       "       [325.93915 ],\n",
       "       [155.29973 ],\n",
       "       [107.951675],\n",
       "       [ 91.6836  ],\n",
       "       [199.8965  ],\n",
       "       [ 98.07656 ],\n",
       "       [ 96.667076],\n",
       "       [177.26721 ],\n",
       "       [104.25322 ],\n",
       "       [381.9376  ],\n",
       "       [489.8933  ],\n",
       "       [198.69714 ],\n",
       "       [ 57.068913],\n",
       "       [173.38101 ],\n",
       "       [137.07437 ],\n",
       "       [ 94.33805 ],\n",
       "       [ 77.998764],\n",
       "       [430.51425 ],\n",
       "       [246.7104  ],\n",
       "       [188.11897 ],\n",
       "       [315.2862  ],\n",
       "       [213.49066 ],\n",
       "       [396.7091  ],\n",
       "       [338.78293 ],\n",
       "       [392.53812 ],\n",
       "       [ 90.85628 ],\n",
       "       [446.9511  ],\n",
       "       [465.4096  ],\n",
       "       [524.9233  ],\n",
       "       [ 63.942146],\n",
       "       [203.45146 ],\n",
       "       [361.26035 ],\n",
       "       [173.30603 ],\n",
       "       [259.03754 ],\n",
       "       [163.94394 ],\n",
       "       [319.00708 ],\n",
       "       [460.76773 ],\n",
       "       [ 54.63291 ],\n",
       "       [460.3075  ],\n",
       "       [400.52112 ],\n",
       "       [399.46512 ],\n",
       "       [176.56848 ],\n",
       "       [359.60974 ],\n",
       "       [387.0302  ],\n",
       "       [ 68.847305],\n",
       "       [109.70752 ],\n",
       "       [443.45242 ],\n",
       "       [139.83025 ],\n",
       "       [147.76762 ],\n",
       "       [116.04649 ],\n",
       "       [ 77.45726 ],\n",
       "       [ 95.44869 ],\n",
       "       [172.25269 ],\n",
       "       [ 92.70474 ],\n",
       "       [247.04459 ],\n",
       "       [231.22723 ],\n",
       "       [236.108   ],\n",
       "       [168.28485 ],\n",
       "       [557.6163  ],\n",
       "       [324.98596 ],\n",
       "       [174.56946 ],\n",
       "       [ 72.70747 ],\n",
       "       [281.57242 ],\n",
       "       [246.37979 ],\n",
       "       [182.62198 ],\n",
       "       [206.00844 ],\n",
       "       [210.95113 ],\n",
       "       [258.93945 ],\n",
       "       [565.6817  ],\n",
       "       [219.5816  ],\n",
       "       [ 89.333954],\n",
       "       [227.45595 ],\n",
       "       [173.21884 ],\n",
       "       [122.7138  ],\n",
       "       [469.76212 ],\n",
       "       [155.43289 ],\n",
       "       [232.3573  ],\n",
       "       [170.76193 ],\n",
       "       [395.45093 ],\n",
       "       [214.11458 ],\n",
       "       [435.88104 ],\n",
       "       [164.16856 ],\n",
       "       [136.46487 ],\n",
       "       [423.42023 ],\n",
       "       [373.8127  ],\n",
       "       [128.70697 ],\n",
       "       [207.73668 ],\n",
       "       [186.73822 ],\n",
       "       [368.7639  ],\n",
       "       [417.151   ],\n",
       "       [386.17566 ],\n",
       "       [415.24835 ],\n",
       "       [434.98697 ],\n",
       "       [143.21661 ],\n",
       "       [504.63467 ],\n",
       "       [218.00601 ],\n",
       "       [252.2127  ],\n",
       "       [152.65417 ],\n",
       "       [182.03436 ],\n",
       "       [ 37.725037],\n",
       "       [199.33008 ],\n",
       "       [237.01666 ],\n",
       "       [126.87466 ],\n",
       "       [212.75946 ],\n",
       "       [ 80.05762 ],\n",
       "       [210.78854 ],\n",
       "       [247.36804 ],\n",
       "       [201.50021 ],\n",
       "       [335.88516 ],\n",
       "       [ 54.295994],\n",
       "       [213.78946 ],\n",
       "       [330.93573 ],\n",
       "       [ 64.64043 ],\n",
       "       [ 61.41445 ],\n",
       "       [312.4168  ],\n",
       "       [ 52.066654],\n",
       "       [459.51917 ],\n",
       "       [383.2283  ],\n",
       "       [500.5954  ],\n",
       "       [165.89073 ],\n",
       "       [182.89107 ],\n",
       "       [ 75.20189 ],\n",
       "       [222.99748 ],\n",
       "       [142.94958 ],\n",
       "       [191.35837 ],\n",
       "       [188.38194 ],\n",
       "       [455.56625 ],\n",
       "       [111.977264],\n",
       "       [ 81.529045],\n",
       "       [397.8982  ],\n",
       "       [338.49664 ],\n",
       "       [166.33423 ],\n",
       "       [173.1927  ],\n",
       "       [233.7894  ],\n",
       "       [ 42.790833],\n",
       "       [423.10184 ],\n",
       "       [222.30719 ],\n",
       "       [514.2171  ],\n",
       "       [ 51.999203],\n",
       "       [437.11365 ],\n",
       "       [239.26752 ],\n",
       "       [113.417435],\n",
       "       [351.51526 ],\n",
       "       [186.9158  ],\n",
       "       [466.36945 ],\n",
       "       [290.05103 ],\n",
       "       [132.28442 ],\n",
       "       [396.1631  ],\n",
       "       [ 81.88468 ],\n",
       "       [204.45796 ],\n",
       "       [210.96127 ],\n",
       "       [ 73.722275],\n",
       "       [167.47137 ],\n",
       "       [419.54266 ],\n",
       "       [208.85277 ],\n",
       "       [174.27228 ],\n",
       "       [107.7222  ],\n",
       "       [156.03029 ],\n",
       "       [206.1576  ],\n",
       "       [188.51599 ],\n",
       "       [135.16158 ],\n",
       "       [468.65787 ],\n",
       "       [254.33273 ],\n",
       "       [ 80.250275],\n",
       "       [150.66104 ],\n",
       "       [179.4578  ],\n",
       "       [128.72041 ],\n",
       "       [ 77.084015],\n",
       "       [128.46251 ],\n",
       "       [204.03394 ],\n",
       "       [253.22108 ],\n",
       "       [408.04102 ],\n",
       "       [218.88422 ],\n",
       "       [425.22824 ],\n",
       "       [361.4726  ],\n",
       "       [222.01678 ],\n",
       "       [162.33163 ],\n",
       "       [159.28668 ],\n",
       "       [ 82.980865],\n",
       "       [ 85.07941 ],\n",
       "       [158.04736 ],\n",
       "       [234.1456  ],\n",
       "       [266.7356  ],\n",
       "       [ 52.869423],\n",
       "       [158.97137 ],\n",
       "       [ 84.46469 ],\n",
       "       [202.06633 ],\n",
       "       [157.55112 ],\n",
       "       [211.19342 ],\n",
       "       [257.69266 ],\n",
       "       [ 74.17545 ],\n",
       "       [171.14557 ],\n",
       "       [175.76228 ],\n",
       "       [370.94766 ],\n",
       "       [201.72644 ],\n",
       "       [ 39.745796],\n",
       "       [ 58.733128],\n",
       "       [366.63843 ],\n",
       "       [447.4835  ],\n",
       "       [200.48544 ],\n",
       "       [344.4508  ],\n",
       "       [213.23465 ],\n",
       "       [182.84662 ],\n",
       "       [214.55478 ],\n",
       "       [386.6138  ],\n",
       "       [134.80547 ],\n",
       "       [195.90947 ],\n",
       "       [148.54205 ],\n",
       "       [422.84546 ],\n",
       "       [ 51.277935],\n",
       "       [422.20142 ],\n",
       "       [ 94.373985],\n",
       "       [411.15955 ],\n",
       "       [244.22684 ],\n",
       "       [257.86197 ],\n",
       "       [361.373   ],\n",
       "       [231.4841  ],\n",
       "       [219.71704 ],\n",
       "       [452.3695  ],\n",
       "       [203.7473  ],\n",
       "       [ 67.54126 ],\n",
       "       [188.63373 ],\n",
       "       [347.45892 ],\n",
       "       [ 86.51808 ],\n",
       "       [227.06197 ],\n",
       "       [414.12692 ],\n",
       "       [335.14542 ],\n",
       "       [171.01611 ],\n",
       "       [ 66.87251 ],\n",
       "       [239.94351 ],\n",
       "       [ 53.938953],\n",
       "       [239.60417 ],\n",
       "       [ 53.17544 ],\n",
       "       [189.0611  ],\n",
       "       [107.01341 ],\n",
       "       [400.48013 ],\n",
       "       [250.5404  ],\n",
       "       [179.733   ],\n",
       "       [155.71161 ],\n",
       "       [144.07945 ],\n",
       "       [156.54642 ],\n",
       "       [133.92505 ],\n",
       "       [279.6244  ],\n",
       "       [386.06583 ],\n",
       "       [239.28671 ],\n",
       "       [171.52441 ],\n",
       "       [137.00322 ],\n",
       "       [455.68933 ],\n",
       "       [147.45761 ],\n",
       "       [510.77902 ],\n",
       "       [251.42514 ],\n",
       "       [159.94531 ],\n",
       "       [217.80057 ],\n",
       "       [ 96.021866],\n",
       "       [139.35846 ],\n",
       "       [211.59799 ],\n",
       "       [367.729   ],\n",
       "       [429.2738  ],\n",
       "       [108.969406],\n",
       "       [114.5213  ],\n",
       "       [183.95615 ],\n",
       "       [ 87.87697 ],\n",
       "       [516.9567  ],\n",
       "       [186.11194 ],\n",
       "       [107.06177 ],\n",
       "       [358.52008 ],\n",
       "       [244.74207 ],\n",
       "       [452.328   ],\n",
       "       [121.18526 ],\n",
       "       [138.49329 ],\n",
       "       [104.01097 ],\n",
       "       [257.58783 ],\n",
       "       [213.46794 ],\n",
       "       [561.32465 ],\n",
       "       [572.27    ],\n",
       "       [106.55514 ],\n",
       "       [ 85.506996],\n",
       "       [202.83275 ],\n",
       "       [202.97195 ],\n",
       "       [112.822464],\n",
       "       [444.5357  ],\n",
       "       [222.9412  ],\n",
       "       [ 54.08428 ],\n",
       "       [ 38.050163],\n",
       "       [166.62296 ],\n",
       "       [ 86.277306],\n",
       "       [333.37244 ],\n",
       "       [337.0853  ],\n",
       "       [254.39015 ],\n",
       "       [203.57954 ],\n",
       "       [174.38876 ],\n",
       "       [ 73.264244],\n",
       "       [215.71866 ],\n",
       "       [329.30035 ],\n",
       "       [499.04507 ],\n",
       "       [ 81.65787 ],\n",
       "       [164.18326 ],\n",
       "       [212.26752 ],\n",
       "       [436.74124 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.78</td>\n",
       "      <td>53.165981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.74</td>\n",
       "      <td>165.909821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246.79</td>\n",
       "      <td>234.891754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187.82</td>\n",
       "      <td>186.915985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201.62</td>\n",
       "      <td>200.988892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>496.39</td>\n",
       "      <td>499.045074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>85.95</td>\n",
       "      <td>81.657867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>166.01</td>\n",
       "      <td>164.183258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>207.90</td>\n",
       "      <td>212.267517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>434.45</td>\n",
       "      <td>436.741241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual   Predicted\n",
       "0     48.78   53.165981\n",
       "1    168.74  165.909821\n",
       "2    246.79  234.891754\n",
       "3    187.82  186.915985\n",
       "4    201.62  200.988892\n",
       "..      ...         ...\n",
       "970  496.39  499.045074\n",
       "971   85.95   81.657867\n",
       "972  166.01  164.183258\n",
       "973  207.90  212.267517\n",
       "974  434.45  436.741241\n",
       "\n",
       "[975 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions.flatten()\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.376630204107822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate the residuals (errors)\n",
    "comparison_df['Error'] = abs(comparison_df['Actual'] - comparison_df['Predicted'])\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(comparison_df['Actual'], comparison_df['Predicted'])\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot individual absolute errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(comparison_df)), comparison_df['Error'])\n",
    "plt.title('Absolute Errors for Predictions')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNN_Stock_Prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
